<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="pandoc.css" type="text/css" />
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h2 id="show-tell-and-discriminate-image-captioning-by-self-retrieval-with-partially-labeled-data">Show, Tell and Discriminate: Image Captioning by Self-retrieval with Partially Labeled Data</h2>
<p>Xihui Liu, Hongsheng Li, Jing Shao, Dapeng Chen, Xiaogang Wang</p>
<p><strong>ECCV 2018 submission</strong> | <a href="http://arxiv.org/abs/1803.08314v1">arxiv</a> | <a href="https://github.com/">code*</a> | <a href="https://github.com/">code</a> |</p>
<h4 id="problematic">Problematic:</h4>
<p>Captioning models replicate frequent phrases and follow generic templates occurring in the training corpus.</p>
<h4 id="contributions">Contributions:</h4>
<p>Improve discriminativeness and fidelity by involving a self-retrieval module.</p>
<p>Discriminativeness: How well can the caption distinguish its associated image from the rest.</p>
<p>The captioning model generates a caption while the self-retrieval module conducts text-to-image retrieval i.e. retrieve the source image from the generated caption.</p>
<h2 id="issues">Issues:</h2>
<p>Generating each word of the caption contains non-differentiable operations (max or sampling). The paper adopts reinforcement learning with the negative retrieval loss as reward.</p>
<h2 id="advantages">Advantages:</h2>
<p>Can easily integrate unlabeled images to train the self-retrieval module. This allows for mining hard negative-examples from unlabeled data and boost the discriminativeness.</p>
<h2 id="details">Details:</h2>
<p><img src="https://i.imgur.com/uNAlPgo.png" /> Text-to-image matching is performed only in the mini-batch.</p>
<h3 id="training-the-retrieval-module">Training the retrieval module:</h3>
<p>Given an image encoder <span class="math inline">\(E_i\)</span>, a GRU encoder <span class="math inline">\(E_c\)</span>, an LSTM decoder <span class="math inline">\(D_c\)</span> and a batch of pairs <span class="math inline">\((I_i,C_i)_i\)</span> (image and caption):</p>
<ol style="list-style-type: decimal">
<li>Encode the image $ v_i = E_i(I_i), i$ and the caption <span class="math inline">\(c_i=E_c(C_i), \forall i\)</span>.</li>
<li>Compute the similarity between each image <span class="math inline">\(i\)</span> and caption <span class="math inline">\(j\)</span>: <span class="math inline">\(s(c_i, v_j)\)</span>.</li>
<li>Evaluate the retrieval loss: <span class="math inline">\(L_{ret}(C_i,\{I_1,...I_n})= \max_{j\neq i} (0, m-s(c_i, v_i) + s(c_i, v_j))\)</span></li>
</ol>
<p>The retrieval reward is simply the negative retrieval loss: <span class="math display">\[r_{ret} =  - L_{ret}$.
### Training the captioning module with RL:
\]</span> L_{RL}() = - E_{C^s p_}[r(C^s)] <span class="math display">\[
With REINFORCE, the gradient is estimated via a single monte carlo sample from $p_\theta$:
\]</span> <em>L</em>{RL}() (r(C^s) -b) <em>p</em>(C^s) $$ where the bias <span class="math inline">\(b\)</span> is introduced to reduce the variance (Sutton 1998). It can be formulated as the reward of the caption generated by greedy decoding (pick the argmax <span class="math inline">\(\forall t\)</span>).</p>
<p>The reward in this case is a weighted summation of CIDEr and the retrieval reward: <span class="math display">\[
r(C_i^s) = r_{cider}(C_i^s) + \alpha . r_{ret}(C_i^s, \{I_1, ..., I_n\})
\]</span></p>
<h2 id="inclusion-of-unlabeled-images">Inclusion of unlabeled images:</h2>
<p>Since the ground truth captions of the images are not required to compute the retrieval reward, we can easily add unlabeled images to the mix.</p>
<p>Unlabeled images are added to the minibatch, for which the CIDEr reward term is dropped. <span class="math display">\[
r(C_i^u) = r_{ret}(C_i^u, \{I_1, ... I_n}\cup\\{I^u_1, ...I_m^u\})
\]</span></p>
<h3 id="mining-hard-negative-pairs">Mining hard negative pairs:</h3>
<div class="figure">
<img src="https://i.imgur.com/iXCPwnl.png" />

</div>
<p>A simple ranking of the unlabeled images given a caption is not optimal since image-caption pairs do not follow a one-to-one mapping (not discriminative!!), the top negative images may perfectly match the query.</p>
<p>Solution: Use <strong>moderately hard</strong> negatives. Instead of selecting the top ranking images, sample from a range <span class="math inline">\([h_{min}, h_{max}]\)</span></p>
<h2 id="training-strategy">Training strategy:</h2>
<p>The captioning and self-retrieval modules share the same CNN 0) Image encoder pre-trained on ImageNet. 1a) Train the retrieval module on labeled data: - First the GRU and projection layers are trained (30 epochs). - Train the whole module (cnn included) (15 epochs). 1b) Pre-train the captioning module with MLE (self-retrieval is fixed 'not even used') and scheduled sampling. 2) Train the captioning module with the retrieval module fixed (including the CNN).</p>
<h4 id="experiments">Experiments:</h4>
<ul>
<li>CNN encoder : Resnet-101 <span class="math inline">\(dim(v) = 2048\)</span></li>
<li>GRU Encoder : <span class="math inline">\(d=1024\)</span></li>
<li>Both features projected to the similarity space with dimension 1024</li>
<li>The similarity used is cosine.</li>
<li>Vocbulary size 9k (freq &gt; 5).</li>
<li>The LSTM decoder with top-down attention (same as mine). (e=h=512)</li>
<li><span class="math inline">\(\alpha=1\)</span> and labeled-unlabeled images in a minibatch is 1:1.</li>
<li><span class="math inline">\(h_{min} = 100\)</span> and <span class="math inline">\(h_{max}=1000\)</span>.</li>
</ul>
<h3 id="mscoo-results">MSCOO results:</h3>
<div class="figure">
<img src="https://i.imgur.com/4B1WsCt.png" />

</div>
<h3 id="bi-product-captions-uniqueness-and-novelty">Bi-product: captions' uniqueness and novelty:</h3>
<p>uniqueness : percentage of unique captions in all the generated captions. novelty: percentage of captions never seen in the training set. o</p>
<p><img src="https://i.imgur.com/nL2oL4z.png" /> * evaluated on Karpathy's test split.</p>
<p>Check restart (Sgdr: stochastic gradient descent with warm restarts Loshchilov et al. 2016)</p>
</body>
</html>
