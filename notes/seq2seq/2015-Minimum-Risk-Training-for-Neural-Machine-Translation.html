<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="pandoc.css" type="text/css" />
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="minimum-risk-training-for-neural-machine-translation.">Minimum Risk Training for Neural Machine Translation.</h1>
<p><a href="https://arxiv.org/abs/1512.02433">link</a></p>
<h2 id="problematic">Problematic:</h2>
<p>It' preferable to optimize w.r.t to an evaluation metric rather than MLE. caveat: not differentiable.</p>
<h2 id="proposed-solution">Proposed solution:</h2>
<p>Approach already in use in SMT. (add ref to Och et al. 2013)</p>
<p>Not unlike, mixer (<a href="https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2015-Sequence-Level-Training-with-Recurrent-Neural-Networks.html">link</a>), minimizes the expected risk on the training data.</p>
<p>Slightly different terminology: - instead of a reward, use a discrepancy/distance <span class="math inline">\(\Delta(y, y^*)\)</span> where <span class="math inline">\(y\)</span> is generated by the model and <span class="math inline">\(y^*\)</span> the gold standard.</p>
<p>The optimized loss is thus: <span class="math display">\[ E_{p_\theta}[ \Delta(y, y^*)]\]</span></p>
<h2 id="issues">Issues:</h2>
<p>The expectation above (in the gradient) is intractable due to the exponentially large search space <span class="math inline">\(\mathcal Y\)</span>, the non decomposability of <span class="math inline">\(\Delta\)</span> and the context sensitiveness of NMT.</p>
<p>To alleviate the problem: consider a subest of the search space <span class="math inline">\(S\subset \mathcal Y\)</span>:</p>
<p><span class="math display">\[
\tilde R = \sum_{y \in S} Q(y, \alpha) \Delta(y, y^*)
\]</span></p>
<p><span class="math inline">\(Q\)</span> is a distribution defined on <span class="math inline">\(S\)</span>:</p>
<p><span class="math display">\[
Q(y, \alpha) \propto p_theta(y)^\alpha,
\]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> controls the sharpness.</p>
<p><span class="math inline">\(S\)</span> is a set of k distinct candidates sampled from the model (decoding mode, not top-k, k around 100)</p>
</body>
</html>
