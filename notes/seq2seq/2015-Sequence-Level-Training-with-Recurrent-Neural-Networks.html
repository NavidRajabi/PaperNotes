<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="pandoc.css" type="text/css" />
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="sequence-level-training-with-recurrent-neural-networks.">Sequence-level training with Recurrent Neural Networks.</h1>
<p>Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, Wojciech Zaremba</p>
<p><strong>ICLR 2016</strong> | <a href="http://arxiv.org/abs/1511.06732">arxiv</a> | <a href="https://github.com/facebookresearch/MIXER">code*</a> | <a href="https://github.com/violet-zct/pytorch_NMT">code</a></p>
<h2 id="problematic">Problematic:</h2>
<p>Raise issues with training sequence prediction models with MLE in teacher-forcing mode.</p>
<ul>
<li><p>MLE is not what the model is evaluated on in test time.</p></li>
<li><p>Integrating these metrics in the loss is not straighj-forward; mostly not differentiable and so do not allow for classic back-propagation.</p></li>
<li><p>Exposure bias ie discrepancy between the distribution during training (teacher-foricng) and the distribtion at evaluation time where the total error can escalate quickly.</p></li>
</ul>
<h2 id="proposed-solution">Proposed solution:</h2>
<p>Use reinforcement learning. Specifically, the REINFORCE algorithm. &gt; A method for doing back-propagation on computational graphs that output a probability distribution on actions.</p>
<p>In this case the loss is :</p>
<p><span class="math inline">\(E_{p_\theta}[r(y | y^*)]\)</span></p>
<p>This is easily applicable to RNNs, which output a soft-max probability distribution at each time step. Besides, to resolve the exposure bias, the outputed distribution will be generated in evalution mode (decoding), eventualy with beam-search to generate multiple candidates.</p>
<p>Training a model from scratch with this approach is infeasible, the other suggest warming-up with MLE then optimizing for a reward. These steps makes what the authors refer to as MIXER (Mixed Incremental Cross-Entropy Reinforce) which goes as follows:</p>
<ol style="list-style-type: decimal">
<li>Train model to optimize the likelihood of the target sequence, i.e. minimize the per time-step cross-entropy loss.</li>
<li>Then, for a target sequence of size T, optimize the cross-entropy for the T-Δ first time steps of the sequence and use Reinforce to get a gradient on the expected loss (e.g. negative BLEU) for the recursive generation of the rest of the Δ time steps.</li>
<li>Increase Δ and go back to 2., until Δ is equal to T.</li>
</ol>
<h2 id="experiments">Experiments:</h2>
<p>Results on 3 benchmarks, summarization, machine translation and image captioning, show that this approach generates better outputs even without the beam-search.</p>
<h2 id="issues">Issues:</h2>
<p>Does not outperform scheduled sampling (DaD data as demonstrator)</p>
</body>
</html>
