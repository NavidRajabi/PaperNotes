<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="pandoc.css" type="text/css" />
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="translating-phrases-in-neural-machine-tanslation">Translating Phrases in Neural Machine Tanslation</h1>
<p><a href="https://arxiv.org/abs/1706.05565">ArXiv</a></p>
<h2 id="problematic">Problematic:</h2>
<p>NMT uses mostly word-by-word (or char) generation; difficult to translate mutli-word expression/phrases/axioms... meaning of the phrase &gt; sum of the words' meanings.</p>
<h2 id="proposed-solution">Proposed solution:</h2>
<p>Integrate a phrase-based SMT into the NMT model. The SMT guided by the NMT propose a set of relevant phrases, The NMT scores the propsed phrases and select the most probable.</p>
<ul>
<li>Syntactic chunk inforation is integrated into the encoder to assist the NMT decoder.</li>
</ul>
<p>A sequence <span class="math inline">\(y\)</span> can be decomposed with words <span class="math inline">\((w_1,....w_K)\)</span> generated by the NMT and phrases <span class="math inline">\((p_1,...p_L)\)</span> generated by the SMT.</p>
<p>The probability of generating the sequence is defined as: <span class="math display">\[
p(y) = \prd_w (1-\lambda_{t(w)})P_{word}(w) \times \prod_p \lamba_{t(p)} P_{phrase}(p)
\]</span> <span class="math inline">\(t(.)\)</span> is the decoding step corresponding to the word (resp. the phrase) <span class="math inline">\(\lambda\)</span> is estimated <em>the balancer</em> an MLP taking for input the NMT's context vector, the previous decoding stat and the previously generated word. Intuitively, it's the importance weight of phrase over the word.</p>
<p>## To be continued...</p>
<p>## Check the followings: - Encoder-Decoder models with attached external structures (Gulcehre at al 2016, Gu et al 2016, Tang et al 2016 and Wang et al. 2017)</p>
</body>
</html>
