<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="pandoc.css" type="text/css" />
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h2 id="non-autoregressive-neural-machine-translation">Non-Autoregressive Neural Machine Translation</h2>
<p>Jiatao Gu, James Bradbury, Caiming Xiong, Victor O.K. Li, Richard Socher</p>
<p><strong>ICLR 2018</strong> | <a href="https://github.com/nyu-dl/dl4mt-nonauto">code</a> | <a href="https://openreview.net/forum?id=B1l8BtlCb">openreview</a></p>
<p>Propose a non-autoregressive neural sequence model based on iterative refinement</p>
<h2 id="problematic">Problematic:</h2>
<p>Auto-regressive models (e.g. RNNs) are often non-Markovian and nonlinear and must resort to suboptimal approximate decoding (beam-search) to find the sequence with the highest probability. The paper dubs this issue &quot;The decoding gap&quot; which is induced by the &quot;modeling gap&quot; i.e. the model's expressiveness or ability to model the data. The total performance gap would be the sum of the two.</p>
<h2 id="contributions">Contributions:</h2>
<p>The authors suggest an non-autoregressive model with a larger &quot;modeling gap&quot; but with which the decoding gap is non-existent thus minimizing the overall gap.</p>
<div class="figure">
<img src="https://i.imgur.com/3vsZTSJ.png" />

</div>
<p>The new model generates a full sequence with a decoder in the form of a transformer block. <img src="https://i.imgur.com/XBUlyFd.png" /></p>
<p><span class="math display">\[ P(Y|x) = \prod_t p(y_t|x) \]</span></p>
<p>However latent variables are introduced in a refinement process where we keep modifying the sequence until a stopping criterion is met.</p>
<p>The final sequence is obtained by marginalizing the latent intermediate sequences: <span class="math display">\[
P(Y|x) = \sum_{Y^0, ...Y^L} \prod_l P(Y^l| Y^{l-1}, x)
\]</span></p>
<p>The decoder shared at all the passages from a sequence to the next one.</p>
<p>Since the summation above is intractable a deterministic lower bound is used instead. <span class="math display">\[
\log P(Y|x) \geq \sum_l \log( P(\hat Y^l | \hat Y^{l-1}, x) )
\]</span> where each sequence is selected as: <span class="math display">\[
\hat Y = \arg\max P(.|\hat Y^{l-1}, x)
\]</span></p>
<h2 id="experiments">Experiments:</h2>
<div class="figure">
<img src="https://i.imgur.com/MuZA22q.png" />

</div>
</body>
</html>
