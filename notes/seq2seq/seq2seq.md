---
layout: post
title:  "Sequence-to-Sequence models"
category: Deep learning
tags: [DL, NLP, CV, CL]
---





<center> Update: 23/07/2018_15:12:17</center>

  	
1. [ Thesis - Contributions](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/Thesis---Contributions.html)
2. [ Stable and Effective Trainable Greedy Decoding for Sequence to Sequence Learning](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2018-Stable-and-Effective-Trainable-Greedy-Decoding-for-Sequence-to-Sequence-Learning.html)
3. [ Soft Actor-Critic](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2018-Soft-Actor-Critic-Off-Policy-Maximum-Entropy-Deep-Reinforcement-Learning-with-a-Stochastic-Actor.html)
4. [ {SEARNN}](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2018-SEARNN-Training-RNNs-with-global-local-losses.html)
5. [ Neural Lattice Language Models](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2018-Neural-Lattice-Language-Models.html)
6. [ MaskGAN](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2018-MaskGAN-Better-Text-Generation-via-Filling-in-the.html)
7. [ Latent Alignment and Variational Attention](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2018-Latent-Alignment-and-Variational-Attention.html)
8. [ Generating Contradictory, Neutral, and Entailing Sentences](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2018-Generating-Contradictory-Neutral-and-Entailing-Sentences.html)
9. [ Discrete Autoencoders for Sequence Models](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2018-Discrete-Autoencoders-for-Sequence-Models.html)
10. [ Differentiable Dynamic Programming for Structured Prediction and](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2018-Differentiable-Dynamic-Programming-for-Structured-Prediction-and-Attention.html)
11. [ Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2018-Deterministic-Non-Autoregressive-Neural-Sequence-Modeling-by-Iterative-Refinement.html)
12. [ Breaking the Softmax Bottleneck](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2018-Breaking-the-Softmax-Bottleneck-A-High-Rank-RNN-Language-Model.html)
13. [ Bi-Directional Block Self-Attention for Fast and Memory-Efficient Sequence](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2018-Bi-Directional-Block-Self-Attention-for-Fast-and-Memory-Efficient-Sequence-Modeling.html)
14. [ A Stochastic Decoder for Neural Machine Translation](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2018-A-Stochastic-Decoder-for-Neural-Machine-Translation.html)
15. [ Using stochastic computation graphs formalism for optimization of sequence-to-sequence model](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2017-Using-stochastic-computation-graphs-formalism-for-optimization-of-sequence-to-sequence-model.html)
16. [ Translating Phrases in Neural Machine Translation](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2017-Translating-Phrases-in-Neural-Machine-Translation.html)
17. [ Towards Neural Phrase-based Machine Translation](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2017-Towards-Neural-Phrase-based-Machine-Translation.html)
18. [ Synthetic and Natural Noise Both Break Neural Machine Translation](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2017-Synthetic-and-Natural-Noise-Both-Break-Neural-Machine-Translation.html)
19. [ Sequence Modeling via Segmentations](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2017-Sequence-Modeling-via-Segmentations.html)
20. [ Differentiable lower bound for expected BLEU score](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2017-Differentiable-lower-bound-for-expected-BLEU-score.html)
21. [ Data Noising as Smoothing in Neural Network Language Models](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2017-Data-Noising-as-Smoothing-in-Neural-Network-Language-Models.html)
22. [ Comparative Study of CNN and RNN for Natural Language Processing](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2017-Comparative-Study-of-CNN-and-RNN-for-Natural-Language-Processing.html)
23. [ Classical Structured Prediction Losses for Sequence to Sequence Learning](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2017-Classical-Structured-Prediction-Losses-for-Sequence-to-Sequence-Learning.html)
24. [ Reward Augmented Maximum Likelihood for Neural Structured Prediction](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2016-Reward-Augmented-Maximum-Likelihood-for-Neural-Structured-Prediction.html)
25. [ Multimodal Pivots for Image Caption Translation](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2016-Multimodal-Pivots-for-Image-Caption-Translation.html)
26. [ An Actor-Critic Algorithm for Sequence Prediction](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2016-An-Actor-Critic-Algorithm-for-Sequence-Prediction.html)
27. [ Sequence Level Training with Recurrent Neural Networks](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2015-Sequence-Level-Training-with-Recurrent-Neural-Networks.html)
28. [ Scheduled Sampling for Sequence Prediction with Recurrent Neural](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2015-Scheduled-Sampling-for-Sequence-Prediction-with-Recurrent-Neural-Networks.html)
29. [ Neural Machine Translation of Rare Words with Subword Units](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2015-Neural-Machine-Translation-of-Rare-Words-with-Subword-Units.html)
30. [ Minimum Risk Training for Neural Machine Translation](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2015-Minimum-Risk-Training-for-Neural-Machine-Translation.html)
31. [ Sequence to Sequence Learning with Neural Networks](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2014-Sequence-to-Sequence-Learning-with-Neural-Networks.html)
32. [ On Using Very Large Target Vocabulary for Neural Machine Translation](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2014-On-Using-Very-Large-Target-Vocabulary-for-Neural-Machine-Translation.html)
33. [ Neural Machine Translation by Jointly Learning to Align and Translate](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2014-Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate.html)
