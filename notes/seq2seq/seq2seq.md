---
layout: post
title:  "Sequence-to-Sequence models"
category: Deep learning
tags: [DL, NLP, CV, CL]
---





<center> Update: 08/03/2018_18:37:00</center>

  	
1. [ Stable and Effective Trainable Greedy Decoding for Sequence to Sequence Learning](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2018-Stable-and-Effective-Trainable-Greedy-Decoding-for-Sequence-to-Sequence-Learning.html)
2. [ Soft Actor-Critic](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2018-Soft-Actor-Critic-Off-Policy-Maximum-Entropy-Deep-Reinforcement-Learning-with-a-Stochastic-Actor.html)
3. [ MaskGAN](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2018-MaskGAN-Better-Text-Generation-via-Filling-in-the.html)
4. [ Discrete Autoencoders for Sequence Models](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2018-Discrete-Autoencoders-for-Sequence-Models.html)
5. [ Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2018-Deterministic-Non-Autoregressive-Neural-Sequence-Modeling-by-Iterative-Refinement.html)
6. [ Breaking the Softmax Bottleneck](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2018-Breaking-the-Softmax-Bottleneck-A-High-Rank-RNN-Language-Model.html)
7. [ Using stochastic computation graphs formalism for optimization of sequence-to-sequence model](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2017-Using-stochastic-computation-graphs-formalism-for-optimization-of-sequence-to-sequence-model.html)
8. [ Translating Phrases in Neural Machine Translation](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2017-Translating-Phrases-in-Neural-Machine-Translation.html)
9. [ Towards Neural Phrase-based Machine Translation](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2017-Towards-Neural-Phrase-based-Machine-Translation.html)
10. [ Synthetic and Natural Noise Both Break Neural Machine Translation](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2017-Synthetic-and-Natural-Noise-Both-Break-Neural-Machine-Translation.html)
11. [ Sequence Modeling via Segmentations](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2017-Sequence-Modeling-via-Segmentations.html)
12. [ Differentiable lower bound for expected BLEU score](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2017-Differentiable-lower-bound-for-expected-BLEU-score.html)
13. [ Data Noising as Smoothing in Neural Network Language Models](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2017-Data-Noising-as-Smoothing-in-Neural-Network-Language-Models.html)
14. [ An Actor-Critic Algorithm for Sequence Prediction](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2016-An-Actor-Critic-Algorithm-for-Sequence-Prediction.html)
15. [ Sequence Level Training with Recurrent Neural Networks](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2015-Sequence-Level-Training-with-Recurrent-Neural-Networks.html)
16. [ Minimum Risk Training for Neural Machine Translation](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2015-Minimum-Risk-Training-for-Neural-Machine-Translation.html)
17. [ On Using Very Large Target Vocabulary for Neural Machine Translation](https://rawgit.com/elbayadm/PaperNotes/master/notes/seq2seq/2014-On-Using-Very-Large-Target-Vocabulary-for-Neural-Machine-Translation.html)
