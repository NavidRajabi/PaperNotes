<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="pandoc.css" type="text/css" />
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h2 id="semi-supervised-learning-with-deep-generative-models">Semi-Supervised Learning with Deep Generative Models</h2>
<p>Diederik P. Kingma, Danilo J. Rezende, Shakir Mohamed, Max Welling</p>
<p><strong>NIPS 2014</strong> | <a href="http://arxiv.org/abs/1406.5298">arxiv</a> |</p>
<p>Given paired data <span class="math inline">\(\\{(x\_i, y_i)\\}\_{1\leq i \leq N}\)</span> with the labels <span class="math inline">\(y\_i\)</span> in <span class="math inline">\(\\{1,..,L\\}\)</span>. The observations have corresponding latent variables <span class="math inline">\(z\_i\)</span>. The empirical distribution over the labelled and unlabelled subsets are denoted as <span class="math inline">\(\tilde p\_l(x,y)\)</span> and <span class="math inline">\(\tilde p\_u(x)\)</span>. #### Latent feature discriminative model (M1): 1) Train an encoder/ generative model that provides a feature representation. 2) Train a separate classifier in the latent space. The generative model used is: <span class="math inline">\(p(z) =\mathcal N(z|0,I)\)</span> with <span class="math inline">\(p\_\theta(x|z) = f(x;z,\theta)\)</span>. <span class="math inline">\(f\)</span> is a suitable likelihood function whose probabilities are formed by a non-linear transformation <span class="math inline">\(\equiv\)</span> deep neural network.</p>
<p>The features are set to the approximate samples from the posterior <span class="math inline">\(p(z|x)\)</span> and a classifier such as transductive SVM (few unlabeled data near the margins) or multinomial regression is trained on this features. #### Generative semi-supervised model (M2): A probabilistic model that describes the data as being generated by a latent class variable <span class="math inline">\(y\)</span> as well as the continuous latent variable <span class="math inline">\(z\)</span>: <span class="math inline">\(p(y) = Cat(y|\pi)\)</span>; <span class="math inline">\(p(z) = \mathcal N(z|0,I)\)</span> and <span class="math inline">\(p\_\theta(x|y,z) = f(x;y,z,\theta)\)</span></p>
<p><span class="math inline">\(Cat(y|\pi)\)</span> is the multinomial distrib. and the class labels are treated as latent variable if the data point is unlabelled. ==The two latent variables are marginally independent==.</p>
<h4 id="stacked-generative-semi-supervised-model-m1m2">Stacked generative semi-supervised model (M1+M2):</h4>
<ol style="list-style-type: decimal">
<li>learn a latent variable <span class="math inline">\(z\_1\)</span> using <strong>M1</strong>.</li>
<li>use <strong>M2</strong> on <span class="math inline">\(z\_1\)</span> instead of the raw data.</li>
</ol>
In all models a VAE approach is used where we approximate the true posterior with <span class="math inline">\(q\_\phi(z|x)\)</span> of parameters <span class="math inline">\(\phi\)</span>. For <strong>M1</strong> we use a gaussian inference network and for <strong>M2</strong> we assume a factorized form:
<p>
<span class="math display">\[ q_\phi(z, y|x) = q_\phi(z|x)q_\phi(y|x)\]</span>
<p>
specified as gaussian and multinomial distributions.
<p>
<span class="math display">\[
M1: q_\phi(z|x) = \mathcal N(z | \mu_\phi(x), diag(\sigma^2_\phi(x))\\
M2: q_\phi(z|y, x) = \mathcal N(z | \mu_\phi(y,x), diag(\sigma^2_\phi(x));\:q_\phi(y|x) = Cat(y|\pi_\phi(x))
\]</span>
</p>
The functions parameterized by <span class="math inline">\(\phi\)</span> are represented as MLPs. For <strong>M1</strong>, the variational bound is:
<p>
<span class="math display">\[
 -\mathcal J(x) = E_{q_\phi(z|x)}\left[\log p_\theta(x|z)\right] - KL[q_\phi(z|x)|| p_\theta(z)]
\]</span>
</p>
<p>While for <strong>M2</strong>, we distinguish between the labelled and unlabelled bound:</p>
<p><span class="math display">\[
\begin{align}
-\mathcal L(x, y) &amp; = E_{q_\phi(y,z|x)}\left[\log p_\theta(x|y,z) + \log p_\theta(y) + \log p(z) - \log q_\phi(z|x,y)\right] - KL[q_\phi(z|x)|| p_\theta(z)]\\
-\mathcal U(x) &amp; = \sum_y q_\phi(y|x)(-\mathcal L(x,y)) + \mathcal H(q_\phi(y|x))\\
\mathcal J &amp; = \sum_{(x,y)\sim \tilde p_l} \mathcal L(x,y) + \sum_{x \sim \tilde p_u} \mathcal U(x)
\end{align}
\]</span></p>
Unfortunately the label predictive distribution <span class="math inline">\(q\_\phi(y|x)\)</span> intervene only on the unlabelled term of the bound. To remedy this, an additional classification loss on labelled data is added.
<p>
<span class="math display">\[
\mathcal J^\alpha = \mathcal J + \alpha E_{\tilde p_l(x,y)}\left[- \log q_\phi(y|x)\right]
\]</span>
</p>
<p>In experiments <span class="math inline">\(\alpha = 0.1 N\)</span> The optimization of the variational lower bound is perfrmed via AEVB.</p>
</body>
</html>
