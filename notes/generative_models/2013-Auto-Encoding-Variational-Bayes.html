<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="pandoc.css" type="text/css" />
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h2 id="auto-encoding-variational-bayes">Auto-Encoding Variational Bayes</h2>
<p>Diederik P Kingma, Max Welling</p>
<p><strong>ICLR 2014</strong> | <a href="http://arxiv.org/abs/1312.6114">arxiv</a> |</p>
<p><strong>Reminder: Bayes theorem</strong></p>
<p><span class="math display">\[ P(H|D) = \frac{P(D|H)P(H)}{P(D)}\]</span> P (H), ==the prior==, is the initial degree of belief in H. P (H|D), the ==posterior==, is the degree of belief in the hypothesis H having accounted for D. the quotient P(D|H)/P(D) represents the support D provides for H.</p>
<p><strong>Objective and motivation:</strong> Scalable inference and learning in Directed probabilistic graphical models in the presence of a continuous latent variable <span class="math inline">\(z\)</span> with intractable posterior.</p>
<h4 id="contributions">Contributions:</h4>
<ol style="list-style-type: decimal">
<li>A reparametrization of the variational lower bound into an unbiased and differentiable estimator, <span class="math inline">\(\equiv\)</span> stochastic gradient variational Bayes (SGVB).</li>
<li>Posterior inference with an approximate model fitted to the intractable poster using the lower bound (in the case of iid dataset and continuous latent variable per data point), <span class="math inline">\(\equiv\)</span> Auto encoding VB (AEVB).</li>
</ol>
<h3 id="typical-graphical-models">Typical graphical models:</h3>
<p><img src="http://ghost-redgns.s3.amazonaws.com/kingma1.png" alt="images" /> Solid lines for the generative model <span class="math inline">\(p\_\theta(z)p\_\theta(x|z)\)</span> and dashed lines for the variational approximation <span class="math inline">\(q\_\phi(z|x)\)</span> to the intractable posterior <span class="math inline">\(p\_\theta(z|x)\)</span> The recognition model <span class="math inline">\(q\_\phi(z|x)\)</span> is a probabilistic encoder and <span class="math inline">\(p\_\theta(x|z)\)</span> is a probablistic decoder. #### Setting: Given an N iid samples <span class="math inline">\(\mathcal X = \\{x\_i\\}\_{1\leq i \leq N}\)</span> of some continuous or discrete variable <span class="math inline">\(x\)</span>. We assume the data is generated by some random process involving an unobserved, continuoius random latent variable <span class="math inline">\(z\)</span>. The process consists of two steps:</p>
<ul>
<li>Generate <span class="math inline">\(z\_i\)</span> from some prior <span class="math inline">\(p\_{\theta^\*}(z)\)</span>.</li>
<li><span class="math inline">\(x\_i\)</span> is generated from a conditional distrib <span class="math inline">\(p\_{\theta^*}(x|z)\)</span> (likelihood).</li>
</ul>
<p>Both the prior and likelihood come from parametric families of distributions <span class="math inline">\(\theta \in \Theta\)</span> with PDFs that are differentiable a.s. w.r.t both <span class="math inline">\(\theta\)</span> and <span class="math inline">\(z\)</span></p>
<h4 id="the-variational-lower-bound">The variational lower bound:</h4>
<span class="math display">\[ML(\mathcal X) = \sum\_{i=1}^N \log p\_\theta(x\_i)\]</span> with:
<p>
<span class="math display">\[\log p_\theta(x) = \log(p_\theta(x|z)) + \log(p_\theta(z)) - \log(p_\theta(z|x))\]</span>
</p>
Given that:
<p>
<span class="math display">\[D_{KL}(q_\phi(z|x)||p_\theta(z|x)) = E_{q_\phi(z|x)}\left[\log(q_\phi(z|x)) - \log(p_\theta(z|x))\right]\]</span> <span class="math display">\[D_{KL}(q_\phi(z|x)||p_\theta(z)) = E_{q_\phi(z|x)}\left[\log(q_\phi(z|x)) - \log(p_\theta(z))\right]\]</span>
</p>
We can rewrite the ML of a sample <span class="math inline">\(x\)</span> as:
<p>
<span class="math display">\[\log p_\theta(x) = D_{KL}(q_\phi(z|x)||p_\theta(z|x)) - D_{KL}(q_\phi(z|x)||p_\theta(z)) + E_{q_\phi(z|x)}\left[\log(p_\theta(x|z)\right] \]</span>
</p>
Since the KL divergence is nonnegative we set <span class="math inline">\(\mathcal L(\theta, \phi;x)\)</span> as the variational lower bound:
<p>
<span class="math display">\[\mathcal L(\theta, \phi;x) = - D_{KL}(q_\phi(z|x)||p_\theta(z)) + E_{q_\phi(z|x)}\left[\log(p_\theta(x|z)\right] \]</span>
</p>
<p>The gradient of this variational lower bound, as is, w.r.t to the recognition/variational parameters <span class="math inline">\(\phi\)</span> is problematic and is usually estimated via MC.</p>
<h4 id="sgvb">SGVB:</h4>
For a chosen approximate posterior <span class="math inline">\(q\_\phi(z|x)\)</span> we can reparameterize the latent variable <span class="math inline">\(z\)</span> using a differentiable transformation <span class="math inline">\(\tilde z = g\_\phi(\epsilon, x)\)</span> where <span class="math inline">\(\epsilon \sim p(\epsilon)\)</span> is an auxiliary noise variable. Now the MC estimation of the expectation of some function <span class="math inline">\(f(z)\)</span> w.r.t to <span class="math inline">\(q\_\phi(z|x)\)</span> is:
<p>
<span class="math display">\[\begin{align}
E_{q_\phi(z|x)}\left[f(z)\right]  &amp; = E_{p(\epsilon)}\left[f(g_\phi(\epsilon, x))\right]\\
&amp; \approx \frac{1}{L_{MC}} \sum_l f(g_\phi(\epsilon_l, x)),\phantom{abcd} \epsilon_l \sim p(\epsilon)
\end{align}\]</span>
</p>
Applied to the lower bound <span class="math inline">\(\mathcal L\)</span> we build the SGVB estimator denoted <span class="math inline">\(\tilde{\mathcal L}\)</span>
<p>
<span class="math display">\[\tilde{\mathcal L}(\theta, \phi;x) = - D_{KL}(q_\phi(z|x)||p_\theta(z)) + \frac{1}{L_{MC}} \sum_l \log(p_\theta(x|z_l)) \]</span>
</p>
<p>where <span class="math inline">\(z\_l = g\_\phi(\epsilon\_l, x)\)</span> and <span class="math inline">\(\epsilon\_l\sim p(\epsilon)\)</span> The KL divergence can be interpreted as a regularization term for <span class="math inline">\(\phi\)</span> encouraging the approximate posterior to be close to the prior.</p>
<p>This term can be integrated analytically (in most cases) and only the reconstruction error needs MC sampling.</p>
e.g.in the case where both the prior and approximate posterior are Gaussians: <span class="math inline">\(p\_\theta(z) = \mathcal N(0,I)\)</span> and <span class="math inline">\(q\_\phi(z|x) = \mathcal N(\mu, \sigma^2)\)</span> with <span class="math inline">\(J\)</span> the dimension of <span class="math inline">\(z\)</span>:
<p>
<span class="math display">\[
- D_{KL}(q_\phi(z|x)||p_\theta(z)) = \frac{1}{2} \sum_j^J (1 + \log(\sigma_j^2) - \mu_j^2 - \sigma_j^2)
\]</span>
</p>
<p>Note: For large batch sizes in SGD, we can set <span class="math inline">\(L\_{MC} = 1\)</span>.</p>
<h4 id="aevb">AEVB:</h4>
<p>Use an SGD or adagrad to optimize the variational bound <span class="math inline">\(\tilde{\mathcal L}\)</span>.</p>
</body>
</html>
