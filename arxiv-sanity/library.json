[{"abstract": "Variational methods that rely on a recognition network to approximate the\nposterior of directed graphical models offer better inference and learning than\nprevious methods. Recent advances that exploit the capacity and flexibility in\nthis approach have expanded what kinds of models can be trained. However, as a\nproposal for the posterior, the capacity of the recognition network is limited,\nwhich can constrain the representational power of the generative model and\nincrease the variance of Monte Carlo estimates. To address these issues, we\nintroduce an iterative refinement procedure for improving the approximate\nposterior of the recognition network and show that training with the refined\nposterior is competitive with state-of-the-art methods. The advantages of\nrefinement are further evident in an increased effective sample size, which\nimplies a lower variance of gradient estimates.", "authors": ["R Devon Hjelm", "Kyunghyun Cho", "Junyoung Chung", "Russ Salakhutdinov", "Vince Calhoun", "Nebojsa Jojic"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1511.06382v6.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1511.06382v6", "num_discussion": 0, "originally_published_time": "11/19/2015", "pid": "1511.06382v6", "published_time": "2/20/2018", "rawpid": "1511.06382", "tags": ["cs.LG", "stat.ML"], "title": "Iterative Refinement of the Approximate Posterior for Directed Belief\n  Networks"}, {"abstract": "We propose a conditional non-autoregressive neural sequence model based on\niterative refinement. The proposed model is designed based on the principles of\nlatent variable models and denoising autoencoders, and is generally applicable\nto any sequence generation task. We extensively evaluate the proposed model on\nmachine translation (En-De and En-Ro) and image caption generation, and observe\nthat it significantly speeds up decoding while maintaining the generation\nquality comparable to the autoregressive counterpart.", "authors": ["Jason Lee", "Elman Mansimov", "Kyunghyun Cho"], "category": "cs.LG", "comment": "Submitted to ICML 2018", "img": "/static/thumbs/1802.06901v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1802.06901v1", "num_discussion": 0, "originally_published_time": "2/19/2018", "pid": "1802.06901v1", "published_time": "2/19/2018", "rawpid": "1802.06901", "tags": ["cs.LG", "cs.CL", "stat.ML"], "title": "Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative\n  Refinement"}, {"abstract": "Image-to-image translation is a class of vision and graphics problems where\nthe goal is to learn the mapping between an input image and an output image\nusing a training set of aligned image pairs. However, for many tasks, paired\ntraining data will not be available. We present an approach for learning to\ntranslate an image from a source domain $X$ to a target domain $Y$ in the\nabsence of paired examples. Our goal is to learn a mapping $G: X \\rightarrow Y$\nsuch that the distribution of images from $G(X)$ is indistinguishable from the\ndistribution $Y$ using an adversarial loss. Because this mapping is highly\nunder-constrained, we couple it with an inverse mapping $F: Y \\rightarrow X$\nand introduce a cycle consistency loss to push $F(G(X)) \\approx X$ (and vice\nversa). Qualitative results are presented on several tasks where paired\ntraining data does not exist, including collection style transfer, object\ntransfiguration, season transfer, photo enhancement, etc. Quantitative\ncomparisons against several prior methods demonstrate the superiority of our\napproach.", "authors": ["Jun-Yan Zhu", "Taesung Park", "Phillip Isola", "Alexei A. Efros"], "category": "cs.CV", "comment": "An extended version of our ICCV 2017 paper, v4 updates the\n  implementation details in the appendix", "img": "/static/thumbs/1703.10593v4.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1703.10593v4", "num_discussion": 0, "originally_published_time": "3/30/2017", "pid": "1703.10593v4", "published_time": "2/19/2018", "rawpid": "1703.10593", "tags": ["cs.CV"], "title": "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial\n  Networks"}, {"abstract": "In unsupervised data generation tasks, besides the generation of a sample\nbased on previous observations, one would often like to give hints to the model\nin order to bias the generation towards desirable metrics. We propose a method\nthat combines Generative Adversarial Networks (GANs) and reinforcement learning\n(RL) in order to accomplish exactly that. While RL biases the data generation\nprocess towards arbitrary metrics, the GAN component of the reward function\nensures that the model still remembers information learned from data. We build\nupon previous results that incorporated GANs and RL in order to generate\nsequence data and test this model in several settings for the generation of\nmolecules encoded as text sequences (SMILES) and in the context of music\ngeneration, showing for each case that we can effectively bias the generation\nprocess towards desired metrics.", "authors": ["Gabriel Lima Guimaraes", "Benjamin Sanchez-Lengeling", "Carlos Outeiral", "Pedro Luis Cunha Farias", "Al\u00e1n Aspuru-Guzik"], "category": "stat.ML", "comment": "10 pages, 7 figures", "img": "/static/thumbs/1705.10843v3.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1705.10843v3", "num_discussion": 0, "originally_published_time": "5/30/2017", "pid": "1705.10843v3", "published_time": "2/7/2018", "rawpid": "1705.10843", "tags": ["stat.ML", "cs.LG"], "title": "Objective-Reinforced Generative Adversarial Networks (ORGAN) for\n  Sequence Generation Models"}, {"abstract": "Many image-to-image translation problems are ambiguous, as a single input\nimage may correspond to multiple possible outputs. In this work, we aim to\nmodel a \\emph{distribution} of possible outputs in a conditional generative\nmodeling setting. The ambiguity of the mapping is distilled in a\nlow-dimensional latent vector, which can be randomly sampled at test time. A\ngenerator learns to map the given input, combined with this latent code, to the\noutput. We explicitly encourage the connection between output and the latent\ncode to be invertible. This helps prevent a many-to-one mapping from the latent\ncode to the output during training, also known as the problem of mode collapse,\nand produces more diverse results. We explore several variants of this approach\nby employing different training objectives, network architectures, and methods\nof injecting the latent code. Our proposed method encourages bijective\nconsistency between the latent encoding and output modes. We present a\nsystematic comparison of our method and other variants on both perceptual\nrealism and diversity.", "authors": ["Jun-Yan Zhu", "Richard Zhang", "Deepak Pathak", "Trevor Darrell", "Alexei A. Efros", "Oliver Wang", "Eli Shechtman"], "category": "cs.CV", "comment": "NIPS 2017 Final paper. v2 adds implementation details; Website:\n  https://junyanz.github.io/BicycleG...", "img": "/static/thumbs/1711.11586v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1711.11586v2", "num_discussion": 0, "originally_published_time": "11/30/2017", "pid": "1711.11586v2", "published_time": "2/1/2018", "rawpid": "1711.11586", "tags": ["cs.CV", "cs.GR", "stat.ML"], "title": "Toward Multimodal Image-to-Image Translation"}, {"abstract": "Recurrent neural networks (RNNs) are important class of architectures among\nneural networks useful for language modeling and sequential prediction.\nHowever, optimizing RNNs is known to be harder compared to feed-forward neural\nnetworks. A number of techniques have been proposed in literature to address\nthis problem. In this paper we propose a simple technique called fraternal\ndropout that takes advantage of dropout to achieve this goal. Specifically, we\npropose to train two identical copies of an RNN (that share parameters) with\ndifferent dropout masks while minimizing the difference between their\n(pre-softmax) predictions. In this way our regularization encourages the\nrepresentations of RNNs to be invariant to dropout mask, thus being robust. We\nshow that our regularization term is upper bounded by the expectation-linear\ndropout objective which has been shown to address the gap due to the difference\nbetween the train and inference phases of dropout. We evaluate our model and\nachieve state-of-the-art results in sequence modeling tasks on two benchmark\ndatasets - Penn Treebank and Wikitext-2. We also show that our approach leads\nto performance improvement by a significant margin in image captioning\n(Microsoft COCO) and semi-supervised (CIFAR-10) tasks.", "authors": ["Konrad Zolna", "Devansh Arpit", "Dendi Suhubdy", "Yoshua Bengio"], "category": "stat.ML", "comment": "Accepted to ICLR 2018. Extended appendix. Added official GitHub code\n  for replication: https://gith...", "img": "/static/thumbs/1711.00066v3.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1711.00066v3", "num_discussion": 0, "originally_published_time": "10/31/2017", "pid": "1711.00066v3", "published_time": "1/31/2018", "rawpid": "1711.00066", "tags": ["stat.ML", "cs.AI", "cs.LG"], "title": "Fraternal Dropout"}, {"abstract": "Automatic image captioning has recently approached human-level performance\ndue to the latest advances in computer vision and natural language\nunderstanding. However, most of the current models can only generate plain\nfactual descriptions about the content of a given image. However, for human\nbeings, image caption writing is quite flexible and diverse, where additional\nlanguage dimensions, such as emotion, humor and language styles, are often\nincorporated to produce diverse, emotional, or appealing captions. In\nparticular, we are interested in generating sentiment-conveying image\ndescriptions, which has received little attention. The main challenge is how to\neffectively inject sentiments into the generated captions without altering the\nsemantic matching between the visual content and the generated descriptions. In\nthis work, we propose two different models, which employ different schemes for\ninjecting sentiments into image captions. Compared with the few existing\napproaches, the proposed models are much simpler and yet more effective. The\nexperimental results show that our model outperform the state-of-the-art models\nin generating sentimental (i.e., sentiment-bearing) image captions. In\naddition, we can also easily manipulate the model by assigning different\nsentiments to the testing image to generate captions with the corresponding\nsentiments.", "authors": ["Quanzeng You", "Hailin Jin", "Jiebo Luo"], "category": "cs.CV", "comment": "8 pages, 5 figures and 4 tables", "img": "/static/thumbs/1801.10121v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1801.10121v1", "num_discussion": 0, "originally_published_time": "1/30/2018", "pid": "1801.10121v1", "published_time": "1/30/2018", "rawpid": "1801.10121", "tags": ["cs.CV"], "title": "Image Captioning at Will: A Versatile Scheme for Effectively Injecting\n  Sentiments into Image Descriptions"}, {"abstract": "State-of-the-art methods for learning cross-lingual word embeddings have\nrelied on bilingual dictionaries or parallel corpora. Recent studies showed\nthat the need for parallel data supervision can be alleviated with\ncharacter-level information. While these methods showed encouraging results,\nthey are not on par with their supervised counterparts and are limited to pairs\nof languages sharing a common alphabet. In this work, we show that we can build\na bilingual dictionary between two languages without using any parallel\ncorpora, by aligning monolingual word embedding spaces in an unsupervised way.\nWithout using any character information, our model even outperforms existing\nsupervised methods on cross-lingual tasks for some language pairs. Our\nexperiments demonstrate that our method works very well also for distant\nlanguage pairs, like English-Russian or English-Chinese. We finally describe\nexperiments on the English-Esperanto low-resource language pair, on which there\nonly exists a limited amount of parallel data, to show the potential impact of\nour method in fully unsupervised machine translation. Our code, embeddings and\ndictionaries are publicly available.", "authors": ["Alexis Conneau", "Guillaume Lample", "Marc'Aurelio Ranzato", "Ludovic Denoyer", "Herv\u00e9 J\u00e9gou"], "category": "cs.CL", "comment": "ICLR 2018", "img": "/static/thumbs/1710.04087v3.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1710.04087v3", "num_discussion": 0, "originally_published_time": "10/11/2017", "pid": "1710.04087v3", "published_time": "1/30/2018", "rawpid": "1710.04087", "tags": ["cs.CL"], "title": "Word Translation Without Parallel Data"}, {"abstract": "In this paper, we present Neural Phrase-based Machine Translation (NPMT). Our\nmethod explicitly models the phrase structures in output sequences using\nSleep-WAke Networks (SWAN), a recently proposed segmentation-based sequence\nmodeling method. To mitigate the monotonic alignment requirement of SWAN, we\nintroduce a new layer to perform (soft) local reordering of input sequences.\nDifferent from existing neural machine translation (NMT) approaches, NPMT does\nnot use attention-based decoding mechanisms. Instead, it directly outputs\nphrases in a sequential order and can decode in linear time. Our experiments\nshow that NPMT achieves superior performances on IWSLT 2014\nGerman-English/English-German and IWSLT 2015 English-Vietnamese machine\ntranslation tasks compared with strong NMT baselines. We also observe that our\nmethod produces meaningful phrases in output languages.", "authors": ["Po-Sen Huang", "Chong Wang", "Sitao Huang", "Dengyong Zhou", "Li Deng"], "category": "cs.CL", "comment": "accepted in International Conference on Learning Representations\n  (ICLR) 2018", "img": "/static/thumbs/1706.05565v5.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1706.05565v5", "num_discussion": 0, "originally_published_time": "6/17/2017", "pid": "1706.05565v5", "published_time": "1/29/2018", "rawpid": "1706.05565", "tags": ["cs.CL", "stat.ML"], "title": "Towards Neural Phrase-based Machine Translation"}, {"abstract": "This paper strives to find amidst a set of sentences the one best describing\nthe content of a given image or video. Different from existing works, which\nrely on a joint subspace for their image and video caption retrieval, we\npropose to do so in a visual space exclusively. Apart from this conceptual\nnovelty, we contribute \\emph{Word2VisualVec}, a deep neural network\narchitecture that learns to predict a visual feature representation from\ntextual input. Example captions are encoded into a textual embedding based on\nmulti-scale sentence vectorization and further transferred into a deep visual\nfeature of choice via a simple multi-layer perceptron. We further generalize\nWord2VisualVec for video caption retrieval, by predicting from text both 3-D\nconvolutional neural network features as well as a visual-audio representation.\nExperiments on Flickr8k, Flickr30k, the Microsoft Video Description dataset and\nthe very recent NIST TrecVid challenge for video caption retrieval detail\nWord2VisualVec's properties, its benefit over textual embeddings, the potential\nfor multimodal query composition and its state-of-the-art results.", "authors": ["Jianfeng Dong", "Xirong Li", "Cees G. M. Snoek"], "category": "cs.CV", "comment": "12 pages, 7 figures, under view of TMM", "img": "/static/thumbs/1709.01362v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1709.01362v2", "num_discussion": 0, "originally_published_time": "9/5/2017", "pid": "1709.01362v2", "published_time": "1/29/2018", "rawpid": "1709.01362", "tags": ["cs.CV"], "title": "Predicting Visual Features from Text for Image and Video Caption\n  Retrieval"}, {"abstract": "Recent work has shown that convolutional networks can be substantially\ndeeper, more accurate, and efficient to train if they contain shorter\nconnections between layers close to the input and those close to the output. In\nthis paper, we embrace this observation and introduce the Dense Convolutional\nNetwork (DenseNet), which connects each layer to every other layer in a\nfeed-forward fashion. Whereas traditional convolutional networks with L layers\nhave L connections - one between each layer and its subsequent layer - our\nnetwork has L(L+1)/2 direct connections. For each layer, the feature-maps of\nall preceding layers are used as inputs, and its own feature-maps are used as\ninputs into all subsequent layers. DenseNets have several compelling\nadvantages: they alleviate the vanishing-gradient problem, strengthen feature\npropagation, encourage feature reuse, and substantially reduce the number of\nparameters. We evaluate our proposed architecture on four highly competitive\nobject recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet).\nDenseNets obtain significant improvements over the state-of-the-art on most of\nthem, whilst requiring less computation to achieve high performance. Code and\npre-trained models are available at https://github.com/liuzhuang13/DenseNet .", "authors": ["Gao Huang", "Zhuang Liu", "Laurens van der Maaten", "Kilian Q. Weinberger"], "category": "cs.CV", "comment": "CVPR 2017", "img": "/static/thumbs/1608.06993v5.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1608.06993v5", "num_discussion": 0, "originally_published_time": "8/25/2016", "pid": "1608.06993v5", "published_time": "1/28/2018", "rawpid": "1608.06993", "tags": ["cs.CV", "cs.LG"], "title": "Densely Connected Convolutional Networks"}, {"abstract": "Generic generation and manipulation of text is challenging and has limited\nsuccess compared to recent deep generative modeling in visual domain. This\npaper aims at generating plausible natural language sentences, whose attributes\nare dynamically controlled by learning disentangled latent representations with\ndesignated semantics. We propose a new neural generative model which combines\nvariational auto-encoders and holistic attribute discriminators for effective\nimposition of semantic structures. With differentiable approximation to\ndiscrete text samples, explicit constraints on independent attribute controls,\nand efficient collaborative learning of generator and discriminators, our model\nlearns highly interpretable representations from even only word annotations,\nand produces realistic sentences with desired attributes. Quantitative\nevaluation validates the accuracy of sentence and attribute generation.", "authors": ["Zhiting Hu", "Zichao Yang", "Xiaodan Liang", "Ruslan Salakhutdinov", "Eric P. Xing"], "category": "cs.LG", "comment": "Fixed typos; ICML 2017", "img": "/static/thumbs/1703.00955v3.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1703.00955v3", "num_discussion": 0, "originally_published_time": "3/2/2017", "pid": "1703.00955v3", "published_time": "1/23/2018", "rawpid": "1703.00955", "tags": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "title": "Toward Controlled Generation of Text"}, {"abstract": "Partially inspired by successful applications of variational recurrent neural\nnetworks, we propose a novel variational recurrent neural machine translation\n(VRNMT) model in this paper. Different from the variational NMT, VRNMT\nintroduces a series of latent random variables to model the translation\nprocedure of a sentence in a generative way, instead of a single latent\nvariable. Specifically, the latent random variables are included into the\nhidden states of the NMT decoder with elements from the variational\nautoencoder. In this way, these variables are recurrently generated, which\nenables them to further capture strong and complex dependencies among the\noutput translations at different timesteps. In order to deal with the\nchallenges in performing efficient posterior inference and large-scale training\nduring the incorporation of latent variables, we build a neural posterior\napproximator, and equip it with a reparameterization technique to estimate the\nvariational lower bound. Experiments on Chinese-English and English-German\ntranslation tasks demonstrate that the proposed model achieves significant\nimprovements over both the conventional and variational NMT models.", "authors": ["Jinsong Su", "Shan Wu", "Deyi Xiong", "Yaojie Lu", "Xianpei Han", "Biao Zhang"], "category": "cs.CL", "comment": "accepted by AAAI 18", "img": "/static/thumbs/1801.05119v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1801.05119v1", "num_discussion": 0, "originally_published_time": "1/16/2018", "pid": "1801.05119v1", "published_time": "1/16/2018", "rawpid": "1801.05119", "tags": ["cs.CL"], "title": "Variational Recurrent Neural Machine Translation"}, {"abstract": "As machine learning algorithms are increasingly applied to high impact yet\nhigh risk tasks, such as medical diagnosis or autonomous driving, it is\ncritical that researchers can explain how such algorithms arrived at their\npredictions. In recent years, a number of image saliency methods have been\ndeveloped to summarize where highly complex neural networks \"look\" in an image\nfor evidence for their predictions. However, these techniques are limited by\ntheir heuristic nature and architectural constraints. In this paper, we make\ntwo main contributions: First, we propose a general framework for learning\ndifferent kinds of explanations for any black box algorithm. Second, we\nspecialise the framework to find the part of an image most responsible for a\nclassifier decision. Unlike previous works, our method is model-agnostic and\ntestable because it is grounded in explicit and interpretable image\nperturbations.", "authors": ["Ruth Fong", "Andrea Vedaldi"], "category": "cs.CV", "comment": "Final camera-ready paper published at ICCV 2017 (Supplementary\n  materials:\n  http://openaccess.thec...", "img": "/static/thumbs/1704.03296v3.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1704.03296v3", "num_discussion": 0, "originally_published_time": "4/11/2017", "pid": "1704.03296v3", "published_time": "1/10/2018", "rawpid": "1704.03296", "tags": ["cs.CV", "cs.AI", "cs.LG", "stat.ML"], "title": "Interpretable Explanations of Black Boxes by Meaningful Perturbation"}, {"abstract": "Neural networks are vulnerable to adversarial examples and researchers have\nproposed many heuristic attack and defense mechanisms. We take the principled\nview of distributionally robust optimization, which guarantees performance\nunder adversarial input perturbations. By considering a Lagrangian penalty\nformulation of perturbation of the underlying data distribution in a\nWasserstein ball, we provide a training procedure that augments model parameter\nupdates with worst-case perturbations of training data. For smooth losses, our\nprocedure provably achieves moderate levels of robustness with little\ncomputational or statistical cost relative to empirical risk minimization.\nFurthermore, our statistical guarantees allow us to efficiently certify\nrobustness for the population loss. For imperceptible perturbations, our method\nmatches or outperforms heuristic approaches.", "authors": ["Aman Sinha", "Hongseok Namkoong", "John Duchi"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1710.10571v3.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1710.10571v3", "num_discussion": 0, "originally_published_time": "10/29/2017", "pid": "1710.10571v3", "published_time": "1/9/2018", "rawpid": "1710.10571", "tags": ["stat.ML", "cs.LG"], "title": "Certifiable Distributional Robustness with Principled Adversarial\n  Training"}, {"abstract": "Neural embeddings are a popular set of methods for representing words,\nphrases or text as a low dimensional vector (typically 50-500 dimensions).\nHowever, it is difficult to interpret these dimensions in a meaningful manner,\nand creating neural embeddings requires extensive training and tuning of\nmultiple parameters and hyperparameters. We present here a simple unsupervised\nmethod for representing words, phrases or text as a low dimensional vector, in\nwhich the meaning and relative importance of dimensions is transparent to\ninspection. We have created a near-comprehensive vector representation of\nwords, and selected bigrams, trigrams and abbreviations, using the set of\ntitles and abstracts in PubMed as a corpus. This vector is used to create\nseveral novel implicit word-word and text-text similarity metrics. The implicit\nword-word similarity metrics correlate well with human judgement of word pair\nsimilarity and relatedness, and outperform or equal all other reported methods\non a variety of biomedical benchmarks, including several implementations of\nneural embeddings trained on PubMed corpora. Our implicit word-word metrics\ncapture different aspects of word-word relatedness than word2vec-based metrics\nand are only partially correlated (rho = ~0.5-0.8 depending on task and\ncorpus). The vector representations of words, bigrams, trigrams, abbreviations,\nand PubMed title+abstracts are all publicly available from\nhttp://arrowsmith.psych.uic.edu for release under CC-BY-NC license. Several\npublic web query interfaces are also available at the same site, including one\nwhich allows the user to specify a given word and view its most closely related\nterms according to direct co-occurrence as well as different implicit\nsimilarity metrics.", "authors": ["Neil R. Smalheiser", "Gary Bonifield"], "category": "cs.CL", "comment": "27 pages, 9 tables, and 6 supplemental files which can be accessed at\n  http://arrowsmith.psych.uic....", "img": "/static/thumbs/1801.01884v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1801.01884v2", "num_discussion": 0, "originally_published_time": "1/5/2018", "pid": "1801.01884v2", "published_time": "1/9/2018", "rawpid": "1801.01884", "tags": ["cs.CL", "cs.IR", "I.2.7; I.7.2"], "title": "Unsupervised Low-Dimensional Vector Representations for Words, Phrases\n  and Text that are Transparent, Scalable, and produce Similarity Metrics that\n  are Complementary to Neural Embeddings"}, {"abstract": "Recurrent neural networks are nowadays successfully used in an abundance of\napplications, going from text, speech and image processing to recommender\nsystems. Backpropagation through time is the algorithm that is commonly used to\ntrain these networks on specific tasks. Many deep learning frameworks have\ntheir own implementation of training and sampling procedures for recurrent\nneural networks, while there are in fact multiple other possibilities to choose\nfrom and other parameters to tune. In existing literature this is very often\noverlooked or ignored. In this paper we therefore give an overview of possible\ntraining and sampling schemes for character-level recurrent neural networks to\nsolve the task of predicting the next token in a given sequence. We test these\ndifferent schemes on a variety of datasets, neural network architectures and\nparameter settings, and formulate a number of take-home recommendations. The\nchoice of training and sampling scheme turns out to be subject to a number of\ntrade-offs, such as training stability, sampling time, model performance and\nimplementation effort, but is largely independent of the data. Perhaps the most\nsurprising result is that transferring hidden states for correctly initializing\nthe model on subsequences often leads to unstable training behavior depending\non the dataset.", "authors": ["Cedric De Boom", "Thomas Demeester", "Bart Dhoedt"], "category": "cs.LG", "comment": "23 pages, 11 figures, 4 tables", "img": "/static/thumbs/1801.00632v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1801.00632v2", "num_discussion": 0, "originally_published_time": "1/2/2018", "pid": "1801.00632v2", "published_time": "1/9/2018", "rawpid": "1801.00632", "tags": ["cs.LG", "cs.CL", "stat.ML"], "title": "Character-level Recurrent Neural Networks in Practice: Comparing\n  Training and Sampling Schemes"}, {"abstract": "To perform tasks specified by natural language instructions, autonomous\nagents need to extract semantically meaningful representations of language and\nmap it to visual elements and actions in the environment. This problem is\ncalled task-oriented language grounding. We propose an end-to-end trainable\nneural architecture for task-oriented language grounding in 3D environments\nwhich assumes no prior linguistic or perceptual knowledge and requires only raw\npixels from the environment and the natural language instruction as input. The\nproposed model combines the image and text representations using a\nGated-Attention mechanism and learns a policy to execute the natural language\ninstruction using standard reinforcement and imitation learning methods. We\nshow the effectiveness of the proposed model on unseen instructions as well as\nunseen maps, both quantitatively and qualitatively. We also introduce a novel\nenvironment based on a 3D game engine to simulate the challenges of\ntask-oriented language grounding over a rich set of instructions and\nenvironment states.", "authors": ["Devendra Singh Chaplot", "Kanthashree Mysore Sathyendra", "Rama Kumar Pasumarthi", "Dheeraj Rajagopal", "Ruslan Salakhutdinov"], "category": "cs.LG", "comment": "To appear in AAAI-18", "img": "/static/thumbs/1706.07230v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1706.07230v2", "num_discussion": 0, "originally_published_time": "6/22/2017", "pid": "1706.07230v2", "published_time": "1/9/2018", "rawpid": "1706.07230", "tags": ["cs.LG", "cs.AI", "cs.CL", "cs.RO"], "title": "Gated-Attention Architectures for Task-Oriented Language Grounding"}, {"abstract": "Recurrent neural networks (RNNs) are capable of learning features and long\nterm dependencies from sequential and time-series data. The RNNs have a stack\nof non-linear units where at least one connection between units forms a\ndirected cycle. A well-trained RNN can model any dynamical system; however,\ntraining RNNs is mostly plagued by issues in learning long-term dependencies.\nIn this paper, we present a survey on RNNs and several new advances for\nnewcomers and professionals in the field. The fundamentals and recent advances\nare explained and the research challenges are introduced.", "authors": ["Hojjat Salehinejad", "Julianne Baarbe", "Sharan Sankar", "Joseph Barfett", "Errol Colak", "Shahrokh Valaee"], "category": "cs.NE", "comment": "", "img": "/static/thumbs/1801.01078v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1801.01078v2", "num_discussion": 0, "originally_published_time": "12/29/2017", "pid": "1801.01078v2", "published_time": "1/8/2018", "rawpid": "1801.01078", "tags": ["cs.NE"], "title": "Recent Advances in Recurrent Neural Networks"}, {"abstract": "Neural network training relies on our ability to find \"good\" minimizers of\nhighly non-convex loss functions. It is well known that certain network\narchitecture designs (e.g., skip connections) produce loss functions that train\neasier, and well-chosen training parameters (batch size, learning rate,\noptimizer) produce minimizers that generalize better. However, the reasons for\nthese differences, and their effect on the underlying loss landscape, is not\nwell understood. In this paper, we explore the structure of neural loss\nfunctions, and the effect of loss landscapes on generalization, using a range\nof visualization methods. First, we introduce a simple \"filter normalization\"\nmethod that helps us visualize loss function curvature, and make meaningful\nside-by-side comparisons between loss functions. Then, using a variety of\nvisualizations, we explore how network architecture affects the loss landscape,\nand how training parameters affect the shape of minimizers.", "authors": ["Hao Li", "Zheng Xu", "Gavin Taylor", "Tom Goldstein"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1712.09913v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1712.09913v1", "num_discussion": 0, "originally_published_time": "12/28/2017", "pid": "1712.09913v1", "published_time": "12/28/2017", "rawpid": "1712.09913", "tags": ["cs.LG", "cs.CV", "stat.ML"], "title": "Visualizing the Loss Landscape of Neural Nets"}, {"abstract": "Captioning models are typically trained using the cross-entropy loss.\nHowever, their performance is evaluated on other metrics designed to better\ncorrelate with human assessments. Recently, it has been shown that\nreinforcement learning (RL) can directly optimize these metrics in tasks such\nas captioning. However, this is computationally costly and requires specifying\na baseline reward at each step to make training converge. We propose a fast\napproach to optimize one's objective of interest through the REINFORCE\nalgorithm. First we show that, by replacing model samples with ground-truth\nsentences, RL training can be seen as a form of weighted cross-entropy loss,\ngiving a fast, RL-based pre-training algorithm. Second, we propose to use the\nconsensus among ground-truth captions of the same video as the baseline reward.\nThis can be computed very efficiently. We call the complete proposal\nConsensus-based Sequence Training (CST). Applied to the MSRVTT video captioning\nbenchmark, our proposals train significantly faster than comparable methods and\nestablish a new state-of-the-art on the task, improving the CIDEr score from\n47.3 to 54.2.", "authors": ["Sang Phan", "Gustav Eje Henter", "Yusuke Miyao", "Shin'ichi Satoh"], "category": "cs.CV", "comment": "11 pages, 4 figures, 5 tables. Github repo at\n  https://github.com/mynlp/cst_captioning", "img": "/static/thumbs/1712.09532v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1712.09532v1", "num_discussion": 0, "originally_published_time": "12/27/2017", "pid": "1712.09532v1", "published_time": "12/27/2017", "rawpid": "1712.09532", "tags": ["cs.CV"], "title": "Consensus-based Sequence Training for Video Captioning"}, {"abstract": "Existing neural machine translation systems do not explicitly model what has\nbeen translated and what has not during the decoding phase. To address this\nproblem, we propose a novel mechanism that separates the source information\ninto two parts: translated Past contents and untranslated Future contents,\nwhich are modeled by two additional recurrent layers. The Past and Future\ncontents are fed to both the attention model and the decoder states, which\noffers NMT systems the knowledge of translated and untranslated contents.\nExperimental results show that the proposed approach significantly improves\ntranslation performance in Chinese-English, German-English and English-German\ntranslation tasks. Specifically, the proposed model outperforms the\nconventional coverage model in both of the translation quality and the\nalignment error rate.", "authors": ["Zaixiang Zheng", "Hao Zhou", "Shujian Huang", "Lili Mou", "Xinyu Dai", "Jiajun Chen", "Zhaopeng Tu"], "category": "cs.CL", "comment": "Accepted by Transaction of ACL", "img": "/static/thumbs/1711.09502v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1711.09502v2", "num_discussion": 0, "originally_published_time": "11/27/2017", "pid": "1711.09502v2", "published_time": "12/26/2017", "rawpid": "1711.09502", "tags": ["cs.CL"], "title": "Modeling Past and Future for Neural Machine Translation"}, {"abstract": "Common recurrent neural network architectures scale poorly due to the\nintrinsic difficulty in parallelizing their state computations. In this work,\nwe propose the Simple Recurrent Unit (SRU) architecture, a recurrent unit that\nsimplifies the computation and exposes more parallelism. In SRU, the majority\nof computation for each step is independent of the recurrence and can be easily\nparallelized. SRU is as fast as a convolutional layer and 5-10x faster than an\noptimized LSTM implementation. We study SRUs on a wide range of applications,\nincluding classification, question answering, language modeling, translation\nand speech recognition. Our experiments demonstrate the effectiveness of SRU\nand the trade-off it enables between speed and performance. We open source our\nimplementation in PyTorch and CNTK.", "authors": ["Tao Lei", "Yu Zhang", "Yoav Artzi"], "category": "cs.CL", "comment": "submission version", "img": "/static/thumbs/1709.02755v4.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1709.02755v4", "num_discussion": 0, "originally_published_time": "9/8/2017", "pid": "1709.02755v4", "published_time": "12/26/2017", "rawpid": "1709.02755", "tags": ["cs.CL", "cs.NE"], "title": "Training RNNs as Fast as CNNs"}, {"abstract": "Recurrent neural language models are the state-of-the-art models for language\nmodeling. When the vocabulary size is large, the space taken to store the model\nparameters becomes the bottleneck for the use of recurrent neural language\nmodels. In this paper, we introduce a simple space compression method that\nrandomly shares the structured parameters at both the input and output\nembedding layers of the recurrent neural language models to significantly\nreduce the size of model parameters, but still compactly represent the original\ninput and output embedding layers. The method is easy to implement and tune.\nExperiments on several data sets show that the new method can get similar\nperplexity and BLEU score results while only using a very tiny fraction of\nparameters.", "authors": ["Zhongliang Li", "Raymond Kulhanek", "Shaojun Wang", "Yunxin Zhao", "Shuang Wu"], "category": "cs.CL", "comment": "To appear at AAAI 2018", "img": "/static/thumbs/1711.09873v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1711.09873v2", "num_discussion": 0, "originally_published_time": "11/27/2017", "pid": "1711.09873v2", "published_time": "12/22/2017", "rawpid": "1711.09873", "tags": ["cs.CL"], "title": "Slim Embedding Layers for Recurrent Neural Language Models"}, {"abstract": "The variational encoder-decoder (VED) encodes source information as a set of\nrandom variables using a neural network, which in turn is decoded into target\ndata using another neural network. In natural language processing,\nsequence-to-sequence (Seq2Seq) models typically serve as encoder-decoder\nnetworks. When combined with a traditional (deterministic) attention mechanism,\nthe variational latent space may be bypassed by the attention model, making the\ngenerated sentences less diversified. In our paper, we propose a variational\nattention mechanism for VED, where the attention vector is modeled as normally\ndistributed random variables. Experiments show that variational attention\nincreases diversity while retaining high quality. We also show that the model\nis not sensitive to hyperparameters.", "authors": ["Hareesh Bahuleyan", "Lili Mou", "Olga Vechtomova", "Pascal Poupart"], "category": "cs.CL", "comment": "8 pages, 4 figures, 2 tables", "img": "/static/thumbs/1712.08207v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1712.08207v1", "num_discussion": 0, "originally_published_time": "12/21/2017", "pid": "1712.08207v1", "published_time": "12/21/2017", "rawpid": "1712.08207", "tags": ["cs.CL"], "title": "Variational Attention for Sequence-to-Sequence Models"}, {"abstract": "Generative Adversarial Networks (GANs) have shown great promise recently in\nimage generation. Training GANs for language generation has proven to be more\ndifficult, because of the non-differentiable nature of generating text with\nrecurrent neural networks. Consequently, past work has either resorted to\npre-training with maximum-likelihood or used convolutional networks for\ngeneration. In this work, we show that recurrent neural networks can be trained\nto generate text with GANs from scratch using curriculum learning, by slowly\nteaching the model to generate sequences of increasing and variable length. We\nempirically show that our approach vastly improves the quality of generated\nsequences compared to a convolutional baseline.", "authors": ["Ofir Press", "Amir Bar", "Ben Bogin", "Jonathan Berant", "Lior Wolf"], "category": "cs.CL", "comment": "Presented at the 1st Workshop on Learning to Generate Natural\n  Language at ICML 2017", "img": "/static/thumbs/1706.01399v3.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1706.01399v3", "num_discussion": 0, "originally_published_time": "6/5/2017", "pid": "1706.01399v3", "published_time": "12/21/2017", "rawpid": "1706.01399", "tags": ["cs.CL"], "title": "Language Generation with Recurrent Generative Adversarial Networks\n  without Pre-training"}, {"abstract": "Inspired by recent development of artificial satellite, remote sensing images\nhave attracted extensive attention. Recently, noticeable progress has been made\nin scene classification and target detection.However, it is still not clear how\nto describe the remote sensing image content with accurate and concise\nsentences. In this paper, we investigate to describe the remote sensing images\nwith accurate and flexible sentences. First, some annotated instructions are\npresented to better describe the remote sensing images considering the special\ncharacteristics of remote sensing images. Second, in order to exhaustively\nexploit the contents of remote sensing images, a large-scale aerial image data\nset is constructed for remote sensing image caption. Finally, a comprehensive\nreview is presented on the proposed data set to fully advance the task of\nremote sensing caption. Extensive experiments on the proposed data set\ndemonstrate that the content of the remote sensing image can be completely\ndescribed by generating language descriptions. The data set is available at\nhttps://github.com/201528014227051/RSICD_optimal", "authors": ["Xiaoqiang Lu", "Binqiang Wang", "Xiangtao Zheng", "Xuelong Li"], "category": "cs.CV", "comment": "14 pages, 8 figures", "img": "/static/thumbs/1712.07835v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1712.07835v1", "num_discussion": 0, "originally_published_time": "12/21/2017", "pid": "1712.07835v1", "published_time": "12/21/2017", "rawpid": "1712.07835", "tags": ["cs.CV", "68"], "title": "Exploring Models and Data for Remote Sensing Image Caption Generation"}, {"abstract": "Generating novel pairs of image and text is a problem that combines computer\nvision and natural language processing. In this paper, we present strategies\nfor generating novel image and caption pairs based on existing captioning\ndatasets. The model takes advantage of recent advances in generative\nadversarial networks and sequence-to-sequence modeling. We make generalizations\nto generate paired samples from multiple domains. Furthermore, we study cycles\n-- generating from image to text then back to image and vise versa, as well as\nits connection with autoencoders.", "authors": ["Jason Xie", "Tingwen Bao"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1712.06682v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1712.06682v1", "num_discussion": 0, "originally_published_time": "12/18/2017", "pid": "1712.06682v1", "published_time": "12/18/2017", "rawpid": "1712.06682", "tags": ["cs.CV", "cs.CL", "cs.LG"], "title": "Synthesizing Novel Pairs of Image and Text"}, {"abstract": "In Multimodal Neural Machine Translation (MNMT), a neural model generates a\ntranslated sentence that describes an image, given the image itself and one\nsource descriptions in English. This is considered as the multimodal image\ncaption translation task. The images are processed with Convolutional Neural\nNetwork (CNN) to extract visual features exploitable by the translation model.\nSo far, the CNNs used are pre-trained on object detection and localization\ntask. We hypothesize that richer architecture, such as dense captioning models,\nmay be more suitable for MNMT and could lead to improved translations. We\nextend this intuition to the word-embeddings, where we compute both linguistic\nand visual representation for our corpus vocabulary. We combine and compare\ndifferent confi", "authors": ["Jean-Benoit Delbrouck", "St\u00e9phane Dupont", "Omar Seddati"], "category": "cs.CL", "comment": "Accepted to GLU 2017. arXiv admin note: text overlap with\n  arXiv:1707.00995", "img": "/static/thumbs/1707.01009v5.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1707.01009v5", "num_discussion": 0, "originally_published_time": "7/4/2017", "pid": "1707.01009v5", "published_time": "12/16/2017", "rawpid": "1707.01009", "tags": ["cs.CL"], "title": "Visually Grounded Word Embeddings and Richer Visual Features for\n  Improving Multimodal Neural Machine Translation"}, {"abstract": "We describe Sockeye (version 1.12), an open-source sequence-to-sequence\ntoolkit for Neural Machine Translation (NMT). Sockeye is a production-ready\nframework for training and applying models as well as an experimental platform\nfor researchers. Written in Python and built on MXNet, the toolkit offers\nscalable training and inference for the three most prominent encoder-decoder\narchitectures: attentional recurrent neural networks, self-attentional\ntransformers, and fully convolutional networks. Sockeye also supports a wide\nrange of optimizers, normalization and regularization techniques, and inference\nimprovements from current NMT literature. Users can easily run standard\ntraining recipes, explore different model settings, and incorporate new ideas.\nIn this paper, we highlight Sockeye's features and benchmark it against other\nNMT toolkits on two language arcs from the 2017 Conference on Machine\nTranslation (WMT): English-German and Latvian-English. We report competitive\nBLEU scores across all three architectures, including an overall best score for\nSockeye's transformer implementation. To facilitate further comparison, we\nrelease all system outputs and training scripts used in our experiments. The\nSockeye toolkit is free software released under the Apache 2.0 license.", "authors": ["Felix Hieber", "Tobias Domhan", "Michael Denkowski", "David Vilar", "Artem Sokolov", "Ann Clifton", "Matt Post"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1712.05690v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1712.05690v1", "num_discussion": 0, "originally_published_time": "12/15/2017", "pid": "1712.05690v1", "published_time": "12/15/2017", "rawpid": "1712.05690", "tags": ["cs.CL", "cs.LG", "stat.ML"], "title": "Sockeye: A Toolkit for Neural Machine Translation"}, {"abstract": "Sequence-to-sequence models with soft attention have been successfully\napplied to a wide variety of problems, but their decoding process incurs a\nquadratic time and space cost and is inapplicable to real-time sequence\ntransduction. To address these issues, we propose Monotonic Chunkwise Attention\n(MoChA), which adaptively splits the input sequence into small chunks over\nwhich soft attention is computed. We show that models utilizing MoChA can be\ntrained efficiently with standard backpropagation while allowing online and\nlinear-time decoding at test time. When applied to online speech recognition,\nwe obtain state-of-the-art results and match the performance of a model using\nan offline soft attention mechanism. In document summarization experiments\nwhere we do not expect monotonic alignments, we show significantly improved\nperformance compared to a baseline monotonic attention-based model.", "authors": ["Chung-Cheng Chiu", "Colin Raffel"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1712.05382v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1712.05382v1", "num_discussion": 0, "originally_published_time": "12/14/2017", "pid": "1712.05382v1", "published_time": "12/14/2017", "rawpid": "1712.05382", "tags": ["cs.CL", "stat.ML"], "title": "Monotonic Chunkwise Attention"}, {"abstract": "The paper approaches the problem of image-to-text with attention-based\nencoder-decoder networks that are trained to handle sequences of characters\nrather than words. We experiment on lines of text from a popular handwriting\ndatabase with different attention mechanisms for the decoder. The model trained\nwith softmax attention achieves the lowest test error, outperforming several\nother RNN-based models. Our results show that softmax attention is able to\nlearn a linear alignment whereas the alignment generated by sigmoid attention\nis linear but much less precise.", "authors": ["Jason Poulos", "Rafael Valle"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1712.04046v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1712.04046v1", "num_discussion": 0, "originally_published_time": "12/11/2017", "pid": "1712.04046v1", "published_time": "12/11/2017", "rawpid": "1712.04046", "tags": ["cs.CV", "cs.CL", "stat.ML"], "title": "Attention networks for image-to-text"}, {"abstract": "Automatically generating coherent and semantically meaningful text has many\napplications in machine translation, dialogue systems, image captioning, etc.\nRecently, by combining with policy gradient, Generative Adversarial Nets (GAN)\nthat use a discriminative model to guide the training of the generative model\nas a reinforcement learning policy has shown promising results in text\ngeneration. However, the scalar guiding signal is only available after the\nentire text has been generated and lacks intermediate information about text\nstructure during the generative process. As such, it limits its success when\nthe length of the generated text samples is long (more than 20 words). In this\npaper, we propose a new framework, called LeakGAN, to address the problem for\nlong text generation. We allow the discriminative net to leak its own\nhigh-level extracted features to the generative net to further help the\nguidance. The generator incorporates such informative signals into all\ngeneration steps through an additional Manager module, which takes the\nextracted features of current generated words and outputs a latent vector to\nguide the Worker module for next-word generation. Our extensive experiments on\nsynthetic data and various real-world tasks with Turing test demonstrate that\nLeakGAN is highly effective in long text generation and also improves the\nperformance in short text generation scenarios. More importantly, without any\nsupervision, LeakGAN would be able to implicitly learn sentence structures only\nthrough the interaction between Manager and Worker.", "authors": ["Jiaxian Guo", "Sidi Lu", "Han Cai", "Weinan Zhang", "Yong Yu", "Jun Wang"], "category": "cs.CL", "comment": "14 pages, AAAI 2018", "img": "/static/thumbs/1709.08624v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1709.08624v2", "num_discussion": 0, "originally_published_time": "9/24/2017", "pid": "1709.08624v2", "published_time": "12/8/2017", "rawpid": "1709.08624", "tags": ["cs.CL", "cs.AI", "cs.LG"], "title": "Long Text Generation via Adversarial Training with Leaked Information"}, {"abstract": "Modern neural image captioning systems typically adopt the encoder-decoder\nframework consisting of two principal components: a convolutional neural\nnetwork (CNN) for image feature extraction and a recurrent neural network (RNN)\nfor caption generation. Inspired by the robustness analysis of CNN-based image\nclassifiers to adversarial perturbations, we propose \\textbf{Show-and-Fool}, a\nnovel algorithm for crafting adversarial examples in neural image captioning.\nUnlike image classification tasks with a finite set of class labels, finding\nvisually-similar adversarial examples in an image captioning system is much\nmore challenging since the space of possible captions in a captioning system is\nalmost infinite. In this paper, we design three approaches for crafting\nadversarial examples in image captioning: (i) targeted caption method; (ii)\ntargeted keyword method; and (iii) untargeted method. We formulate the process\nof finding adversarial perturbations as optimization problems and design novel\nloss functions for efficient search. Experimental results on the Show-and-Tell\nmodel and MSCOCO data set show that Show-and-Fool can successfully craft\nvisually-similar adversarial examples with randomly targeted captions, and the\nadversarial examples can be made highly transferable to the\nShow-Attend-and-Tell model. Consequently, the presence of adversarial examples\nleads to new robustness implications of neural image captioning. To the best of\nour knowledge, this is the first work on crafting effective adversarial\nexamples for image captioning tasks.", "authors": ["Hongge Chen", "Huan Zhang", "Pin-Yu Chen", "Jinfeng Yi", "Cho-Jui Hsieh"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1712.02051v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1712.02051v1", "num_discussion": 0, "originally_published_time": "12/6/2017", "pid": "1712.02051v1", "published_time": "12/6/2017", "rawpid": "1712.02051", "tags": ["cs.CV"], "title": "Show-and-Fool: Crafting Adversarial Examples for Neural Image Captioning"}, {"abstract": "The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks in an encoder-decoder configuration. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer, based\nsolely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to be\nsuperior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\nEnglish-to-German translation task, improving over the existing best results,\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\ntranslation task, our model establishes a new single-model state-of-the-art\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\nof the training costs of the best models from the literature. We show that the\nTransformer generalizes well to other tasks by applying it successfully to\nEnglish constituency parsing both with large and limited training data.", "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin"], "category": "cs.CL", "comment": "15 pages, 5 figures", "img": "/static/thumbs/1706.03762v5.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1706.03762v5", "num_discussion": 2, "originally_published_time": "6/12/2017", "pid": "1706.03762v5", "published_time": "12/6/2017", "rawpid": "1706.03762", "tags": ["cs.CL", "cs.LG"], "title": "Attention Is All You Need"}, {"abstract": "We focus on grounding (i.e., localizing or linking) referring expressions in\nimages, e.g., \"largest elephant standing behind baby elephant\". This is a\ngeneral yet challenging vision-language task since it does not only require the\nlocalization of objects, but also the multimodal comprehension of context ---\nvisual attributes (e.g., \"largest\", \"baby\") and relationships (e.g., \"behind\")\nthat help to distinguish the referent from other objects, especially those of\nthe same category. Due to the exponential complexity involved in modeling the\ncontext associated with multiple image regions, existing work oversimplifies\nthis task to pairwise region modeling by multiple instance learning. In this\npaper, we propose a variational Bayesian method, called Variational Context, to\nsolve the problem of complex context modeling in referring expression\ngrounding. Our model exploits the reciprocal relation between the referent and\ncontext, i.e., either of them influences the estimation of the posterior\ndistribution of the other, and thereby the search space of context can be\ngreatly reduced, resulting in better localization of referent. We develop a\nnovel cue-specific language-vision embedding network that learns this\nreciprocity model end-to-end. We also extend the model to the unsupervised\nsetting where no annotation for the referent is available. Extensive\nexperiments on various benchmarks show consistent improvement over\nstate-of-the-art methods in both supervised and unsupervised settings.", "authors": ["Hanwang Zhang", "Yulei Niu", "Shih-Fu Chang"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1712.01892v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1712.01892v1", "num_discussion": 0, "originally_published_time": "12/5/2017", "pid": "1712.01892v1", "published_time": "12/5/2017", "rawpid": "1712.01892", "tags": ["cs.CV"], "title": "Grounding Referring Expressions in Images by Variational Context"}, {"abstract": "Factored neural machine translation (FNMT) is founded on the idea of using\nthe morphological and grammatical decomposition of the words (factors) at the\noutput side of the neural network. This architecture addresses two well-known\nproblems occurring in MT, namely the size of target language vocabulary and the\nnumber of unknown tokens produced in the translation. FNMT system is designed\nto manage larger vocabulary and reduce the training time (for systems with\nequivalent target language vocabulary size). Moreover, we can produce\ngrammatically correct words that are not part of the vocabulary. FNMT model is\nevaluated on IWSLT'15 English to French task and compared to the baseline\nword-based and BPE-based NMT systems. Promising qualitative and quantitative\nresults (in terms of BLEU and METEOR) are reported.", "authors": ["Mercedes Garc\u00eda-Mart\u00ednez", "Lo\u00efc Barrault", "Fethi Bougares"], "category": "cs.CL", "comment": "11 pages, 3 figues, SLSP conference", "img": "/static/thumbs/1712.01821v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1712.01821v1", "num_discussion": 0, "originally_published_time": "12/5/2017", "pid": "1712.01821v1", "published_time": "12/5/2017", "rawpid": "1712.01821", "tags": ["cs.CL"], "title": "Neural Machine Translation by Generating Multiple Linguistic Factors"}, {"abstract": "Sequence-to-sequence models, such as attention-based models in automatic\nspeech recognition (ASR), are typically trained to optimize the cross-entropy\ncriterion which corresponds to improving the log-likelihood of the data.\nHowever, system performance is usually measured in terms of word error rate\n(WER), not log-likelihood. Traditional ASR systems benefit from discriminative\nsequence training which optimizes criteria such as the state-level minimum\nBayes risk (sMBR) which are more closely related to WER. In the present work,\nwe explore techniques to train attention-based models to directly minimize\nexpected word error rate. We consider two loss functions which approximate the\nexpected number of word errors: either by sampling from the model, or by using\nN-best lists of decoded hypotheses, which we find to be more effective than the\nsampling-based method. In experimental evaluations, we find that the proposed\ntraining procedure improves performance by up to 8.2% relative to the baseline\nsystem. This allows us to train grapheme-based, uni-directional attention-based\nmodels which match the performance of a traditional, state-of-the-art,\ndiscriminative sequence-trained system on a mobile voice-search task.", "authors": ["Rohit Prabhavalkar", "Tara N. Sainath", "Yonghui Wu", "Patrick Nguyen", "Zhifeng Chen", "Chung-Cheng Chiu", "Anjuli Kannan"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1712.01818v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1712.01818v1", "num_discussion": 0, "originally_published_time": "12/5/2017", "pid": "1712.01818v1", "published_time": "12/5/2017", "rawpid": "1712.01818", "tags": ["cs.CL", "eess.AS", "stat.ML"], "title": "Minimum Word Error Rate Training for Attention-based\n  Sequence-to-Sequence Models"}, {"abstract": "Generative adversarial networks (GANs) have great successes on synthesizing\ndata. However, the existing GANs restrict the discriminator to be a binary\nclassifier, and thus limit their learning capacity for tasks that need to\nsynthesize output with rich structures such as natural language descriptions.\nIn this paper, we propose a novel generative adversarial network, RankGAN, for\ngenerating high-quality language descriptions. Rather than training the\ndiscriminator to learn and assign absolute binary predicate for individual data\nsample, the proposed RankGAN is able to analyze and rank a collection of\nhuman-written and machine-written sentences by giving a reference group. By\nviewing a set of data samples collectively and evaluating their quality through\nrelative ranking scores, the discriminator is able to make better assessment\nwhich in turn helps to learn a better generator. The proposed RankGAN is\noptimized through the policy gradient technique. Experimental results on\nmultiple public datasets clearly demonstrate the effectiveness of the proposed\napproach.", "authors": ["Kevin Lin", "Dianqi Li", "Xiaodong He", "Zhengyou Zhang", "Ming-Ting Sun"], "category": "cs.CL", "comment": "NIPS2017", "img": "/static/thumbs/1705.11001v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1705.11001v2", "num_discussion": 0, "originally_published_time": "5/31/2017", "pid": "1705.11001v2", "published_time": "12/3/2017", "rawpid": "1705.11001", "tags": ["cs.CL", "cs.LG"], "title": "Adversarial Ranking for Language Generation"}, {"abstract": "We study the problem of segmenting moving objects in unconstrained videos.\nGiven a video, the task is to segment all the objects that exhibit independent\nmotion in at least one frame. We formulate this as a learning problem and\ndesign our framework with three cues: (i) independent object motion between a\npair of frames, which complements object recognition, (ii) object appearance,\nwhich helps to correct errors in motion estimation, and (iii) temporal\nconsistency, which imposes additional constraints on the segmentation. The\nframework is a two-stream neural network with an explicit memory module. The\ntwo streams encode appearance and motion cues in a video sequence respectively,\nwhile the memory module captures the evolution of objects over time, exploiting\nthe temporal consistency. The motion stream is a convolutional neural network\ntrained on synthetic videos to segment independently moving objects in the\noptical flow field. The module to build a 'visual memory' in video, i.e., a\njoint representation of all the video frames, is realized with a convolutional\nrecurrent unit learned from a small number of training video sequences.\n  For every pixel in a frame of a test video, our approach assigns an object or\nbackground label based on the learned spatio-temporal features as well as the\n'visual memory' specific to the video. We evaluate our method extensively on\nthree benchmarks, DAVIS, Freiburg-Berkeley motion segmentation dataset and\nSegTrack. In addition, we provide an extensive ablation study to investigate\nboth the choice of the training data and the influence of each component in the\nproposed framework.", "authors": ["Pavel Tokmakov", "Cordelia Schmid", "Karteek Alahari"], "category": "cs.CV", "comment": "arXiv admin note: text overlap with arXiv:1704.05737,\n  arXiv:1612.07217", "img": "/static/thumbs/1712.01127v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1712.01127v1", "num_discussion": 0, "originally_published_time": "12/1/2017", "pid": "1712.01127v1", "published_time": "12/1/2017", "rawpid": "1712.01127", "tags": ["cs.CV"], "title": "Learning to Segment Moving Objects"}, {"abstract": "In this paper, we propose a model using generative adversarial net (GAN) to\ngenerate realistic text. Instead of using standard GAN, we combine variational\nautoencoder (VAE) with generative adversarial net. The use of high-level latent\nrandom variables is helpful to learn the data distribution and solve the\nproblem that generative adversarial net always emits the similar data. We\npropose the VGAN model where the generative model is composed of recurrent\nneural network and VAE. The discriminative model is a convolutional neural\nnetwork. We train the model via policy gradient. We apply the proposed model to\nthe task of text generation and compare it to other recent neural network based\nmodels, such as recurrent neural network language model and SeqGAN. We evaluate\nthe performance of the model by calculating negative log-likelihood and the\nBLEU score. We conduct experiments on three benchmark datasets, and results\nshow that our model outperforms other previous models.", "authors": ["Heng Wang", "Zengchang Qin", "Tao Wan"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1712.00170v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1712.00170v1", "num_discussion": 0, "originally_published_time": "12/1/2017", "pid": "1712.00170v1", "published_time": "12/1/2017", "rawpid": "1712.00170", "tags": ["cs.CL"], "title": "Text Generation Based on Generative Adversarial Nets with Latent\n  Variable"}, {"abstract": "Generative modeling of high-dimensional data is a key problem in machine\nlearning. Successful approaches include latent variable models and\nautoregressive models. The complementary strengths of these approaches, to\nmodel global and local image statistics respectively, suggest hybrid models\ncombining the strengths of both models. Our contribution is to train such\nhybrid models using an auxiliary loss function that controls which information\nis captured by the latent variables and what is left to the autoregressive\ndecoder. In contrast, prior work on such hybrid models needed to limit the\ncapacity of the autoregressive decoder to prevent degenerate models that ignore\nthe latent variables and only rely on autoregressive modeling. Our approach\nresults in models with meaningful latent variable representations, and which\nrely on powerful autoregressive decoders to model image details. Our model\ngenerates qualitatively convincing samples, and yields state-of-the-art\nquantitative results.", "authors": ["Thomas Lucas", "Jakob Verbeek"], "category": "cs.CV", "comment": "Under review as a conference paper at ICLR 2018", "img": "/static/thumbs/1711.11479v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1711.11479v1", "num_discussion": 0, "originally_published_time": "11/30/2017", "pid": "1711.11479v1", "published_time": "11/30/2017", "rawpid": "1711.11479", "tags": ["cs.CV"], "title": "Auxiliary Guided Autoregressive Variational Autoencoders"}, {"abstract": "In this paper, we propose an Attentional Generative Adversarial Network\n(AttnGAN) that allows attention-driven, multi-stage refinement for fine-grained\ntext-to-image generation. With a novel attentional generative network, the\nAttnGAN can synthesize fine-grained details at different subregions of the\nimage by paying attentions to the relevant words in the natural language\ndescription. In addition, a deep attentional multimodal similarity model is\nproposed to compute a fine-grained image-text matching loss for training the\ngenerator. The proposed AttnGAN significantly outperforms the previous state of\nthe art, boosting the best reported inception score by 14.14% on the CUB\ndataset and 170.25% on the more challenging COCO dataset. A detailed analysis\nis also performed by visualizing the attention layers of the AttnGAN. It for\nthe first time shows that the layered attentional GAN is able to automatically\nselect the condition at the word level for generating different parts of the\nimage.", "authors": ["Tao Xu", "Pengchuan Zhang", "Qiuyuan Huang", "Han Zhang", "Zhe Gan", "Xiaolei Huang", "Xiaodong He"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1711.10485v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1711.10485v1", "num_discussion": 0, "originally_published_time": "11/28/2017", "pid": "1711.10485v1", "published_time": "11/28/2017", "rawpid": "1711.10485", "tags": ["cs.CV"], "title": "AttnGAN: Fine-Grained Text to Image Generation with Attentional\n  Generative Adversarial Networks"}, {"abstract": "We investigate the integration of a planning mechanism into\nsequence-to-sequence models using attention. We develop a model which can plan\nahead in the future when it computes its alignments between input and output\nsequences, constructing a matrix of proposed future alignments and a commitment\nvector that governs whether to follow or recompute the plan. This mechanism is\ninspired by the recently proposed strategic attentive reader and writer (STRAW)\nmodel for Reinforcement Learning. Our proposed model is end-to-end trainable\nusing primarily differentiable operations. We show that it outperforms a strong\nbaseline on character-level translation tasks from WMT'15, the algorithmic task\nof finding Eulerian circuits of graphs, and question generation from the text.\nOur analysis demonstrates that the model computes qualitatively intuitive\nalignments, converges faster than the baselines, and achieves superior\nperformance with fewer parameters.", "authors": ["Francis Dutil", "Caglar Gulcehre", "Adam Trischler", "Yoshua Bengio"], "category": "cs.LG", "comment": "NIPS 2017", "img": "/static/thumbs/1711.10462v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1711.10462v1", "num_discussion": 0, "originally_published_time": "11/28/2017", "pid": "1711.10462v1", "published_time": "11/28/2017", "rawpid": "1711.10462", "tags": ["cs.LG", "stat.ML"], "title": "Plan, Attend, Generate: Planning for Sequence-to-Sequence Models"}, {"abstract": "Generating natural language descriptions of images is an important capability\nfor a robot or other visual-intelligence driven AI agent that may need to\ncommunicate with human users about what it is seeing. Such image captioning\nmethods are typically trained by maximising the likelihood of ground-truth\nannotated caption given the image. While simple and easy to implement, this\napproach does not directly maximise the language quality metrics we care about\nsuch as CIDEr. In this paper we investigate training image captioning methods\nbased on actor-critic reinforcement learning in order to directly optimise\nnon-differentiable quality metrics of interest. By formulating a per-token\nadvantage and value computation strategy in this novel reinforcement learning\nbased captioning model, we show that it is possible to achieve the state of the\nart performance on the widely used MSCOCO benchmark.", "authors": ["Li Zhang", "Flood Sung", "Feng Liu", "Tao Xiang", "Shaogang Gong", "Yongxin Yang", "Timothy M. Hospedales"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1706.09601v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1706.09601v2", "num_discussion": 0, "originally_published_time": "6/29/2017", "pid": "1706.09601v2", "published_time": "11/28/2017", "rawpid": "1706.09601", "tags": ["cs.CV"], "title": "Actor-Critic Sequence Training for Image Captioning"}, {"abstract": "Deep learning methods have recently achieved great empirical success on\nmachine translation, dialogue response generation, summarization, and other\ntext generation tasks. At a high level, the technique has been to train\nend-to-end neural network models consisting of an encoder model to produce a\nhidden representation of the source text, followed by a decoder model to\ngenerate the target. While such models have significantly fewer pieces than\nearlier systems, significant tuning is still required to achieve good\nperformance. For text generation models in particular, the decoder can behave\nin undesired ways, such as by generating truncated or repetitive outputs,\noutputting bland and generic responses, or in some cases producing\nungrammatical gibberish. This paper is intended as a practical guide for\nresolving such undesired behavior in text generation models, with the aim of\nhelping enable real-world applications.", "authors": ["Ziang Xie"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1711.09534v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1711.09534v1", "num_discussion": 0, "originally_published_time": "11/27/2017", "pid": "1711.09534v1", "published_time": "11/27/2017", "rawpid": "1711.09534", "tags": ["cs.CL", "cs.LG", "stat.ML"], "title": "Neural Text Generation: A Practical Guide"}, {"abstract": "Image captioning is an important but challenging task, applicable to virtual\nassistants, editing tools, image indexing, and support of the disabled. Its\nchallenges are due to the variability and ambiguity of possible image\ndescriptions. In recent years significant progress has been made in image\ncaptioning, using Recurrent Neural Networks powered by long-short-term-memory\n(LSTM) units. Despite mitigating the vanishing gradient problem, and despite\ntheir compelling ability to memorize dependencies, LSTM units are complex and\ninherently sequential across time. To address this issue, recent work has shown\nbenefits of convolutional networks for machine translation and conditional\nimage generation. Inspired by their success, in this paper, we develop a\nconvolutional image captioning technique. We demonstrate its efficacy on the\nchallenging MSCOCO dataset and demonstrate performance on par with the\nbaseline, while having a faster training time per number of parameters. We also\nperform a detailed analysis, providing compelling reasons in favor of\nconvolutional language generation approaches.", "authors": ["Jyoti Aneja", "Aditya Deshpande", "Alexander Schwing"], "category": "cs.CV", "comment": "11 pages, 9 Figures", "img": "/static/thumbs/1711.09151v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1711.09151v1", "num_discussion": 0, "originally_published_time": "11/24/2017", "pid": "1711.09151v1", "published_time": "11/24/2017", "rawpid": "1711.09151", "tags": ["cs.CV"], "title": "Convolutional Image Captioning"}, {"abstract": "Linguistic sequence labeling is a general modeling approach that encompasses\na variety of problems, such as part-of-speech tagging and named entity\nrecognition. Recent advances in neural networks (NNs) make it possible to build\nreliable models without handcrafted features. However, in many cases, it is\nhard to obtain sufficient annotations to train these models. In this study, we\ndevelop a novel neural framework to extract abundant knowledge hidden in raw\ntexts to empower the sequence labeling task. Besides word-level knowledge\ncontained in pre-trained word embeddings, character-aware neural language\nmodels are incorporated to extract character-level knowledge. Transfer learning\ntechniques are further adopted to mediate different components and guide the\nlanguage model towards the key knowledge. Comparing to previous methods, these\ntask-specific knowledge allows us to adopt a more concise model and conduct\nmore efficient training. Different from most transfer learning methods, the\nproposed framework does not rely on any additional supervision. It extracts\nknowledge from self-contained order information of training sequences.\nExtensive experiments on benchmark datasets demonstrate the effectiveness of\nleveraging character-level knowledge and the efficiency of co-training. For\nexample, on the CoNLL03 NER task, model training completes in about 6 hours on\na single GPU, reaching F1 score of 91.71$\\pm$0.10 without using any extra\nannotation.", "authors": ["Liyuan Liu", "Jingbo Shang", "Frank F. Xu", "Xiang Ren", "Huan Gui", "Jian Peng", "Jiawei Han"], "category": "cs.CL", "comment": "AAAI 2018", "img": "/static/thumbs/1709.04109v4.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1709.04109v4", "num_discussion": 0, "originally_published_time": "9/13/2017", "pid": "1709.04109v4", "published_time": "11/23/2017", "rawpid": "1709.04109", "tags": ["cs.CL", "cs.LG"], "title": "Empower Sequence Labeling with Task-Aware Neural Language Model"}, {"abstract": "This paper presents an approach for grounding phrases in images which jointly\nlearns multiple text-conditioned embeddings in a single end-to-end model. In\norder to differentiate text phrases into semantically distinct subspaces, we\npropose a concept weight branch that automatically assigns phrases to\nembeddings, whereas prior works predefine such assignments. Our proposed\nsolution simplifies the representation requirements for individual embeddings\nand allows the underrepresented concepts to take advantage of the shared\nrepresentations before feeding them into concept-specific layers. Comprehensive\nexperiments verify the effectiveness of our approach across three phrase\ngrounding datasets, Flickr30K Entities, ReferIt Game, and Visual Genome, where\nwe obtain a (resp.) 3.5%, 2%, and 3.5% improvement in grounding performance\nover a strong region-phrase embedding baseline.", "authors": ["Bryan A. Plummer", "Paige Kordas", "M. Hadi Kiapour", "Shuai Zheng", "Robinson Piramuthu", "Svetlana Lazebnik"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1711.08389v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1711.08389v1", "num_discussion": 0, "originally_published_time": "11/22/2017", "pid": "1711.08389v1", "published_time": "11/22/2017", "rawpid": "1711.08389", "tags": ["cs.CV"], "title": "Conditional Image-Text Embedding Networks"}, {"abstract": "Deep learning achieves remarkable generalization capability with overwhelming\nnumber of model parameters. Theoretical understanding of deep learning\ngeneralization receives recent attention yet remains not fully explored. This\npaper attempts to provide an alternative understanding from the perspective of\nmaximum entropy. We first derive two feature conditions that softmax regression\nstrictly apply maximum entropy principle. DNN is then regarded as approximating\nthe feature conditions with multilayer feature learning, and proved to be a\nrecursive solution towards maximum entropy principle. The connection between\nDNN and maximum entropy well explains why typical designs such as shortcut and\nregularization improves model generalization, and provides instructions for\nfuture model development.", "authors": ["Guanhua Zheng", "Jitao Sang", "Changsheng Xu"], "category": "cs.LG", "comment": "13 pages,2 figures", "img": "/static/thumbs/1711.07758v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1711.07758v1", "num_discussion": 0, "originally_published_time": "11/21/2017", "pid": "1711.07758v1", "published_time": "11/21/2017", "rawpid": "1711.07758", "tags": ["cs.LG"], "title": "Understanding Deep Learning Generalization by Maximum Entropy"}, {"abstract": "Ongoing innovations in recurrent neural network architectures have provided a\nsteady influx of apparently state-of-the-art results on language modelling\nbenchmarks. However, these have been evaluated using differing code bases and\nlimited computational resources, which represent uncontrolled sources of\nexperimental variation. We reevaluate several popular architectures and\nregularisation methods with large-scale automatic black-box hyperparameter\ntuning and arrive at the somewhat surprising conclusion that standard LSTM\narchitectures, when properly regularised, outperform more recent models. We\nestablish a new state of the art on the Penn Treebank and Wikitext-2 corpora,\nas well as strong baselines on the Hutter Prize dataset.", "authors": ["G\u00e1bor Melis", "Chris Dyer", "Phil Blunsom"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1707.05589v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1707.05589v2", "num_discussion": 2, "originally_published_time": "7/18/2017", "pid": "1707.05589v2", "published_time": "11/20/2017", "rawpid": "1707.05589", "tags": ["cs.CL"], "title": "On the State of the Art of Evaluation in Neural Language Models"}, {"abstract": "This paper explores image caption generation using conditional variational\nauto-encoders (CVAEs). Standard CVAEs with a fixed Gaussian prior yield\ndescriptions with too little variability. Instead, we propose two models that\nexplicitly structure the latent space around $K$ components corresponding to\ndifferent types of image content, and combine components to create priors for\nimages that contain multiple types of content simultaneously (e.g., several\nkinds of objects). Our first model uses a Gaussian Mixture model (GMM) prior,\nwhile the second one defines a novel Additive Gaussian (AG) prior that linearly\ncombines component means. We show that both models produce captions that are\nmore diverse and more accurate than a strong LSTM baseline or a \"vanilla\" CVAE\nwith a fixed Gaussian prior, with AG-CVAE showing particular promise.", "authors": ["Liwei Wang", "Alexander G. Schwing", "Svetlana Lazebnik"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1711.07068v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1711.07068v1", "num_discussion": 0, "originally_published_time": "11/19/2017", "pid": "1711.07068v1", "published_time": "11/19/2017", "rawpid": "1711.07068", "tags": ["cs.CV"], "title": "Diverse and Accurate Image Description Using a Variational Auto-Encoder\n  with an Additive Gaussian Encoding Space"}, {"abstract": "The Generative Adversarial Network (GAN) has achieved great success in\ngenerating realistic (real-valued) synthetic data. However, convergence issues\nand difficulties dealing with discrete data hinder the applicability of GAN to\ntext. We propose a framework for generating realistic text via adversarial\ntraining. We employ a long short-term memory network as generator, and a\nconvolutional network as discriminator. Instead of using the standard objective\nof GAN, we propose matching the high-dimensional latent feature distributions\nof real and synthetic sentences, via a kernelized discrepancy metric. This\neases adversarial training by alleviating the mode-collapsing problem. Our\nexperiments show superior performance in quantitative evaluation, and\ndemonstrate that our model can generate realistic-looking sentences.", "authors": ["Yizhe Zhang", "Zhe Gan", "Kai Fan", "Zhi Chen", "Ricardo Henao", "Dinghan Shen", "Lawrence Carin"], "category": "stat.ML", "comment": "Accepted by ICML 2017", "img": "/static/thumbs/1706.03850v3.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1706.03850v3", "num_discussion": 0, "originally_published_time": "6/12/2017", "pid": "1706.03850v3", "published_time": "11/18/2017", "rawpid": "1706.03850", "tags": ["stat.ML", "cs.CL", "cs.LG"], "title": "Adversarial Feature Matching for Text Generation"}, {"abstract": "We investigate the problem of Language-Based Image Editing (LBIE) in this\nwork. Given a source image and a natural language description, we want to\ngenerate a target image by editing the source im- age based on the description.\nWe propose a generic modeling framework for two sub-tasks of LBIE:\nlanguage-based image segmentation and image colorization. The framework uses\nrecurrent attentive models to fuse image and language features. Instead of\nusing a fixed step size, we introduce for each re- gion of the image a\ntermination gate to dynamically determine in each inference step whether to\ncontinue extrapolating additional information from the textual description. The\neffectiveness of the framework has been validated on three datasets. First, we\nintroduce a synthetic dataset, called CoSaL, to evaluate the end-to-end\nperformance of our LBIE system. Second, we show that the framework leads to\nstate-of-the- art performance on image segmentation on the ReferIt dataset.\nThird, we present the first language-based colorization result on the\nOxford-102 Flowers dataset, laying the foundation for future research.", "authors": ["Jianbo Chen", "Yelong Shen", "Jianfeng Gao", "Jingjing Liu", "Xiaodong Liu"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1711.06288v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1711.06288v1", "num_discussion": 0, "originally_published_time": "11/16/2017", "pid": "1711.06288v1", "published_time": "11/16/2017", "rawpid": "1711.06288", "tags": ["cs.CV", "cs.CL", "cs.LG"], "title": "Language-Based Image Editing with Recurrent Attentive Models"}, {"abstract": "Recently it has been shown that policy-gradient methods for reinforcement\nlearning can be utilized to train deep end-to-end systems directly on\nnon-differentiable metrics for the task at hand. In this paper we consider the\nproblem of optimizing image captioning systems using reinforcement learning,\nand show that by carefully optimizing our systems using the test metrics of the\nMSCOCO task, significant gains in performance can be realized. Our systems are\nbuilt using a new optimization approach that we call self-critical sequence\ntraining (SCST). SCST is a form of the popular REINFORCE algorithm that, rather\nthan estimating a \"baseline\" to normalize the rewards and reduce variance,\nutilizes the output of its own test-time inference algorithm to normalize the\nrewards it experiences. Using this approach, estimating the reward signal (as\nactor-critic methods must do) and estimating normalization (as REINFORCE\nalgorithms typically do) is avoided, while at the same time harmonizing the\nmodel with respect to its test-time inference procedure. Empirically we find\nthat directly optimizing the CIDEr metric with SCST and greedy decoding at\ntest-time is highly effective. Our results on the MSCOCO evaluation sever\nestablish a new state-of-the-art on the task, improving the best result in\nterms of CIDEr from 104.9 to 114.7.", "authors": ["Steven J. Rennie", "Etienne Marcheret", "Youssef Mroueh", "Jarret Ross", "Vaibhava Goel"], "category": "cs.LG", "comment": "CVPR 2017 + additional analysis + fixed baseline results, 16 pages", "img": "/static/thumbs/1612.00563v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1612.00563v2", "num_discussion": 0, "originally_published_time": "12/2/2016", "pid": "1612.00563v2", "published_time": "11/16/2017", "rawpid": "1612.00563", "tags": ["cs.LG", "cs.AI", "cs.CV"], "title": "Self-critical Sequence Training for Image Captioning"}, {"abstract": "Recurrent neural networks like long short-term memory (LSTM) are important\narchitectures for sequential prediction tasks. LSTMs (and RNNs in general)\nmodel sequences along the forward time direction. Bidirectional LSTMs\n(Bi-LSTMs) on the other hand model sequences along both forward and backward\ndirections and are generally known to perform better at such tasks because they\ncapture a richer representation of the data. In the training of Bi-LSTMs, the\nforward and backward paths are learned independently. We propose a variant of\nthe Bi-LSTM architecture, which we call Variational Bi-LSTM, that creates a\nchannel between the two paths (during training, but which may be omitted during\ninference); thus optimizing the two paths jointly. We arrive at this joint\nobjective for our model by minimizing a variational lower bound of the joint\nlikelihood of the data sequence. Our model acts as a regularizer and encourages\nthe two networks to inform each other in making their respective predictions\nusing distinct information. We perform ablation studies to better understand\nthe different components of our model and evaluate the method on various\nbenchmarks, showing state-of-the-art performance.", "authors": ["Samira Shabanian", "Devansh Arpit", "Adam Trischler", "Yoshua Bengio"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1711.05717v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1711.05717v1", "num_discussion": 0, "originally_published_time": "11/15/2017", "pid": "1711.05717v1", "published_time": "11/15/2017", "rawpid": "1711.05717", "tags": ["stat.ML", "cs.LG"], "title": "Variational Bi-LSTMs"}, {"abstract": "Automatic generation of caption to describe the content of an image has been\ngaining a lot of research interests recently, where most of the existing works\ntreat the image caption as pure sequential data. Natural language, however\npossess a temporal hierarchy structure, with complex dependencies between each\nsubsequence. In this paper, we propose a phrase-based hierarchical Long\nShort-Term Memory (phi-LSTM) model to generate image description. In contrast\nto the conventional solutions that generate caption in a pure sequential\nmanner, our proposed model decodes image caption from phrase to sentence. It\nconsists of a phrase decoder at the bottom hierarchy to decode noun phrases of\nvariable length, and an abbreviated sentence decoder at the upper hierarchy to\ndecode an abbreviated form of the image description. A complete image caption\nis formed by combining the generated phrases with sentence during the inference\nstage. Empirically, our proposed model shows a better or competitive result on\nthe Flickr8k, Flickr30k and MS-COCO datasets in comparison to the state-of-the\nart models. We also show that our proposed model is able to generate more novel\ncaptions (not seen in the training data) which are richer in word contents in\nall these three datasets.", "authors": ["Ying Hua Tan", "Chee Seng Chan"], "category": "cs.CV", "comment": "17 pages, 12 figures, ACCV2016 extension, phrase-based image\n  captioning", "img": "/static/thumbs/1711.05557v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1711.05557v1", "num_discussion": 0, "originally_published_time": "11/11/2017", "pid": "1711.05557v1", "published_time": "11/11/2017", "rawpid": "1711.05557", "tags": ["cs.CV", "cs.AI", "cs.CL"], "title": "Phrase-based Image Captioning with Hierarchical LSTM Model"}, {"abstract": "The interpretation of spatial references is highly contextual, requiring\njoint inference over both language and the environment. We consider the task of\nspatial reasoning in a simulated environment, where an agent can act and\nreceive rewards. The proposed model learns a representation of the world\nsteered by instruction text. This design allows for precise alignment of local\nneighborhoods with corresponding verbalizations, while also handling global\nreferences in the instructions. We train our model with reinforcement learning\nusing a variant of generalized value iteration. The model outperforms\nstate-of-the-art approaches on several metrics, yielding a 45% reduction in\ngoal localization error.", "authors": ["Michael Janner", "Karthik Narasimhan", "Regina Barzilay"], "category": "cs.CL", "comment": "Accepted to TACL 2017, code:\n  https://github.com/jannerm/spatial-reasoning", "img": "/static/thumbs/1707.03938v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1707.03938v2", "num_discussion": 0, "originally_published_time": "7/13/2017", "pid": "1707.03938v2", "published_time": "11/11/2017", "rawpid": "1707.03938", "tags": ["cs.CL", "cs.AI", "cs.LG"], "title": "Representation Learning for Grounded Spatial Reasoning"}, {"abstract": "A significant amount of the world's knowledge is stored in relational\ndatabases. However, the ability for users to retrieve facts from a database is\nlimited due to a lack of understanding of query languages such as SQL. We\npropose Seq2SQL, a deep neural network for translating natural language\nquestions to corresponding SQL queries. Our model leverages the structure of\nSQL queries to significantly reduce the output space of generated queries.\nMoreover, we use rewards from in-the-loop query execution over the database to\nlearn a policy to generate unordered parts of the query, which we show are less\nsuitable for optimization via cross entropy loss. In addition, we will publish\nWikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL\nqueries distributed across 24241 tables from Wikipedia. This dataset is\nrequired to train our model and is an order of magnitude larger than comparable\ndatasets. By applying policy-based reinforcement learning with a query\nexecution environment to WikiSQL, our model Seq2SQL outperforms attentional\nsequence to sequence models, improving execution accuracy from 35.9% to 59.4%\nand logical form accuracy from 23.4% to 48.3%.", "authors": ["Victor Zhong", "Caiming Xiong", "Richard Socher"], "category": "cs.CL", "comment": "12 pages, 5 figures", "img": "/static/thumbs/1709.00103v7.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1709.00103v7", "num_discussion": 0, "originally_published_time": "8/31/2017", "pid": "1709.00103v7", "published_time": "11/9/2017", "rawpid": "1709.00103", "tags": ["cs.CL", "cs.AI"], "title": "Seq2SQL: Generating Structured Queries from Natural Language using\n  Reinforcement Learning"}, {"abstract": "Long short-term memory (LSTM) recurrent neural networks are renowned for\nbeing uninterpretable \"black boxes\". In the medical domain where LSTMs have\nshown promise, this is specifically concerning because it is imperative to\nunderstand the decisions made by machine learning models in such acute\nsituations. This study employs techniques used in the convolutional neural\nnetwork domain to elucidate the inputs that are important when LSTMs classify\nelectrocardiogram signals. Of the various techniques available to determine\ninput feature saliency, it was found that learning an occlusion mask is the\nmost effective.", "authors": ["Jos van der Westhuizen", "Joan Lasenby"], "category": "stat.ML", "comment": "4 pages, 4 figures", "img": "/static/thumbs/1705.08153v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1705.08153v2", "num_discussion": 0, "originally_published_time": "5/23/2017", "pid": "1705.08153v2", "published_time": "11/8/2017", "rawpid": "1705.08153", "tags": ["stat.ML", "cs.LG"], "title": "What does an LSTM look for in classifying heartbeats?"}, {"abstract": "This paper proposes an approach for applying GANs to NMT. We build a\nconditional sequence generative adversarial net which comprises of two\nadversarial sub models, a generator and a discriminator. The generator aims to\ngenerate sentences which are hard to be discriminated from human-translated\nsentences ( i.e., the golden target sentences); And the discriminator makes\nefforts to discriminate the machine-generated sentences from human-translated\nones. The two sub models play a mini-max game and achieve the win-win situation\nwhen they reach a Nash Equilibrium. Additionally, the static sentence-level\nBLEU is utilized as the reinforced objective for the generator, which biases\nthe generation towards high BLEU points. During training, both the dynamic\ndiscriminator and the static BLEU objective are employed to evaluate the\ngenerated sentences and feedback the evaluations to guide the learning of the\ngenerator. Experimental results show that the proposed model consistently\noutperforms the traditional RNNSearch and the newly emerged state-of-the-art\nTransformer on English-German and Chinese-English translation tasks.", "authors": ["Zhen Yang", "Wei Chen", "Feng Wang", "Bo Xu"], "category": "cs.CL", "comment": "More strong baselines and experiments, update the equation and\n  pictures", "img": "/static/thumbs/1703.04887v3.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1703.04887v3", "num_discussion": 0, "originally_published_time": "3/15/2017", "pid": "1703.04887v3", "published_time": "11/8/2017", "rawpid": "1703.04887", "tags": ["cs.CL"], "title": "Improving Neural Machine Translation with Conditional Sequence\n  Generative Adversarial Nets"}, {"abstract": "While strong progress has been made in image captioning over the last years,\nmachine and human captions are still quite distinct. A closer look reveals that\nthis is due to the deficiencies in the generated word distribution, vocabulary\nsize, and strong bias in the generators towards frequent captions. Furthermore,\nhumans -- rightfully so -- generate multiple, diverse captions, due to the\ninherent ambiguity in the captioning task which is not considered in today's\nsystems.\n  To address these challenges, we change the training objective of the caption\ngenerator from reproducing groundtruth captions to generating a set of captions\nthat is indistinguishable from human generated captions. Instead of\nhandcrafting such a learning target, we employ adversarial training in\ncombination with an approximate Gumbel sampler to implicitly match the\ngenerated distribution to the human one. While our method achieves comparable\nperformance to the state-of-the-art in terms of the correctness of the\ncaptions, we generate a set of diverse captions, that are significantly less\nbiased and match the word statistics better in several aspects.", "authors": ["Rakshith Shetty", "Marcus Rohrbach", "Lisa Anne Hendricks", "Mario Fritz", "Bernt Schiele"], "category": "cs.CV", "comment": "16 pages, Published in ICCV 2017", "img": "/static/thumbs/1703.10476v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1703.10476v2", "num_discussion": 0, "originally_published_time": "3/30/2017", "pid": "1703.10476v2", "published_time": "11/6/2017", "rawpid": "1703.10476", "tags": ["cs.CV", "cs.AI", "cs.CL"], "title": "Speaking the Same Language: Matching Machine to Human Captions by\n  Adversarial Training"}, {"abstract": "Image captioning has been recently gaining a lot of attention thanks to the\nimpressive achievements shown by deep captioning architectures, which combine\nConvolutional Neural Networks to extract image representations, and Recurrent\nNeural Networks to generate the corresponding captions. At the same time, a\nsignificant research effort has been dedicated to the development of saliency\nprediction models, which can predict human eye fixations. Even though saliency\ninformation could be useful to condition an image captioning architecture, by\nproviding an indication of what is salient and what is not, research is still\nstruggling to incorporate these two techniques. In this work, we propose an\nimage captioning approach in which a generative recurrent neural network can\nfocus on different parts of the input image during the generation of the\ncaption, by exploiting the conditioning given by a saliency prediction model on\nwhich parts of the image are salient and which are contextual. We show, through\nextensive quantitative and qualitative experiments on large scale datasets,\nthat our model achieves superior performances with respect to captioning\nbaselines with and without saliency, and to different state of the art\napproaches combining saliency and captioning.", "authors": ["Marcella Cornia", "Lorenzo Baraldi", "Giuseppe Serra", "Rita Cucchiara"], "category": "cs.CV", "comment": "Submitted to ACM Transactions on Multimedia Computing, Communications\n  and Applications", "img": "/static/thumbs/1706.08474v3.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1706.08474v3", "num_discussion": 0, "originally_published_time": "6/26/2017", "pid": "1706.08474v3", "published_time": "11/4/2017", "rawpid": "1706.08474", "tags": ["cs.CV"], "title": "Paying More Attention to Saliency: Image Captioning with Saliency and\n  Context Attention"}, {"abstract": "There has recently been significant interest in hard attention models for\ntasks such as object recognition, visual captioning and speech recognition.\nHard attention can offer benefits over soft attention such as decreased\ncomputational cost, but training hard attention models can be difficult because\nof the discrete latent variables they introduce. Previous work used REINFORCE\nand Q-learning to approach these issues, but those methods can provide\nhigh-variance gradient estimates and be slow to train. In this paper, we tackle\nthe problem of learning hard attention for a sequential task using variational\ninference methods, specifically the recently introduced VIMCO and NVIL.\nFurthermore, we propose a novel baseline that adapts VIMCO to this setting. We\ndemonstrate our method on a phoneme recognition task in clean and noisy\nenvironments and show that our method outperforms REINFORCE, with the\ndifference being greater for a more complicated task.", "authors": ["Dieterich Lawson", "Chung-Cheng Chiu", "George Tucker", "Colin Raffel", "Kevin Swersky", "Navdeep Jaitly"], "category": "cs.AI", "comment": "", "img": "/static/thumbs/1705.05524v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1705.05524v2", "num_discussion": 0, "originally_published_time": "5/16/2017", "pid": "1705.05524v2", "published_time": "11/1/2017", "rawpid": "1705.05524", "tags": ["cs.AI", "cs.LG", "stat.ML"], "title": "Learning Hard Alignments with Variational Inference"}, {"abstract": "Machine translation has recently achieved impressive performance thanks to\nrecent advances in deep learning and the availability of large-scale parallel\ncorpora. There have been numerous attempts to extend these successes to\nlow-resource language pairs, yet requiring tens of thousands of parallel\nsentences. In this work, we take this research direction to the extreme and\ninvestigate whether it is possible to learn to translate even without any\nparallel data. We propose a model that takes sentences from monolingual corpora\nin two different languages and maps them into the same latent space. By\nlearning to reconstruct in both languages from this shared feature space, the\nmodel effectively learns to translate without using any labeled data. We\ndemonstrate our model on two widely used datasets and two language pairs,\nreporting BLEU scores up to 32.8, without using even a single parallel sentence\nat training time.", "authors": ["Guillaume Lample", "Ludovic Denoyer", "Marc'Aurelio Ranzato"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1711.00043v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1711.00043v1", "num_discussion": 0, "originally_published_time": "10/31/2017", "pid": "1711.00043v1", "published_time": "10/31/2017", "rawpid": "1711.00043", "tags": ["cs.CL", "cs.AI"], "title": "Unsupervised Machine Translation Using Monolingual Corpora Only"}, {"abstract": "Maximum Likelihood Estimation (MLE) suffers from data sparsity problem in\nsequence prediction tasks where training resource is rare. In order to\nalleviate this problem, in this paper, we propose a novel generative bridging\nnetwork (GBN) to train sequence prediction models, which contains a generator\nand a bridge. Unlike MLE directly maximizing the likelihood of the ground\ntruth, the bridge extends the point-wise ground truth to a bridge distribution\n(containing inexhaustible examples), and the generator is trained to minimize\ntheir KL-divergence. In order to guide the training of generator with\nadditional signals, the bridge distribution can be set or trained to possess\nspecific properties, by using different constraints. More specifically, to\nincrease output diversity, enhance language smoothness and relieve learning\nburden, three different regularization constraints are introduced to construct\nbridge distributions. By combining these bridges with a sequence generator,\nthree independent GBNs are proposed, namely uniform GBN, language-model GBN and\ncoaching GBN. Experiment conducted on two recognized sequence prediction tasks\n(machine translation and abstractive text summarization) shows that our\nproposed GBNs can yield significant improvements over strong baseline systems.\nFurthermore, by analyzing samples drawn from bridge distributions, expected\ninfluences on the sequence model training are verified.", "authors": ["Wenhu Chen", "Guanlin Li", "Shuo Ren", "Shujie Liu", "Zhirui Zhang", "Mu Li", "Ming Zhou"], "category": "cs.AI", "comment": "A submission for AAAI 2018", "img": "/static/thumbs/1706.09152v4.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1706.09152v4", "num_discussion": 0, "originally_published_time": "6/28/2017", "pid": "1706.09152v4", "published_time": "10/31/2017", "rawpid": "1706.09152", "tags": ["cs.AI", "cs.LG", "stat.ML"], "title": "Generative Bridging Network in Neural Sequence Prediction"}, {"abstract": "We present a new technique for learning visual-semantic embeddings for\ncross-modal retrieval. Inspired by the use of hard negatives in structured\nprediction, and ranking loss functions used in retrieval, we introduce a simple\nchange to common loss functions used to learn multi-modal embeddings. That,\ncombined with fine-tuning and the use of augmented data, yields significant\ngains in retrieval performance. We showcase our approach, dubbed VSE++, on the\nMS-COCO and Flickr30K datasets, using ablation studies and comparisons with\nexisting methods. On MS-COCO our approach outperforms state-of-the-art methods\nby 8.8% in caption retrieval, and 11.3% in image retrieval (based on R@1).", "authors": ["Fartash Faghri", "David J. Fleet", "Jamie Ryan Kiros", "Sanja Fidler"], "category": "cs.LG", "comment": "Under review as a conference paper at ICLR 2018", "img": "/static/thumbs/1707.05612v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1707.05612v2", "num_discussion": 0, "originally_published_time": "7/18/2017", "pid": "1707.05612v2", "published_time": "10/30/2017", "rawpid": "1707.05612", "tags": ["cs.LG", "cs.CL", "cs.CV"], "title": "VSE++: Improving Visual-Semantic Embeddings with Hard Negatives"}, {"abstract": "The ability to compare two degenerate probability distributions (i.e. two\nprobability distributions supported on two distinct low-dimensional manifolds\nliving in a much higher-dimensional space) is a crucial problem arising in the\nestimation of generative models for high-dimensional observations such as those\narising in computer vision or natural language. It is known that optimal\ntransport metrics can represent a cure for this problem, since they were\nspecifically designed as an alternative to information divergences to handle\nsuch problematic scenarios. Unfortunately, training generative machines using\nOT raises formidable computational and statistical challenges, because of (i)\nthe computational burden of evaluating OT losses, (ii) the instability and lack\nof smoothness of these losses, (iii) the difficulty to estimate robustly these\nlosses and their gradients in high dimension. This paper presents the first\ntractable computational method to train large scale generative models using an\noptimal transport loss, and tackles these three issues by relying on two key\nideas: (a) entropic smoothing, which turns the original OT loss into one that\ncan be computed using Sinkhorn fixed point iterations; (b) algorithmic\n(automatic) differentiation of these iterations. These two approximations\nresult in a robust and differentiable approximation of the OT loss with\nstreamlined GPU execution. Entropic smoothing generates a family of losses\ninterpolating between Wasserstein (OT) and Maximum Mean Discrepancy (MMD), thus\nallowing to find a sweet spot leveraging the geometry of OT and the favorable\nhigh-dimensional sample complexity of MMD which comes with unbiased gradient\nestimates. The resulting computational architecture complements nicely standard\ndeep network generative models by a stack of extra layers implementing the loss\nfunction.", "authors": ["Aude Genevay", "Gabriel Peyr\u00e9", "Marco Cuturi"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1706.00292v3.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1706.00292v3", "num_discussion": 0, "originally_published_time": "6/1/2017", "pid": "1706.00292v3", "published_time": "10/20/2017", "rawpid": "1706.00292", "tags": ["stat.ML"], "title": "Learning Generative Models with Sinkhorn Divergences"}, {"abstract": "We present the results from the second shared task on multimodal machine\ntranslation and multilingual image description. Nine teams submitted 19 systems\nto two tasks. The multimodal translation task, in which the source sentence is\nsupplemented by an image, was extended with a new language (French) and two new\ntest sets. The multilingual image description task was changed such that at\ntest time, only the image is given. Compared to last year, multimodal systems\nimproved, but text-only systems remain competitive.", "authors": ["Desmond Elliott", "Stella Frank", "Lo\u00efc Barrault", "Fethi Bougares", "Lucia Specia"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1710.07177v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1710.07177v1", "num_discussion": 0, "originally_published_time": "10/19/2017", "pid": "1710.07177v1", "published_time": "10/19/2017", "rawpid": "1710.07177", "tags": ["cs.CL", "cs.CV"], "title": "Findings of the Second Shared Task on Multimodal Machine Translation and\n  Multilingual Image Description"}, {"abstract": "Images in the wild encapsulate rich knowledge about varied abstract concepts\nand cannot be sufficiently described with models built only using image-caption\npairs containing selected objects. We propose to handle such a task with the\nguidance of a knowledge base that incorporate many abstract concepts. Our\nmethod is a two-step process where we first build a multi-entity-label image\nrecognition model to predict abstract concepts as image labels and then\nleverage them in the second step as an external semantic attention and\nconstrained inference in the caption generation model for describing images\nthat depict unseen/novel objects. Evaluations show that our models outperform\nmost of the prior work for out-of-domain captioning on MSCOCO and are useful\nfor integration of knowledge and vision in general.", "authors": ["Aditya Mogadala", "Umanga Bista", "Lexing Xie", "Achim Rettinger"], "category": "cs.CV", "comment": "10 pages, 5 figures", "img": "/static/thumbs/1710.06303v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1710.06303v1", "num_discussion": 0, "originally_published_time": "10/17/2017", "pid": "1710.06303v1", "published_time": "10/17/2017", "rawpid": "1710.06303", "tags": ["cs.CV", "cs.CL"], "title": "Describing Natural Images Containing Novel Objects with Knowledge Guided\n  Assitance"}, {"abstract": "This paper proposes a general method for improving the structure and quality\nof sequences generated by a recurrent neural network (RNN), while maintaining\ninformation originally learned from data, as well as sample diversity. An RNN\nis first pre-trained on data using maximum likelihood estimation (MLE), and the\nprobability distribution over the next token in the sequence learned by this\nmodel is treated as a prior policy. Another RNN is then trained using\nreinforcement learning (RL) to generate higher-quality outputs that account for\ndomain-specific incentives while retaining proximity to the prior policy of the\nMLE RNN. To formalize this objective, we derive novel off-policy RL methods for\nRNNs from KL-control. The effectiveness of the approach is demonstrated on two\napplications; 1) generating novel musical melodies, and 2) computational\nmolecular generation. For both problems, we show that the proposed method\nimproves the desired properties and structure of the generated sequences, while\nmaintaining information learned from data.", "authors": ["Natasha Jaques", "Shixiang Gu", "Dzmitry Bahdanau", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Richard E. Turner", "Douglas Eck"], "category": "cs.LG", "comment": "Add supplementary material", "img": "/static/thumbs/1611.02796v9.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1611.02796v9", "num_discussion": 0, "originally_published_time": "11/9/2016", "pid": "1611.02796v9", "published_time": "10/16/2017", "rawpid": "1611.02796", "tags": ["cs.LG", "cs.AI"], "title": "Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models\n  with KL-control"}, {"abstract": "We explore two solutions to the problem of mistranslating rare words in\nneural machine translation. First, we argue that the standard output layer,\nwhich computes the inner product of a vector representing the context with all\npossible output word embeddings, rewards frequent words disproportionately, and\nwe propose to fix the norms of both vectors to a constant value. Second, we\nintegrate a simple lexical module which is jointly trained with the rest of the\nmodel. We evaluate our approaches on eight language pairs with data sizes\nranging from 100k to 8M words, and achieve improvements of up to +4.5 BLEU,\nsurpassing phrase-based translation in nearly all settings.", "authors": ["Toan Q. Nguyen", "David Chiang"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1710.01329v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1710.01329v2", "num_discussion": 0, "originally_published_time": "10/3/2017", "pid": "1710.01329v2", "published_time": "10/6/2017", "rawpid": "1710.01329", "tags": ["cs.CL"], "title": "Improving Lexical Choice in Neural Machine Translation"}, {"abstract": "Neural machine translation (NMT) has recently achieved impressive results. A\npotential problem of the existing NMT algorithm, however, is that the decoding\nis conducted from left to right, without considering the right context. This\npaper proposes an two-stage approach to solve the problem. In the first stage,\na conventional attention-based NMT system is used to produce a draft\ntranslation, and in the second stage, a novel double-attention NMT system is\nused to refine the translation, by looking at the original input as well as the\ndraft translation. This drafting-and-refinement can obtain the right-context\ninformation from the draft, hence producing more consistent translations. We\nevaluated this approach using two Chinese-English translation tasks, one with\n44k pairs and 1M pairs respectively. The experiments showed that our approach\nachieved positive improvements over the conventional NMT system: the\nimprovements are 2.4 and 0.9 BLEU points on the small-scale and large-scale\ntasks, respectively.", "authors": ["Aodong Li", "Shiyue Zhang", "Dong Wang", "Thomas Fang Zheng"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1710.01789v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1710.01789v1", "num_discussion": 0, "originally_published_time": "10/4/2017", "pid": "1710.01789v1", "published_time": "10/4/2017", "rawpid": "1710.01789", "tags": ["cs.CL"], "title": "Enhanced Neural Machine Translation by Learning from Draft"}, {"abstract": "Since their invention, generative adversarial networks (GANs) have become a\npopular approach for learning to model a distribution of real (unlabeled) data.\nConvergence problems during training are overcome by Wasserstein GANs which\nminimize the distance between the model and the empirical distribution in terms\nof a different metric, but thereby introduce a Lipschitz constraint into the\noptimization problem. A simple way to enforce the Lipschitz constraint on the\nclass of functions, which can be modeled by the neural network, is weight\nclipping. It was proposed that training can be improved by instead augmenting\nthe loss by a regularization term that penalizes the deviation of the gradient\nof the critic (as a function of the network's input) from one. We present\ntheoretical arguments why using a weaker regularization term enforcing the\nLipschitz constraint is preferable. These arguments are supported by\nexperimental results on toy data sets.", "authors": ["Henning Petzka", "Asja Fischer", "Denis Lukovnicov"], "category": "stat.ML", "comment": "* Henning Petzka and Asja Fischer contributed equally to this work\n  (15 pages +7 pages appendix)", "img": "/static/thumbs/1709.08894v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1709.08894v1", "num_discussion": 0, "originally_published_time": "9/26/2017", "pid": "1709.08894v1", "published_time": "9/26/2017", "rawpid": "1709.08894", "tags": ["stat.ML", "cs.LG"], "title": "On the regularization of Wasserstein GANs"}, {"abstract": "Advances in neural variational inference have facilitated the learning of\npowerful directed graphical models with continuous latent variables, such as\nvariational autoencoders. The hope is that such models will learn to represent\nrich, multi-modal latent factors in real-world data, such as natural language\ntext. However, current models often assume simplistic priors on the latent\nvariables - such as the uni-modal Gaussian distribution - which are incapable\nof representing complex latent factors efficiently. To overcome this\nrestriction, we propose the simple, but highly flexible, piecewise constant\ndistribution. This distribution has the capacity to represent an exponential\nnumber of modes of a latent target distribution, while remaining mathematically\ntractable. Our results demonstrate that incorporating this new latent\ndistribution into different models yields substantial improvements in natural\nlanguage processing tasks such as document modeling and natural language\ngeneration for dialogue.", "authors": ["Iulian V. Serban", "Alexander G. Ororbia II", "Joelle Pineau", "Aaron Courville"], "category": "cs.CL", "comment": "19 pages, 2 figures, 8 tables; EMNLP 2017", "img": "/static/thumbs/1612.00377v4.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1612.00377v4", "num_discussion": 0, "originally_published_time": "12/1/2016", "pid": "1612.00377v4", "published_time": "9/23/2017", "rawpid": "1612.00377", "tags": ["cs.CL", "cs.AI", "cs.LG", "cs.NE", "I.5.1; I.2.7"], "title": "Piecewise Latent Variables for Neural Variational Text Processing"}, {"abstract": "Attention mechanisms have been widely applied in the Visual Question\nAnswering (VQA) task, as they help to focus on the area-of-interest of both\nvisual and textual information. To answer the questions correctly, the model\nneeds to selectively target different areas of an image, which suggests that an\nattention-based model may benefit from an explicit attention supervision. In\nthis work, we aim to address the problem of adding attention supervision to VQA\nmodels. Since there is a lack of human attention data, we first propose a Human\nAttention Network (HAN) to generate human-like attention maps, training on a\nrecently released dataset called Human ATtention Dataset (VQA-HAT). Then, we\napply the pre-trained HAN on the VQA v2.0 dataset to automatically produce the\nhuman-like attention maps for all image-question pairs. The generated\nhuman-like attention map dataset for the VQA v2.0 dataset is named as\nHuman-Like ATtention (HLAT) dataset. Finally, we apply human-like attention\nsupervision to an attention-based VQA model. The experiments show that adding\nhuman-like supervision yields a more accurate attention together with a better\nperformance, showing a promising future for human-like attention supervision in\nVQA.", "authors": ["Tingting Qiao", "Jianfeng Dong", "Duanqing Xu"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1709.06308v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1709.06308v1", "num_discussion": 0, "originally_published_time": "9/19/2017", "pid": "1709.06308v1", "published_time": "9/19/2017", "rawpid": "1709.06308", "tags": ["cs.CV"], "title": "Exploring Human-like Attention Supervision in Visual Question Answering"}, {"abstract": "Paraphrase generation is an important problem in NLP, especially in question\nanswering, information retrieval, information extraction, conversation systems,\nto name a few. In this paper, we address the problem of generating paraphrases\nautomatically. Our proposed method is based on a combination of deep generative\nmodels (VAE) with sequence-to-sequence models (LSTM) to generate paraphrases,\ngiven an input sentence. Traditional VAEs when combined with recurrent neural\nnetworks can generate free text but they are not suitable for paraphrase\ngeneration for a given sentence. We address this problem by conditioning the\nboth, encoder and decoder sides of VAE, on the original sentence, so that it\ncan generate the given sentence's paraphrases. Unlike most existing models, our\nmodel is simple, modular and can generate multiple paraphrases, for a given\nsentence. Quantitative evaluation of the proposed method on a benchmark\nparaphrase dataset demonstrates its efficacy, and its performance improvement\nover the state-of-the-art methods by a significant margin, whereas qualitative\nhuman evaluation indicate that the generated paraphrases are well-formed,\ngrammatically correct, and are relevant to the input sentence. Furthermore, we\nevaluate our method on a newly released question paraphrase dataset, and\nestablish a new baseline for future research.", "authors": ["Ankush Gupta", "Arvind Agarwal", "Prawaan Singh", "Piyush Rai"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1709.05074v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1709.05074v1", "num_discussion": 0, "originally_published_time": "9/15/2017", "pid": "1709.05074v1", "published_time": "9/15/2017", "rawpid": "1709.05074", "tags": ["cs.CL"], "title": "A Deep Generative Framework for Paraphrase Generation"}, {"abstract": "Compared with artificial neural networks (ANNs), spiking neural networks\n(SNNs) are promising to explore the brain-like behaviors since the spikes could\nencode more spatio-temporal information. Although pre-training from ANN or\ndirect training based on backpropagation (BP) makes the supervised training of\nSNNs possible, these methods only exploit the networks' spatial domain\ninformation which leads to the performance bottleneck and requires many\ncomplicated training skills. Another fundamental issue is that the spike\nactivity is naturally non-differentiable which causes great difficulties in\ntraining SNNs. To this end, we build an iterative LIF model that is more\nfriendly for gradient descent training. By simultaneously considering the\nlayer-by-layer spatial domain (SD) and the timing-dependent temporal domain\n(TD) in the training phase, as well as an approximated derivative for the spike\nactivity, we propose a spatio-temporal backpropagation (STBP) training\nframework without using any complicated technology. We achieve the best\nperformance of multi-layered perceptron (MLP) compared with existing\nstate-of-the-art algorithms over the static MNIST and the dynamic N-MNIST\ndataset as well as a custom object detection dataset. This work provides a new\nperspective to explore the high-performance SNNs for future brain-like\ncomputing paradigm with rich spatio-temporal dynamics.", "authors": ["Yujie Wu", "Lei Deng", "Guoqi Li", "Jun Zhu", "Luping Shi"], "category": "cs.NE", "comment": "", "img": "/static/thumbs/1706.02609v3.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1706.02609v3", "num_discussion": 0, "originally_published_time": "6/8/2017", "pid": "1706.02609v3", "published_time": "9/12/2017", "rawpid": "1706.02609", "tags": ["cs.NE", "q-bio.NC", "stat.ML"], "title": "Spatio-Temporal Backpropagation for Training High-performance Spiking\n  Neural Networks"}, {"abstract": "The existing image captioning approaches typically train a one-stage sentence\ndecoder, which is difficult to generate rich fine-grained descriptions. On the\nother hand, multi-stage image caption model is hard to train due to the\nvanishing gradient problem. In this paper, we propose a coarse-to-fine\nmulti-stage prediction framework for image captioning, composed of multiple\ndecoders each of which operates on the output of the previous stage, producing\nincreasingly refined image descriptions. Our proposed learning approach\naddresses the difficulty of vanishing gradients during training by providing a\nlearning objective function that enforces intermediate supervisions.\nParticularly, we optimize our model with a reinforcement learning approach\nwhich utilizes the output of each intermediate decoder's test-time inference\nalgorithm as well as the output of its preceding decoder to normalize the\nrewards, which simultaneously solves the well-known exposure bias problem and\nthe loss-evaluation mismatch problem. We extensively evaluate the proposed\napproach on MSCOCO and show that our approach can achieve the state-of-the-art\nperformance.", "authors": ["Jiuxiang Gu", "Jianfei Cai", "Gang Wang", "Tsuhan Chen"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1709.03376v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1709.03376v2", "num_discussion": 0, "originally_published_time": "9/11/2017", "pid": "1709.03376v2", "published_time": "9/12/2017", "rawpid": "1709.03376", "tags": ["cs.CV"], "title": "Stack-Captioning: Coarse-to-Fine Learning for Image Captioning"}, {"abstract": "Previous work combines word-level and character-level representations using\nconcatenation or scalar weighting, which is suboptimal for high-level tasks\nlike reading comprehension. We present a fine-grained gating mechanism to\ndynamically combine word-level and character-level representations based on\nproperties of the words. We also extend the idea of fine-grained gating to\nmodeling the interaction between questions and paragraphs for reading\ncomprehension. Experiments show that our approach can improve the performance\non reading comprehension tasks, achieving new state-of-the-art results on the\nChildren's Book Test dataset. To demonstrate the generality of our gating\nmechanism, we also show improved results on a social media tag prediction task.", "authors": ["Zhilin Yang", "Bhuwan Dhingra", "Ye Yuan", "Junjie Hu", "William W. Cohen", "Ruslan Salakhutdinov"], "category": "cs.CL", "comment": "Accepted as a conference paper at ICLR 2017", "img": "/static/thumbs/1611.01724v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1611.01724v2", "num_discussion": 0, "originally_published_time": "11/6/2016", "pid": "1611.01724v2", "published_time": "9/11/2017", "rawpid": "1611.01724", "tags": ["cs.CL", "cs.LG"], "title": "Words or Characters? Fine-grained Gating for Reading Comprehension"}, {"abstract": "The pre-dominant approach to language modeling to date is based on recurrent\nneural networks. Their success on this task is often linked to their ability to\ncapture unbounded context. In this paper we develop a finite context approach\nthrough stacked convolutions, which can be more efficient since they allow\nparallelization over sequential tokens. We propose a novel simplified gating\nmechanism that outperforms Oord et al (2016) and investigate the impact of key\narchitectural decisions. The proposed approach achieves state-of-the-art on the\nWikiText-103 benchmark, even though it features long-term dependencies, as well\nas competitive results on the Google Billion Words benchmark. Our model reduces\nthe latency to score a sentence by an order of magnitude compared to a\nrecurrent baseline. To our knowledge, this is the first time a non-recurrent\napproach is competitive with strong recurrent models on these large scale\nlanguage tasks.", "authors": ["Yann N. Dauphin", "Angela Fan", "Michael Auli", "David Grangier"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1612.08083v3.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1612.08083v3", "num_discussion": 0, "originally_published_time": "12/23/2016", "pid": "1612.08083v3", "published_time": "9/8/2017", "rawpid": "1612.08083", "tags": ["cs.CL"], "title": "Language Modeling with Gated Convolutional Networks"}, {"abstract": "Deep Learning has revolutionized vision via convolutional neural networks\n(CNNs) and natural language processing via recurrent neural networks (RNNs).\nHowever, success stories of Deep Learning with standard feed-forward neural\nnetworks (FNNs) are rare. FNNs that perform well are typically shallow and,\ntherefore cannot exploit many levels of abstract representations. We introduce\nself-normalizing neural networks (SNNs) to enable high-level abstract\nrepresentations. While batch normalization requires explicit normalization,\nneuron activations of SNNs automatically converge towards zero mean and unit\nvariance. The activation function of SNNs are \"scaled exponential linear units\"\n(SELUs), which induce self-normalizing properties. Using the Banach fixed-point\ntheorem, we prove that activations close to zero mean and unit variance that\nare propagated through many network layers will converge towards zero mean and\nunit variance -- even under the presence of noise and perturbations. This\nconvergence property of SNNs allows to (1) train deep networks with many\nlayers, (2) employ strong regularization, and (3) to make learning highly\nrobust. Furthermore, for activations not close to unit variance, we prove an\nupper and lower bound on the variance, thus, vanishing and exploding gradients\nare impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning\nrepository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with\nstandard FNNs and other machine learning methods such as random forests and\nsupport vector machines. SNNs significantly outperformed all competing FNN\nmethods at 121 UCI tasks, outperformed all competing methods at the Tox21\ndataset, and set a new record at an astronomy data set. The winning SNN\narchitectures are often very deep. Implementations are available at:\ngithub.com/bioinf-jku/SNNs.", "authors": ["G\u00fcnter Klambauer", "Thomas Unterthiner", "Andreas Mayr", "Sepp Hochreiter"], "category": "cs.LG", "comment": "9 pages (+ 93 pages appendix)", "img": "/static/thumbs/1706.02515v5.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1706.02515v5", "num_discussion": 0, "originally_published_time": "6/8/2017", "pid": "1706.02515v5", "published_time": "9/7/2017", "rawpid": "1706.02515", "tags": ["cs.LG", "stat.ML"], "title": "Self-Normalizing Neural Networks"}, {"abstract": "In neural image captioning systems, a recurrent neural network (RNN) is\ntypically viewed as the primary `generation' component. This view suggests that\nthe image features should be `injected' into the RNN. This is in fact the\ndominant view in the literature. Alternatively, the RNN can instead be viewed\nas only encoding the previously generated words. This view suggests that the\nRNN should only be used to encode linguistic features and that only the final\nrepresentation should be `merged' with the image features at a later stage.\nThis paper compares these two architectures. We find that, in general, late\nmerging outperforms injection, suggesting that RNNs are better viewed as\nencoders, rather than generators.", "authors": ["Marc Tanti", "Albert Gatt", "Kenneth P. Camilleri"], "category": "cs.CL", "comment": "Appears in: Proceedings of the 10th International Conference on\n  Natural Language Generation (INLG'...", "img": "/static/thumbs/1708.02043v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1708.02043v2", "num_discussion": 0, "originally_published_time": "8/7/2017", "pid": "1708.02043v2", "published_time": "8/25/2017", "rawpid": "1708.02043", "tags": ["cs.CL", "cs.CV", "cs.NE"], "title": "What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption\n  Generator?"}, {"abstract": "We propose \"Areas of Attention\", a novel attention-based model for automatic\nimage captioning. Our approach models the dependencies between image regions,\ncaption words, and the state of an RNN language model, using three pairwise\ninteractions. In contrast to previous attention-based approaches that associate\nimage regions only to the RNN state, our method allows a direct association\nbetween caption words and image regions. During training these associations are\ninferred from image-level captions, akin to weakly-supervised object detector\ntraining. These associations help to improve captioning by localizing the\ncorresponding regions during testing. We also propose and compare different\nways of generating attention areas: CNN activation grids, object proposals, and\nspatial transformers nets applied in a convolutional fashion. Spatial\ntransformers give the best results. They allow for image specific attention\nareas, and can be trained jointly with the rest of the network. Our attention\nmechanism and spatial transformer attention areas together yield\nstate-of-the-art results on the MSCOCO dataset.o meaningful latent semantic\nstructure in the generated captions.", "authors": ["Marco Pedersoli", "Thomas Lucas", "Cordelia Schmid", "Jakob Verbeek"], "category": "cs.CV", "comment": "Accepted in ICCV 2017", "img": "/static/thumbs/1612.01033v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1612.01033v2", "num_discussion": 0, "originally_published_time": "12/3/2016", "pid": "1612.01033v2", "published_time": "8/25/2017", "rawpid": "1612.01033", "tags": ["cs.CV"], "title": "Areas of Attention for Image Captioning"}, {"abstract": "Despite their success for object detection, convolutional neural networks are\nill-equipped for incremental learning, i.e., adapting the original model\ntrained on a set of classes to additionally detect objects of new classes, in\nthe absence of the initial training data. They suffer from \"catastrophic\nforgetting\" - an abrupt degradation of performance on the original set of\nclasses, when the training objective is adapted to the new classes. We present\na method to address this issue, and learn object detectors incrementally, when\nneither the original training data nor annotations for the original classes in\nthe new training set are available. The core of our proposed solution is a loss\nfunction to balance the interplay between predictions on the new classes and a\nnew distillation loss which minimizes the discrepancy between responses for old\nclasses from the original and the updated networks. This incremental learning\ncan be performed multiple times, for a new set of classes in each step, with a\nmoderate drop in performance compared to the baseline network trained on the\nensemble of data. We present object detection results on the PASCAL VOC 2007\nand COCO datasets, along with a detailed empirical analysis of the approach.", "authors": ["Konstantin Shmelkov", "Cordelia Schmid", "Karteek Alahari"], "category": "cs.CV", "comment": "To appear in ICCV 2017", "img": "/static/thumbs/1708.06977v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1708.06977v1", "num_discussion": 0, "originally_published_time": "8/23/2017", "pid": "1708.06977v1", "published_time": "8/23/2017", "rawpid": "1708.06977", "tags": ["cs.CV"], "title": "Incremental Learning of Object Detectors without Catastrophic Forgetting"}, {"abstract": "Existing Natural Language Generation (NLG) systems are weak AI systems and\nexhibit limited capabilities when language generation tasks demand higher\nlevels of creativity, originality and brevity. Effective solutions or, at least\nevaluations of modern NLG paradigms for such creative tasks have been elusive,\nunfortunately. This paper introduces and addresses the task of coherent story\ngeneration from independent descriptions, describing a scene or an event.\nTowards this, we explore along two popular text-generation paradigms -- (1)\nStatistical Machine Translation (SMT), posing story generation as a translation\nproblem and (2) Deep Learning, posing story generation as a sequence to\nsequence learning problem. In SMT, we chose two popular methods such as phrase\nbased SMT (PB-SMT) and syntax based SMT (SYNTAX-SMT) to `translate' the\nincoherent input text into stories. We then implement a deep recurrent neural\nnetwork (RNN) architecture that encodes sequence of variable length input\ndescriptions to corresponding latent representations and decodes them to\nproduce well formed comprehensive story like summaries. The efficacy of the\nsuggested approaches is demonstrated on a publicly available dataset with the\nhelp of popular machine translation and summarization evaluation metrics.", "authors": ["Parag Jain", "Priyanka Agrawal", "Abhijit Mishra", "Mohak Sukhwani", "Anirban Laha", "Karthik Sankaranarayanan"], "category": "cs.CL", "comment": "Accepted in SIGKDD Workshop on Machine Learning for Creativity\n  (ML4Creativity), 2017", "img": "/static/thumbs/1707.05501v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1707.05501v2", "num_discussion": 0, "originally_published_time": "7/18/2017", "pid": "1707.05501v2", "published_time": "8/21/2017", "rawpid": "1707.05501", "tags": ["cs.CL"], "title": "Story Generation from Sequence of Independent Short Descriptions"}, {"abstract": "A number of recent works have proposed techniques for end-to-end learning of\ncommunication protocols among cooperative multi-agent populations, and have\nsimultaneously found the emergence of grounded human-interpretable language in\nthe protocols developed by the agents, all learned without any human\nsupervision!\n  In this paper, using a Task and Tell reference game between two agents as a\ntestbed, we present a sequence of 'negative' results culminating in a\n'positive' one -- showing that while most agent-invented languages are\neffective (i.e. achieve near-perfect task rewards), they are decidedly not\ninterpretable or compositional.\n  In essence, we find that natural language does not emerge 'naturally',\ndespite the semblance of ease of natural-language-emergence that one may gather\nfrom recent literature. We discuss how it is possible to coax the invented\nlanguages to become more and more human-like and compositional by increasing\nrestrictions on how two agents may communicate.", "authors": ["Satwik Kottur", "Jos\u00e9 M. F. Moura", "Stefan Lee", "Dhruv Batra"], "category": "cs.CL", "comment": "9 pages, 7 figures, 2 tables, accepted at EMNLP 2017 as short paper", "img": "/static/thumbs/1706.08502v3.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1706.08502v3", "num_discussion": 0, "originally_published_time": "6/26/2017", "pid": "1706.08502v3", "published_time": "8/20/2017", "rawpid": "1706.08502", "tags": ["cs.CL", "cs.AI", "cs.CV"], "title": "Natural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog"}, {"abstract": "There is a pressing need to build an architecture that could subsume these\nnetworks undera unified framework that achieves both higher performance and\nless overhead. To this end, two fundamental issues are yet to be addressed. The\nfirst one is how to implement the back propagation when neuronal activations\nare discrete. The second one is how to remove the full-precision hidden weights\nin the training phase to break the bottlenecks of memory/computation\nconsumption. To address the first issue, we present a multistep neuronal\nactivation discretization method and a derivative approximation technique that\nenable the implementing the back propagation algorithm on discrete DNNs. While\nfor the second issue, we propose a discrete state transition (DST) methodology\nto constrain the weights in a discrete space without saving the hidden weights.\nIn this way, we build a unified framework that subsumes the binary or ternary\nnetworks as its special cases.More particularly, we find that when both the\nweights and activations become ternary values, the DNNs can be reduced to gated\nXNOR networks (or sparse binary networks) since only the event of non-zero\nweight and non-zero activation enables the control gate to start the XNOR logic\noperations in the original binary networks. This promises the event-driven\nhardware design for efficient mobile intelligence. We achieve advanced\nperformance compared with state-of-the-art algorithms. Furthermore,the\ncomputational sparsity and the number of states in the discrete space can be\nflexibly modified to make it suitable for various hardware platforms.", "authors": ["Lei Deng", "Peng Jiao", "Jing Pei", "Zhenzhi Wu", "Guoqi Li"], "category": "cs.LG", "comment": "9 pages, 10 figures", "img": "/static/thumbs/1705.09283v3.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1705.09283v3", "num_discussion": 0, "originally_published_time": "5/25/2017", "pid": "1705.09283v3", "published_time": "8/18/2017", "rawpid": "1705.09283", "tags": ["cs.LG", "cs.CV", "stat.ML"], "title": "Gated XNOR Networks: Deep Neural Networks with Ternary Weights and\n  Activations under a Unified Discretization Framework"}, {"abstract": "Image captioning often requires a large set of training image-sentence pairs.\nIn practice, however, acquiring sufficient training pairs is always expensive,\nmaking the recent captioning models limited in their ability to describe\nobjects outside of training corpora (i.e., novel objects). In this paper, we\npresent Long Short-Term Memory with Copying Mechanism (LSTM-C) --- a new\narchitecture that incorporates copying into the Convolutional Neural Networks\n(CNN) plus Recurrent Neural Networks (RNN) image captioning framework, for\ndescribing novel objects in captions. Specifically, freely available object\nrecognition datasets are leveraged to develop classifiers for novel objects.\nOur LSTM-C then nicely integrates the standard word-by-word sentence generation\nby a decoder RNN with copying mechanism which may instead select words from\nnovel objects at proper places in the output sentence. Extensive experiments\nare conducted on both MSCOCO image captioning and ImageNet datasets,\ndemonstrating the ability of our proposed LSTM-C architecture to describe novel\nobjects. Furthermore, superior results are reported when compared to\nstate-of-the-art deep models.", "authors": ["Ting Yao", "Yingwei Pan", "Yehao Li", "Tao Mei"], "category": "cs.CV", "comment": "CVPR17", "img": "/static/thumbs/1708.05271v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1708.05271v1", "num_discussion": 0, "originally_published_time": "8/17/2017", "pid": "1708.05271v1", "published_time": "8/17/2017", "rawpid": "1708.05271", "tags": ["cs.CV", "cs.CL"], "title": "Incorporating Copying Mechanism in Image Captioning for Learning Novel\n  Objects"}, {"abstract": "Impressive image captioning results are achieved in domains with plenty of\ntraining image and sentence pairs (e.g., MSCOCO). However, transferring to a\ntarget domain with significant domain shifts but no paired training data\n(referred to as cross-domain image captioning) remains largely unexplored. We\npropose a novel adversarial training procedure to leverage unpaired data in the\ntarget domain. Two critic networks are introduced to guide the captioner,\nnamely domain critic and multi-modal critic. The domain critic assesses whether\nthe generated sentences are indistinguishable from sentences in the target\ndomain. The multi-modal critic assesses whether an image and its generated\nsentence are a valid pair. During training, the critics and captioner act as\nadversaries -- captioner aims to generate indistinguishable sentences, whereas\ncritics aim at distinguishing them. The assessment improves the captioner\nthrough policy gradient updates. During inference, we further propose a novel\ncritic-based planning method to select high-quality sentences without\nadditional supervision (e.g., tags). To evaluate, we use MSCOCO as the source\ndomain and four other datasets (CUB-200-2011, Oxford-102, TGIF, and Flickr30k)\nas the target domains. Our method consistently performs well on all datasets.\nIn particular, on CUB-200-2011, we achieve 21.8% CIDEr-D improvement after\nadaptation. Utilizing critics during inference further gives another 4.5%\nboost.", "authors": ["Tseng-Hung Chen", "Yuan-Hong Liao", "Ching-Yao Chuang", "Wan-Ting Hsu", "Jianlong Fu", "Min Sun"], "category": "cs.CV", "comment": "ICCV 2017", "img": "/static/thumbs/1705.00930v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1705.00930v2", "num_discussion": 0, "originally_published_time": "5/2/2017", "pid": "1705.00930v2", "published_time": "8/14/2017", "rawpid": "1705.00930", "tags": ["cs.CV", "cs.AI", "cs.LG"], "title": "Show, Adapt and Tell: Adversarial Training of Cross-domain Image\n  Captioner"}, {"abstract": "Top-down visual attention mechanisms have been used extensively in image\ncaptioning and visual question answering (VQA) to enable deeper image\nunderstanding through fine-grained analysis and even multiple steps of\nreasoning. In this work, we propose a combined bottom-up and top-down attention\nmechanism that enables attention to be calculated at the level of objects and\nother salient image regions. This is the natural basis for attention to be\nconsidered. Within our approach, the bottom-up mechanism (based on Faster\nR-CNN) proposes image regions, each with an associated feature vector, while\nthe top-down mechanism determines feature weightings. Applying this approach to\nimage captioning, our results on the MSCOCO test server establish a new\nstate-of-the-art for the task, improving the best published result in terms of\nCIDEr score from 114.7 to 117.9 and BLEU-4 from 35.2 to 36.9. Demonstrating the\nbroad applicability of the method, applying the same approach to VQA we obtain\nfirst place in the 2017 VQA Challenge.", "authors": ["Peter Anderson", "Xiaodong He", "Chris Buehler", "Damien Teney", "Mark Johnson", "Stephen Gould", "Lei Zhang"], "category": "cs.CV", "comment": "Winner of the Visual Question Answering Challenge at CVPR 2017", "img": "/static/thumbs/1707.07998v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1707.07998v2", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.07998v2", "published_time": "8/10/2017", "rawpid": "1707.07998", "tags": ["cs.CV"], "title": "Bottom-Up and Top-Down Attention for Image Captioning and Visual\n  Question Answering"}, {"abstract": "In this work we formulate the problem of image captioning as a multimodal\ntranslation task. Analogous to machine translation, we present a\nsequence-to-sequence recurrent neural networks (RNN) model for image caption\ngeneration. Different from most existing work where the whole image is\nrepresented by convolutional neural network (CNN) feature, we propose to\nrepresent the input image as a sequence of detected objects which feeds as the\nsource sequence of the RNN model. In this way, the sequential representation of\nan image can be naturally translated to a sequence of words, as the target\nsequence of the RNN model. To represent the image in a sequential way, we\nextract the objects features in the image and arrange them in a order using\nconvolutional neural networks. To further leverage the visual information from\nthe encoded objects, a sequential attention layer is introduced to selectively\nattend to the objects that are related to generate corresponding words in the\nsentences. Extensive experiments are conducted to validate the proposed\napproach on popular benchmark dataset, i.e., MS COCO, and the proposed model\nsurpasses the state-of-the-art methods in all metrics following the dataset\nsplits of previous work. The proposed approach is also evaluated by the\nevaluation server of MS COCO captioning challenge, and achieves very\ncompetitive results, e.g., a CIDEr of 1.029 (c5) and 1.064 (c40).", "authors": ["Chang Liu", "Fuchun Sun", "Changhu Wang", "Feng Wang", "Alan Yuille"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1702.05658v3.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1702.05658v3", "num_discussion": 0, "originally_published_time": "2/18/2017", "pid": "1702.05658v3", "published_time": "8/10/2017", "rawpid": "1702.05658", "tags": ["cs.CV"], "title": "MAT: A Multimodal Attentive Translator for Image Captioning"}, {"abstract": "Real-time scene understanding has become crucial in many applications such as\nautonomous driving. In this paper, we propose a deep architecture, called\nBlitzNet, that jointly performs object detection and semantic segmentation in\none forward pass, allowing real-time computations. Besides the computational\ngain of having a single network to perform several tasks, we show that object\ndetection and semantic segmentation benefit from each other in terms of\naccuracy. Experimental results for VOC and COCO datasets show state-of-the-art\nperformance for object detection and segmentation among real time systems.", "authors": ["Nikita Dvornik", "Konstantin Shmelkov", "Julien Mairal", "Cordelia Schmid"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1708.02813v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1708.02813v1", "num_discussion": 0, "originally_published_time": "8/9/2017", "pid": "1708.02813v1", "published_time": "8/9/2017", "rawpid": "1708.02813", "tags": ["cs.CV"], "title": "BlitzNet: A Real-Time Deep Network for Scene Understanding"}, {"abstract": "Dense captioning is a newly emerging computer vision topic for understanding\nimages with dense language descriptions. The goal is to densely detect visual\nconcepts (e.g., objects, object parts, and interactions between them) from\nimages, labeling each with a short descriptive phrase. We identify two key\nchallenges of dense captioning that need to be properly addressed when tackling\nthe problem. First, dense visual concept annotations in each image are\nassociated with highly overlapping target regions, making accurate localization\nof each visual concept challenging. Second, the large amount of visual concepts\nmakes it hard to recognize each of them by appearance alone. We propose a new\nmodel pipeline based on two novel ideas, joint inference and context fusion, to\nalleviate these two challenges. We design our model architecture in a\nmethodical manner and thoroughly evaluate the variations in architecture. Our\nfinal model, compact and efficient, achieves state-of-the-art accuracy on\nVisual Genome for dense captioning with a relative gain of 73\\% compared to the\nprevious best algorithm. Qualitative experiments also reveal the semantic\ncapabilities of our model in dense captioning.", "authors": ["Linjie Yang", "Kevin Tang", "Jianchao Yang", "Li-Jia Li"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1611.06949v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1611.06949v2", "num_discussion": 0, "originally_published_time": "11/21/2016", "pid": "1611.06949v2", "published_time": "8/7/2017", "rawpid": "1611.06949", "tags": ["cs.CV"], "title": "Dense Captioning with Joint Inference and Visual Context"}, {"abstract": "Recurrent neural networks (RNNs), such as long short-term memory networks\n(LSTMs), serve as a fundamental building block for many sequence learning\ntasks, including machine translation, language modeling, and question\nanswering. In this paper, we consider the specific problem of word-level\nlanguage modeling and investigate strategies for regularizing and optimizing\nLSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on\nhidden-to-hidden weights as a form of recurrent regularization. Further, we\nintroduce NT-ASGD, a variant of the averaged stochastic gradient method,\nwherein the averaging trigger is determined using a non-monotonic condition as\nopposed to being tuned by the user. Using these and other regularization\nstrategies, we achieve state-of-the-art word level perplexities on two data\nsets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the\neffectiveness of a neural cache in conjunction with our proposed model, we\nachieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and\n52.0 on WikiText-2.", "authors": ["Stephen Merity", "Nitish Shirish Keskar", "Richard Socher"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1708.02182v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1708.02182v1", "num_discussion": 0, "originally_published_time": "8/7/2017", "pid": "1708.02182v1", "published_time": "8/7/2017", "rawpid": "1708.02182", "tags": ["cs.CL", "cs.NE"], "title": "Regularizing and Optimizing LSTM Language Models"}, {"abstract": "Synthesizing high-quality images from text descriptions is a challenging\nproblem in computer vision and has many practical applications. Samples\ngenerated by existing text-to-image approaches can roughly reflect the meaning\nof the given descriptions, but they fail to contain necessary details and vivid\nobject parts. In this paper, we propose Stacked Generative Adversarial Networks\n(StackGAN) to generate 256x256 photo-realistic images conditioned on text\ndescriptions. We decompose the hard problem into more manageable sub-problems\nthrough a sketch-refinement process. The Stage-I GAN sketches the primitive\nshape and colors of the object based on the given text description, yielding\nStage-I low-resolution images. The Stage-II GAN takes Stage-I results and text\ndescriptions as inputs, and generates high-resolution images with\nphoto-realistic details. It is able to rectify defects in Stage-I results and\nadd compelling details with the refinement process. To improve the diversity of\nthe synthesized images and stabilize the training of the conditional-GAN, we\nintroduce a novel Conditioning Augmentation technique that encourages\nsmoothness in the latent conditioning manifold. Extensive experiments and\ncomparisons with state-of-the-arts on benchmark datasets demonstrate that the\nproposed method achieves significant improvements on generating photo-realistic\nimages conditioned on text descriptions.", "authors": ["Han Zhang", "Tao Xu", "Hongsheng Li", "Shaoting Zhang", "Xiaogang Wang", "Xiaolei Huang", "Dimitris Metaxas"], "category": "cs.CV", "comment": "ICCV 2017 Oral Presentation", "img": "/static/thumbs/1612.03242v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1612.03242v2", "num_discussion": 0, "originally_published_time": "12/10/2016", "pid": "1612.03242v2", "published_time": "8/5/2017", "rawpid": "1612.03242", "tags": ["cs.CV", "cs.AI", "stat.ML"], "title": "StackGAN: Text to Photo-realistic Image Synthesis with Stacked\n  Generative Adversarial Networks"}, {"abstract": "This article was withdrawn because (1) it was uploaded without the\nco-authors' knowledge or consent, and (2) there are allegations of plagiarism.", "authors": ["Jun Qi"], "category": "cs.LG", "comment": "The paper has been withdrawn. See the abstract for the reason", "img": "/static/thumbs/1707.05721v3.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1707.05721v3", "num_discussion": 0, "originally_published_time": "7/18/2017", "pid": "1707.05721v3", "published_time": "8/3/2017", "rawpid": "1707.05721", "tags": ["cs.LG"], "title": "Submodular Mini-Batch Training in Generative Moment Matching Networks"}, {"abstract": "Recurrent neural networks (RNNs) serve as a fundamental building block for\nmany sequence tasks across natural language processing. Recent research has\nfocused on recurrent dropout techniques or custom RNN cells in order to improve\nperformance. Both of these can require substantial modifications to the machine\nlearning model or to the underlying RNN configurations. We revisit traditional\nregularization techniques, specifically L2 regularization on RNN activations\nand slowness regularization over successive hidden states, to improve the\nperformance of RNNs on the task of language modeling. Both of these techniques\nrequire minimal modification to existing RNN architectures and result in\nperformance improvements comparable or superior to more complicated\nregularization techniques or custom cell architectures. These regularization\ntechniques can be used without any modification on optimized LSTM\nimplementations such as the NVIDIA cuDNN LSTM.", "authors": ["Stephen Merity", "Bryan McCann", "Richard Socher"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1708.01009v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1708.01009v1", "num_discussion": 0, "originally_published_time": "8/3/2017", "pid": "1708.01009v1", "published_time": "8/3/2017", "rawpid": "1708.01009", "tags": ["cs.CL", "cs.NE"], "title": "Revisiting Activation Regularization for Language RNNs"}, {"abstract": "Language Models based on recurrent neural networks have dominated recent\nimage caption generation tasks. In this paper, we introduce a Language CNN\nmodel which is suitable for statistical language modeling tasks and shows\ncompetitive performance in image captioning. In contrast to previous models\nwhich predict next word based on one previous word and hidden state, our\nlanguage CNN is fed with all the previous words and can model the long-range\ndependencies of history words, which are critical for image captioning. The\neffectiveness of our approach is validated on two datasets MS COCO and\nFlickr30K. Our extensive experimental results show that our method outperforms\nthe vanilla recurrent neural network based language models and is competitive\nwith the state-of-the-art methods.", "authors": ["Jiuxiang Gu", "Gang Wang", "Jianfei Cai", "Tsuhan Chen"], "category": "cs.CV", "comment": "Comments: 10 pages, In proceedings of ICCV 2017", "img": "/static/thumbs/1612.07086v3.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1612.07086v3", "num_discussion": 0, "originally_published_time": "12/21/2016", "pid": "1612.07086v3", "published_time": "8/2/2017", "rawpid": "1612.07086", "tags": ["cs.CV", "cs.LG"], "title": "An Empirical Study of Language CNN for Image Captioning"}, {"abstract": "We introduce the task of Visual Dialog, which requires an AI agent to hold a\nmeaningful dialog with humans in natural, conversational language about visual\ncontent. Specifically, given an image, a dialog history, and a question about\nthe image, the agent has to ground the question in image, infer context from\nhistory, and answer the question accurately. Visual Dialog is disentangled\nenough from a specific downstream task so as to serve as a general test of\nmachine intelligence, while being grounded in vision enough to allow objective\nevaluation of individual responses and benchmark progress. We develop a novel\ntwo-person chat data-collection protocol to curate a large-scale Visual Dialog\ndataset (VisDial). VisDial v0.9 has been released and contains 1 dialog with 10\nquestion-answer pairs on ~120k images from COCO, with a total of ~1.2M dialog\nquestion-answer pairs.\n  We introduce a family of neural encoder-decoder models for Visual Dialog with\n3 encoders -- Late Fusion, Hierarchical Recurrent Encoder and Memory Network --\nand 2 decoders (generative and discriminative), which outperform a number of\nsophisticated baselines. We propose a retrieval-based evaluation protocol for\nVisual Dialog where the AI agent is asked to sort a set of candidate answers\nand evaluated on metrics such as mean-reciprocal-rank of human response. We\nquantify gap between machine and human performance on the Visual Dialog task\nvia human studies. Putting it all together, we demonstrate the first 'visual\nchatbot'! Our dataset, code, trained models and visual chatbot are available on\nhttps://visualdialog.org", "authors": ["Abhishek Das", "Satwik Kottur", "Khushi Gupta", "Avi Singh", "Deshraj Yadav", "Jos\u00e9 M. F. Moura", "Devi Parikh", "Dhruv Batra"], "category": "cs.CV", "comment": "23 pages, 18 figures, CVPR 2017 camera-ready, results on VisDial v0.9\n  dataset, Webpage: http://vis...", "img": "/static/thumbs/1611.08669v5.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1611.08669v5", "num_discussion": 0, "originally_published_time": "11/26/2016", "pid": "1611.08669v5", "published_time": "8/1/2017", "rawpid": "1611.08669", "tags": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "title": "Visual Dialog"}, {"abstract": "Computer vision has benefited from initializing multiple deep layers with\nweights pretrained on large supervised training sets like ImageNet. Natural\nlanguage processing (NLP) typically sees initialization of only the lowest\nlayer of deep models with pretrained word vectors. In this paper, we use a deep\nLSTM encoder from an attentional sequence-to-sequence model trained for machine\ntranslation (MT) to contextualize word vectors. We show that adding these\ncontext vectors (CoVe) improves performance over using only unsupervised word\nand character vectors on a wide variety of common NLP tasks: sentiment analysis\n(SST, IMDb), question classification (TREC), entailment (SNLI), and question\nanswering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe\nimproves performance of our baseline models to the state of the art.", "authors": ["Bryan McCann", "James Bradbury", "Caiming Xiong", "Richard Socher"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1708.00107v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1708.00107v1", "num_discussion": 0, "originally_published_time": "8/1/2017", "pid": "1708.00107v1", "published_time": "8/1/2017", "rawpid": "1708.00107", "tags": ["cs.CL", "cs.AI", "cs.LG"], "title": "Learned in Translation: Contextualized Word Vectors"}, {"abstract": "The prevalent approach to sequence to sequence learning maps an input\nsequence to a variable length output sequence via recurrent neural networks. We\nintroduce an architecture based entirely on convolutional neural networks.\nCompared to recurrent models, computations over all elements can be fully\nparallelized during training and optimization is easier since the number of\nnon-linearities is fixed and independent of the input length. Our use of gated\nlinear units eases gradient propagation and we equip each decoder layer with a\nseparate attention module. We outperform the accuracy of the deep LSTM setup of\nWu et al. (2016) on both WMT'14 English-German and WMT'14 English-French\ntranslation at an order of magnitude faster speed, both on GPU and CPU.", "authors": ["Jonas Gehring", "Michael Auli", "David Grangier", "Denis Yarats", "Yann N. Dauphin"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1705.03122v3.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1705.03122v3", "num_discussion": 0, "originally_published_time": "5/8/2017", "pid": "1705.03122v3", "published_time": "7/25/2017", "rawpid": "1705.03122", "tags": ["cs.CL"], "title": "Convolutional Sequence to Sequence Learning"}, {"abstract": "The prevalent approach to neural machine translation relies on bi-directional\nLSTMs to encode the source sentence. In this paper we present a faster and\nsimpler architecture based on a succession of convolutional layers. This allows\nto encode the entire source sentence simultaneously compared to recurrent\nnetworks for which computation is constrained by temporal dependencies. On\nWMT'16 English-Romanian translation we achieve competitive accuracy to the\nstate-of-the-art and we outperform several recently published results on the\nWMT'15 English-German task. Our models obtain almost the same accuracy as a\nvery deep LSTM setup on WMT'14 English-French translation. Our convolutional\nencoder speeds up CPU decoding by more than two times at the same or higher\naccuracy as a strong bi-directional LSTM baseline.", "authors": ["Jonas Gehring", "Michael Auli", "David Grangier", "Yann N. Dauphin"], "category": "cs.CL", "comment": "13 pages", "img": "/static/thumbs/1611.02344v3.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1611.02344v3", "num_discussion": 0, "originally_published_time": "11/7/2016", "pid": "1611.02344v3", "published_time": "7/25/2017", "rawpid": "1611.02344", "tags": ["cs.CL"], "title": "A Convolutional Encoder Model for Neural Machine Translation"}, {"abstract": "Transfer and multi-task learning have traditionally focused on either a\nsingle source-target pair or very few, similar tasks. Ideally, the linguistic\nlevels of morphology, syntax and semantics would benefit each other by being\ntrained in a single model. We introduce a joint many-task model together with a\nstrategy for successively growing its depth to solve increasingly complex\ntasks. Higher layers include shortcut connections to lower-level task\npredictions to reflect linguistic hierarchies. We use a simple regularization\nterm to allow for optimizing all model weights to improve one task's loss\nwithout exhibiting catastrophic interference of the other tasks. Our single\nend-to-end model obtains state-of-the-art or competitive results on five\ndifferent tasks from tagging, parsing, relatedness, and entailment tasks.", "authors": ["Kazuma Hashimoto", "Caiming Xiong", "Yoshimasa Tsuruoka", "Richard Socher"], "category": "cs.CL", "comment": "Accepted as a full paper at the 2017 Conference on Empirical Methods\n  in Natural Language Processin...", "img": "/static/thumbs/1611.01587v5.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1611.01587v5", "num_discussion": 0, "originally_published_time": "11/5/2016", "pid": "1611.01587v5", "published_time": "7/24/2017", "rawpid": "1611.01587", "tags": ["cs.CL", "cs.AI"], "title": "A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks"}, {"abstract": "Word embeddings improve generalization over lexical features by placing each\nword in a lower-dimensional space, using distributional information obtained\nfrom unlabeled data. However, the effectiveness of word embeddings for\ndownstream NLP tasks is limited by out-of-vocabulary (OOV) words, for which\nembeddings do not exist. In this paper, we present MIMICK, an approach to\ngenerating OOV word embeddings compositionally, by learning a function from\nspellings to distributional embeddings. Unlike prior work, MIMICK does not\nrequire re-training on the original word embedding corpus; instead, learning is\nperformed at the type level. Intrinsic and extrinsic evaluations demonstrate\nthe power of this simple approach. On 23 languages, MIMICK improves performance\nover a word-based baseline for tagging part-of-speech and morphosyntactic\nattributes. It is competitive with (and complementary to) a supervised\ncharacter-based model in low-resource settings.", "authors": ["Yuval Pinter", "Robert Guthrie", "Jacob Eisenstein"], "category": "cs.CL", "comment": "EMNLP 2017", "img": "/static/thumbs/1707.06961v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1707.06961v1", "num_discussion": 0, "originally_published_time": "7/21/2017", "pid": "1707.06961v1", "published_time": "7/21/2017", "rawpid": "1707.06961", "tags": ["cs.CL"], "title": "Mimicking Word Embeddings using Subword RNNs"}, {"abstract": "Many modern NLP systems rely on word embeddings, previously trained in an\nunsupervised manner on large corpora, as base features. Efforts to obtain\nembeddings for larger chunks of text, such as sentences, have however not been\nso successful. Several attempts at learning unsupervised representations of\nsentences have not reached satisfactory enough performance to be widely\nadopted. In this paper, we show how universal sentence representations trained\nusing the supervised data of the Stanford Natural Language Inference datasets\ncan consistently outperform unsupervised methods like SkipThought vectors on a\nwide range of transfer tasks. Much like how computer vision uses ImageNet to\nobtain features, which can then be transferred to other tasks, our work tends\nto indicate the suitability of natural language inference for transfer learning\nto other NLP tasks. Our encoder is publicly available.", "authors": ["Alexis Conneau", "Douwe Kiela", "Holger Schwenk", "Loic Barrault", "Antoine Bordes"], "category": "cs.CL", "comment": "Accepted to EMNLP 2017", "img": "/static/thumbs/1705.02364v4.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1705.02364v4", "num_discussion": 0, "originally_published_time": "5/5/2017", "pid": "1705.02364v4", "published_time": "7/21/2017", "rawpid": "1705.02364", "tags": ["cs.CL"], "title": "Supervised Learning of Universal Sentence Representations from Natural\n  Language Inference Data"}, {"abstract": "Recent captioning models are limited in their ability to scale and describe\nconcepts unseen in paired image-text corpora. We propose the Novel Object\nCaptioner (NOC), a deep visual semantic captioning model that can describe a\nlarge number of object categories not present in existing image-caption\ndatasets. Our model takes advantage of external sources -- labeled images from\nobject recognition datasets, and semantic knowledge extracted from unannotated\ntext. We propose minimizing a joint objective which can learn from these\ndiverse data sources and leverage distributional semantic embeddings, enabling\nthe model to generalize and describe novel objects outside of image-caption\ndatasets. We demonstrate that our model exploits semantic information to\ngenerate captions for hundreds of object categories in the ImageNet object\nrecognition dataset that are not observed in MSCOCO image-caption training\ndata, as well as many categories that are observed very rarely. Both automatic\nevaluations and human judgements show that our model considerably outperforms\nprior work in being able to describe many more categories of objects.", "authors": ["Subhashini Venugopalan", "Lisa Anne Hendricks", "Marcus Rohrbach", "Raymond Mooney", "Trevor Darrell", "Kate Saenko"], "category": "cs.CV", "comment": "CVPR 2017 Camera ready version. 17 pages (8 + 9 supplement), 12\n  figures, 8 tables. Includes projec...", "img": "/static/thumbs/1606.07770v3.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1606.07770v3", "num_discussion": 0, "originally_published_time": "6/24/2016", "pid": "1606.07770v3", "published_time": "7/20/2017", "rawpid": "1606.07770", "tags": ["cs.CV", "cs.CL"], "title": "Captioning Images with Diverse Objects"}, {"abstract": "We introduce a variety of models, trained on a supervised image captioning\ncorpus to predict the image features for a given caption, to perform sentence\nrepresentation grounding. We train a grounded sentence encoder that achieves\ngood performance on COCO caption and image retrieval and subsequently show that\nthis encoder can successfully be transferred to various NLP tasks, with\nimproved performance over text-only models. Lastly, we analyze the contribution\nof grounding, and show that word embeddings learned by this system outperform\nnon-grounded ones.", "authors": ["Douwe Kiela", "Alexis Conneau", "Allan Jabri", "Maximilian Nickel"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1707.06320v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1707.06320v1", "num_discussion": 0, "originally_published_time": "7/19/2017", "pid": "1707.06320v1", "published_time": "7/19/2017", "rawpid": "1707.06320", "tags": ["cs.CL", "cs.CV"], "title": "Learning Visually Grounded Sentence Representations"}, {"abstract": "Existing image captioning models do not generalize well to out-of-domain\nimages containing novel scenes or objects. This limitation severely hinders the\nuse of these models in real world applications dealing with images in the wild.\nWe address this problem using a flexible approach that enables existing deep\ncaptioning architectures to take advantage of image taggers at test time,\nwithout re-training. Our method uses constrained beam search to force the\ninclusion of selected tag words in the output, and fixed, pretrained word\nembeddings to facilitate vocabulary expansion to previously unseen tag words.\nUsing this approach we achieve state of the art results for out-of-domain\ncaptioning on MSCOCO (and improved results for in-domain captioning). Perhaps\nsurprisingly, our results significantly outperform approaches that incorporate\nthe same tag predictions into the learning algorithm. We also show that we can\nsignificantly improve the quality of generated ImageNet captions by leveraging\nground-truth labels.", "authors": ["Peter Anderson", "Basura Fernando", "Mark Johnson", "Stephen Gould"], "category": "cs.CV", "comment": "EMNLP 2017", "img": "/static/thumbs/1612.00576v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1612.00576v2", "num_discussion": 0, "originally_published_time": "12/2/2016", "pid": "1612.00576v2", "published_time": "7/19/2017", "rawpid": "1612.00576", "tags": ["cs.CV"], "title": "Guided Open Vocabulary Image Captioning with Constrained Beam Search"}, {"abstract": "In this paper, we introduce the novel concept of densely connected layers\ninto recurrent neural networks. We evaluate our proposed architecture on the\nPenn Treebank language modeling task. We show that we can obtain similar\nperplexity scores with six times fewer parameters compared to a standard\nstacked 2-layer LSTM model trained with dropout (Zaremba et al. 2014). In\ncontrast with the current usage of skip connections, we show that densely\nconnecting only a few stacked layers with skip connections already yields\nsignificant perplexity reductions.", "authors": ["Fr\u00e9deric Godin", "Joni Dambre", "Wesley De Neve"], "category": "cs.CL", "comment": "Accepted at Workshop on Representation Learning, ACL2017", "img": "/static/thumbs/1707.06130v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1707.06130v1", "num_discussion": 0, "originally_published_time": "7/19/2017", "pid": "1707.06130v1", "published_time": "7/19/2017", "rawpid": "1707.06130", "tags": ["cs.CL"], "title": "Improving Language Modeling using Densely Connected Recurrent Neural\n  Networks"}, {"abstract": "The attention mechanisms in deep neural networks are inspired by human's\nattention that sequentially focuses on the most relevant parts of the\ninformation over time to generate prediction output. The attention parameters\nin those models are implicitly trained in an end-to-end manner, yet there have\nbeen few trials to explicitly incorporate human gaze tracking to supervise the\nattention models. In this paper, we investigate whether attention models can\nbenefit from explicit human gaze labels, especially for the task of video\ncaptioning. We collect a new dataset called VAS, consisting of movie clips, and\ncorresponding multiple descriptive sentences along with human gaze tracking\ndata. We propose a video captioning model named Gaze Encoding Attention Network\n(GEAN) that can leverage gaze tracking information to provide the spatial and\ntemporal attention for sentence generation. Through evaluation of language\nsimilarity metrics and human assessment via Amazon mechanical Turk, we\ndemonstrate that spatial attentions guided by human gaze data indeed improve\nthe performance of multiple captioning methods. Moreover, we show that the\nproposed approach achieves the state-of-the-art performance for both gaze\nprediction and video captioning not only in our VAS dataset but also in\nstandard datasets (e.g. LSMDC and Hollywood2).", "authors": ["Youngjae Yu", "Jongwook Choi", "Yeonhwa Kim", "Kyung Yoo", "Sang-Hun Lee", "Gunhee Kim"], "category": "cs.CV", "comment": "In CVPR 2017. 9 pages + supplementary 17 pages", "img": "/static/thumbs/1707.06029v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1707.06029v1", "num_discussion": 0, "originally_published_time": "7/19/2017", "pid": "1707.06029v1", "published_time": "7/19/2017", "rawpid": "1707.06029", "tags": ["cs.CV"], "title": "Supervising Neural Attention Models for Video Captioning by Human Gaze\n  Data"}, {"abstract": "Generative Adversarial Networks (GANs) have been shown to be able to sample\nimpressively realistic images. GAN training consists of a saddle point\noptimization problem that can be thought of as an adversarial game between a\ngenerator which produces the images, and a discriminator, which judges if the\nimages are real. Both the generator and the discriminator are commonly\nparametrized as deep convolutional neural networks. The goal of this paper is\nto disentangle the contribution of the optimization procedure and the network\nparametrization to the success of GANs. To this end we introduce and study\nGenerative Latent Optimization (GLO), a framework to train a generator without\nthe need to learn a discriminator, thus avoiding challenging adversarial\noptimization problems. We show experimentally that GLO enjoys many of the\ndesirable properties of GANs: learning from large data, synthesizing\nvisually-appealing samples, interpolating meaningfully between samples, and\nperforming linear arithmetic with noise vectors.", "authors": ["Piotr Bojanowski", "Armand Joulin", "David Lopez-Paz", "Arthur Szlam"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1707.05776v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1707.05776v1", "num_discussion": 0, "originally_published_time": "7/18/2017", "pid": "1707.05776v1", "published_time": "7/18/2017", "rawpid": "1707.05776", "tags": ["stat.ML", "cs.CV", "cs.LG"], "title": "Optimizing the Latent Space of Generative Networks"}, {"abstract": "We propose an algorithm for meta-learning that is model-agnostic, in the\nsense that it is compatible with any model trained with gradient descent and\napplicable to a variety of different learning problems, including\nclassification, regression, and reinforcement learning. The goal of\nmeta-learning is to train a model on a variety of learning tasks, such that it\ncan solve new learning tasks using only a small number of training samples. In\nour approach, the parameters of the model are explicitly trained such that a\nsmall number of gradient steps with a small amount of training data from a new\ntask will produce good generalization performance on that task. In effect, our\nmethod trains the model to be easy to fine-tune. We demonstrate that this\napproach leads to state-of-the-art performance on two few-shot image\nclassification benchmarks, produces good results on few-shot regression, and\naccelerates fine-tuning for policy gradient reinforcement learning with neural\nnetwork policies.", "authors": ["Chelsea Finn", "Pieter Abbeel", "Sergey Levine"], "category": "cs.LG", "comment": "ICML 2017. Code at https://github.com/cbfinn/maml, Videos of RL\n  results at https://sites.google.co...", "img": "/static/thumbs/1703.03400v3.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1703.03400v3", "num_discussion": 1, "originally_published_time": "3/9/2017", "pid": "1703.03400v3", "published_time": "7/18/2017", "rawpid": "1703.03400", "tags": ["cs.LG", "cs.AI", "cs.CV", "cs.NE"], "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"}, {"abstract": "We present a method for generating a video of a talking face. The method\ntakes as inputs: (i) still images of the target face, and (ii) an audio speech\nsegment; and outputs a video of the target face lip synched with the audio. The\nmethod runs in real time and is applicable to faces and audio not seen at\ntraining time.\n  To achieve this we propose an encoder-decoder CNN model that uses a joint\nembedding of the face and audio to generate synthesised talking face video\nframes. The model is trained on tens of hours of unlabelled videos.\n  We also show results of re-dubbing videos using speech from a different\nperson.", "authors": ["Joon Son Chung", "Amir Jamaludin", "Andrew Zisserman"], "category": "cs.CV", "comment": "https://youtu.be/LeufDSb15Kc British Machine Vision Conference\n  (BMVC), 2017", "img": "/static/thumbs/1705.02966v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1705.02966v2", "num_discussion": 0, "originally_published_time": "5/8/2017", "pid": "1705.02966v2", "published_time": "7/18/2017", "rawpid": "1705.02966", "tags": ["cs.CV"], "title": "You said that?"}, {"abstract": "This paper exploits a memory-augmented neural network to predict accurate\nanswers to visual questions, even when those answers occur rarely in the\ntraining set. The memory network incorporates both internal and external memory\nblocks and selectively pays attention to each training exemplar. We show that\nmemory-augmented neural networks are able to maintain a relatively long-term\nmemory of scarce training exemplars, which is important for visual question\nanswering due to the heavy-tailed distribution of answers in a general VQA\nsetting. Experimental results on two large scale benchmark datasets show the\nfavorable performance of the proposed algorithm with a comparison to state of\nthe art.", "authors": ["Chao Ma", "Chunhua Shen", "Anthony Dick", "Anton van den Hengel"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1707.04968v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1707.04968v1", "num_discussion": 0, "originally_published_time": "7/17/2017", "pid": "1707.04968v1", "published_time": "7/17/2017", "rawpid": "1707.04968", "tags": ["cs.CV", "cs.CL"], "title": "Visual Question Answering with Memory-Augmented Networks"}, {"abstract": "Deep generative models have achieved impressive success in recent years.\nGenerative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as\npowerful frameworks for deep generative model learning, have largely been\nconsidered as two distinct paradigms and received extensive independent study\nrespectively. This paper establishes formal connections between deep generative\nmodeling approaches through a new formulation of GANs and VAEs. We show that\nGANs and VAEs are essentially minimizing KL divergences of respective posterior\nand inference distributions with opposite directions, extending the two\nlearning phases of classic wake-sleep algorithm, respectively. The unified view\nprovides a powerful tool to analyze a diverse set of existing model variants,\nand enables to exchange ideas across research lines in a principled way. For\nexample, we transfer the importance weighting method in VAE literatures for\nimproved GAN learning, and enhance VAEs with an adversarial mechanism for\nleveraging generated samples. Quantitative experiments show generality and\neffectiveness of the imported extensions.", "authors": ["Zhiting Hu", "Zichao Yang", "Ruslan Salakhutdinov", "Eric P. Xing"], "category": "cs.LG", "comment": "major revision: added more materials/discussions/figures, fixed typos", "img": "/static/thumbs/1706.00550v3.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1706.00550v3", "num_discussion": 0, "originally_published_time": "6/2/2017", "pid": "1706.00550v3", "published_time": "7/16/2017", "rawpid": "1706.00550", "tags": ["cs.LG", "stat.ML"], "title": "On Unifying Deep Generative Models"}, {"abstract": "In this paper, we describe our submissions to the WMT17 Multimodal\nTranslation Task. For Task 1 (multimodal translation), our best scoring system\nis a purely textual neural translation of the source image caption to the\ntarget language. The main feature of the system is the use of additional data\nthat was acquired by selecting similar sentences from parallel corpora and by\ndata synthesis with back-translation. For Task 2 (cross-lingual image\ncaptioning), our best submitted system generates an English caption which is\nthen translated by the best system used in Task 1. We also present negative\nresults, which are based on ideas that we believe have potential of making\nimprovements, but did not prove to be useful in our particular setup.", "authors": ["Jind\u0159ich Helcl", "Jind\u0159ich Libovick\u00fd"], "category": "cs.CL", "comment": "8 pages; Camera-ready submission to WMT17", "img": "/static/thumbs/1707.04550v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1707.04550v1", "num_discussion": 0, "originally_published_time": "7/14/2017", "pid": "1707.04550v1", "published_time": "7/14/2017", "rawpid": "1707.04550", "tags": ["cs.CL", "cs.NE", "I.2.7"], "title": "CUNI System for the WMT17 Multimodal Translation Task"}, {"abstract": "This paper describes the monomodal and multimodal Neural Machine Translation\nsystems developed by LIUM and CVC for WMT17 Shared Task on Multimodal\nTranslation. We mainly explored two multimodal architectures where either\nglobal visual features or convolutional feature maps are integrated in order to\nbenefit from visual context. Our final systems ranked first for both En-De and\nEn-Fr language pairs according to the automatic evaluation metrics METEOR and\nBLEU.", "authors": ["Ozan Caglayan", "Walid Aransa", "Adrien Bardet", "Mercedes Garc\u00eda-Mart\u00ednez", "Fethi Bougares", "Lo\u00efc Barrault", "Marc Masana", "Luis Herranz", "Joost van de Weijer"], "category": "cs.CL", "comment": "MMT System Description Paper for WMT17", "img": "/static/thumbs/1707.04481v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1707.04481v1", "num_discussion": 0, "originally_published_time": "7/14/2017", "pid": "1707.04481v1", "published_time": "7/14/2017", "rawpid": "1707.04481", "tags": ["cs.CL"], "title": "LIUM-CVC Submissions for WMT17 Multimodal Translation Task"}, {"abstract": "We study in this work the importance of depth in convolutional models for\ntext classification, either when character or word inputs are considered. We\nshow on 5 standard text classification and sentiment analysis tasks that deep\nmodels indeed give better performances than shallow networks when the text\ninput is represented as a sequence of characters. However, a simple\nshallow-and-wide network outperforms deep models such as DenseNet with word\ninputs. Our shallow word model further establishes new state-of-the-art\nperformances on two datasets: Yelp Binary (95.9\\%) and Yelp Full (64.9\\%).", "authors": ["Hoa T. Le", "Christophe Cerisara", "Alexandre Denis"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1707.04108v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1707.04108v1", "num_discussion": 0, "originally_published_time": "7/13/2017", "pid": "1707.04108v1", "published_time": "7/13/2017", "rawpid": "1707.04108", "tags": ["cs.CL"], "title": "Do Convolutional Networks need to be Deep for Text Classification ?"}, {"abstract": "The natural world is infinitely diverse, yet this diversity arises from a\nrelatively small set of coherent properties and rules, such as the laws of\nphysics or chemistry. We conjecture that biological intelligent systems are\nable to survive within their diverse environments by discovering the\nregularities that arise from these rules primarily through unsupervised\nexperiences, and representing this knowledge as abstract concepts. Such\nrepresentations possess useful properties of compositionality and hierarchical\norganisation, which allow intelligent agents to recombine a finite set of\nconceptual building blocks into an exponentially large set of useful new\nconcepts. This paper describes SCAN (Symbol-Concept Association Network), a new\nframework for learning such concepts in the visual domain. We first use the\npreviously published beta-VAE (Higgins et al., 2017a) architecture to learn a\ndisentangled representation of the latent structure of the visual world, before\ntraining SCAN to extract abstract concepts grounded in such disentangled visual\nprimitives through fast symbol association. Our approach requires very few\npairings between symbols and images and makes no assumptions about the choice\nof symbol representations. Once trained, SCAN is capable of multimodal\nbi-directional inference, generating a diverse set of image samples from\nsymbolic descriptions and vice versa. It also allows for traversal and\nmanipulation of the implicit hierarchy of compositional visual concepts through\nsymbolic instructions and learnt logical recombination operations. Such\nmanipulations enable SCAN to invent and learn novel visual concepts through\nrecombination of the few learnt concepts.", "authors": ["Irina Higgins", "Nicolas Sonnerat", "Loic Matthey", "Arka Pal", "Christopher P Burgess", "Matthew Botvinick", "Demis Hassabis", "Alexander Lerchner"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1707.03389v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1707.03389v2", "num_discussion": 0, "originally_published_time": "7/11/2017", "pid": "1707.03389v2", "published_time": "7/12/2017", "rawpid": "1707.03389", "tags": ["stat.ML", "cs.LG"], "title": "SCAN: Learning Abstract Hierarchical Compositional Visual Concepts"}, {"abstract": "This paper addresses the task of segmenting moving objects in unconstrained\nvideos. We introduce a novel two-stream neural network with an explicit memory\nmodule to achieve this. The two streams of the network encode spatial and\ntemporal features in a video sequence respectively, while the memory module\ncaptures the evolution of objects over time. The module to build a \"visual\nmemory\" in video, i.e., a joint representation of all the video frames, is\nrealized with a convolutional recurrent unit learned from a small number of\ntraining video sequences. Given a video frame as input, our approach assigns\neach pixel an object or background label based on the learned spatio-temporal\nfeatures as well as the \"visual memory\" specific to the video, acquired\nautomatically without any manually-annotated frames. The visual memory is\nimplemented with convolutional gated recurrent units, which allows to propagate\nspatial information over time. We evaluate our method extensively on two\nbenchmarks, DAVIS and Freiburg-Berkeley motion segmentation datasets, and show\nstate-of-the-art results. For example, our approach outperforms the top method\non the DAVIS dataset by nearly 6%. We also provide an extensive ablative\nanalysis to investigate the influence of each component in the proposed\nframework.", "authors": ["Pavel Tokmakov", "Karteek Alahari", "Cordelia Schmid"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1704.05737v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1704.05737v2", "num_discussion": 0, "originally_published_time": "4/19/2017", "pid": "1704.05737v2", "published_time": "7/12/2017", "rawpid": "1704.05737", "tags": ["cs.CV"], "title": "Learning Video Object Segmentation with Visual Memory"}, {"abstract": "Natural language generation (NLG) is an important component in spoken\ndialogue systems. This paper presents a model called Encoder-Aggregator-Decoder\nwhich is an extension of an Recurrent Neural Network based Encoder-Decoder\narchitecture. The proposed Semantic Aggregator consists of two components: an\nAligner and a Refiner. The Aligner is a conventional attention calculated over\nthe encoded input information, while the Refiner is another attention or gating\nmechanism stacked over the attentive Aligner in order to further select and\naggregate the semantic elements. The proposed model can be jointly trained both\nsentence planning and surface realization to produce natural language\nutterances. The model was extensively assessed on four different NLG domains,\nin which the experimental results showed that the proposed generator\nconsistently outperforms the previous methods on all the NLG domains.", "authors": ["Van-Khanh Tran", "Le-Minh Nguyen"], "category": "cs.CL", "comment": "To be appear at SIGDIAL 2017. arXiv admin note: text overlap with\n  arXiv:1706.00134, arXiv:1706.001...", "img": "/static/thumbs/1706.06714v3.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1706.06714v3", "num_discussion": 0, "originally_published_time": "6/21/2017", "pid": "1706.06714v3", "published_time": "7/11/2017", "rawpid": "1706.06714", "tags": ["cs.CL", "cs.LG"], "title": "Neural-based Natural Language Generation in Dialogue using RNN\n  Encoder-Decoder with Semantic Aggregation"}, {"abstract": "We decompose multimodal translation into two sub-tasks: learning to translate\nand learning visually grounded representations. In a multitask learning\nframework, translations are learned in an attention-based encoder-decoder, and\ngrounded representations are learned through image representation prediction.\nOur approach improves translation performance compared to the state of the art\non the Multi30K dataset. Furthermore, it is equally effective if we train the\nimage prediction task on the external MS COCO dataset, and we find improvements\nif we train the translation model on the external News Commentary parallel\ntext.", "authors": ["Desmond Elliott", "\u00c1kos K\u00e1d\u00e1r"], "category": "cs.CL", "comment": "Clarified main contributions, minor correction to Equation 8,\n  additional comparisons in Table 2, a...", "img": "/static/thumbs/1705.04350v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1705.04350v2", "num_discussion": 0, "originally_published_time": "5/11/2017", "pid": "1705.04350v2", "published_time": "7/7/2017", "rawpid": "1705.04350", "tags": ["cs.CL", "cs.CV"], "title": "Imagination improves Multimodal Translation"}, {"abstract": "Do GANS (Generative Adversarial Nets) actually learn the target distribution?\nThe foundational paper of (Goodfellow et al 2014) suggested they do, if they\nwere given sufficiently large deep nets, sample size, and computation time. A\nrecent theoretical analysis in Arora et al (to appear at ICML 2017) raised\ndoubts whether the same holds when discriminator has finite size. It showed\nthat the training objective can approach its optimum value even if the\ngenerated distribution has very low support ---in other words, the training\nobjective is unable to prevent mode collapse. The current note reports\nexperiments suggesting that such problems are not merely theoretical. It\npresents empirical evidence that well-known GANs approaches do learn\ndistributions of fairly low support, and thus presumably are not learning the\ntarget distribution. The main technical contribution is a new proposed test,\nbased upon the famous birthday paradox, for estimating the support size of the\ngenerated distribution.", "authors": ["Sanjeev Arora", "Yi Zhang"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1706.08224v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1706.08224v2", "num_discussion": 0, "originally_published_time": "6/26/2017", "pid": "1706.08224v2", "published_time": "7/1/2017", "rawpid": "1706.08224", "tags": ["cs.LG"], "title": "Do GANs actually learn the distribution? An empirical study"}, {"abstract": "We propose a new neural sequence model training method in which the objective\nfunction is defined by $\\alpha$-divergence. We demonstrate that the objective\nfunction generalizes the maximum-likelihood (ML)-based and reinforcement\nlearning (RL)-based objective functions as special cases (i.e., ML corresponds\nto $\\alpha \\to 0$ and RL to $\\alpha \\to1$). We also show that the gradient of\nthe objective function can be considered a mixture of ML- and RL-based\nobjective gradients. The experimental results of a machine translation task\nshow that minimizing the objective function with $\\alpha > 0$ outperforms\n$\\alpha \\to 0$, which corresponds to ML-based methods.", "authors": ["Sotetsu Koyamada", "Yuta Kikuchi", "Atsunori Kanemura", "Shin-ichi Maeda", "Shin Ishii"], "category": "stat.ML", "comment": "2017 ICML Workshop on Learning to Generate Natural Language (LGNL\n  2017)", "img": "/static/thumbs/1706.10031v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1706.10031v1", "num_discussion": 0, "originally_published_time": "6/30/2017", "pid": "1706.10031v1", "published_time": "6/30/2017", "rawpid": "1706.10031", "tags": ["stat.ML", "cs.LG"], "title": "Neural Sequence Model Training via $\u03b1$-divergence Minimization"}, {"abstract": "Recurrent neural network models with an attention mechanism have proven to be\nextremely effective on a wide variety of sequence-to-sequence problems.\nHowever, the fact that soft attention mechanisms perform a pass over the entire\ninput sequence when producing each element in the output sequence precludes\ntheir use in online settings and results in a quadratic time complexity. Based\non the insight that the alignment between input and output sequence elements is\nmonotonic in many problems of interest, we propose an end-to-end differentiable\nmethod for learning monotonic alignments which, at test time, enables computing\nattention online and in linear time. We validate our approach on sentence\nsummarization, machine translation, and online speech recognition problems and\nachieve results competitive with existing sequence-to-sequence models.", "authors": ["Colin Raffel", "Minh-Thang Luong", "Peter J. Liu", "Ron J. Weiss", "Douglas Eck"], "category": "cs.LG", "comment": "ICML camera-ready version; 10 pages + 9 page appendix", "img": "/static/thumbs/1704.00784v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1704.00784v2", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00784v2", "published_time": "6/29/2017", "rawpid": "1704.00784", "tags": ["cs.LG", "cs.CL"], "title": "Online and Linear-Time Attention by Enforcing Monotonic Alignments"}, {"abstract": "In this paper we propose a neural network model with a novel Sequential\nAttention layer that extends soft attention by assigning weights to words in an\ninput sequence in a way that takes into account not just how well that word\nmatches a query, but how well surrounding words match. We evaluate this\napproach on the task of reading comprehension (on the Who did What and CNN\ndatasets) and show that it dramatically improves a strong baseline--the\nStanford Reader--and is competitive with the state of the art.", "authors": ["Sebastian Brarda", "Philip Yeres", "Samuel R. Bowman"], "category": "cs.CL", "comment": "To appear in ACL 2017 2nd Workshop on Representation Learning for\n  NLP. Contains additional experim...", "img": "/static/thumbs/1705.02269v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1705.02269v2", "num_discussion": 0, "originally_published_time": "5/5/2017", "pid": "1705.02269v2", "published_time": "6/26/2017", "rawpid": "1705.02269", "tags": ["cs.CL", "cs.LG"], "title": "Sequential Attention: A Context-Aware Alignment Function for Machine\n  Reading"}, {"abstract": "State-of-the-art approaches for image captioning require supervised training\ndata consisting of captions with paired image data. These methods are typically\nunable to use unsupervised data such as textual data with no corresponding\nimages, which is a much more abundant commodity. We here propose a novel way of\nusing such textual data by artificially generating missing visual information.\nWe evaluate this learning approach on a newly designed model that detects\nvisual concepts present in an image and feed them to a reviewer-decoder\narchitecture with an attention mechanism. Unlike previous approaches that\nencode visual concepts using word embeddings, we instead suggest using regional\nimage features which capture more intrinsic information. The main benefit of\nthis architecture is that it synthesizes meaningful thought vectors that\ncapture salient image properties and then applies a soft attentive decoder to\ndecode the thought vectors and generate image captions. We evaluate our model\non both Microsoft COCO and Flickr30K datasets and demonstrate that this model\ncombined with our semi-supervised learning method can largely improve\nperformance and help the model to generate more accurate and diverse captions.", "authors": ["Wenhu Chen", "Aurelien Lucchi", "Thomas Hofmann"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1611.05321v3.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1611.05321v3", "num_discussion": 0, "originally_published_time": "11/16/2016", "pid": "1611.05321v3", "published_time": "6/24/2017", "rawpid": "1611.05321", "tags": ["cs.CV"], "title": "A Semi-supervised Framework for Image Captioning"}, {"abstract": "In this paper, we study a new learning paradigm for Neural Machine\nTranslation (NMT). Instead of maximizing the likelihood of the human\ntranslation as in previous works, we minimize the distinction between human\ntranslation and the translation given by an NMT model. To achieve this goal,\ninspired by the recent success of generative adversarial networks (GANs), we\nemploy an adversarial training architecture and name it as Adversarial-NMT. In\nAdversarial-NMT, the training of the NMT model is assisted by an adversary,\nwhich is an elaborately designed Convolutional Neural Network (CNN). The goal\nof the adversary is to differentiate the translation result generated by the\nNMT model from that by human. The goal of the NMT model is to produce high\nquality translations so as to cheat the adversary. A policy gradient method is\nleveraged to co-train the NMT model and the adversary. Experimental results on\nEnglish$\\rightarrow$French and German$\\rightarrow$English translation tasks\nshow that Adversarial-NMT can achieve significantly better translation quality\nthan several strong baselines.", "authors": ["Lijun Wu", "Yingce Xia", "Li Zhao", "Fei Tian", "Tao Qin", "Jianhuang Lai", "Tie-Yan Liu"], "category": "cs.CL", "comment": "11 pages, 4 figures, 4 tables", "img": "/static/thumbs/1704.06933v3.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1704.06933v3", "num_discussion": 0, "originally_published_time": "4/20/2017", "pid": "1704.06933v3", "published_time": "6/24/2017", "rawpid": "1704.06933", "tags": ["cs.CL", "cs.LG", "stat.ML"], "title": "Adversarial Neural Machine Translation"}, {"abstract": "Recent work in computer vision has yielded impressive results in\nautomatically describing images with natural language. Most of these systems\ngenerate captions in a sin- gle language, requiring multiple language-specific\nmodels to build a multilingual captioning system. We propose a very simple\ntechnique to build a single unified model across languages, using artificial\ntokens to control the language, making the captioning system more compact. We\nevaluate our approach on generating English and Japanese captions, and show\nthat a typical neural captioning architecture is capable of learning a single\nmodel that can switch between two different languages.", "authors": ["Satoshi Tsutsui", "David Crandall"], "category": "cs.CV", "comment": "This work appears as an Extended Abstract at the 2017 CVPR Language\n  and Vision Workshop", "img": "/static/thumbs/1706.06275v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1706.06275v1", "num_discussion": 0, "originally_published_time": "6/20/2017", "pid": "1706.06275v1", "published_time": "6/20/2017", "rawpid": "1706.06275", "tags": ["cs.CV"], "title": "Using Artificial Tokens to Control Languages for Multilingual Image\n  Caption Generation"}, {"abstract": "Recent work on generative modeling of text has found that variational\nauto-encoders (VAE) incorporating LSTM decoders perform worse than simpler LSTM\nlanguage models (Bowman et al., 2015). This negative result is so far poorly\nunderstood, but has been attributed to the propensity of LSTM decoders to\nignore conditioning information from the encoder. In this paper, we experiment\nwith a new type of decoder for VAE: a dilated CNN. By changing the decoder's\ndilation architecture, we control the effective context from previously\ngenerated words. In experiments, we find that there is a trade off between the\ncontextual capacity of the decoder and the amount of encoding information used.\nWe show that with the right decoder, VAE can outperform LSTM language models.\nWe demonstrate perplexity gains on two datasets, representing the first\npositive experimental result on the use VAE for generative modeling of text.\nFurther, we conduct an in-depth investigation of the use of VAE (with our new\ndecoding architecture) for semi-supervised and unsupervised labeling tasks,\ndemonstrating gains over several strong baselines.", "authors": ["Zichao Yang", "Zhiting Hu", "Ruslan Salakhutdinov", "Taylor Berg-Kirkpatrick"], "category": "cs.NE", "comment": "camera ready", "img": "/static/thumbs/1702.08139v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1702.08139v2", "num_discussion": 0, "originally_published_time": "2/27/2017", "pid": "1702.08139v2", "published_time": "6/18/2017", "rawpid": "1702.08139", "tags": ["cs.NE", "cs.CL", "cs.LG"], "title": "Improved Variational Autoencoders for Text Modeling using Dilated\n  Convolutions"}, {"abstract": "Deep learning yields great results across many fields, from speech\nrecognition, image classification, to translation. But for each problem,\ngetting a deep model to work well involves research into the architecture and a\nlong period of tuning. We present a single model that yields good results on a\nnumber of problems spanning multiple domains. In particular, this single model\nis trained concurrently on ImageNet, multiple translation tasks, image\ncaptioning (COCO dataset), a speech recognition corpus, and an English parsing\ntask. Our model architecture incorporates building blocks from multiple\ndomains. It contains convolutional layers, an attention mechanism, and\nsparsely-gated layers. Each of these computational blocks is crucial for a\nsubset of the tasks we train on. Interestingly, even if a block is not crucial\nfor a task, we observe that adding it never hurts performance and in most cases\nimproves it on all tasks. We also show that tasks with less data benefit\nlargely from joint training with other tasks, while performance on large tasks\ndegrades only slightly if at all.", "authors": ["Lukasz Kaiser", "Aidan N. Gomez", "Noam Shazeer", "Ashish Vaswani", "Niki Parmar", "Llion Jones", "Jakob Uszkoreit"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1706.05137v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1706.05137v1", "num_discussion": 0, "originally_published_time": "6/16/2017", "pid": "1706.05137v1", "published_time": "6/16/2017", "rawpid": "1706.05137", "tags": ["cs.LG", "stat.ML"], "title": "One Model To Learn Them All"}, {"abstract": "Depthwise separable convolutions reduce the number of parameters and\ncomputation used in convolutional operations while increasing representational\nefficiency. They have been shown to be successful in image classification\nmodels, both in obtaining better models than previously possible for a given\nparameter count (the Xception architecture) and considerably reducing the\nnumber of parameters required to perform at a given level (the MobileNets\nfamily of architectures). Recently, convolutional sequence-to-sequence networks\nhave been applied to machine translation tasks with good results. In this work,\nwe study how depthwise separable convolutions can be applied to neural machine\ntranslation. We introduce a new architecture inspired by Xception and ByteNet,\ncalled SliceNet, which enables a significant reduction of the parameter count\nand amount of computation needed to obtain results like ByteNet, and, with a\nsimilar parameter count, achieves new state-of-the-art results. In addition to\nshowing that depthwise separable convolutions perform well for machine\ntranslation, we investigate the architectural changes that they enable: we\nobserve that thanks to depthwise separability, we can increase the length of\nconvolution windows, removing the need for filter dilation. We also introduce a\nnew \"super-separable\" convolution operation that further reduces the number of\nparameters and computational cost for obtaining state-of-the-art results.", "authors": ["Lukasz Kaiser", "Aidan N. Gomez", "Francois Chollet"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1706.03059v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1706.03059v2", "num_discussion": 1, "originally_published_time": "6/9/2017", "pid": "1706.03059v2", "published_time": "6/16/2017", "rawpid": "1706.03059", "tags": ["cs.CL", "cs.LG"], "title": "Depthwise Separable Convolutions for Neural Machine Translation"}, {"abstract": "Gradient descent optimization algorithms, while increasingly popular, are\noften used as black-box optimizers, as practical explanations of their\nstrengths and weaknesses are hard to come by. This article aims to provide the\nreader with intuitions with regard to the behaviour of different algorithms\nthat will allow her to put them to use. In the course of this overview, we look\nat different variants of gradient descent, summarize challenges, introduce the\nmost common optimization algorithms, review architectures in a parallel and\ndistributed setting, and investigate additional strategies for optimizing\ngradient descent.", "authors": ["Sebastian Ruder"], "category": "cs.LG", "comment": "Added derivations of AdaMax and Nadam", "img": "/static/thumbs/1609.04747v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1609.04747v2", "num_discussion": 0, "originally_published_time": "9/15/2016", "pid": "1609.04747v2", "published_time": "6/15/2017", "rawpid": "1609.04747", "tags": ["cs.LG"], "title": "An overview of gradient descent optimization algorithms"}, {"abstract": "In the last few years, Recurrent Neural Networks (RNNs) have proved effective\non several NLP tasks. Despite such great success, their ability to model\n\\emph{sequence labeling} is still limited. This lead research toward solutions\nwhere RNNs are combined with models which already proved effective in this\ndomain, such as CRFs. In this work we propose a solution far simpler but very\neffective: an evolution of the simple Jordan RNN, where labels are re-injected\nas input into the network, and converted into embeddings, in the same way as\nwords. We compare this RNN variant to all the other RNN models, Elman and\nJordan RNN, LSTM and GRU, on two well-known tasks of Spoken Language\nUnderstanding (SLU). Thanks to label embeddings and their combination at the\nhidden layer, the proposed variant, which uses more parameters than Elman and\nJordan RNNs, but far fewer than LSTM and GRU, is more effective than other\nRNNs, but also outperforms sophisticated CRF models.", "authors": ["Yoann Dupont", "Marco Dinarelli", "Isabelle Tellier"], "category": "cs.CL", "comment": "22 pages, 3 figures. Accepted at CICling 2017 conference. Best\n  Verifiability, Reproducibility, and...", "img": "/static/thumbs/1706.01740v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1706.01740v1", "num_discussion": 0, "originally_published_time": "6/6/2017", "pid": "1706.01740v1", "published_time": "6/6/2017", "rawpid": "1706.01740", "tags": ["cs.CL"], "title": "Label-Dependencies Aware Recurrent Neural Networks"}, {"abstract": "Attention-based neural encoder-decoder frameworks have been widely adopted\nfor image captioning. Most methods force visual attention to be active for\nevery generated word. However, the decoder likely requires little to no visual\ninformation from the image to predict non-visual words such as \"the\" and \"of\".\nOther words that may seem visual can often be predicted reliably just from the\nlanguage model e.g., \"sign\" after \"behind a red stop\" or \"phone\" following\n\"talking on a cell\". In this paper, we propose a novel adaptive attention model\nwith a visual sentinel. At each time step, our model decides whether to attend\nto the image (and if so, to which regions) or to the visual sentinel. The model\ndecides whether to attend to the image and where, in order to extract\nmeaningful information for sequential word generation. We test our method on\nthe COCO image captioning 2015 challenge dataset and Flickr30K. Our approach\nsets the new state-of-the-art by a significant margin.", "authors": ["Jiasen Lu", "Caiming Xiong", "Devi Parikh", "Richard Socher"], "category": "cs.CV", "comment": "12 pages, 11 figures, CVPR2017 camera ready", "img": "/static/thumbs/1612.01887v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1612.01887v2", "num_discussion": 0, "originally_published_time": "12/6/2016", "pid": "1612.01887v2", "published_time": "6/6/2017", "rawpid": "1612.01887", "tags": ["cs.CV", "cs.AI"], "title": "Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image\n  Captioning"}, {"abstract": "Words in natural language follow a Zipfian distribution whereby some words\nare frequent but most are rare. Learning representations for words in the \"long\ntail\" of this distribution requires enormous amounts of data. Representations\nof rare words trained directly on end-tasks are usually poor, requiring us to\npre-train embeddings on external data, or treat all rare words as\nout-of-vocabulary words with a unique representation. We provide a method for\npredicting embeddings of rare words on the fly from small amounts of auxiliary\ndata with a network trained against the end task. We show that this improves\nresults against baselines where embeddings are trained on the end task in a\nreading comprehension task, a recognizing textual entailment task, and in\nlanguage modelling.", "authors": ["Dzmitry Bahdanau", "Tom Bosc", "Stanis\u0142aw Jastrz\u0119bski", "Edward Grefenstette", "Pascal Vincent", "Yoshua Bengio"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1706.00286v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1706.00286v2", "num_discussion": 0, "originally_published_time": "6/1/2017", "pid": "1706.00286v2", "published_time": "6/5/2017", "rawpid": "1706.00286", "tags": ["cs.LG", "cs.CL"], "title": "Learning to Compute Word Embeddings On the Fly"}, {"abstract": "Robots will eventually be part of every household. It is thus critical to\nenable algorithms to learn from and be guided by non-expert users. In this\npaper, we bring a human in the loop, and enable a human teacher to give\nfeedback to a learning agent in the form of natural language. We argue that a\ndescriptive sentence can provide a much stronger learning signal than a numeric\nreward in that it can easily point to where the mistakes are and how to correct\nthem. We focus on the problem of image captioning in which the quality of the\noutput can easily be judged by non-experts. We propose a hierarchical\nphrase-based captioning model trained with policy gradients, and design a\nfeedback network that provides reward to the learner by conditioning on the\nhuman-provided feedback. We show that by exploiting descriptive feedback our\nmodel learns to perform better than when given independently written human\ncaptions.", "authors": ["Huan Ling", "Sanja Fidler"], "category": "cs.CL", "comment": "13 pages", "img": "/static/thumbs/1706.00130v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1706.00130v2", "num_discussion": 0, "originally_published_time": "6/1/2017", "pid": "1706.00130v2", "published_time": "6/5/2017", "rawpid": "1706.00130", "tags": ["cs.CL", "cs.AI", "cs.CV", "cs.HC"], "title": "Teaching Machines to Describe Images via Natural Language Feedback"}, {"abstract": "Translating information between text and image is a fundamental problem in\nartificial intelligence that connects natural language processing and computer\nvision. In the past few years, performance in image caption generation has seen\nsignificant improvement through the adoption of recurrent neural networks\n(RNN). Meanwhile, text-to-image generation begun to generate plausible images\nusing datasets of specific categories like birds and flowers. We've even seen\nimage generation from multi-category datasets such as the Microsoft Common\nObjects in Context (MSCOCO) through the use of generative adversarial networks\n(GANs). Synthesizing objects with a complex shape, however, is still\nchallenging. For example, animals and humans have many degrees of freedom,\nwhich means that they can take on many complex shapes. We propose a new\ntraining method called Image-Text-Image (I2T2I) which integrates text-to-image\nand image-to-text (image captioning) synthesis to improve the performance of\ntext-to-image synthesis. We demonstrate that %the capability of our method to\nunderstand the sentence descriptions, so as to I2T2I can generate better\nmulti-categories images using MSCOCO than the state-of-the-art. We also\ndemonstrate that I2T2I can achieve transfer learning by using a pre-trained\nimage captioning module to generate human images on the MPII Human Pose", "authors": ["Hao Dong", "Jingqing Zhang", "Douglas McIlwraith", "Yike Guo"], "category": "cs.CV", "comment": "International Conference on Image Processing (ICIP) 2017", "img": "/static/thumbs/1703.06676v3.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1703.06676v3", "num_discussion": 0, "originally_published_time": "3/20/2017", "pid": "1703.06676v3", "published_time": "6/3/2017", "rawpid": "1703.06676", "tags": ["cs.CV", "cs.CL"], "title": "I2T2I: Learning Text to Image Synthesis with Textual Data Augmentation"}, {"abstract": "Generative Adversarial Networks (GANs) have gathered a lot of attention from\nthe computer vision community, yielding impressive results for image\ngeneration. Advances in the adversarial generation of natural language from\nnoise however are not commensurate with the progress made in generating images,\nand still lag far behind likelihood based methods. In this paper, we take a\nstep towards generating natural language with a GAN objective alone. We\nintroduce a simple baseline that addresses the discrete output space problem\nwithout relying on gradient estimators and show that it is able to achieve\nstate-of-the-art results on a Chinese poem generation dataset. We present\nquantitative results on generating sentences from context-free and\nprobabilistic context-free grammars, and qualitative language modeling results.\nA conditional version is also described that can generate sequences conditioned\non sentence characteristics.", "authors": ["Sai Rajeswar", "Sandeep Subramanian", "Francis Dutil", "Christopher Pal", "Aaron Courville"], "category": "cs.CL", "comment": "11 pages, 3 figures, 5 tables", "img": "/static/thumbs/1705.10929v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1705.10929v1", "num_discussion": 0, "originally_published_time": "5/31/2017", "pid": "1705.10929v1", "published_time": "5/31/2017", "rawpid": "1705.10929", "tags": ["cs.CL", "cs.AI", "cs.NE", "stat.ML"], "title": "Adversarial Generation of Natural Language"}, {"abstract": "Generic text embeddings are successfully used in a variety of tasks. However,\nthey are often learnt by capturing the co-occurrence structure from pure text\ncorpora, resulting in limitations of their ability to generalize. In this\npaper, we explore models that incorporate visual information into the text\nrepresentation. Based on comprehensive ablation studies, we propose a\nconceptually simple, yet well performing architecture. It outperforms previous\nmultimodal approaches on a set of well established benchmarks. We also improve\nthe state-of-the-art results for image-related text datasets, using orders of\nmagnitude less data.", "authors": ["Karol Kurach", "Sylvain Gelly", "Michal Jastrzebski", "Philip Haeusser", "Olivier Teytaud", "Damien Vincent", "Olivier Bousquet"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1705.08386v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1705.08386v2", "num_discussion": 0, "originally_published_time": "5/23/2017", "pid": "1705.08386v2", "published_time": "5/26/2017", "rawpid": "1705.08386", "tags": ["cs.CL", "cs.CV"], "title": "Better Text Understanding Through Image-To-Text Transfer"}, {"abstract": "Deep learning exploits large volumes of labeled data to learn powerful\nmodels. When the target dataset is small, it is a common practice to perform\ntransfer learning using pre-trained models to learn new task specific\nrepresentations. However, pre-trained CNNs for image recognition are provided\nwith limited information about the image during training, which is label alone.\nTasks such as scene retrieval suffer from features learned from this weak\nsupervision and require stronger supervision to better understand the contents\nof the image. In this paper, we exploit the features learned from caption\ngenerating models to learn novel task specific image representations. In\nparticular, we consider the state-of-the art captioning system Show and\nTell~\\cite{SnT-pami-2016} and the dense region description model\nDenseCap~\\cite{densecap-cvpr-2016}. We demonstrate that, owing to richer\nsupervision provided during the process of training, the features learned by\nthe captioning system perform better than those of CNNs. Further, we train a\nsiamese network with a modified pair-wise loss to fuse the features learned\nby~\\cite{SnT-pami-2016} and~\\cite{densecap-cvpr-2016} and learn image\nrepresentations suitable for retrieval. Experiments show that the proposed\nfusion exploits the complementary nature of the individual features and yields\nstate-of-the art retrieval results on benchmark datasets.", "authors": ["Konda Reddy Mopuri", "Vishal B. Athreya", "R. Venkatesh Babu"], "category": "cs.CV", "comment": "ICME 2017", "img": "/static/thumbs/1705.09142v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1705.09142v1", "num_discussion": 0, "originally_published_time": "5/25/2017", "pid": "1705.09142v1", "published_time": "5/25/2017", "rawpid": "1705.09142", "tags": ["cs.CV"], "title": "Deep image representations using caption generators"}, {"abstract": "We develop the first approximate inference algorithm for 1-Best (and M-Best)\ndecoding in bidirectional neural sequence models by extending Beam Search (BS)\nto reason about both forward and backward time dependencies. Beam Search (BS)\nis a widely used approximate inference algorithm for decoding sequences from\nunidirectional neural sequence models. Interestingly, approximate inference in\nbidirectional models remains an open problem, despite their significant\nadvantage in modeling information from both the past and future. To enable the\nuse of bidirectional models, we present Bidirectional Beam Search (BiBS), an\nefficient algorithm for approximate bidirectional inference.To evaluate our\nmethod and as an interesting problem in its own right, we introduce a novel\nFill-in-the-Blank Image Captioning task which requires reasoning about both\npast and future sentence structure to reconstruct sensible image descriptions.\nWe use this task as well as the Visual Madlibs dataset to demonstrate the\neffectiveness of our approach, consistently outperforming all baseline methods.", "authors": ["Qing Sun", "Stefan Lee", "Dhruv Batra"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1705.08759v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1705.08759v1", "num_discussion": 0, "originally_published_time": "5/24/2017", "pid": "1705.08759v1", "published_time": "5/24/2017", "rawpid": "1705.08759", "tags": ["cs.CV"], "title": "Bidirectional Beam Search: Forward-Backward Inference in Neural Sequence\n  Models for Fill-in-the-Blank Image Captioning"}, {"abstract": "Visual question answering (or VQA) is a new and exciting problem that\ncombines natural language processing and computer vision techniques. We present\na survey of the various datasets and models that have been used to tackle this\ntask. The first part of the survey details the various datasets for VQA and\ncompares them along some common factors. The second part of this survey details\nthe different approaches for VQA, classified into four types: non-deep learning\nmodels, deep learning models without attention, deep learning models with\nattention, and other models which do not fit into the first three. Finally, we\ncompare the performances of these approaches and provide some directions for\nfuture work.", "authors": ["Akshay Kumar Gupta"], "category": "cs.CL", "comment": "10 pages, 3 figures, 3 tables Added references, corrected typos, made\n  references less wordy", "img": "/static/thumbs/1705.03865v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1705.03865v2", "num_discussion": 0, "originally_published_time": "5/10/2017", "pid": "1705.03865v2", "published_time": "5/11/2017", "rawpid": "1705.03865", "tags": ["cs.CL", "cs.AI", "cs.CV"], "title": "Survey of Visual Question Answering: Datasets and Techniques"}, {"abstract": "Existing methods for visual reasoning attempt to directly map inputs to\noutputs using black-box architectures without explicitly modeling the\nunderlying reasoning processes. As a result, these black-box models often learn\nto exploit biases in the data rather than learning to perform visual reasoning.\nInspired by module networks, this paper proposes a model for visual reasoning\nthat consists of a program generator that constructs an explicit representation\nof the reasoning process to be performed, and an execution engine that executes\nthe resulting program to produce an answer. Both the program generator and the\nexecution engine are implemented by neural networks, and are trained using a\ncombination of backpropagation and REINFORCE. Using the CLEVR benchmark for\nvisual reasoning, we show that our model significantly outperforms strong\nbaselines and generalizes better in a variety of settings.", "authors": ["Justin Johnson", "Bharath Hariharan", "Laurens van der Maaten", "Judy Hoffman", "Li Fei-Fei", "C. Lawrence Zitnick", "Ross Girshick"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1705.03633v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1705.03633v1", "num_discussion": 0, "originally_published_time": "5/10/2017", "pid": "1705.03633v1", "published_time": "5/10/2017", "rawpid": "1705.03633", "tags": ["cs.CV", "cs.CL", "cs.LG"], "title": "Inferring and Executing Programs for Visual Reasoning"}, {"abstract": "In this paper, we aim to understand whether current language and vision\n(LaVi) models truly grasp the interaction between the two modalities. To this\nend, we propose an extension of the MSCOCO dataset, FOIL-COCO, which associates\nimages with both correct and \"foil\" captions, that is, descriptions of the\nimage that are highly similar to the original ones, but contain one single\nmistake (\"foil word\"). We show that current LaVi models fall into the traps of\nthis data and perform badly on three tasks: a) caption classification (correct\nvs. foil); b) foil word detection; c) foil word correction. Humans, in\ncontrast, have near-perfect performance on those tasks. We demonstrate that\nmerely utilising language cues is not enough to model FOIL-COCO and that it\nchallenges the state-of-the-art by requiring a fine-grained understanding of\nthe relation between text and image.", "authors": ["Ravi Shekhar", "Sandro Pezzelle", "Yauhen Klimovich", "Aurelie Herbelot", "Moin Nabi", "Enver Sangineto", "Raffaella Bernardi"], "category": "cs.CV", "comment": "To appear at ACL 2017", "img": "/static/thumbs/1705.01359v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1705.01359v1", "num_discussion": 0, "originally_published_time": "5/3/2017", "pid": "1705.01359v1", "published_time": "5/3/2017", "rawpid": "1705.01359", "tags": ["cs.CV", "cs.CL", "cs.MM"], "title": "FOIL it! Find One mismatch between Image and Language caption"}, {"abstract": "Automatic description generation from natural images is a challenging problem\nthat has recently received a large amount of interest from the computer vision\nand natural language processing communities. In this survey, we classify the\nexisting approaches based on how they conceptualize this problem, viz., models\nthat cast description as either generation problem or as a retrieval problem\nover a visual or multimodal representational space. We provide a detailed\nreview of existing models, highlighting their advantages and disadvantages.\nMoreover, we give an overview of the benchmark image datasets and the\nevaluation measures that have been developed to assess the quality of\nmachine-generated image descriptions. Finally we extrapolate future directions\nin the area of automatic image description generation.", "authors": ["Raffaella Bernardi", "Ruket Cakici", "Desmond Elliott", "Aykut Erdem", "Erkut Erdem", "Nazli Ikizler-Cinbis", "Frank Keller", "Adrian Muscat", "Barbara Plank"], "category": "cs.CL", "comment": "Journal of Artificial Intelligence Research 55, 409-442, 2016", "img": "/static/thumbs/1601.03896v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1601.03896v2", "num_discussion": 0, "originally_published_time": "1/15/2016", "pid": "1601.03896v2", "published_time": "4/24/2017", "rawpid": "1601.03896", "tags": ["cs.CL", "cs.CV"], "title": "Automatic Description Generation from Images: A Survey of Models,\n  Datasets, and Evaluation Measures"}, {"abstract": "Fully convolutional neural networks (FCNNs) trained on a large number of\nimages with strong pixel-level annotations have become the new state of the art\nfor the semantic segmentation task. While there have been recent attempts to\nlearn FCNNs from image-level weak annotations, they need additional\nconstraints, such as the size of an object, to obtain reasonable performance.\nTo address this issue, we present motion-CNN (M-CNN), a novel FCNN framework\nwhich incorporates motion cues and is learned from video-level weak\nannotations. Our learning scheme to train the network uses motion segments as\nsoft constraints, thereby handling noisy motion information. When trained on\nweakly-annotated videos, our method outperforms the state-of-the-art EM-Adapt\napproach on the PASCAL VOC 2012 image segmentation benchmark. We also\ndemonstrate that the performance of M-CNN learned with 150 weak video\nannotations is on par with state-of-the-art weakly-supervised methods trained\nwith thousands of images. Finally, M-CNN substantially outperforms recent\napproaches in a related task of video co-localization on the YouTube-Objects\ndataset.", "authors": ["Pavel Tokmakov", "Karteek Alahari", "Cordelia Schmid"], "category": "cs.CV", "comment": "Extended version of our ECCV 2016 paper", "img": "/static/thumbs/1603.07188v3.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1603.07188v3", "num_discussion": 0, "originally_published_time": "3/23/2016", "pid": "1603.07188v3", "published_time": "4/21/2017", "rawpid": "1603.07188", "tags": ["cs.CV"], "title": "Weakly-Supervised Semantic Segmentation using Motion Cues"}, {"abstract": "We propose an approach for semi-automatic annotation of object instances.\nWhile most current methods treat object segmentation as a pixel-labeling\nproblem, we here cast it as a polygon prediction task, mimicking how most\ncurrent datasets have been annotated. In particular, our approach takes as\ninput an image crop and sequentially produces vertices of the polygon outlining\nthe object. This allows a human annotator to interfere at any time and correct\na vertex if needed, producing as accurate segmentation as desired by the\nannotator. We show that our approach speeds up the annotation process by a\nfactor of 4.7 across all classes in Cityscapes, while achieving 78.4% agreement\nin IoU with original ground-truth, matching the typical agreement between human\nannotators. For cars, our speed-up factor is 7.3 for an agreement of 82.2%. We\nfurther show generalization capabilities of our approach to unseen datasets.", "authors": ["Lluis Castrejon", "Kaustav Kundu", "Raquel Urtasun", "Sanja Fidler"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1704.05548v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1704.05548v1", "num_discussion": 0, "originally_published_time": "4/18/2017", "pid": "1704.05548v1", "published_time": "4/18/2017", "rawpid": "1704.05548", "tags": ["cs.CV"], "title": "Annotating Object Instances with a Polygon-RNN"}, {"abstract": "Associating image regions with text queries has been recently explored as a\nnew way to bridge visual and linguistic representations. A few pioneering\napproaches have been proposed based on recurrent neural language models trained\ngeneratively (e.g., generating captions), but achieving somewhat limited\nlocalization accuracy. To better address natural-language-based visual entity\nlocalization, we propose a discriminative approach. We formulate a\ndiscriminative bimodal neural network (DBNet), which can be trained by a\nclassifier with extensive use of negative samples. Our training objective\nencourages better localization on single images, incorporates text phrases in a\nbroad range, and properly pairs image regions with text phrases into positive\nand negative examples. Experiments on the Visual Genome dataset demonstrate the\nproposed DBNet significantly outperforms previous state-of-the-art methods both\nfor localization on single images and for detection on multiple images. We we\nalso establish an evaluation protocol for natural-language visual detection.", "authors": ["Yuting Zhang", "Luyao Yuan", "Yijie Guo", "Zhiyuan He", "I-An Huang", "Honglak Lee"], "category": "cs.CV", "comment": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n  2017", "img": "/static/thumbs/1704.03944v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1704.03944v2", "num_discussion": 0, "originally_published_time": "4/12/2017", "pid": "1704.03944v2", "published_time": "4/17/2017", "rawpid": "1704.03944", "tags": ["cs.CV", "stat.ML"], "title": "Discriminative Bimodal Networks for Visual Localization and Detection\n  with Natural Language Queries"}, {"abstract": "In recent years we have seen rapid and significant progress in automatic\nimage description but what are the open problems in this area? Most work has\nbeen evaluated using text-based similarity metrics, which only indicate that\nthere have been improvements, without explaining what has improved. In this\npaper, we present a detailed error analysis of the descriptions generated by a\nstate-of-the-art attention-based model. Our analysis operates on two levels:\nfirst we check the descriptions for accuracy, and then we categorize the types\nof errors we observe in the inaccurate descriptions. We find only 20% of the\ndescriptions are free from errors, and surprisingly that 26% are unrelated to\nthe image. Finally, we manually correct the most frequently occurring error\ntypes (e.g. gender identification) to estimate the performance reward for\naddressing these errors, observing gains of 0.2--1 BLEU point per type.", "authors": ["Emiel van Miltenburg", "Desmond Elliott"], "category": "cs.CL", "comment": "Submitted", "img": "/static/thumbs/1704.04198v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1704.04198v1", "num_discussion": 0, "originally_published_time": "4/13/2017", "pid": "1704.04198v1", "published_time": "4/13/2017", "rawpid": "1704.04198", "tags": ["cs.CL"], "title": "Room for improvement in automatic image description: an error analysis"}, {"abstract": "Neural image/video captioning models can generate accurate descriptions, but\ntheir internal process of mapping regions to words is a black box and therefore\ndifficult to explain. Top-down neural saliency methods can find important\nregions given a high-level semantic task such as object classification, but\ncannot use a natural language sentence as the top-down input for the task. In\nthis paper, we propose Caption-Guided Visual Saliency to expose the\nregion-to-word mapping in modern encoder-decoder networks and demonstrate that\nit is learned implicitly from caption training data, without any pixel-level\nannotations. Our approach can produce spatial or spatiotemporal heatmaps for\nboth predicted captions, and for arbitrary query sentences. It recovers\nsaliency without the overhead of introducing explicit attention layers, and can\nbe used to analyze a variety of existing model architectures and improve their\ndesign. Evaluation on large-scale video and image datasets demonstrates that\nour approach achieves comparable captioning performance with existing methods\nwhile providing more accurate saliency heatmaps. Our code is available at\nvisionlearninggroup.github.io/caption-guided-saliency/.", "authors": ["Vasili Ramanishka", "Abir Das", "Jianming Zhang", "Kate Saenko"], "category": "cs.CV", "comment": "CVPR 2017 camera ready version", "img": "/static/thumbs/1612.07360v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1612.07360v2", "num_discussion": 0, "originally_published_time": "12/21/2016", "pid": "1612.07360v2", "published_time": "4/12/2017", "rawpid": "1612.07360", "tags": ["cs.CV"], "title": "Top-down Visual Saliency Guided by Captions"}, {"abstract": "Image captioning is a challenging problem owing to the complexity in\nunderstanding the image content and diverse ways of describing it in natural\nlanguage. Recent advances in deep neural networks have substantially improved\nthe performance of this task. Most state-of-the-art approaches follow an\nencoder-decoder framework, which generates captions using a sequential\nrecurrent prediction model. However, in this paper, we introduce a novel\ndecision-making framework for image captioning. We utilize a \"policy network\"\nand a \"value network\" to collaboratively generate captions. The policy network\nserves as a local guidance by providing the confidence of predicting the next\nword according to the current state. Additionally, the value network serves as\na global and lookahead guidance by evaluating all possible extensions of the\ncurrent state. In essence, it adjusts the goal of predicting the correct words\ntowards the goal of generating captions similar to the ground truth captions.\nWe train both networks using an actor-critic reinforcement learning model, with\na novel reward defined by visual-semantic embedding. Extensive experiments and\nanalyses on the Microsoft COCO dataset show that the proposed framework\noutperforms state-of-the-art approaches across different evaluation metrics.", "authors": ["Zhou Ren", "Xiaoyu Wang", "Ning Zhang", "Xutao Lv", "Li-Jia Li"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1704.03899v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1704.03899v1", "num_discussion": 0, "originally_published_time": "4/12/2017", "pid": "1704.03899v1", "published_time": "4/12/2017", "rawpid": "1704.03899", "tags": ["cs.CV", "cs.AI"], "title": "Deep Reinforcement Learning-based Image Captioning with Embedding Reward"}, {"abstract": "Visual attention has been successfully applied in structural prediction tasks\nsuch as visual captioning and question answering. Existing visual attention\nmodels are generally spatial, i.e., the attention is modeled as spatial\nprobabilities that re-weight the last conv-layer feature map of a CNN encoding\nan input image. However, we argue that such spatial attention does not\nnecessarily conform to the attention mechanism --- a dynamic feature extractor\nthat combines contextual fixations over time, as CNN features are naturally\nspatial, channel-wise and multi-layer. In this paper, we introduce a novel\nconvolutional neural network dubbed SCA-CNN that incorporates Spatial and\nChannel-wise Attentions in a CNN. In the task of image captioning, SCA-CNN\ndynamically modulates the sentence generation context in multi-layer feature\nmaps, encoding where (i.e., attentive spatial locations at multiple layers) and\nwhat (i.e., attentive channels) the visual attention is. We evaluate the\nproposed SCA-CNN architecture on three benchmark image captioning datasets:\nFlickr8K, Flickr30K, and MSCOCO. It is consistently observed that SCA-CNN\nsignificantly outperforms state-of-the-art visual attention-based image\ncaptioning methods.", "authors": ["Long Chen", "Hanwang Zhang", "Jun Xiao", "Liqiang Nie", "Jian Shao", "Wei Liu", "Tat-Seng Chua"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1611.05594v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1611.05594v2", "num_discussion": 0, "originally_published_time": "11/17/2016", "pid": "1611.05594v2", "published_time": "4/12/2017", "rawpid": "1611.05594", "tags": ["cs.CV"], "title": "SCA-CNN: Spatial and Channel-wise Attention in Convolutional Networks\n  for Image Captioning"}, {"abstract": "Generating diverse questions for given images is an important task for\ncomputational education, entertainment and AI assistants. Different from many\nconventional prediction techniques is the need for algorithms to generate a\ndiverse set of plausible questions, which we refer to as \"creativity\". In this\npaper we propose a creative algorithm for visual question generation which\ncombines the advantages of variational autoencoders with long short-term memory\nnetworks. We demonstrate that our framework is able to generate a large set of\nvarying questions given a single input image.", "authors": ["Unnat Jain", "Ziyu Zhang", "Alexander Schwing"], "category": "cs.CV", "comment": "Accepted to CVPR 2017", "img": "/static/thumbs/1704.03493v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1704.03493v1", "num_discussion": 0, "originally_published_time": "4/11/2017", "pid": "1704.03493v1", "published_time": "4/11/2017", "rawpid": "1704.03493", "tags": ["cs.CV"], "title": "Creativity: Generating Diverse Questions using Variational Autoencoders"}, {"abstract": "In this work we explore a straightforward variational Bayes scheme for\nRecurrent Neural Networks. Firstly, we show that a simple adaptation of\ntruncated backpropagation through time can yield good quality uncertainty\nestimates and superior regularisation at only a small extra computational cost\nduring training. Secondly, we demonstrate how a novel kind of posterior\napproximation yields further improvements to the performance of Bayesian RNNs.\nWe incorporate local gradient information into the approximate posterior to\nsharpen it around the current batch statistics. This technique is not exclusive\nto recurrent neural networks and can be applied more widely to train Bayesian\nneural networks. We also empirically demonstrate how Bayesian RNNs are superior\nto traditional RNNs on a language modelling benchmark and an image captioning\ntask, as well as showing how each of these methods improve our model over a\nvariety of other schemes for training them. We also introduce a new benchmark\nfor studying uncertainty for language models so future methods can be easily\ncompared.", "authors": ["Meire Fortunato", "Charles Blundell", "Oriol Vinyals"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1704.02798v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1704.02798v2", "num_discussion": 0, "originally_published_time": "4/10/2017", "pid": "1704.02798v2", "published_time": "4/11/2017", "rawpid": "1704.02798", "tags": ["cs.LG", "stat.ML"], "title": "Bayesian Recurrent Neural Networks"}, {"abstract": "Recent progress on image captioning has made it possible to generate novel\nsentences describing images in natural language, but compressing an image into\na single sentence can describe visual content in only coarse detail. While one\nnew captioning approach, dense captioning, can potentially describe images in\nfiner levels of detail by captioning many regions within an image, it in turn\nis unable to produce a coherent story for an image. In this paper we overcome\nthese limitations by generating entire paragraphs for describing images, which\ncan tell detailed, unified stories. We develop a model that decomposes both\nimages and paragraphs into their constituent parts, detecting semantic regions\nin images and using a hierarchical recurrent neural network to reason about\nlanguage. Linguistic analysis confirms the complexity of the paragraph\ngeneration task, and thorough experiments on a new dataset of image and\nparagraph pairs demonstrate the effectiveness of our approach.", "authors": ["Jonathan Krause", "Justin Johnson", "Ranjay Krishna", "Li Fei-Fei"], "category": "cs.CV", "comment": "CVPR 2017 spotlight", "img": "/static/thumbs/1611.06607v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1611.06607v2", "num_discussion": 0, "originally_published_time": "11/20/2016", "pid": "1611.06607v2", "published_time": "4/10/2017", "rawpid": "1611.06607", "tags": ["cs.CV", "cs.CL"], "title": "A Hierarchical Approach for Generating Descriptive Image Paragraphs"}, {"abstract": "The problem of determining whether an object is in motion, irrespective of\ncamera motion, is far from being solved. We address this challenging task by\nlearning motion patterns in videos. The core of our approach is a fully\nconvolutional network, which is learned entirely from synthetic video\nsequences, and their ground-truth optical flow and motion segmentation. This\nencoder-decoder style architecture first learns a coarse representation of the\noptical flow field features, and then refines it iteratively to produce motion\nlabels at the original high-resolution. We further improve this labeling with\nan objectness map and a conditional random field, to account for errors in\noptical flow, and also to focus on moving \"things\" rather than \"stuff\". The\noutput label of each pixel denotes whether it has undergone independent motion,\ni.e., irrespective of camera motion. We demonstrate the benefits of this\nlearning framework on the moving object segmentation task, where the goal is to\nsegment all objects in motion. Our approach outperforms the top method on the\nrecently released DAVIS benchmark dataset, comprising real-world sequences, by\n5.6%. We also evaluate on the Berkeley motion segmentation database, achieving\nstate-of-the-art results.", "authors": ["Pavel Tokmakov", "Karteek Alahari", "Cordelia Schmid"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1612.07217v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1612.07217v2", "num_discussion": 0, "originally_published_time": "12/21/2016", "pid": "1612.07217v2", "published_time": "4/10/2017", "rawpid": "1612.07217", "tags": ["cs.CV"], "title": "Learning Motion Patterns in Videos"}, {"abstract": "Learning how to generate descriptions of images or videos received major\ninterest both in the Computer Vision and Natural Language Processing\ncommunities. While a few works have proposed to learn a grounding during the\ngeneration process in an unsupervised way (via an attention mechanism), it\nremains unclear how good the quality of the grounding is and whether it\nbenefits the description quality. In this work we propose a movie description\nmodel which learns to generate description and jointly ground (localize) the\nmentioned characters as well as do visual co-reference resolution between pairs\nof consecutive sentences/clips. We also propose to use weak localization\nsupervision through character mentions provided in movie descriptions to learn\nthe character grounding. At training time, we first learn how to localize\ncharacters by relating their visual appearance to mentions in the descriptions\nvia a semi-supervised approach. We then provide this (noisy) supervision into\nour description model which greatly improves its performance. Our proposed\ndescription model improves over prior work w.r.t. generated description quality\nand additionally provides grounding and local co-reference resolution. We\nevaluate it on the MPII Movie Description dataset using automatic and human\nevaluation measures and using our newly collected grounding and co-reference\ndata for characters.", "authors": ["Anna Rohrbach", "Marcus Rohrbach", "Siyu Tang", "Seong Joon Oh", "Bernt Schiele"], "category": "cs.CV", "comment": "Accepted to CVPR 2017", "img": "/static/thumbs/1704.01518v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1704.01518v1", "num_discussion": 0, "originally_published_time": "4/5/2017", "pid": "1704.01518v1", "published_time": "4/5/2017", "rawpid": "1704.01518", "tags": ["cs.CV"], "title": "Generating Descriptions with Grounded and Co-Referenced People"}, {"abstract": "The ability of the Generative Adversarial Networks (GANs) framework to learn\ngenerative models mapping from simple latent distributions to arbitrarily\ncomplex data distributions has been demonstrated empirically, with compelling\nresults showing that the latent space of such generators captures semantic\nvariation in the data distribution. Intuitively, models trained to predict\nthese semantic latent representations given data may serve as useful feature\nrepresentations for auxiliary problems where semantics are relevant. However,\nin their existing form, GANs have no means of learning the inverse mapping --\nprojecting data back into the latent space. We propose Bidirectional Generative\nAdversarial Networks (BiGANs) as a means of learning this inverse mapping, and\ndemonstrate that the resulting learned feature representation is useful for\nauxiliary supervised discrimination tasks, competitive with contemporary\napproaches to unsupervised and self-supervised feature learning.", "authors": ["Jeff Donahue", "Philipp Kr\u00e4henb\u00fchl", "Trevor Darrell"], "category": "cs.LG", "comment": "Published as a conference paper at ICLR 2017. Changelog: (v7) Table 2\n  results improved 1-2% due to...", "img": "/static/thumbs/1605.09782v7.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1605.09782v7", "num_discussion": 0, "originally_published_time": "5/31/2016", "pid": "1605.09782v7", "published_time": "4/3/2017", "rawpid": "1605.09782", "tags": ["cs.LG", "cs.AI", "cs.CV", "cs.NE", "stat.ML"], "title": "Adversarial Feature Learning"}, {"abstract": "Of late, weakly supervised object detection is with great importance in\nobject recognition. Based on deep learning, weakly supervised detectors have\nachieved many promising results. However, compared with fully supervised\ndetection, it is more challenging to train deep network based detectors in a\nweakly supervised manner. Here we formulate weakly supervised detection as a\nMultiple Instance Learning (MIL) problem, where instance classifiers (object\ndetectors) are put into the network as hidden nodes. We propose a novel online\ninstance classifier refinement algorithm to integrate MIL and the instance\nclassifier refinement procedure into a single deep network, and train the\nnetwork end-to-end with only image-level supervision, i.e., without object\nlocation information. More precisely, instance labels inferred from weak\nsupervision are propagated to their spatially overlapped instances to refine\ninstance classifier online. The iterative instance classifier refinement\nprocedure is implemented using multiple streams in deep network, where each\nstream supervises its latter stream. Weakly supervised object detection\nexperiments are carried out on the challenging PASCAL VOC 2007 and 2012\nbenchmarks. We obtain 47% mAP on VOC 2007 that significantly outperforms the\nprevious state-of-the-art.", "authors": ["Peng Tang", "Xinggang Wang", "Xiang Bai", "Wenyu Liu"], "category": "cs.CV", "comment": "Accepted by CVPR 2017, IEEE Conference on Computer Vision and Pattern\n  Recognition 2017", "img": "/static/thumbs/1704.00138v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1704.00138v1", "num_discussion": 0, "originally_published_time": "4/1/2017", "pid": "1704.00138v1", "published_time": "4/1/2017", "rawpid": "1704.00138", "tags": ["cs.CV"], "title": "Multiple Instance Detection Network with Online Instance Classifier\n  Refinement"}, {"abstract": "A Semantic Compositional Network (SCN) is developed for image captioning, in\nwhich semantic concepts (i.e., tags) are detected from the image, and the\nprobability of each tag is used to compose the parameters in a long short-term\nmemory (LSTM) network. The SCN extends each weight matrix of the LSTM to an\nensemble of tag-dependent weight matrices. The degree to which each member of\nthe ensemble is used to generate an image caption is tied to the\nimage-dependent probability of the corresponding tag. In addition to captioning\nimages, we also extend the SCN to generate captions for video clips. We\nqualitatively analyze semantic composition in SCNs, and quantitatively evaluate\nthe algorithm on three benchmark datasets: COCO, Flickr30k, and Youtube2Text.\nExperimental results show that the proposed method significantly outperforms\nprior state-of-the-art approaches, across multiple evaluation metrics.", "authors": ["Zhe Gan", "Chuang Gan", "Xiaodong He", "Yunchen Pu", "Kenneth Tran", "Jianfeng Gao", "Lawrence Carin", "Li Deng"], "category": "cs.CV", "comment": "Accepted in CVPR 2017", "img": "/static/thumbs/1611.08002v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1611.08002v2", "num_discussion": 0, "originally_published_time": "11/23/2016", "pid": "1611.08002v2", "published_time": "3/28/2017", "rawpid": "1611.08002", "tags": ["cs.CV", "cs.CL", "cs.LG"], "title": "Semantic Compositional Networks for Visual Captioning"}, {"abstract": "When a neural language model is used for caption generation, the image\ninformation can be fed to the neural network either by directly incorporating\nit in a recurrent neural network -- conditioning the language model by\ninjecting image features -- or in a layer following the recurrent neural\nnetwork -- conditioning the language model by merging the image features. While\nmerging implies that visual features are bound at the end of the caption\ngeneration process, injecting can bind the visual features at a variety stages.\nIn this paper we empirically show that late binding is superior to early\nbinding in terms of different evaluation metrics. This suggests that the\ndifferent modalities (visual and linguistic) for caption generation should not\nbe jointly encoded by the RNN; rather, the multimodal integration should be\ndelayed to a subsequent stage. Furthermore, this suggests that recurrent neural\nnetworks should not be viewed as actually generating text, but only as encoding\nit for prediction in a subsequent layer.", "authors": ["Marc Tanti", "Albert Gatt", "Kenneth P. Camilleri"], "category": "cs.NE", "comment": "under review, 29 pages, 5 figures, 6 tables", "img": "/static/thumbs/1703.09137v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1703.09137v1", "num_discussion": 0, "originally_published_time": "3/27/2017", "pid": "1703.09137v1", "published_time": "3/27/2017", "rawpid": "1703.09137", "tags": ["cs.NE", "cs.CL", "cs.CV"], "title": "Where to put the Image in an Image Caption Generator"}, {"abstract": "A natural image usually conveys rich semantic content and can be viewed from\ndifferent angles. Existing image description methods are largely restricted by\nsmall sets of biased visual paragraph annotations, and fail to cover rich\nunderlying semantics. In this paper, we investigate a semi-supervised paragraph\ngenerative framework that is able to synthesize diverse and semantically\ncoherent paragraph descriptions by reasoning over local semantic regions and\nexploiting linguistic knowledge. The proposed Recurrent Topic-Transition\nGenerative Adversarial Network (RTT-GAN) builds an adversarial framework\nbetween a structured paragraph generator and multi-level paragraph\ndiscriminators. The paragraph generator generates sentences recurrently by\nincorporating region-based visual and language attention mechanisms at each\nstep. The quality of generated paragraph sentences is assessed by multi-level\nadversarial discriminators from two aspects, namely, plausibility at sentence\nlevel and topic-transition coherence at paragraph level. The joint adversarial\ntraining of RTT-GAN drives the model to generate realistic paragraphs with\nsmooth logical transition between sentence topics. Extensive quantitative\nexperiments on image and video paragraph datasets demonstrate the effectiveness\nof our RTT-GAN in both supervised and semi-supervised settings. Qualitative\nresults on telling diverse stories for an image also verify the\ninterpretability of RTT-GAN.", "authors": ["Xiaodan Liang", "Zhiting Hu", "Hao Zhang", "Chuang Gan", "Eric P. Xing"], "category": "cs.CV", "comment": "10 pages, 6 figures", "img": "/static/thumbs/1703.07022v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1703.07022v2", "num_discussion": 0, "originally_published_time": "3/21/2017", "pid": "1703.07022v2", "published_time": "3/23/2017", "rawpid": "1703.07022", "tags": ["cs.CV", "cs.AI", "cs.LG"], "title": "Recurrent Topic-Transition GAN for Visual Paragraph Generation"}, {"abstract": "Neural Machine Translation (NMT) has shown remarkable progress over the past\nfew years with production systems now being deployed to end-users. One major\ndrawback of current architectures is that they are expensive to train,\ntypically requiring days to weeks of GPU time to converge. This makes\nexhaustive hyperparameter search, as is commonly done with other neural network\narchitectures, prohibitively expensive. In this work, we present the first\nlarge-scale analysis of NMT architecture hyperparameters. We report empirical\nresults and variance numbers for several hundred experimental runs,\ncorresponding to over 250,000 GPU hours on the standard WMT English to German\ntranslation task. Our experiments lead to novel insights and practical advice\nfor building and extending NMT architectures. As part of this contribution, we\nrelease an open-source NMT framework that enables researchers to easily\nexperiment with novel techniques and reproduce state of the art results.", "authors": ["Denny Britz", "Anna Goldie", "Minh-Thang Luong", "Quoc Le"], "category": "cs.CL", "comment": "9 pages, 2 figures, 8 tables, submitted to ACL 2017, open source code\n  at https://github.com/google...", "img": "/static/thumbs/1703.03906v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1703.03906v2", "num_discussion": 0, "originally_published_time": "3/11/2017", "pid": "1703.03906v2", "published_time": "3/21/2017", "rawpid": "1703.03906", "tags": ["cs.CL"], "title": "Massive Exploration of Neural Machine Translation Architectures"}, {"abstract": "Many of the existing methods for learning joint embedding of images and text\nuse only supervised information from paired images and its textual attributes.\nTaking advantage of the recent success of unsupervised learning in deep neural\nnetworks, we propose an end-to-end learning framework that is able to extract\nmore robust multi-modal representations across domains. The proposed method\ncombines representation learning models (i.e., auto-encoders) together with\ncross-domain learning criteria (i.e., Maximum Mean Discrepancy loss) to learn\njoint embeddings for semantic and visual features. A novel technique of\nunsupervised-data adaptation inference is introduced to construct more\ncomprehensive embeddings for both labeled and unlabeled data. We evaluate our\nmethod on Animals with Attributes and Caltech-UCSD Birds 200-2011 dataset with\na wide range of applications, including zero and few-shot image recognition and\nretrieval, from inductive to transductive settings. Empirically, we show that\nour framework improves over the current state of the art on many of the\nconsidered tasks.", "authors": ["Yao-Hung Hubert Tsai", "Liang-Kang Huang", "Ruslan Salakhutdinov"], "category": "cs.CV", "comment": "12 pages", "img": "/static/thumbs/1703.05908v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1703.05908v2", "num_discussion": 0, "originally_published_time": "3/17/2017", "pid": "1703.05908v2", "published_time": "3/20/2017", "rawpid": "1703.05908", "tags": ["cs.CV", "cs.CL", "cs.LG"], "title": "Learning Robust Visual-Semantic Embeddings"}, {"abstract": "Current image captioning methods are usually trained via (penalized) maximum\nlikelihood estimation. However, the log-likelihood score of a caption does not\ncorrelate well with human assessments of quality. Standard syntactic evaluation\nmetrics, such as BLEU, METEOR and ROUGE, are also not well correlated. The\nnewer SPICE and CIDEr metrics are better correlated, but have traditionally\nbeen hard to optimize for. In this paper, we show how to use a policy gradient\n(PG) method to directly optimize a linear combination of SPICE and CIDEr (a\ncombination we call SPIDEr): the SPICE score ensures our captions are\nsemantically faithful to the image, while CIDEr score ensures our captions are\nsyntactically fluent. The PG method we propose improves on the prior MIXER\napproach, by using Monte Carlo rollouts instead of mixing MLE training with PG.\nWe show empirically that our algorithm leads to easier optimization and\nimproved results compared to MIXER. Finally, we show that using our PG method\nwe can optimize any of the metrics, including the proposed SPIDEr metric which\nresults in image captions that are strongly preferred by human raters compared\nto captions generated by the same model but trained to optimize MLE or the COCO\nmetrics.", "authors": ["Siqi Liu", "Zhenhai Zhu", "Ning Ye", "Sergio Guadarrama", "Kevin Murphy"], "category": "cs.CV", "comment": "Under review at ICCV 2017", "img": "/static/thumbs/1612.00370v3.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1612.00370v3", "num_discussion": 0, "originally_published_time": "12/1/2016", "pid": "1612.00370v3", "published_time": "3/18/2017", "rawpid": "1612.00370", "tags": ["cs.CV", "cs.CL"], "title": "Improved Image Captioning via Policy Gradient optimization of SPIDEr"}, {"abstract": "This paper proposes a new model for extracting an interpretable sentence\nembedding by introducing self-attention. Instead of using a vector, we use a\n2-D matrix to represent the embedding, with each row of the matrix attending on\na different part of the sentence. We also propose a self-attention mechanism\nand a special regularization term for the model. As a side effect, the\nembedding comes with an easy way of visualizing what specific parts of the\nsentence are encoded into the embedding. We evaluate our model on 3 different\ntasks: author profiling, sentiment classification, and textual entailment.\nResults show that our model yields a significant performance gain compared to\nother sentence embedding methods in all of the 3 tasks.", "authors": ["Zhouhan Lin", "Minwei Feng", "Cicero Nogueira dos Santos", "Mo Yu", "Bing Xiang", "Bowen Zhou", "Yoshua Bengio"], "category": "cs.CL", "comment": "15 pages with appendix, 7 figures, 4 tables. Conference paper in 5th\n  International Conference on L...", "img": "/static/thumbs/1703.03130v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1703.03130v1", "num_discussion": 0, "originally_published_time": "3/9/2017", "pid": "1703.03130v1", "published_time": "3/9/2017", "rawpid": "1703.03130", "tags": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "title": "A Structured Self-attentive Sentence Embedding"}, {"abstract": "Data noising is an effective technique for regularizing neural network\nmodels. While noising is widely adopted in application domains such as vision\nand speech, commonly used noising primitives have not been developed for\ndiscrete sequence-level settings such as language modeling. In this paper, we\nderive a connection between input noising in neural network language models and\nsmoothing in $n$-gram models. Using this connection, we draw upon ideas from\nsmoothing to develop effective noising schemes. We demonstrate performance\ngains when applying the proposed schemes to language modeling and machine\ntranslation. Finally, we provide empirical analysis validating the relationship\nbetween noising and smoothing.", "authors": ["Ziang Xie", "Sida I. Wang", "Jiwei Li", "Daniel L\u00e9vy", "Aiming Nie", "Dan Jurafsky", "Andrew Y. Ng"], "category": "cs.LG", "comment": "ICLR 2017", "img": "/static/thumbs/1703.02573v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1703.02573v1", "num_discussion": 0, "originally_published_time": "3/7/2017", "pid": "1703.02573v1", "published_time": "3/7/2017", "rawpid": "1703.02573", "tags": ["cs.LG", "cs.CL"], "title": "Data Noising as Smoothing in Neural Network Language Models"}, {"abstract": "This tutorial introduces a new and powerful set of techniques variously\ncalled \"neural machine translation\" or \"neural sequence-to-sequence models\".\nThese techniques have been used in a number of tasks regarding the handling of\nhuman language, and can be a powerful tool in the toolbox of anyone who wants\nto model sequential data of some sort. The tutorial assumes that the reader\nknows the basics of math and programming, but does not assume any particular\nexperience with neural networks or natural language processing. It attempts to\nexplain the intuition behind the various methods covered, then delves into them\nwith enough mathematical detail to understand them concretely, and culiminates\nwith a suggestion for an implementation exercise, where readers can test that\nthey understood the content in practice.", "authors": ["Graham Neubig"], "category": "cs.CL", "comment": "65 Pages", "img": "/static/thumbs/1703.01619v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1703.01619v1", "num_discussion": 0, "originally_published_time": "3/5/2017", "pid": "1703.01619v1", "published_time": "3/5/2017", "rawpid": "1703.01619", "tags": ["cs.CL", "cs.LG", "stat.ML"], "title": "Neural Machine Translation and Sequence-to-sequence Models: A Tutorial"}, {"abstract": "Two potential bottlenecks on the expressiveness of recurrent neural networks\n(RNNs) are their ability to store information about the task in their\nparameters, and to store information about the input history in their units. We\nshow experimentally that all common RNN architectures achieve nearly the same\nper-task and per-unit capacity bounds with careful training, for a variety of\ntasks and stacking depths. They can store an amount of task information which\nis linear in the number of parameters, and is approximately 5 bits per\nparameter. They can additionally store approximately one real number from their\ninput history per hidden unit. We further find that for several tasks it is the\nper-task parameter capacity bound that determines performance. These results\nsuggest that many previous results comparing RNN architectures are driven\nprimarily by differences in training effectiveness, rather than differences in\ncapacity. Supporting this observation, we compare training difficulty for\nseveral architectures, and show that vanilla RNNs are far more difficult to\ntrain, yet have slightly higher capacity. Finally, we propose two novel RNN\narchitectures, one of which is easier to train than the LSTM or GRU for deeply\nstacked architectures.", "authors": ["Jasmine Collins", "Jascha Sohl-Dickstein", "David Sussillo"], "category": "stat.ML", "comment": "Published as a conference paper at ICLR 2017", "img": "/static/thumbs/1611.09913v3.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1611.09913v3", "num_discussion": 0, "originally_published_time": "11/29/2016", "pid": "1611.09913v3", "published_time": "3/3/2017", "rawpid": "1611.09913", "tags": ["stat.ML", "cs.AI", "cs.LG", "cs.NE"], "title": "Capacity and Trainability in Recurrent Neural Networks"}, {"abstract": "We present an approach to training neural networks to generate sequences\nusing actor-critic methods from reinforcement learning (RL). Current\nlog-likelihood training methods are limited by the discrepancy between their\ntraining and testing modes, as models must generate tokens conditioned on their\nprevious guesses rather than the ground-truth tokens. We address this problem\nby introducing a \\textit{critic} network that is trained to predict the value\nof an output token, given the policy of an \\textit{actor} network. This results\nin a training procedure that is much closer to the test phase, and allows us to\ndirectly optimize for a task-specific score such as BLEU. Crucially, since we\nleverage these techniques in the supervised learning setting rather than the\ntraditional RL setting, we condition the critic network on the ground-truth\noutput. We show that our method leads to improved performance on both a\nsynthetic task, and for German-English machine translation. Our analysis paves\nthe way for such methods to be applied in natural language generation tasks,\nsuch as machine translation, caption generation, and dialogue modelling.", "authors": ["Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1607.07086v3.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1607.07086v3", "num_discussion": 0, "originally_published_time": "7/24/2016", "pid": "1607.07086v3", "published_time": "3/3/2017", "rawpid": "1607.07086", "tags": ["cs.LG"], "title": "An Actor-Critic Algorithm for Sequence Prediction"}, {"abstract": "The focus of past machine learning research for Reading Comprehension tasks\nhas been primarily on the design of novel deep learning architectures. Here we\nshow that seemingly minor choices made on (1) the use of pre-trained word\nembeddings, and (2) the representation of out-of-vocabulary tokens at test\ntime, can turn out to have a larger impact than architectural choices on the\nfinal performance. We systematically explore several options for these choices,\nand provide recommendations to researchers working in this area.", "authors": ["Bhuwan Dhingra", "Hanxiao Liu", "Ruslan Salakhutdinov", "William W. Cohen"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1703.00993v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1703.00993v1", "num_discussion": 0, "originally_published_time": "3/2/2017", "pid": "1703.00993v1", "published_time": "3/2/2017", "rawpid": "1703.00993", "tags": ["cs.CL"], "title": "A Comparative Study of Word Embeddings for Reading Comprehension"}, {"abstract": "Sophisticated gated recurrent neural network architectures like LSTMs and\nGRUs have been shown to be highly effective in a myriad of applications. We\ndevelop an un-gated unit, the statistical recurrent unit (SRU), that is able to\nlearn long term dependencies in data by only keeping moving averages of\nstatistics. The SRU's architecture is simple, un-gated, and contains a\ncomparable number of parameters to LSTMs; yet, SRUs perform favorably to more\nsophisticated LSTM and GRU alternatives, often outperforming one or both in\nvarious tasks. We show the efficacy of SRUs as compared to LSTMs and GRUs in an\nunbiased manner by optimizing respective architectures' hyperparameters in a\nBayesian optimization scheme for both synthetic and real-world tasks.", "authors": ["Junier B. Oliva", "Barnabas Poczos", "Jeff Schneider"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1703.00381v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1703.00381v1", "num_discussion": 0, "originally_published_time": "3/1/2017", "pid": "1703.00381v1", "published_time": "3/1/2017", "rawpid": "1703.00381", "tags": ["cs.LG", "cs.AI", "stat.ML"], "title": "The Statistical Recurrent Unit"}, {"abstract": "Despite the successes in capturing continuous distributions, the application\nof generative adversarial networks (GANs) to discrete settings, like natural\nlanguage tasks, is rather restricted. The fundamental reason is the difficulty\nof back-propagation through discrete random variables combined with the\ninherent instability of the GAN training objective. To address these problems,\nwe propose Maximum-Likelihood Augmented Discrete Generative Adversarial\nNetworks. Instead of directly optimizing the GAN objective, we derive a novel\nand low-variance objective using the discriminator's output that follows\ncorresponds to the log-likelihood. Compared with the original, the new\nobjective is proved to be consistent in theory and beneficial in practice. The\nexperimental results on various discrete datasets demonstrate the effectiveness\nof the proposed approach.", "authors": ["Tong Che", "Yanran Li", "Ruixiang Zhang", "R Devon Hjelm", "Wenjie Li", "Yangqiu Song", "Yoshua Bengio"], "category": "cs.AI", "comment": "11 pages, 3 figures", "img": "/static/thumbs/1702.07983v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1702.07983v1", "num_discussion": 0, "originally_published_time": "2/26/2017", "pid": "1702.07983v1", "published_time": "2/26/2017", "rawpid": "1702.07983", "tags": ["cs.AI", "cs.CL", "cs.LG"], "title": "Maximum-Likelihood Augmented Discrete Generative Adversarial Networks"}, {"abstract": "Deep neural networks are currently among the most commonly used classifiers.\nDespite easily achieving very good performance, one of the best selling points\nof these models is their modular design - one can conveniently adapt their\narchitecture to specific needs, change connectivity patterns, attach\nspecialised layers, experiment with a large amount of activation functions,\nnormalisation schemes and many others. While one can find impressively wide\nspread of various configurations of almost every aspect of the deep nets, one\nelement is, in authors' opinion, underrepresented - while solving\nclassification problems, vast majority of papers and applications simply use\nlog loss. In this paper we try to investigate how particular choices of loss\nfunctions affect deep models and their learning dynamics, as well as resulting\nclassifiers robustness to various effects. We perform experiments on classical\ndatasets, as well as provide some additional, theoretical insights into the\nproblem. In particular we show that L1 and L2 losses are, quite surprisingly,\njustified classification objectives for deep nets, by providing probabilistic\ninterpretation in terms of expected misclassification. We also introduce two\nlosses which are not typically used as deep nets objectives and show that they\nare viable alternatives to the existing ones.", "authors": ["Katarzyna Janocha", "Wojciech Marian Czarnecki"], "category": "cs.LG", "comment": "Presented at Theoretical Foundations of Machine Learning 2017 (TFML\n  2017)", "img": "/static/thumbs/1702.05659v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1702.05659v1", "num_discussion": 0, "originally_published_time": "2/18/2017", "pid": "1702.05659v1", "published_time": "2/18/2017", "rawpid": "1702.05659", "tags": ["cs.LG"], "title": "On Loss Functions for Deep Neural Networks in Classification"}, {"abstract": "Grounding (i.e. localizing) arbitrary, free-form textual phrases in visual\ncontent is a challenging problem with many applications for human-computer\ninteraction and image-text reference resolution. Few datasets provide the\nground truth spatial localization of phrases, thus it is desirable to learn\nfrom data with no or little grounding supervision. We propose a novel approach\nwhich learns grounding by reconstructing a given phrase using an attention\nmechanism, which can be either latent or optimized directly. During training\nour approach encodes the phrase using a recurrent network language model and\nthen learns to attend to the relevant image region in order to reconstruct the\ninput phrase. At test time, the correct attention, i.e., the grounding, is\nevaluated. If grounding supervision is available it can be directly applied via\na loss over the attention mechanism. We demonstrate the effectiveness of our\napproach on the Flickr 30k Entities and ReferItGame datasets with different\nlevels of supervision, ranging from no supervision over partial supervision to\nfull supervision. Our supervised variant improves by a large margin over the\nstate-of-the-art on both datasets.", "authors": ["Anna Rohrbach", "Marcus Rohrbach", "Ronghang Hu", "Trevor Darrell", "Bernt Schiele"], "category": "cs.CV", "comment": "published at ECCV 2016 (oral); updated to final version", "img": "/static/thumbs/1511.03745v4.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1511.03745v4", "num_discussion": 0, "originally_published_time": "11/12/2015", "pid": "1511.03745v4", "published_time": "2/17/2017", "rawpid": "1511.03745", "tags": ["cs.CV", "cs.CL", "cs.LG"], "title": "Grounding of Textual Phrases in Images by Reconstruction"}, {"abstract": "Several deep learning models have been proposed for question answering.\nHowever, due to their single-pass nature, they have no way to recover from\nlocal maxima corresponding to incorrect answers. To address this problem, we\nintroduce the Dynamic Coattention Network (DCN) for question answering. The DCN\nfirst fuses co-dependent representations of the question and the document in\norder to focus on relevant parts of both. Then a dynamic pointing decoder\niterates over potential answer spans. This iterative procedure enables the\nmodel to recover from initial local maxima corresponding to incorrect answers.\nOn the Stanford question answering dataset, a single DCN model improves the\nprevious state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains\n80.4% F1.", "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "category": "cs.CL", "comment": "14 pages, 7 figures, International Conference on Learning\n  Representations 2017", "img": "/static/thumbs/1611.01604v3.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1611.01604v3", "num_discussion": 0, "originally_published_time": "11/5/2016", "pid": "1611.01604v3", "published_time": "2/13/2017", "rawpid": "1611.01604", "tags": ["cs.CL", "cs.AI"], "title": "Dynamic Coattention Networks For Question Answering"}, {"abstract": "In this work we implement a training of a Language Model (LM), using\nRecurrent Neural Network (RNN) and GloVe word embeddings, introduced by\nPennigton et al. in [1]. The implementation is following the general idea of\ntraining RNNs for LM tasks presented in [2], but is rather using Gated\nRecurrent Unit (GRU) [3] for a memory cell, and not the more commonly used LSTM\n[4].", "authors": ["Victor Makarenkov", "Bracha Shapira", "Lior Rokach"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1610.03759v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1610.03759v2", "num_discussion": 0, "originally_published_time": "10/12/2016", "pid": "1610.03759v2", "published_time": "2/5/2017", "rawpid": "1610.03759", "tags": ["cs.CL"], "title": "Language Models with Pre-Trained (GloVe) Word Embeddings"}, {"abstract": "We systematically explore regularizing neural networks by penalizing low\nentropy output distributions. We show that penalizing low entropy output\ndistributions, which has been shown to improve exploration in reinforcement\nlearning, acts as a strong regularizer in supervised learning. Furthermore, we\nconnect a maximum entropy based confidence penalty to label smoothing through\nthe direction of the KL divergence. We exhaustively evaluate the proposed\nconfidence penalty and label smoothing on 6 common benchmarks: image\nclassification (MNIST and Cifar-10), language modeling (Penn Treebank), machine\ntranslation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ).\nWe find that both label smoothing and the confidence penalty improve\nstate-of-the-art models across benchmarks without modifying existing\nhyperparameters, suggesting the wide applicability of these regularizers.", "authors": ["Gabriel Pereyra", "George Tucker", "Jan Chorowski", "\u0141ukasz Kaiser", "Geoffrey Hinton"], "category": "cs.NE", "comment": "Submitted to ICLR 2017", "img": "/static/thumbs/1701.06548v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1701.06548v1", "num_discussion": 0, "originally_published_time": "1/23/2017", "pid": "1701.06548v1", "published_time": "1/23/2017", "rawpid": "1701.06548", "tags": ["cs.NE", "cs.LG"], "title": "Regularizing Neural Networks by Penalizing Confident Output\n  Distributions"}, {"abstract": "We consider generation and comprehension of natural language referring\nexpression for objects in an image. Unlike generic \"image captioning\" which\nlacks natural standard evaluation criteria, quality of a referring expression\nmay be measured by the receiver's ability to correctly infer which object is\nbeing described. Following this intuition, we propose two approaches to utilize\nmodels trained for comprehension task to generate better expressions. First, we\nuse a comprehension module trained on human-generated expressions, as a\n\"critic\" of referring expression generator. The comprehension module serves as\na differentiable proxy of human evaluation, providing training signal to the\ngeneration module. Second, we use the comprehension module in a\ngenerate-and-rerank pipeline, which chooses from candidate expressions\ngenerated by a model according to their performance on the comprehension task.\nWe show that both approaches lead to improved referring expression generation\non multiple benchmark datasets.", "authors": ["Ruotian Luo", "Gregory Shakhnarovich"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1701.03439v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1701.03439v1", "num_discussion": 0, "originally_published_time": "1/12/2017", "pid": "1701.03439v1", "published_time": "1/12/2017", "rawpid": "1701.03439", "tags": ["cs.CV"], "title": "Comprehension-guided referring expressions"}, {"abstract": "A key problem in structured output prediction is direct optimization of the\ntask reward function that matters for test evaluation. This paper presents a\nsimple and computationally efficient approach to incorporate task reward into a\nmaximum likelihood framework. By establishing a link between the log-likelihood\nand expected reward objectives, we show that an optimal regularized expected\nreward is achieved when the conditional distribution of the outputs given the\ninputs is proportional to their exponentiated scaled rewards. Accordingly, we\npresent a framework to smooth the predictive probability of the outputs using\ntheir corresponding rewards. We optimize the conditional log-probability of\naugmented outputs that are sampled proportionally to their exponentiated scaled\nrewards. Experiments on neural sequence to sequence models for speech\nrecognition and machine translation show notable improvements over a maximum\nlikelihood baseline by using reward augmented maximum likelihood (RAML), where\nthe rewards are defined as the negative edit distance between the outputs and\nthe ground truth labels.", "authors": ["Mohammad Norouzi", "Samy Bengio", "Zhifeng Chen", "Navdeep Jaitly", "Mike Schuster", "Yonghui Wu", "Dale Schuurmans"], "category": "cs.LG", "comment": "NIPS 2016", "img": "/static/thumbs/1609.00150v3.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1609.00150v3", "num_discussion": 0, "originally_published_time": "9/1/2016", "pid": "1609.00150v3", "published_time": "1/4/2017", "rawpid": "1609.00150", "tags": ["cs.LG"], "title": "Reward Augmented Maximum Likelihood for Neural Structured Prediction"}, {"abstract": "We introduce a new neural architecture to learn the conditional probability\nof an output sequence with elements that are discrete tokens corresponding to\npositions in an input sequence. Such problems cannot be trivially addressed by\nexistent approaches such as sequence-to-sequence and Neural Turing Machines,\nbecause the number of target classes in each step of the output depends on the\nlength of the input, which is variable. Problems such as sorting variable sized\nsequences, and various combinatorial optimization problems belong to this\nclass. Our model solves the problem of variable size output dictionaries using\na recently proposed mechanism of neural attention. It differs from the previous\nattention attempts in that, instead of using attention to blend hidden units of\nan encoder to a context vector at each decoder step, it uses attention as a\npointer to select a member of the input sequence as the output. We call this\narchitecture a Pointer Net (Ptr-Net). We show Ptr-Nets can be used to learn\napproximate solutions to three challenging geometric problems -- finding planar\nconvex hulls, computing Delaunay triangulations, and the planar Travelling\nSalesman Problem -- using training examples alone. Ptr-Nets not only improve\nover sequence-to-sequence with input attention, but also allow us to generalize\nto variable size output dictionaries. We show that the learnt models generalize\nbeyond the maximum lengths they were trained on. We hope our results on these\ntasks will encourage a broader exploration of neural learning for discrete\nproblems.", "authors": ["Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1506.03134v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1506.03134v2", "num_discussion": 0, "originally_published_time": "6/9/2015", "pid": "1506.03134v2", "published_time": "1/2/2017", "rawpid": "1506.03134", "tags": ["stat.ML", "cs.CG", "cs.LG", "cs.NE"], "title": "Pointer Networks"}, {"abstract": "We present a method for detecting objects in images using a single deep\nneural network. Our approach, named SSD, discretizes the output space of\nbounding boxes into a set of default boxes over different aspect ratios and\nscales per feature map location. At prediction time, the network generates\nscores for the presence of each object category in each default box and\nproduces adjustments to the box to better match the object shape. Additionally,\nthe network combines predictions from multiple feature maps with different\nresolutions to naturally handle objects of various sizes. Our SSD model is\nsimple relative to methods that require object proposals because it completely\neliminates proposal generation and subsequent pixel or feature resampling stage\nand encapsulates all computation in a single network. This makes SSD easy to\ntrain and straightforward to integrate into systems that require a detection\ncomponent. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets\nconfirm that SSD has comparable accuracy to methods that utilize an additional\nobject proposal step and is much faster, while providing a unified framework\nfor both training and inference. Compared to other single stage methods, SSD\nhas much better accuracy, even with a smaller input image size. For $300\\times\n300$ input, SSD achieves 72.1% mAP on VOC2007 test at 58 FPS on a Nvidia Titan\nX and for $500\\times 500$ input, SSD achieves 75.1% mAP, outperforming a\ncomparable state of the art Faster R-CNN model. Code is available at\nhttps://github.com/weiliu89/caffe/tree/ssd .", "authors": ["Wei Liu", "Dragomir Anguelov", "Dumitru Erhan", "Christian Szegedy", "Scott Reed", "Cheng-Yang Fu", "Alexander C. Berg"], "category": "cs.CV", "comment": "ECCV 2016", "img": "/static/thumbs/1512.02325v5.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1512.02325v5", "num_discussion": 0, "originally_published_time": "12/8/2015", "pid": "1512.02325v5", "published_time": "12/29/2016", "rawpid": "1512.02325", "tags": ["cs.CV"], "title": "SSD: Single Shot MultiBox Detector"}, {"abstract": "We introduce YOLO9000, a state-of-the-art, real-time object detection system\nthat can detect over 9000 object categories. First we propose various\nimprovements to the YOLO detection method, both novel and drawn from prior\nwork. The improved model, YOLOv2, is state-of-the-art on standard detection\ntasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At\n40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like\nFaster RCNN with ResNet and SSD while still running significantly faster.\nFinally we propose a method to jointly train on object detection and\nclassification. Using this method we train YOLO9000 simultaneously on the COCO\ndetection dataset and the ImageNet classification dataset. Our joint training\nallows YOLO9000 to predict detections for object classes that don't have\nlabelled detection data. We validate our approach on the ImageNet detection\ntask. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite\nonly having detection data for 44 of the 200 classes. On the 156 classes not in\nCOCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes;\nit predicts detections for more than 9000 different object categories. And it\nstill runs in real-time.", "authors": ["Joseph Redmon", "Ali Farhadi"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1612.08242v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1612.08242v1", "num_discussion": 1, "originally_published_time": "12/25/2016", "pid": "1612.08242v1", "published_time": "12/25/2016", "rawpid": "1612.08242", "tags": ["cs.CV"], "title": "YOLO9000: Better, Faster, Stronger"}, {"abstract": "We introduce a new multi-modal task for computer systems, posed as a combined\nvision-language comprehension challenge: identifying the most suitable text\ndescribing a scene, given several similar options. Accomplishing the task\nentails demonstrating comprehension beyond just recognizing \"keywords\" (or\nkey-phrases) and their corresponding visual concepts. Instead, it requires an\nalignment between the representations of the two modalities that achieves a\nvisually-grounded \"understanding\" of various linguistic elements and their\ndependencies. This new task also admits an easy-to-compute and well-studied\nmetric: the accuracy in detecting the true target among the decoys.\n  The paper makes several contributions: an effective and extensible mechanism\nfor generating decoys from (human-created) image captions; an instance of\napplying this mechanism, yielding a large-scale machine comprehension dataset\n(based on the COCO images and captions) that we make publicly available; human\nevaluation results on this dataset, informing a performance upper-bound; and\nseveral baseline and competitive learning approaches that illustrate the\nutility of the proposed task and dataset in advancing both image and language\ncomprehension. We also show that, in a multi-task learning setting, the\nperformance on the proposed task is positively correlated with the end-to-end\ntask of image captioning.", "authors": ["Nan Ding", "Sebastian Goodman", "Fei Sha", "Radu Soricut"], "category": "cs.CL", "comment": "11 pages", "img": "/static/thumbs/1612.07833v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1612.07833v1", "num_discussion": 0, "originally_published_time": "12/22/2016", "pid": "1612.07833v1", "published_time": "12/22/2016", "rawpid": "1612.07833", "tags": ["cs.CL", "cs.CV"], "title": "Understanding Image and Text Simultaneously: a Dual Vision-Language\n  Machine Comprehension Task"}, {"abstract": "LSTMs have become a basic building block for many deep NLP models. In recent\nyears, many improvements and variations have been proposed for deep sequence\nmodels in general, and LSTMs in particular. We propose and analyze a series of\naugmentations and modifications to LSTM networks resulting in improved\nperformance for text classification datasets. We observe compounding\nimprovements on traditional LSTMs using Monte Carlo test-time model averaging,\naverage pooling, and residual connections, along with four other suggested\nmodifications. Our analysis provides a simple, reliable, and high quality\nbaseline model.", "authors": ["Shayne Longpre", "Sabeek Pradhan", "Caiming Xiong", "Richard Socher"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1611.05104v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1611.05104v2", "num_discussion": 0, "originally_published_time": "11/16/2016", "pid": "1611.05104v2", "published_time": "12/17/2016", "rawpid": "1611.05104", "tags": ["cs.CL", "cs.AI"], "title": "A Way out of the Odyssey: Analyzing and Combining Recent Insights for\n  LSTMs"}, {"abstract": "Much recent progress in Vision-to-Language problems has been achieved through\na combination of Convolutional Neural Networks (CNNs) and Recurrent Neural\nNetworks (RNNs). This approach does not explicitly represent high-level\nsemantic concepts, but rather seeks to progress directly from image features to\ntext. In this paper we first propose a method of incorporating high-level\nconcepts into the successful CNN-RNN approach, and show that it achieves a\nsignificant improvement on the state-of-the-art in both image captioning and\nvisual question answering. We further show that the same mechanism can be used\nto incorporate external knowledge, which is critically important for answering\nhigh level visual questions. Specifically, we design a visual question\nanswering model that combines an internal representation of the content of an\nimage with information extracted from a general knowledge base to answer a\nbroad range of image-based questions. It particularly allows questions to be\nasked about the contents of an image, even when the image itself does not\ncontain a complete answer. Our final model achieves the best reported results\non both image captioning and visual question answering on several benchmark\ndatasets.", "authors": ["Qi Wu", "Chunhua Shen", "Anton van den Hengel", "Peng Wang", "Anthony Dick"], "category": "cs.CV", "comment": "14 pages. arXiv admin note: text overlap with arXiv:1511.06973", "img": "/static/thumbs/1603.02814v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1603.02814v2", "num_discussion": 0, "originally_published_time": "3/9/2016", "pid": "1603.02814v2", "published_time": "12/16/2016", "rawpid": "1603.02814", "tags": ["cs.CV"], "title": "Image Captioning and Visual Question Answering Based on Attributes and\n  External Knowledge"}, {"abstract": "Along with the prosperity of recurrent neural network in modelling sequential\ndata and the power of attention mechanism in automatically identify salient\ninformation, image captioning, a.k.a., image description, has been remarkably\nadvanced in recent years. Nonetheless, most existing paradigms may suffer from\nthe deficiency of invariance to images with different scaling, rotation, etc.;\nand effective integration of standalone attention to form a holistic end-to-end\nsystem. In this paper, we propose a novel image captioning architecture, termed\nRecurrent Image Captioner (\\textbf{RIC}), which allows visual encoder and\nlanguage decoder to coherently cooperate in a recurrent manner. Specifically,\nwe first equip CNN-based visual encoder with a differentiable layer to enable\nspatially invariant transformation of visual signals. Moreover, we deploy an\nattention filter module (differentiable) between encoder and decoder to\ndynamically determine salient visual parts. We also employ bidirectional LSTM\nto preprocess sentences for generating better textual representations. Besides,\nwe propose to exploit variational inference to optimize the whole architecture.\nExtensive experimental results on three benchmark datasets (i.e., Flickr8k,\nFlickr30k and MS COCO) demonstrate the superiority of our proposed architecture\nas compared to most of the state-of-the-art methods.", "authors": ["Hao Liu", "Yang Yang", "Fumin Shen", "Lixin Duan", "Heng Tao Shen"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1612.04949v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1612.04949v1", "num_discussion": 0, "originally_published_time": "12/15/2016", "pid": "1612.04949v1", "published_time": "12/15/2016", "rawpid": "1612.04949", "tags": ["cs.CV", "cs.CL"], "title": "Recurrent Image Captioner: Describing Images with Spatial-Invariant\n  Transformation and Attention Filtering"}, {"abstract": "Top-k error is currently a popular performance measure on large scale image\nclassification benchmarks such as ImageNet and Places. Despite its wide\nacceptance, our understanding of this metric is limited as most of the previous\nresearch is focused on its special case, the top-1 error. In this work, we\nexplore two directions that shed more light on the top-k error. First, we\nprovide an in-depth analysis of established and recently proposed single-label\nmulticlass methods along with a detailed account of efficient optimization\nalgorithms for them. Our results indicate that the softmax loss and the smooth\nmulticlass SVM are surprisingly competitive in top-k error uniformly across all\nk, which can be explained by our analysis of multiclass top-k calibration.\nFurther improvements for a specific k are possible with a number of proposed\ntop-k loss functions. Second, we use the top-k methods to explore the\ntransition from multiclass to multilabel learning. In particular, we find that\nit is possible to obtain effective multilabel classifiers on Pascal VOC using a\nsingle label per image for training, while the gap between multiclass and\nmultilabel methods on MS COCO is more significant. Finally, our contribution of\nefficient algorithms for training with the considered top-k and multilabel loss\nfunctions is of independent interest.", "authors": ["Maksim Lapin", "Matthias Hein", "Bernt Schiele"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1612.03663v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1612.03663v1", "num_discussion": 0, "originally_published_time": "12/12/2016", "pid": "1612.03663v1", "published_time": "12/12/2016", "rawpid": "1612.03663", "tags": ["cs.CV", "stat.ML"], "title": "Analysis and Optimization of Loss Functions for Multiclass, Top-k, and\n  Multilabel Classification"}, {"abstract": "Visual attention plays an important role to understand images and\ndemonstrates its effectiveness in generating natural language descriptions of\nimages. On the other hand, recent studies show that language associated with an\nimage can steer visual attention in the scene during our cognitive process.\nInspired by this, we introduce a text-guided attention model for image\ncaptioning, which learns to drive visual attention using associated captions.\nFor this model, we propose an exemplar-based learning approach that retrieves\nfrom training data associated captions with each image, and use them to learn\nattention on visual features. Our attention model enables to describe a\ndetailed state of scenes by distinguishing small or confusable objects\neffectively. We validate our model on MS-COCO Captioning benchmark and achieve\nthe state-of-the-art performance in standard metrics.", "authors": ["Jonghwan Mun", "Minsu Cho", "Bohyung Han"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1612.03557v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1612.03557v1", "num_discussion": 0, "originally_published_time": "12/12/2016", "pid": "1612.03557v1", "published_time": "12/12/2016", "rawpid": "1612.03557", "tags": ["cs.CV"], "title": "Text-guided Attention Model for Image Captioning"}, {"abstract": "Multiple instance learning (MIL) is a form of weakly supervised learning\nwhere training instances are arranged in sets, called bags, and a label is\nprovided for the entire bag. This formulation is gaining interest because it\nnaturally fits various problems and allows to leverage weakly labeled data.\nConsequently, it has been used in diverse application fields such as computer\nvision and document classification. However, learning from bags raises\nimportant challenges that are unique to MIL. This paper provides a\ncomprehensive survey of the characteristics which define and differentiate the\ntypes of MIL problems. Until now, these problem characteristics have not been\nformally identified and described. As a result, the variations in performance\nof MIL algorithms from one data set to another are difficult to explain. In\nthis paper, MIL problem characteristics are grouped into four broad categories:\nthe composition of the bags, the types of data distribution, the ambiguity of\ninstance labels, and the task to be performed. Methods specialized to address\neach category are reviewed. Then, the extent to which these characteristics\nmanifest themselves in key MIL application areas are described. Finally,\nexperiments are conducted to compare the performance of 16 state-of-the-art MIL\nmethods on selected problem characteristics. This paper provides insight on how\nthe problem characteristics affect MIL algorithms, recommendations for future\nbenchmarking and promising avenues for research.", "authors": ["Marc-Andr\u00e9 Carbonneau", "Veronika Cheplygina", "Eric Granger", "Ghyslain Gagnon"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1612.03365v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1612.03365v1", "num_discussion": 0, "originally_published_time": "12/11/2016", "pid": "1612.03365v1", "published_time": "12/11/2016", "rawpid": "1612.03365", "tags": ["cs.CV", "cs.AI", "cs.IR"], "title": "Multiple Instance Learning: A Survey of Problem Characteristics and\n  Applications"}, {"abstract": "This paper investigates how linguistic knowledge mined from large text\ncorpora can aid the generation of natural language descriptions of videos.\nSpecifically, we integrate both a neural language model and distributional\nsemantics trained on large text corpora into a recent LSTM-based architecture\nfor video description. We evaluate our approach on a collection of Youtube\nvideos as well as two large movie description datasets showing significant\nimprovements in grammaticality while modestly improving descriptive quality.", "authors": ["Subhashini Venugopalan", "Lisa Anne Hendricks", "Raymond Mooney", "Kate Saenko"], "category": "cs.CL", "comment": "Accepted at EMNLP 2016. Project page:\n  http://vsubhashini.github.io/language_fusion.html", "img": "/static/thumbs/1604.01729v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1604.01729v2", "num_discussion": 0, "originally_published_time": "4/6/2016", "pid": "1604.01729v2", "published_time": "11/29/2016", "rawpid": "1604.01729", "tags": ["cs.CL", "cs.CV"], "title": "Improving LSTM-based Video Description with Linguistic Knowledge Mined\n  from Text"}, {"abstract": "Attention mechanisms have attracted considerable interest in image captioning\ndue to its powerful performance. However, existing methods use only visual\ncontent as attention and whether textual context can improve attention in image\ncaptioning remains unsolved. To explore this problem, we propose a novel\nattention mechanism, called \\textit{text-conditional attention}, which allows\nthe caption generator to focus on certain image features given previously\ngenerated text. To obtain text-related image features for our attention model,\nwe adopt the guiding Long Short-Term Memory (gLSTM) captioning architecture\nwith CNN fine-tuning. Our proposed method allows joint learning of the image\nembedding, text embedding, text-conditional attention and language model with\none network architecture in an end-to-end manner. We perform extensive\nexperiments on the MS-COCO dataset. The experimental results show that our\nmethod outperforms state-of-the-art captioning methods on various quantitative\nmetrics as well as in human evaluation, which supports the use of our\ntext-conditional attention in image captioning.", "authors": ["Luowei Zhou", "Chenliang Xu", "Parker Koch", "Jason J. Corso"], "category": "cs.CV", "comment": "source code is available online", "img": "/static/thumbs/1606.04621v3.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1606.04621v3", "num_discussion": 0, "originally_published_time": "6/15/2016", "pid": "1606.04621v3", "published_time": "11/24/2016", "rawpid": "1606.04621", "tags": ["cs.CV"], "title": "Watch What You Just Said: Image Captioning with Text-Conditional\n  Attention"}, {"abstract": "Attention mechanisms have recently been introduced in deep learning for\nvarious tasks in natural language processing and computer vision. But despite\ntheir popularity, the \"correctness\" of the implicitly-learned attention maps\nhas only been assessed qualitatively by visualization of several examples. In\nthis paper we focus on evaluating and improving the correctness of attention in\nneural image captioning models. Specifically, we propose a quantitative\nevaluation metric for the consistency between the generated attention maps and\nhuman annotations, using recently released datasets with alignment between\nregions in images and entities in captions. We then propose novel models with\ndifferent levels of explicit supervision for learning attention maps during\ntraining. The supervision can be strong when alignment between regions and\ncaption entities are available, or weak when only object segments and\ncategories are provided. We show on the popular Flickr30k and COCO datasets\nthat introducing supervision of attention maps during training solidly improves\nboth attention correctness and caption quality, showing the promise of making\nmachine perception more human-like.", "authors": ["Chenxi Liu", "Junhua Mao", "Fei Sha", "Alan Yuille"], "category": "cs.CV", "comment": "To appear in AAAI-17. See http://www.cs.jhu.edu/~cxliu/ for\n  supplementary material", "img": "/static/thumbs/1605.09553v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1605.09553v2", "num_discussion": 0, "originally_published_time": "5/31/2016", "pid": "1605.09553v2", "published_time": "11/23/2016", "rawpid": "1605.09553", "tags": ["cs.CV", "cs.CL", "cs.LG"], "title": "Attention Correctness in Neural Image Captioning"}, {"abstract": "Recurrent neural networks are a powerful tool for modeling sequential data,\nbut the dependence of each timestep's computation on the previous timestep's\noutput limits parallelism and makes RNNs unwieldy for very long sequences. We\nintroduce quasi-recurrent neural networks (QRNNs), an approach to neural\nsequence modeling that alternates convolutional layers, which apply in parallel\nacross timesteps, and a minimalist recurrent pooling function that applies in\nparallel across channels. Despite lacking trainable recurrent layers, stacked\nQRNNs have better predictive accuracy than stacked LSTMs of the same hidden\nsize. Due to their increased parallelism, they are up to 16 times faster at\ntrain and test time. Experiments on language modeling, sentiment\nclassification, and character-level neural machine translation demonstrate\nthese advantages and underline the viability of QRNNs as a basic building block\nfor a variety of sequence tasks.", "authors": ["James Bradbury", "Stephen Merity", "Caiming Xiong", "Richard Socher"], "category": "cs.NE", "comment": "Submitted to conference track at ICLR 2017", "img": "/static/thumbs/1611.01576v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1611.01576v2", "num_discussion": 0, "originally_published_time": "11/5/2016", "pid": "1611.01576v2", "published_time": "11/21/2016", "rawpid": "1611.01576", "tags": ["cs.NE", "cs.AI", "cs.CL", "cs.LG"], "title": "Quasi-Recurrent Neural Networks"}, {"abstract": "We introduce a general and simple structural design called Multiplicative\nIntegration (MI) to improve recurrent neural networks (RNNs). MI changes the\nway in which information from difference sources flows and is integrated in the\ncomputational building block of an RNN, while introducing almost no extra\nparameters. The new structure can be easily embedded into many popular RNN\nmodels, including LSTMs and GRUs. We empirically analyze its learning behaviour\nand conduct evaluations on several tasks using different RNN models. Our\nexperimental results demonstrate that Multiplicative Integration can provide a\nsubstantial performance boost over many of the existing RNN models.", "authors": ["Yuhuai Wu", "Saizheng Zhang", "Ying Zhang", "Yoshua Bengio", "Ruslan Salakhutdinov"], "category": "cs.LG", "comment": "10 pages, 2 figures; To appear in NIPS2016", "img": "/static/thumbs/1606.06630v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1606.06630v2", "num_discussion": 0, "originally_published_time": "6/21/2016", "pid": "1606.06630v2", "published_time": "11/12/2016", "rawpid": "1606.06630", "tags": ["cs.LG"], "title": "On Multiplicative Integration with Recurrent Neural Networks"}, {"abstract": "Sequence-to-Sequence (seq2seq) modeling has rapidly become an important\ngeneral-purpose NLP tool that has proven effective for many text-generation and\nsequence-labeling tasks. Seq2seq builds on deep neural language modeling and\ninherits its remarkable accuracy in estimating local, next-word distributions.\nIn this work, we introduce a model and beam-search training scheme, based on\nthe work of Daume III and Marcu (2005), that extends seq2seq to learn global\nsequence scores. This structured approach avoids classical biases associated\nwith local training and unifies the training loss with the test-time usage,\nwhile preserving the proven model architecture of seq2seq and its efficient\ntraining approach. We show that our system outperforms a highly-optimized\nattention-based seq2seq system and other baselines on three different sequence\nto sequence tasks: word ordering, parsing, and machine translation.", "authors": ["Sam Wiseman", "Alexander M. Rush"], "category": "cs.CL", "comment": "EMNLP 2016 camera-ready", "img": "/static/thumbs/1606.02960v2.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1606.02960v2", "num_discussion": 0, "originally_published_time": "6/9/2016", "pid": "1606.02960v2", "published_time": "11/10/2016", "rawpid": "1606.02960", "tags": ["cs.CL", "cs.LG", "cs.NE", "stat.ML"], "title": "Sequence-to-Sequence Learning as Beam-Search Optimization"}, {"abstract": "Automatically describing an image with a natural language has been an\nemerging challenge in both fields of computer vision and natural language\nprocessing. In this paper, we present Long Short-Term Memory with Attributes\n(LSTM-A) - a novel architecture that integrates attributes into the successful\nConvolutional Neural Networks (CNNs) plus Recurrent Neural Networks (RNNs)\nimage captioning framework, by training them in an end-to-end manner. To\nincorporate attributes, we construct variants of architectures by feeding image\nrepresentations and attributes into RNNs in different ways to explore the\nmutual but also fuzzy relationship between them. Extensive experiments are\nconducted on COCO image captioning dataset and our framework achieves superior\nresults when compared to state-of-the-art deep models. Most remarkably, we\nobtain METEOR/CIDEr-D of 25.2%/98.6% on testing data of widely used and\npublicly available splits in (Karpathy & Fei-Fei, 2015) when extracting image\nrepresentations by GoogleNet and achieve to date top-1 performance on COCO\ncaptioning Leaderboard.", "authors": ["Ting Yao", "Yingwei Pan", "Yehao Li", "Zhaofan Qiu", "Tao Mei"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1611.01646v1.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1611.01646v1", "num_discussion": 0, "originally_published_time": "11/5/2016", "pid": "1611.01646v1", "published_time": "11/5/2016", "rawpid": "1611.01646", "tags": ["cs.CV"], "title": "Boosting Image Captioning with Attributes"}, {"abstract": "We propose a novel extension of the encoder-decoder framework, called a\nreview network. The review network is generic and can enhance any existing\nencoder- decoder model: in this paper, we consider RNN decoders with both CNN\nand RNN encoders. The review network performs a number of review steps with\nattention mechanism on the encoder hidden states, and outputs a thought vector\nafter each review step; the thought vectors are used as the input of the\nattention mechanism in the decoder. We show that conventional encoder-decoders\nare a special case of our framework. Empirically, we show that our framework\nimproves over state-of- the-art encoder-decoder systems on the tasks of image\ncaptioning and source code captioning.", "authors": ["Zhilin Yang", "Ye Yuan", "Yuexin Wu", "Ruslan Salakhutdinov", "William W. Cohen"], "category": "cs.LG", "comment": "NIPS 2016", "img": "/static/thumbs/1605.07912v4.pdf.jpg", "in_library": 1, "link": "http://arxiv.org/abs/1605.07912v4", "num_discussion": 0, "originally_published_time": "5/25/2016", "pid": "1605.07912v4", "published_time": "10/27/2016", "rawpid": "1605.07912", "tags": ["cs.LG", "cs.CL", "cs.CV"], "title": "Review Networks for Caption Generation"}]