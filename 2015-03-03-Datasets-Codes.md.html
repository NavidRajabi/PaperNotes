<!DOCTYPE html>
<html>
  <head>
      <meta charset="utf-8" />
      <title>2015-03-03-Datasets-Codes</title>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX","output/HTML-CSS"],
    extensions: [],
    TeX: {
      extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
    },
    showMathMenu: false
  });
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js">
</script>
      <style>.markdown-preview:not([data-use-github-style]) { padding: 2em; font-size: 1.2em; color: rgb(171, 178, 191); overflow: auto; background-color: rgb(40, 44, 52); }
.markdown-preview:not([data-use-github-style]) > :first-child { margin-top: 0px; }
.markdown-preview:not([data-use-github-style]) h1, .markdown-preview:not([data-use-github-style]) h2, .markdown-preview:not([data-use-github-style]) h3, .markdown-preview:not([data-use-github-style]) h4, .markdown-preview:not([data-use-github-style]) h5, .markdown-preview:not([data-use-github-style]) h6 { line-height: 1.2; margin-top: 1.5em; margin-bottom: 0.5em; color: rgb(255, 255, 255); }
.markdown-preview:not([data-use-github-style]) h1 { font-size: 2.4em; font-weight: 300; }
.markdown-preview:not([data-use-github-style]) h2 { font-size: 1.8em; font-weight: 400; }
.markdown-preview:not([data-use-github-style]) h3 { font-size: 1.5em; font-weight: 500; }
.markdown-preview:not([data-use-github-style]) h4 { font-size: 1.2em; font-weight: 600; }
.markdown-preview:not([data-use-github-style]) h5 { font-size: 1.1em; font-weight: 600; }
.markdown-preview:not([data-use-github-style]) h6 { font-size: 1em; font-weight: 600; }
.markdown-preview:not([data-use-github-style]) strong { color: rgb(255, 255, 255); }
.markdown-preview:not([data-use-github-style]) del { color: rgb(124, 135, 156); }
.markdown-preview:not([data-use-github-style]) a, .markdown-preview:not([data-use-github-style]) a code { color: rgb(82, 139, 255); }
.markdown-preview:not([data-use-github-style]) img { max-width: 100%; }
.markdown-preview:not([data-use-github-style]) > p { margin-top: 0px; margin-bottom: 1.5em; }
.markdown-preview:not([data-use-github-style]) > ul, .markdown-preview:not([data-use-github-style]) > ol { margin-bottom: 1.5em; }
.markdown-preview:not([data-use-github-style]) blockquote { margin: 1.5em 0px; font-size: inherit; color: rgb(124, 135, 156); border-color: rgb(75, 83, 98); border-width: 4px; }
.markdown-preview:not([data-use-github-style]) hr { margin: 3em 0px; border-top: 2px dashed rgb(75, 83, 98); background: none; }
.markdown-preview:not([data-use-github-style]) table { margin: 1.5em 0px; }
.markdown-preview:not([data-use-github-style]) th { color: rgb(255, 255, 255); }
.markdown-preview:not([data-use-github-style]) th, .markdown-preview:not([data-use-github-style]) td { padding: 0.66em 1em; border: 1px solid rgb(75, 83, 98); }
.markdown-preview:not([data-use-github-style]) code { color: rgb(255, 255, 255); background-color: rgb(58, 63, 75); }
.markdown-preview:not([data-use-github-style]) pre.editor-colors { margin: 1.5em 0px; padding: 1em; font-size: 0.92em; border-radius: 3px; background-color: rgb(49, 54, 63); }
.markdown-preview:not([data-use-github-style]) kbd { color: rgb(255, 255, 255); border-width: 1px 1px 2px; border-style: solid; border-color: rgb(75, 83, 98) rgb(75, 83, 98) rgb(62, 68, 81); background-color: rgb(58, 63, 75); }
.markdown-preview[data-use-github-style] { font-family: "Helvetica Neue", Helvetica, "Segoe UI", Arial, freesans, sans-serif; line-height: 1.6; word-wrap: break-word; padding: 30px; font-size: 16px; color: rgb(51, 51, 51); overflow: scroll; background-color: rgb(255, 255, 255); }
.markdown-preview[data-use-github-style] > :first-child { margin-top: 0px !important; }
.markdown-preview[data-use-github-style] > :last-child { margin-bottom: 0px !important; }
.markdown-preview[data-use-github-style] a:not([href]) { color: inherit; text-decoration: none; }
.markdown-preview[data-use-github-style] .absent { color: rgb(204, 0, 0); }
.markdown-preview[data-use-github-style] .anchor { position: absolute; top: 0px; left: 0px; display: block; padding-right: 6px; padding-left: 30px; margin-left: -30px; }
.markdown-preview[data-use-github-style] .anchor:focus { outline: none; }
.markdown-preview[data-use-github-style] h1, .markdown-preview[data-use-github-style] h2, .markdown-preview[data-use-github-style] h3, .markdown-preview[data-use-github-style] h4, .markdown-preview[data-use-github-style] h5, .markdown-preview[data-use-github-style] h6 { position: relative; margin-top: 1em; margin-bottom: 16px; font-weight: bold; line-height: 1.4; }
.markdown-preview[data-use-github-style] h1 .octicon-link, .markdown-preview[data-use-github-style] h2 .octicon-link, .markdown-preview[data-use-github-style] h3 .octicon-link, .markdown-preview[data-use-github-style] h4 .octicon-link, .markdown-preview[data-use-github-style] h5 .octicon-link, .markdown-preview[data-use-github-style] h6 .octicon-link { display: none; color: rgb(0, 0, 0); vertical-align: middle; }
.markdown-preview[data-use-github-style] h1:hover .anchor, .markdown-preview[data-use-github-style] h2:hover .anchor, .markdown-preview[data-use-github-style] h3:hover .anchor, .markdown-preview[data-use-github-style] h4:hover .anchor, .markdown-preview[data-use-github-style] h5:hover .anchor, .markdown-preview[data-use-github-style] h6:hover .anchor { padding-left: 8px; margin-left: -30px; text-decoration: none; }
.markdown-preview[data-use-github-style] h1:hover .anchor .octicon-link, .markdown-preview[data-use-github-style] h2:hover .anchor .octicon-link, .markdown-preview[data-use-github-style] h3:hover .anchor .octicon-link, .markdown-preview[data-use-github-style] h4:hover .anchor .octicon-link, .markdown-preview[data-use-github-style] h5:hover .anchor .octicon-link, .markdown-preview[data-use-github-style] h6:hover .anchor .octicon-link { display: inline-block; }
.markdown-preview[data-use-github-style] h1 tt, .markdown-preview[data-use-github-style] h2 tt, .markdown-preview[data-use-github-style] h3 tt, .markdown-preview[data-use-github-style] h4 tt, .markdown-preview[data-use-github-style] h5 tt, .markdown-preview[data-use-github-style] h6 tt, .markdown-preview[data-use-github-style] h1 code, .markdown-preview[data-use-github-style] h2 code, .markdown-preview[data-use-github-style] h3 code, .markdown-preview[data-use-github-style] h4 code, .markdown-preview[data-use-github-style] h5 code, .markdown-preview[data-use-github-style] h6 code { font-size: inherit; }
.markdown-preview[data-use-github-style] h1 { padding-bottom: 0.3em; font-size: 2.25em; line-height: 1.2; border-bottom: 1px solid rgb(238, 238, 238); }
.markdown-preview[data-use-github-style] h1 .anchor { line-height: 1; }
.markdown-preview[data-use-github-style] h2 { padding-bottom: 0.3em; font-size: 1.75em; line-height: 1.225; border-bottom: 1px solid rgb(238, 238, 238); }
.markdown-preview[data-use-github-style] h2 .anchor { line-height: 1; }
.markdown-preview[data-use-github-style] h3 { font-size: 1.5em; line-height: 1.43; }
.markdown-preview[data-use-github-style] h3 .anchor { line-height: 1.2; }
.markdown-preview[data-use-github-style] h4 { font-size: 1.25em; }
.markdown-preview[data-use-github-style] h4 .anchor { line-height: 1.2; }
.markdown-preview[data-use-github-style] h5 { font-size: 1em; }
.markdown-preview[data-use-github-style] h5 .anchor { line-height: 1.1; }
.markdown-preview[data-use-github-style] h6 { font-size: 1em; color: rgb(119, 119, 119); }
.markdown-preview[data-use-github-style] h6 .anchor { line-height: 1.1; }
.markdown-preview[data-use-github-style] p, .markdown-preview[data-use-github-style] blockquote, .markdown-preview[data-use-github-style] ul, .markdown-preview[data-use-github-style] ol, .markdown-preview[data-use-github-style] dl, .markdown-preview[data-use-github-style] table, .markdown-preview[data-use-github-style] pre { margin-top: 0px; margin-bottom: 16px; }
.markdown-preview[data-use-github-style] hr { height: 4px; padding: 0px; margin: 16px 0px; border: 0px none; background-color: rgb(231, 231, 231); }
.markdown-preview[data-use-github-style] ul, .markdown-preview[data-use-github-style] ol { padding-left: 2em; }
.markdown-preview[data-use-github-style] ul.no-list, .markdown-preview[data-use-github-style] ol.no-list { padding: 0px; list-style-type: none; }
.markdown-preview[data-use-github-style] ul ul, .markdown-preview[data-use-github-style] ul ol, .markdown-preview[data-use-github-style] ol ol, .markdown-preview[data-use-github-style] ol ul { margin-top: 0px; margin-bottom: 0px; }
.markdown-preview[data-use-github-style] li > p { margin-top: 16px; }
.markdown-preview[data-use-github-style] dl { padding: 0px; }
.markdown-preview[data-use-github-style] dl dt { padding: 0px; margin-top: 16px; font-size: 1em; font-style: italic; font-weight: bold; }
.markdown-preview[data-use-github-style] dl dd { padding: 0px 16px; margin-bottom: 16px; }
.markdown-preview[data-use-github-style] blockquote { padding: 0px 15px; color: rgb(119, 119, 119); border-left: 4px solid rgb(221, 221, 221); }
.markdown-preview[data-use-github-style] blockquote > :first-child { margin-top: 0px; }
.markdown-preview[data-use-github-style] blockquote > :last-child { margin-bottom: 0px; }
.markdown-preview[data-use-github-style] table { display: block; width: 100%; overflow: auto; word-break: keep-all; }
.markdown-preview[data-use-github-style] table th { font-weight: bold; }
.markdown-preview[data-use-github-style] table th, .markdown-preview[data-use-github-style] table td { padding: 6px 13px; border: 1px solid rgb(221, 221, 221); }
.markdown-preview[data-use-github-style] table tr { border-top: 1px solid rgb(204, 204, 204); background-color: rgb(255, 255, 255); }
.markdown-preview[data-use-github-style] table tr:nth-child(2n) { background-color: rgb(248, 248, 248); }
.markdown-preview[data-use-github-style] img { max-width: 100%; box-sizing: border-box; }
.markdown-preview[data-use-github-style] .emoji { max-width: none; }
.markdown-preview[data-use-github-style] span.frame { display: block; overflow: hidden; }
.markdown-preview[data-use-github-style] span.frame > span { display: block; float: left; width: auto; padding: 7px; margin: 13px 0px 0px; overflow: hidden; border: 1px solid rgb(221, 221, 221); }
.markdown-preview[data-use-github-style] span.frame span img { display: block; float: left; }
.markdown-preview[data-use-github-style] span.frame span span { display: block; padding: 5px 0px 0px; clear: both; color: rgb(51, 51, 51); }
.markdown-preview[data-use-github-style] span.align-center { display: block; overflow: hidden; clear: both; }
.markdown-preview[data-use-github-style] span.align-center > span { display: block; margin: 13px auto 0px; overflow: hidden; text-align: center; }
.markdown-preview[data-use-github-style] span.align-center span img { margin: 0px auto; text-align: center; }
.markdown-preview[data-use-github-style] span.align-right { display: block; overflow: hidden; clear: both; }
.markdown-preview[data-use-github-style] span.align-right > span { display: block; margin: 13px 0px 0px; overflow: hidden; text-align: right; }
.markdown-preview[data-use-github-style] span.align-right span img { margin: 0px; text-align: right; }
.markdown-preview[data-use-github-style] span.float-left { display: block; float: left; margin-right: 13px; overflow: hidden; }
.markdown-preview[data-use-github-style] span.float-left span { margin: 13px 0px 0px; }
.markdown-preview[data-use-github-style] span.float-right { display: block; float: right; margin-left: 13px; overflow: hidden; }
.markdown-preview[data-use-github-style] span.float-right > span { display: block; margin: 13px auto 0px; overflow: hidden; text-align: right; }
.markdown-preview[data-use-github-style] code, .markdown-preview[data-use-github-style] tt { padding: 0.2em 0px; margin: 0px; font-size: 85%; border-radius: 3px; background-color: rgba(0, 0, 0, 0.0392157); }
.markdown-preview[data-use-github-style] code::before, .markdown-preview[data-use-github-style] tt::before, .markdown-preview[data-use-github-style] code::after, .markdown-preview[data-use-github-style] tt::after { letter-spacing: -0.2em; content: " "; }
.markdown-preview[data-use-github-style] code br, .markdown-preview[data-use-github-style] tt br { display: none; }
.markdown-preview[data-use-github-style] del code { text-decoration: inherit; }
.markdown-preview[data-use-github-style] pre > code { padding: 0px; margin: 0px; font-size: 100%; word-break: normal; white-space: pre; border: 0px; background: transparent; }
.markdown-preview[data-use-github-style] .highlight { margin-bottom: 16px; }
.markdown-preview[data-use-github-style] .highlight pre, .markdown-preview[data-use-github-style] pre { padding: 16px; overflow: auto; font-size: 85%; line-height: 1.45; border-radius: 3px; background-color: rgb(247, 247, 247); }
.markdown-preview[data-use-github-style] .highlight pre { margin-bottom: 0px; word-break: normal; }
.markdown-preview[data-use-github-style] pre { word-wrap: normal; }
.markdown-preview[data-use-github-style] pre code, .markdown-preview[data-use-github-style] pre tt { display: inline; max-width: initial; padding: 0px; margin: 0px; overflow: initial; line-height: inherit; word-wrap: normal; border: 0px; background-color: transparent; }
.markdown-preview[data-use-github-style] pre code::before, .markdown-preview[data-use-github-style] pre tt::before, .markdown-preview[data-use-github-style] pre code::after, .markdown-preview[data-use-github-style] pre tt::after { content: normal; }
.markdown-preview[data-use-github-style] kbd { display: inline-block; padding: 3px 5px; font-size: 11px; line-height: 10px; color: rgb(85, 85, 85); vertical-align: middle; border-width: 1px; border-style: solid; border-color: rgb(204, 204, 204) rgb(204, 204, 204) rgb(187, 187, 187); border-radius: 3px; box-shadow: rgb(187, 187, 187) 0px -1px 0px inset; background-color: rgb(252, 252, 252); }
.markdown-preview[data-use-github-style] a { color: rgb(51, 122, 183); }
.markdown-preview[data-use-github-style] code { color: inherit; }
.markdown-preview[data-use-github-style] pre.editor-colors { padding: 0.8em 1em; margin-bottom: 1em; font-size: 0.85em; border-radius: 4px; overflow: auto; }
.scrollbars-visible-always .markdown-preview pre.editor-colors .vertical-scrollbar, .scrollbars-visible-always .markdown-preview pre.editor-colors .horizontal-scrollbar { visibility: hidden; }
.scrollbars-visible-always .markdown-preview pre.editor-colors:hover .vertical-scrollbar, .scrollbars-visible-always .markdown-preview pre.editor-colors:hover .horizontal-scrollbar { visibility: visible; }
.markdown-preview .task-list-item-checkbox { position: absolute; margin: 0.25em 0px 0px -1.4em; }
.markdown-preview:not([data-use-github-style]) { padding: 2em; font-size: 1.2em; color: rgb(171, 178, 191); overflow: auto; background-color: rgb(40, 44, 52); }
.markdown-preview:not([data-use-github-style]) > :first-child { margin-top: 0px; }
.markdown-preview:not([data-use-github-style]) h1, .markdown-preview:not([data-use-github-style]) h2, .markdown-preview:not([data-use-github-style]) h3, .markdown-preview:not([data-use-github-style]) h4, .markdown-preview:not([data-use-github-style]) h5, .markdown-preview:not([data-use-github-style]) h6 { line-height: 1.2; margin-top: 1.5em; margin-bottom: 0.5em; color: rgb(255, 255, 255); }
.markdown-preview:not([data-use-github-style]) h1 { font-size: 2.4em; font-weight: 300; }
.markdown-preview:not([data-use-github-style]) h2 { font-size: 1.8em; font-weight: 400; }
.markdown-preview:not([data-use-github-style]) h3 { font-size: 1.5em; font-weight: 500; }
.markdown-preview:not([data-use-github-style]) h4 { font-size: 1.2em; font-weight: 600; }
.markdown-preview:not([data-use-github-style]) h5 { font-size: 1.1em; font-weight: 600; }
.markdown-preview:not([data-use-github-style]) h6 { font-size: 1em; font-weight: 600; }
.markdown-preview:not([data-use-github-style]) strong { color: rgb(255, 255, 255); }
.markdown-preview:not([data-use-github-style]) del { color: rgb(124, 135, 156); }
.markdown-preview:not([data-use-github-style]) a, .markdown-preview:not([data-use-github-style]) a code { color: rgb(82, 139, 255); }
.markdown-preview:not([data-use-github-style]) img { max-width: 100%; }
.markdown-preview:not([data-use-github-style]) > p { margin-top: 0px; margin-bottom: 1.5em; }
.markdown-preview:not([data-use-github-style]) > ul, .markdown-preview:not([data-use-github-style]) > ol { margin-bottom: 1.5em; }
.markdown-preview:not([data-use-github-style]) blockquote { margin: 1.5em 0px; font-size: inherit; color: rgb(124, 135, 156); border-color: rgb(75, 83, 98); border-width: 4px; }
.markdown-preview:not([data-use-github-style]) hr { margin: 3em 0px; border-top: 2px dashed rgb(75, 83, 98); background: none; }
.markdown-preview:not([data-use-github-style]) table { margin: 1.5em 0px; }
.markdown-preview:not([data-use-github-style]) th { color: rgb(255, 255, 255); }
.markdown-preview:not([data-use-github-style]) th, .markdown-preview:not([data-use-github-style]) td { padding: 0.66em 1em; border: 1px solid rgb(75, 83, 98); }
.markdown-preview:not([data-use-github-style]) pre, .markdown-preview:not([data-use-github-style]) code { color: rgb(255, 255, 255); background-color: rgb(58, 63, 75); }
.markdown-preview:not([data-use-github-style]) pre, .markdown-preview:not([data-use-github-style]) pre.editor-colors { margin: 1.5em 0px; padding: 1em; font-size: 0.92em; border-radius: 3px; background-color: rgb(49, 54, 63); }
.markdown-preview:not([data-use-github-style]) kbd { color: rgb(255, 255, 255); border-width: 1px 1px 2px; border-style: solid; border-color: rgb(75, 83, 98) rgb(75, 83, 98) rgb(62, 68, 81); background-color: rgb(58, 63, 75); }
.markdown-preview[data-use-github-style] { font-family: "Helvetica Neue", Helvetica, "Segoe UI", Arial, freesans, sans-serif; line-height: 1.6; word-wrap: break-word; padding: 30px; font-size: 16px; color: rgb(51, 51, 51); overflow: scroll; background-color: rgb(255, 255, 255); }
.markdown-preview[data-use-github-style] > :first-child { margin-top: 0px !important; }
.markdown-preview[data-use-github-style] > :last-child { margin-bottom: 0px !important; }
.markdown-preview[data-use-github-style] a:not([href]) { color: inherit; text-decoration: none; }
.markdown-preview[data-use-github-style] .absent { color: rgb(204, 0, 0); }
.markdown-preview[data-use-github-style] .anchor { position: absolute; top: 0px; left: 0px; display: block; padding-right: 6px; padding-left: 30px; margin-left: -30px; }
.markdown-preview[data-use-github-style] .anchor:focus { outline: none; }
.markdown-preview[data-use-github-style] h1, .markdown-preview[data-use-github-style] h2, .markdown-preview[data-use-github-style] h3, .markdown-preview[data-use-github-style] h4, .markdown-preview[data-use-github-style] h5, .markdown-preview[data-use-github-style] h6 { position: relative; margin-top: 1em; margin-bottom: 16px; font-weight: bold; line-height: 1.4; }
.markdown-preview[data-use-github-style] h1 .octicon-link, .markdown-preview[data-use-github-style] h2 .octicon-link, .markdown-preview[data-use-github-style] h3 .octicon-link, .markdown-preview[data-use-github-style] h4 .octicon-link, .markdown-preview[data-use-github-style] h5 .octicon-link, .markdown-preview[data-use-github-style] h6 .octicon-link { display: none; color: rgb(0, 0, 0); vertical-align: middle; }
.markdown-preview[data-use-github-style] h1:hover .anchor, .markdown-preview[data-use-github-style] h2:hover .anchor, .markdown-preview[data-use-github-style] h3:hover .anchor, .markdown-preview[data-use-github-style] h4:hover .anchor, .markdown-preview[data-use-github-style] h5:hover .anchor, .markdown-preview[data-use-github-style] h6:hover .anchor { padding-left: 8px; margin-left: -30px; text-decoration: none; }
.markdown-preview[data-use-github-style] h1:hover .anchor .octicon-link, .markdown-preview[data-use-github-style] h2:hover .anchor .octicon-link, .markdown-preview[data-use-github-style] h3:hover .anchor .octicon-link, .markdown-preview[data-use-github-style] h4:hover .anchor .octicon-link, .markdown-preview[data-use-github-style] h5:hover .anchor .octicon-link, .markdown-preview[data-use-github-style] h6:hover .anchor .octicon-link { display: inline-block; }
.markdown-preview[data-use-github-style] h1 tt, .markdown-preview[data-use-github-style] h2 tt, .markdown-preview[data-use-github-style] h3 tt, .markdown-preview[data-use-github-style] h4 tt, .markdown-preview[data-use-github-style] h5 tt, .markdown-preview[data-use-github-style] h6 tt, .markdown-preview[data-use-github-style] h1 code, .markdown-preview[data-use-github-style] h2 code, .markdown-preview[data-use-github-style] h3 code, .markdown-preview[data-use-github-style] h4 code, .markdown-preview[data-use-github-style] h5 code, .markdown-preview[data-use-github-style] h6 code { font-size: inherit; }
.markdown-preview[data-use-github-style] h1 { padding-bottom: 0.3em; font-size: 2.25em; line-height: 1.2; border-bottom: 1px solid rgb(238, 238, 238); }
.markdown-preview[data-use-github-style] h1 .anchor { line-height: 1; }
.markdown-preview[data-use-github-style] h2 { padding-bottom: 0.3em; font-size: 1.75em; line-height: 1.225; border-bottom: 1px solid rgb(238, 238, 238); }
.markdown-preview[data-use-github-style] h2 .anchor { line-height: 1; }
.markdown-preview[data-use-github-style] h3 { font-size: 1.5em; line-height: 1.43; }
.markdown-preview[data-use-github-style] h3 .anchor { line-height: 1.2; }
.markdown-preview[data-use-github-style] h4 { font-size: 1.25em; }
.markdown-preview[data-use-github-style] h4 .anchor { line-height: 1.2; }
.markdown-preview[data-use-github-style] h5 { font-size: 1em; }
.markdown-preview[data-use-github-style] h5 .anchor { line-height: 1.1; }
.markdown-preview[data-use-github-style] h6 { font-size: 1em; color: rgb(119, 119, 119); }
.markdown-preview[data-use-github-style] h6 .anchor { line-height: 1.1; }
.markdown-preview[data-use-github-style] p, .markdown-preview[data-use-github-style] blockquote, .markdown-preview[data-use-github-style] ul, .markdown-preview[data-use-github-style] ol, .markdown-preview[data-use-github-style] dl, .markdown-preview[data-use-github-style] table, .markdown-preview[data-use-github-style] pre { margin-top: 0px; margin-bottom: 16px; }
.markdown-preview[data-use-github-style] hr { height: 4px; padding: 0px; margin: 16px 0px; border: 0px none; background-color: rgb(231, 231, 231); }
.markdown-preview[data-use-github-style] ul, .markdown-preview[data-use-github-style] ol { padding-left: 2em; }
.markdown-preview[data-use-github-style] ul.no-list, .markdown-preview[data-use-github-style] ol.no-list { padding: 0px; list-style-type: none; }
.markdown-preview[data-use-github-style] ul ul, .markdown-preview[data-use-github-style] ul ol, .markdown-preview[data-use-github-style] ol ol, .markdown-preview[data-use-github-style] ol ul { margin-top: 0px; margin-bottom: 0px; }
.markdown-preview[data-use-github-style] li > p { margin-top: 16px; }
.markdown-preview[data-use-github-style] dl { padding: 0px; }
.markdown-preview[data-use-github-style] dl dt { padding: 0px; margin-top: 16px; font-size: 1em; font-style: italic; font-weight: bold; }
.markdown-preview[data-use-github-style] dl dd { padding: 0px 16px; margin-bottom: 16px; }
.markdown-preview[data-use-github-style] blockquote { padding: 0px 15px; color: rgb(119, 119, 119); border-left: 4px solid rgb(221, 221, 221); }
.markdown-preview[data-use-github-style] blockquote > :first-child { margin-top: 0px; }
.markdown-preview[data-use-github-style] blockquote > :last-child { margin-bottom: 0px; }
.markdown-preview[data-use-github-style] table { display: block; width: 100%; overflow: auto; word-break: keep-all; }
.markdown-preview[data-use-github-style] table th { font-weight: bold; }
.markdown-preview[data-use-github-style] table th, .markdown-preview[data-use-github-style] table td { padding: 6px 13px; border: 1px solid rgb(221, 221, 221); }
.markdown-preview[data-use-github-style] table tr { border-top: 1px solid rgb(204, 204, 204); background-color: rgb(255, 255, 255); }
.markdown-preview[data-use-github-style] table tr:nth-child(2n) { background-color: rgb(248, 248, 248); }
.markdown-preview[data-use-github-style] img { max-width: 100%; box-sizing: border-box; }
.markdown-preview[data-use-github-style] .emoji { max-width: none; }
.markdown-preview[data-use-github-style] span.frame { display: block; overflow: hidden; }
.markdown-preview[data-use-github-style] span.frame > span { display: block; float: left; width: auto; padding: 7px; margin: 13px 0px 0px; overflow: hidden; border: 1px solid rgb(221, 221, 221); }
.markdown-preview[data-use-github-style] span.frame span img { display: block; float: left; }
.markdown-preview[data-use-github-style] span.frame span span { display: block; padding: 5px 0px 0px; clear: both; color: rgb(51, 51, 51); }
.markdown-preview[data-use-github-style] span.align-center { display: block; overflow: hidden; clear: both; }
.markdown-preview[data-use-github-style] span.align-center > span { display: block; margin: 13px auto 0px; overflow: hidden; text-align: center; }
.markdown-preview[data-use-github-style] span.align-center span img { margin: 0px auto; text-align: center; }
.markdown-preview[data-use-github-style] span.align-right { display: block; overflow: hidden; clear: both; }
.markdown-preview[data-use-github-style] span.align-right > span { display: block; margin: 13px 0px 0px; overflow: hidden; text-align: right; }
.markdown-preview[data-use-github-style] span.align-right span img { margin: 0px; text-align: right; }
.markdown-preview[data-use-github-style] span.float-left { display: block; float: left; margin-right: 13px; overflow: hidden; }
.markdown-preview[data-use-github-style] span.float-left span { margin: 13px 0px 0px; }
.markdown-preview[data-use-github-style] span.float-right { display: block; float: right; margin-left: 13px; overflow: hidden; }
.markdown-preview[data-use-github-style] span.float-right > span { display: block; margin: 13px auto 0px; overflow: hidden; text-align: right; }
.markdown-preview[data-use-github-style] code, .markdown-preview[data-use-github-style] tt { padding: 0.2em 0px; margin: 0px; font-size: 85%; border-radius: 3px; background-color: rgba(0, 0, 0, 0.0392157); }
.markdown-preview[data-use-github-style] code::before, .markdown-preview[data-use-github-style] tt::before, .markdown-preview[data-use-github-style] code::after, .markdown-preview[data-use-github-style] tt::after { letter-spacing: -0.2em; content: " "; }
.markdown-preview[data-use-github-style] code br, .markdown-preview[data-use-github-style] tt br { display: none; }
.markdown-preview[data-use-github-style] del code { text-decoration: inherit; }
.markdown-preview[data-use-github-style] pre > code { padding: 0px; margin: 0px; font-size: 100%; word-break: normal; white-space: pre; border: 0px; background: transparent; }
.markdown-preview[data-use-github-style] .highlight { margin-bottom: 16px; }
.markdown-preview[data-use-github-style] .highlight pre, .markdown-preview[data-use-github-style] pre { padding: 16px; overflow: auto; font-size: 85%; line-height: 1.45; border-radius: 3px; background-color: rgb(247, 247, 247); }
.markdown-preview[data-use-github-style] .highlight pre { margin-bottom: 0px; word-break: normal; }
.markdown-preview[data-use-github-style] pre { word-wrap: normal; }
.markdown-preview[data-use-github-style] pre code, .markdown-preview[data-use-github-style] pre tt { display: inline; max-width: initial; padding: 0px; margin: 0px; overflow: initial; line-height: inherit; word-wrap: normal; border: 0px; background-color: transparent; }
.markdown-preview[data-use-github-style] pre code::before, .markdown-preview[data-use-github-style] pre tt::before, .markdown-preview[data-use-github-style] pre code::after, .markdown-preview[data-use-github-style] pre tt::after { content: normal; }
.markdown-preview[data-use-github-style] kbd { display: inline-block; padding: 3px 5px; font-size: 11px; line-height: 10px; color: rgb(85, 85, 85); vertical-align: middle; border-width: 1px; border-style: solid; border-color: rgb(204, 204, 204) rgb(204, 204, 204) rgb(187, 187, 187); border-radius: 3px; box-shadow: rgb(187, 187, 187) 0px -1px 0px inset; background-color: rgb(252, 252, 252); }
.markdown-preview[data-use-github-style] a { color: rgb(51, 122, 183); }
.markdown-preview[data-use-github-style] pre, .markdown-preview[data-use-github-style] code { color: inherit; }
.markdown-preview[data-use-github-style] pre, .markdown-preview[data-use-github-style] pre.editor-colors { padding: 0.8em 1em; margin-bottom: 1em; font-size: 0.85em; border-radius: 4px; overflow: auto; }
.scrollbars-visible-always .markdown-preview pre.editor-colors .vertical-scrollbar, .scrollbars-visible-always .markdown-preview pre.editor-colors .horizontal-scrollbar { visibility: hidden; }
.scrollbars-visible-always .markdown-preview pre.editor-colors:hover .vertical-scrollbar, .scrollbars-visible-always .markdown-preview pre.editor-colors:hover .horizontal-scrollbar { visibility: visible; }
.markdown-preview del { text-decoration: none; position: relative; }
.markdown-preview del::after { border-bottom: 1px solid black; content: ""; left: 0px; position: absolute; right: 0px; top: 50%; }
.markdown-preview .flash { animation: flash 1s ease-out 1; outline: rgba(255, 0, 0, 0) solid 1px; }
.markdown-preview .flash:not(li) { display: block; }
.bracket-matcher .region {
  border-bottom: 1px dotted lime;
  position: absolute;
}

.spell-check-misspelling .region {
  border-bottom: 2px dotted rgba(255, 51, 51, 0.75);
}
.spell-check-corrections {
  width: 25em !important;
}

pre.editor-colors {
  background-color: #282c34;
  color: #abb2bf;
}
pre.editor-colors .line.cursor-line {
  background-color: rgba(153, 187, 255, 0.04);
}
pre.editor-colors .invisible {
  color: #abb2bf;
}
pre.editor-colors .cursor {
  border-left: 2px solid #528bff;
}
pre.editor-colors .selection .region {
  background-color: #3e4451;
}
pre.editor-colors .bracket-matcher .region {
  border-bottom: 1px solid #528bff;
  box-sizing: border-box;
}
pre.editor-colors .invisible-character {
  color: rgba(171, 178, 191, 0.15);
}
pre.editor-colors .indent-guide {
  color: rgba(171, 178, 191, 0.15);
}
pre.editor-colors .wrap-guide {
  background-color: rgba(171, 178, 191, 0.15);
}
pre.editor-colors .find-result .region.region.region,
pre.editor-colors .current-result .region.region.region {
  border-radius: 2px;
  background-color: rgba(82, 139, 255, 0.24);
  transition: border-color 0.4s;
}
pre.editor-colors .find-result .region.region.region {
  border: 2px solid transparent;
}
pre.editor-colors .current-result .region.region.region {
  border: 2px solid #528bff;
  transition-duration: .1s;
}
pre.editor-colors .gutter .line-number {
  color: #636d83;
  -webkit-font-smoothing: antialiased;
}
pre.editor-colors .gutter .line-number.cursor-line {
  color: #abb2bf;
  background-color: #2c313a;
}
pre.editor-colors .gutter .line-number.cursor-line-no-selection {
  background-color: transparent;
}
pre.editor-colors .gutter .line-number .icon-right {
  color: #abb2bf;
}
pre.editor-colors .gutter:not(.git-diff-icon) .line-number.git-line-removed.git-line-removed::before {
  bottom: -3px;
}
pre.editor-colors .gutter:not(.git-diff-icon) .line-number.git-line-removed::after {
  content: "";
  position: absolute;
  left: 0px;
  bottom: 0px;
  width: 25px;
  border-bottom: 1px dotted rgba(224, 82, 82, 0.5);
  pointer-events: none;
}
pre.editor-colors .gutter .line-number.folded,
pre.editor-colors .gutter .line-number:after,
pre.editor-colors .fold-marker:after {
  color: #abb2bf;
}
.syntax--comment {
  color: #5c6370;
  font-style: italic;
}
.syntax--comment .syntax--markup.syntax--link {
  color: #5c6370;
}
.syntax--entity.syntax--name.syntax--type {
  color: #e5c07b;
}
.syntax--entity.syntax--other.syntax--inherited-class {
  color: #98c379;
}
.syntax--keyword {
  color: #c678dd;
}
.syntax--keyword.syntax--control {
  color: #c678dd;
}
.syntax--keyword.syntax--operator {
  color: #abb2bf;
}
.syntax--keyword.syntax--other.syntax--special-method {
  color: #61afef;
}
.syntax--keyword.syntax--other.syntax--unit {
  color: #d19a66;
}
.syntax--storage {
  color: #c678dd;
}
.syntax--storage.syntax--type.syntax--annotation,
.syntax--storage.syntax--type.syntax--primitive {
  color: #c678dd;
}
.syntax--storage.syntax--modifier.syntax--package,
.syntax--storage.syntax--modifier.syntax--import {
  color: #abb2bf;
}
.syntax--constant {
  color: #d19a66;
}
.syntax--constant.syntax--variable {
  color: #d19a66;
}
.syntax--constant.syntax--character.syntax--escape {
  color: #56b6c2;
}
.syntax--constant.syntax--numeric {
  color: #d19a66;
}
.syntax--constant.syntax--other.syntax--color {
  color: #56b6c2;
}
.syntax--constant.syntax--other.syntax--symbol {
  color: #56b6c2;
}
.syntax--variable {
  color: #e06c75;
}
.syntax--variable.syntax--interpolation {
  color: #be5046;
}
.syntax--variable.syntax--parameter {
  color: #abb2bf;
}
.syntax--string {
  color: #98c379;
}
.syntax--string.syntax--regexp {
  color: #56b6c2;
}
.syntax--string.syntax--regexp .syntax--source.syntax--ruby.syntax--embedded {
  color: #e5c07b;
}
.syntax--string.syntax--other.syntax--link {
  color: #e06c75;
}
.syntax--punctuation.syntax--definition.syntax--comment {
  color: #5c6370;
}
.syntax--punctuation.syntax--definition.syntax--method-parameters,
.syntax--punctuation.syntax--definition.syntax--function-parameters,
.syntax--punctuation.syntax--definition.syntax--parameters,
.syntax--punctuation.syntax--definition.syntax--separator,
.syntax--punctuation.syntax--definition.syntax--seperator,
.syntax--punctuation.syntax--definition.syntax--array {
  color: #abb2bf;
}
.syntax--punctuation.syntax--definition.syntax--heading,
.syntax--punctuation.syntax--definition.syntax--identity {
  color: #61afef;
}
.syntax--punctuation.syntax--definition.syntax--bold {
  color: #e5c07b;
  font-weight: bold;
}
.syntax--punctuation.syntax--definition.syntax--italic {
  color: #c678dd;
  font-style: italic;
}
.syntax--punctuation.syntax--section.syntax--embedded {
  color: #be5046;
}
.syntax--punctuation.syntax--section.syntax--method,
.syntax--punctuation.syntax--section.syntax--class,
.syntax--punctuation.syntax--section.syntax--inner-class {
  color: #abb2bf;
}
.syntax--support.syntax--class {
  color: #e5c07b;
}
.syntax--support.syntax--type {
  color: #56b6c2;
}
.syntax--support.syntax--function {
  color: #56b6c2;
}
.syntax--support.syntax--function.syntax--any-method {
  color: #61afef;
}
.syntax--entity.syntax--name.syntax--function {
  color: #61afef;
}
.syntax--entity.syntax--name.syntax--class,
.syntax--entity.syntax--name.syntax--type.syntax--class {
  color: #e5c07b;
}
.syntax--entity.syntax--name.syntax--section {
  color: #61afef;
}
.syntax--entity.syntax--name.syntax--tag {
  color: #e06c75;
}
.syntax--entity.syntax--other.syntax--attribute-name {
  color: #d19a66;
}
.syntax--entity.syntax--other.syntax--attribute-name.syntax--id {
  color: #61afef;
}
.syntax--meta.syntax--class {
  color: #e5c07b;
}
.syntax--meta.syntax--class.syntax--body {
  color: #abb2bf;
}
.syntax--meta.syntax--method-call,
.syntax--meta.syntax--method {
  color: #abb2bf;
}
.syntax--meta.syntax--definition.syntax--variable {
  color: #e06c75;
}
.syntax--meta.syntax--link {
  color: #d19a66;
}
.syntax--meta.syntax--require {
  color: #61afef;
}
.syntax--meta.syntax--selector {
  color: #c678dd;
}
.syntax--meta.syntax--separator {
  background-color: #373b41;
  color: #abb2bf;
}
.syntax--meta.syntax--tag {
  color: #abb2bf;
}
.syntax--underline {
  text-decoration: underline;
}
.syntax--none {
  color: #abb2bf;
}
.syntax--invalid.syntax--deprecated {
  color: #523d14 !important;
  background-color: #e0c285 !important;
}
.syntax--invalid.syntax--illegal {
  color: white !important;
  background-color: #e05252 !important;
}
.syntax--markup.syntax--bold {
  color: #d19a66;
  font-weight: bold;
}
.syntax--markup.syntax--changed {
  color: #c678dd;
}
.syntax--markup.syntax--deleted {
  color: #e06c75;
}
.syntax--markup.syntax--italic {
  color: #c678dd;
  font-style: italic;
}
.syntax--markup.syntax--heading {
  color: #e06c75;
}
.syntax--markup.syntax--heading .syntax--punctuation.syntax--definition.syntax--heading {
  color: #61afef;
}
.syntax--markup.syntax--link {
  color: #56b6c2;
}
.syntax--markup.syntax--inserted {
  color: #98c379;
}
.syntax--markup.syntax--quote {
  color: #d19a66;
}
.syntax--markup.syntax--raw {
  color: #98c379;
}
.syntax--source.syntax--c .syntax--keyword.syntax--operator {
  color: #c678dd;
}
.syntax--source.syntax--cpp .syntax--keyword.syntax--operator {
  color: #c678dd;
}
.syntax--source.syntax--cs .syntax--keyword.syntax--operator {
  color: #c678dd;
}
.syntax--source.syntax--css .syntax--property-name,
.syntax--source.syntax--css .syntax--property-value {
  color: #828997;
}
.syntax--source.syntax--css .syntax--property-name.syntax--support,
.syntax--source.syntax--css .syntax--property-value.syntax--support {
  color: #abb2bf;
}
.syntax--source.syntax--gfm .syntax--markup {
  -webkit-font-smoothing: auto;
}
.syntax--source.syntax--gfm .syntax--link .syntax--entity {
  color: #61afef;
}
.syntax--source.syntax--go .syntax--storage.syntax--type.syntax--string {
  color: #c678dd;
}
.syntax--source.syntax--ini .syntax--keyword.syntax--other.syntax--definition.syntax--ini {
  color: #e06c75;
}
.syntax--source.syntax--java .syntax--storage.syntax--modifier.syntax--import {
  color: #e5c07b;
}
.syntax--source.syntax--java .syntax--storage.syntax--type {
  color: #e5c07b;
}
.syntax--source.syntax--java .syntax--keyword.syntax--operator.syntax--instanceof {
  color: #c678dd;
}
.syntax--source.syntax--java-properties .syntax--meta.syntax--key-pair {
  color: #e06c75;
}
.syntax--source.syntax--java-properties .syntax--meta.syntax--key-pair > .syntax--punctuation {
  color: #abb2bf;
}
.syntax--source.syntax--js .syntax--keyword.syntax--operator {
  color: #56b6c2;
}
.syntax--source.syntax--js .syntax--keyword.syntax--operator.syntax--delete,
.syntax--source.syntax--js .syntax--keyword.syntax--operator.syntax--in,
.syntax--source.syntax--js .syntax--keyword.syntax--operator.syntax--of,
.syntax--source.syntax--js .syntax--keyword.syntax--operator.syntax--instanceof,
.syntax--source.syntax--js .syntax--keyword.syntax--operator.syntax--new,
.syntax--source.syntax--js .syntax--keyword.syntax--operator.syntax--typeof,
.syntax--source.syntax--js .syntax--keyword.syntax--operator.syntax--void {
  color: #c678dd;
}
.syntax--source.syntax--json .syntax--meta.syntax--structure.syntax--dictionary.syntax--json > .syntax--string.syntax--quoted.syntax--json {
  color: #e06c75;
}
.syntax--source.syntax--json .syntax--meta.syntax--structure.syntax--dictionary.syntax--json > .syntax--string.syntax--quoted.syntax--json > .syntax--punctuation.syntax--string {
  color: #e06c75;
}
.syntax--source.syntax--json .syntax--meta.syntax--structure.syntax--dictionary.syntax--json > .syntax--value.syntax--json > .syntax--string.syntax--quoted.syntax--json,
.syntax--source.syntax--json .syntax--meta.syntax--structure.syntax--array.syntax--json > .syntax--value.syntax--json > .syntax--string.syntax--quoted.syntax--json,
.syntax--source.syntax--json .syntax--meta.syntax--structure.syntax--dictionary.syntax--json > .syntax--value.syntax--json > .syntax--string.syntax--quoted.syntax--json > .syntax--punctuation,
.syntax--source.syntax--json .syntax--meta.syntax--structure.syntax--array.syntax--json > .syntax--value.syntax--json > .syntax--string.syntax--quoted.syntax--json > .syntax--punctuation {
  color: #98c379;
}
.syntax--source.syntax--json .syntax--meta.syntax--structure.syntax--dictionary.syntax--json > .syntax--constant.syntax--language.syntax--json,
.syntax--source.syntax--json .syntax--meta.syntax--structure.syntax--array.syntax--json > .syntax--constant.syntax--language.syntax--json {
  color: #56b6c2;
}
.syntax--source.syntax--ruby .syntax--constant.syntax--other.syntax--symbol > .syntax--punctuation {
  color: inherit;
}
.syntax--source.syntax--python .syntax--keyword.syntax--operator.syntax--logical.syntax--python {
  color: #c678dd;
}
.syntax--source.syntax--python .syntax--variable.syntax--parameter {
  color: #d19a66;
}
</style>
  </head>
  <body class='markdown-preview'><h1>Code lists</h1>
<table border="3" style="width:100%">
<caption><em><center></center></em></caption>
<th>Name</th><th>Descriptions</th><th>Illustration</th>
<!------------------------------------------------------->
<tr><td><a href=""></a></td>
<td>
</td>
<td><img src="" alt="" width="200"></td></tr>
<!------------------------------------------------------->
<tr><td><a href="https://github.com/utkuozbulak/pytorch-cnn-visualizations">pytorch-cnn-visualizations</a></td>
<td>Pytorch implementation of convolutional neural network visualization techniques
</td>
<td><img src="https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/cat_dog_Cam_On_Image.jpg" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/tensorboy/Pytorch_Mask_RCNN">Pytorch_Mask_RCNN</a></td>
<td>Converted from [tf+keras version MASK-RCNN](https://github.com/matterport/Mask_RCNN)
</td>
<td><img src="https://github.com/tensorboy/Pytorch_Mask_RCNN/raw/master/README/santas_output.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/facebookresearch/MUSE">MUSE</a></td>
<td>A library for Multilingual Unsupervised or Supervised word Embeddings
</td>
<td><img src="https://camo.githubusercontent.com/e8a19eb6772e722fb3fe2cd787e14ed7c4e17ddd/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6172726976616c2f6f75746c696e655f616c6c2e706e67" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/warmspringwinds/pytorch-segmentation-detection">pytorch-segmentation-detection</a></td>
<td>Image Segmentation and Object Detection in Pytorch
</td>
<td><img src="https://github.com/warmspringwinds/pytorch-segmentation-detection/raw/master/pytorch_segmentation_detection/recipes/pascal_voc/segmentation/segmentation_demo_preview.gif?raw=true" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/Yugnaynehc/banet">banet</a></td>
<td>Pytorch implementation of Hierarchical Boundary-Aware Neural Encoder for Video Captioning. The C3D part is implemented but not work well.
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/openai/multiagent-competition">multiagent-competition</a></td>
<td>Repository for competitive multi-agent environments
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/ExplorerFreda/Structured-Self-Attentive-Sentence-Embedding">Structured-Self-Attentive-Sentence-Embedding</a></td>
<td>An open-source implementation of the paper A Structured Self-Attentive Sentence Embedding published by IBM and MILA.
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/jiyanggao/TURN-TAP">TURN-TAP</a></td>
<td>TURN TAP: Temporal Unit Regression Network for Temporal Action Proposals
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/Simon4Yan/Learning-via-Translation"> Learning-via-Translation</a></td>
<td>Image-Image Domain Adaptation with Preserved Self-Similarity and Domain-Dissimilarity for Person Re-identification (https://arxiv.org/pdf/1711.07027.pdf)
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/lium-lst/nmtpytorch">nmtpytorch</a></td>
<td>Neural Machine Translation Framework in PyTorch
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/XenderLiu/Listen-Attend-and-Spell-Pytorch">Listen-Attend-and-Spell-Pytorch</a></td>
<td>Listen Attend and Spell (LAS) implement in pytorch
</td>
<td><img src="" alt="" width="200"></td></tr>
<!------------------------------------------------------->
<tr><td><a href="https://github.com/xingyizhou/pytorch-pose-hg-3d">pytorch-pose-hg-3d</a></td>
<td>PyTorch implementation for 3D human pose estimation
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/ZhixiuYe/NER-pytorch">NER-pytorch</a></td>
<td>Neural Architectures for Named Entity Recognition
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/ruotianluo/NeuralDialog-CVAE-pytorch">NeuralDialog-CVAE-pytorch</a></td>
<td>Knowledge-Guided CVAE for dialog generation
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/kenshohara/3D-ResNets-PyTorch">3D-ResNets-PyTorch</a></td>
<td>3D ResNets for Action Recognition. Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh,
"Learning Spatio-Temporal Features with 3D Residual Networks for Action Recognition",
arXiv preprint, arXiv:1708.07632, 2017.
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/znxlwm/pytorch-generative-model-collections">pytorch-generative-model-collections</a></td>
<td>Pytorch implementation of various GANs.
</td>
<td><img src="https://github.com/gujiuxiang/pytorch-generative-model-collections/raw/master/assets/etc/GAN_structure.png" alt="" width="200"></td></tr>
<tr><td><a href="">bandit-nmt</a></td>
<td>This is code repo for our EMNLP 2017 paper "Reinforcement Learning for Bandit Neural Machine Translation with Simulated Human Feedback", which implements the A2C algorithm on top of a neural encoder-decoder model and benchmarks the combination under simulated noisy rewards.
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/ruotianluo/dbs">Diverse Beam Search</a></td>
<td>
This code implements Diverse Beam Search (DBS) - a replacement for beam search that generates diverse sequences from sequence models like LSTMs. This repository lets you generate diverse image-captions for models trained using the popular neuraltalk2 repository. A demo of our implementation on captioning is available at dbs.cloudcv.org
</td>
<td><img src="https://camo.githubusercontent.com/7dbd345b986b691eede2b8611285114d1fc90e32/68747470733a2f2f7332322e706f7374696d672e6f72672f686f6f7233726963782f64625f636f7665725f325f312e706e67" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/seangal/dcgan_vae_pytorch.git">dcgan_vae_pytorch</a></td>
<td>dcgan combined with vae in pytorch!
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/LiqunChen0606/Triangle-GAN">Triangle-GAN</a></td>
<td>This is an implemtation for NIPS paper: Triangle Generative models
</td>
<td><img src="https://raw.githubusercontent.com/LiqunChen0606/Triangle-GAN/master/figures/model.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/DartML/Stein-Variational-Gradient-Descent">Stein-Variational-Gradient-Descent</a></td>
<td>SVGD is a general purpose variational inference algorithm that forms a natural counterpart of gradient descent for optimization. SVGD iteratively transports a set of particles to match with the target distribution, by applying a form of functional gradient descent that minimizes the KL divergence.
</td>
<td><img src="https://github.com/DartML/Stein-Variational-Gradient-Descent/raw/master/data/1dgmm.gif" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/DeepRNN/object_detection">object_detection</a></td>
<td>Tensorflow implementation of Faster R-CNN and ResNets
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/falcondai/chinese-char-lm">chinese-char-lm</a></td>
<td>This is the code associated with the publication Glyph-aware Embedding of Chinese Characters by Dai and Cai. Please consider to cite the paper if you find the code useful in some way for your research.
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/facebookresearch/SentEval">SentEval</a></td>
<td>SentEval is a library for evaluating the quality of sentence embeddings. We assess their generalization power by using them as features on a broad and diverse set of "transfer" tasks (more details here). Our goal is to ease the study and the development of general-purpose fixed-size sentence representations.
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/fh295/SentenceRepresentation">SentenceRepresentation</a></td>
<td>This code acompanies the paper 'Learning Sentence Representations from Unlabelled Data' Felix Hill, KyungHyun Cho and Anna Korhonen 2016.
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/zsdonghao/seq2seq-chatbot">seq2seq-chatbot</a></td>
<td>Chatbot in 200 lines of code / Please Star --> https://github.com/zsdonghao/tensorlayer
</td>
<td><img src="https://camo.githubusercontent.com/9e88497fcdec5a9c716e0de5bc4b6d1793c6e23f/687474703a2f2f73757269796164656570616e2e6769746875622e696f2f696d672f736571327365712f73657132736571322e706e67" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/dgriff777/rl_a3c_pytorch">rl_a3c_pytorch</a></td>
<td>Reinforcement learning A3C LSTM Atari with Pytorch
</td>
<td><img src="https://github.com/dgriff777/rl_a3c_pytorch/raw/master/demo/MsPacman.gif" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/wayaai/SimGAN">SimGAN</a></td>
<td>Implementation of Apple's Learning from Simulated and Unsupervised Images through Adversarial Training
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/ikostrikov/pytorch-a2c-ppo-acktr">pytorch-a2c-ppo-acktr</a></td>
<td>PyTorch implementation of Advantage Actor Critic (A2C), Proximal Policy Optimization (PPO) and Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation (ACKTR).
</td>
<td><img src="https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/raw/master/imgs/beamrider.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/ezyang/onnx-pytorch">onnx-pytorch</a></td>
<td>Support scripts for PyTorch-ONNX.This repository contains end-to-end tests for PyTorch's ONNX support, including exporting models to Caffe2.
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/YunzhuLi/InfoGAIL">InfoGAIL</a></td>
<td>Source code for our paper, Inferring The Latent Structure of Human Decision-Making from Raw Visual Inputs
</td>
<td><img src="https://github.com/YunzhuLi/InfoGAIL/raw/master/turn_code_0.gif" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/jacobandreas/psketch">psketch</a></td>
<td>
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/mkocaoglu/CausalGAN">CausalGAN</a></td>
<td>CausalGAN/CausalBEGAN in Tensorflow
</td>
<td><img src="https://github.com/mkocaoglu/CausalGAN/raw/master/assets/314393_began_Bald_topdo1_botcond1.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/znxlwm/pytorch-generative-model-collections">pytorch-generative-model-collections</a></td>
<td>Collection of generative models in Pytorch version.
</td>
<td><img src="https://github.com/znxlwm/pytorch-generative-model-collections/raw/master/assets/etc/GAN_structure.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/nottombrown/rl-teacher">rl-teacher</a></td>
<td>rl-teacher is an implementation of Deep Reinforcement Learning from Human Preferences [Christiano et al., 2017].
</td>
<td><img src="https://user-images.githubusercontent.com/306655/28396526-d4ce6334-6cb0-11e7-825c-63a85c8ff533.gif" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/spro/RARNN">RARNN</a></td>
<td>Recursive Application of Recurrent Neural Networks.
A simple model for intent parsing that supports complex nested intents.
</td>
<td><img src="https://camo.githubusercontent.com/f490fbe26a07b98dbb8aca0b96f5673acf399894/68747470733a2f2f692e696d6775722e636f6d2f44424e48726e522e706e67" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/dhpollack/fast-wavenet.pytorch">fast-wavenet.pytorch</a></td>
<td>A PyTorch implementation of fast-wavenet
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/kuangliu/pytorch-retinanet">pytorch-retinanet</a></td>
<td>Train RetinaNet with Focal Loss in PyTorch.
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/vy007vikas/PyTorch-ActorCriticRL">PyTorch-ActorCriticRL</a></td>
<td>PyTorch implementation of DDPG algorithm for continuous action reinforcement learning problem.
</td>
<td><img src="https://camo.githubusercontent.com/1360adc21b25702bf68626692894c089c5bfc4f1/68747470733a2f2f6a2e676966732e636f6d2f4f37316e714c2e676966" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/ruotianluo/irl-imitation">irl-imitation</a></td>
<td>Implementations of model-based Inverse Reinforcement Learning (IRL) algorithms in python/Tensorflow. Deep MaxEnt, MaxEnt, LPIRL
</td>
<td><img src="https://github.com/ruotianluo/irl-imitation/raw/master/imgs/rmap_maxent_10.jpg" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/taolei87/sru">sru</a></td>
<td>Training RNNs as Fast as CNNs (https://arxiv.org/abs/1709.02755)
</td>
<td><img src="https://github.com/taolei87/sru/raw/master/imgs/speed.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/ajbrock/SMASH">SMASH</a></td>
<td>SMASH: One-Shot Model Architecture Search through HyperNetworks
</td>
<td><img src="https://camo.githubusercontent.com/ade9db12f340f43d351a3070b7e3adf6d041ca3f/687474703a2f2f692e696d6775722e636f6d2f4f544f767374572e676966" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/nishnik/Deep-Semantic-Similarity-Model-PyTorch">Deep-Semantic-Similarity-Model-PyTorch</a></td>
<td>[A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/cikm2014_cdssm_final.pdf)
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/bearpaw/pytorch-classification">pytorch-classification</a></td>
<td>Classification on CIFAR-10/100 and ImageNet with PyTorch.
</td>
<td><img src="https://github.com/bearpaw/pytorch-classification/raw/master/utils/images/cifar.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/lichengunc/visdif_emb_guide2_reinforce">visdif_emb_guide2_reinforce</a></td>
<td>Torch implementation of CVPR 2017's referring expression paper "A Joint Speaker-Listener-Reinforcer Model for Referring Expressions"
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/imisra/composing_cvpr17">composing_cvpr17</a></td>
<td>From Red Wine to Red Tomato: Composition with Context
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/ZhouYanzhao/ORN"></a>ORN</td>
<td>Oriented Response Networks, in CVPR 2017 http://yzhou.work/ORN
</td>
<td><img src="https://github.com/ZhouYanzhao/ORN/raw/master/illustration.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/r9y9/tacotron_pytorch">tacotron_pytorch</a></td>
<td>PyTorch implementation of Tacotron speech synthesis model. http://nbviewer.jupyter.org/github/r9…
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/jmtomczak/vae_vampprior">vae_vampprior</a></td>
<td>Code for the paper "VAE with a VampPrior", J.M. Tomczak & M. Welling https://jmtomczak.github.io/deebmed.html
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/Kaixhin/NoisyNet-A3C">NoisyNet-A3C</a></td>
<td>NoisyNet [1] (LSTM) asynchronous advantage actor-critic (A3C) [2] on the CartPole-v1 environment. This repo has a minimalistic design and a classic control environment to enable quick investigation of different hyperparameters.
</td>
<td><img src="https://github.com/Kaixhin/NoisyNet-A3C/raw/master/figures/good-noisynet-a3c.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/LiyuanLucasLiu/LM-LSTM-CRF">LM-LSTM-CRF</a></td>
<td>This project provides high-performance character-aware sequence labeling tools and tutorials. Model details can be accessed here, and the implementation is based on the PyTorch library.
<p>LM-LSTM-CRF achieves F1 score of 91.71+/-0.10 on the CoNLL 2003 NER dataset, without using any additional corpus or resource.</p>
</td>
<td><img src="https://github.com/LiyuanLucasLiu/LM-LSTM-CRF/raw/master/docs/framework.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/markdtw/vqa-winner-cvprw-2017">vqa-winner-cvprw-2017</a></td>
<td>2017 VQA Challenge Winner (CVPR'17 Workshop).Pytorch implementation of Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge by Teney et al.
</td>
<td><img src="https://camo.githubusercontent.com/1194ae93cb823754f0f2c2194dc3f73aa60b0cf5/68747470733a2f2f692e696d6775722e636f6d2f7068424849715a2e706e67" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/Rangozhang/VideoCaption">VideoCaption</a></td>
<td>Video captioning using LSTM and CNN. This is the Visual Learning project done by Rui Zhang, Yujia Huang and Yu Zhang
</td>
<td><img src="https://github.com/Rangozhang/VideoCaption/raw/master/VideoCaption.jpg" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/JianGoForIt/YellowFin_Pytorch">YellowFin_Pytorch</a></td>
<td>auto-tuning momentum SGD optimizer
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/NickShahML/rnn.wgan">rnn.wgan</a></td>
<td>Code for training and evaluation of the model from "Language Generation with Recurrent Generative Adversarial Networks without Pre-training" https://arxiv.org/abs/1706.01399
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/zawlin/cvpr17_vtranse">cvpr17_vtranse</a></td>
<td>This implements "Visual Translation Embedding Network for Visual Relation Detection,Hanwang Zhang, Zawlin Kyaw, Shih-Fu Chang, Tat-Seng Chua (CVPR2017)"
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/facebookresearch/InferSent">InferSent</a></td>
<td>Sentence embeddings (InferSent) and training code for NLI.
</td>
<td><img src="https://camo.githubusercontent.com/eacfae9d9987988db2774e703609473bc10ed311/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f73656e746576616c2f696e66657273656e742f76697375616c697a6174696f6e2e706e67" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/meetshah1995/pytorch-semseg">pytorch-semseg</a></td>
<td>Semantic Segmentation Algorithms Implemented in PyTorch
</td>
<td><img src="https://camo.githubusercontent.com/c1ebf30322adb7208363f8660386c177e4c5d8b6/68747470733a2f2f6d65657473686168313939352e6769746875622e696f2f696d616765732f626c6f672f73732f707473656d7365672e706e67" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/ycszen/pytorch-ss">pytorch-ss</a></td>
<td>Pytorch for Semantic Segmentation
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/ronghanghu/n2nmn">n2nmn</a></td>
<td>R. Hu, J. Andreas, M. Rohrbach, T. Darrell, K. Saenko, Learning to Reason: End-to-End Module Networks for Visual Question Answering. in ICCV, 2017. (PDF)
</td>
<td><img src="http://ronghanghu.com/wp-content/uploads/nmn3-e1492671105251-1024x381.jpg" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/XingxingZhang/dress">dress</a></td>
<td>Sentence Simplification with Deep Reinforcement Learning
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/JannerM/spatial-reasoning">spatial-reasoning</a></td>
<td>Code and data to reproduce the experiments in Representation Learning for Grounded Spatial Reasoning.
</td>
<td><img src="https://github.com/JannerM/spatial-reasoning/raw/master/logs/example/predictions.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/ranjaykrishna/densevid_eval">densevid_eval</a></td>
<td>Evaluation code for Dense-Captioning Events in Videos
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/twtygqyy/pytorch-SRResNet"> pytorch-SRResNet</a></td>
<td>pytorch implementation for Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network arXiv:1609.04802
</td>
<td><img src="https://github.com/twtygqyy/pytorch-SRResNet/raw/master/result/result.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/ruotianluo/pytorch-faster-rcnn">pytorch-faster-rcnn</a></td>
<td>
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/ghliu/pytorch-ddpg">pytorch-ddpg</a></td>
<td>Implementation of the Deep Deterministic Policy Gradient (DDPG) using PyTorch
</td>
<td><img src="https://github.com/ghliu/pytorch-ddpg/raw/master/output/Pendulum-v0-run0/validate_reward.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/leelabcnbc/cnnvis-pytorch"> cnnvis-pytorch</a></td>
<td>visualization of CNN in PyTorch
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/ryanleary/pytorch-ctc">pytorch-ctc</a></td>
<td>PyTorch CTC Decoder bindings
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/alexsax/pytorch-visdom">pytorch-visdom</a></td>
<td>Support powerful visual logging in PyTorch.
</td>
<td><img src="https://user-images.githubusercontent.com/5157485/28799619-2bebef8c-75fe-11e7-898d-202a6c6d3239.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/pkdn/pytorch-smoothgrad">pytorch-smoothgrad</a></td>
<td> Unstar SmoothGrad implementation in PyTorch
</td>
<td><img src="https://github.com/pkdn/pytorch-smoothgrad/raw/master/results/grad/guided_grad.jpg" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/facebookresearch/DrQA">DrQA</a></td>
<td>Reading Wikipedia to Answer Open-Domain Questions
</td>
<td><img src="https://github.com/facebookresearch/DrQA/raw/master/img/drqa.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/MatthieuCourbariaux/BinaryNet">BinaryNet</a></td>
<td>Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/jwyang/stnm.pytorch">stnm.pytorch</a></td>
<td>Pytorch code for spatial transformer network with mask
</td>
<td><img src="https://github.com/jwyang/stnm.pytorch/raw/master/images/stnm_figure.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/jinfagang/rl_atari_pytorch">rl_atari_pytorch</a></td>
<td>ReinforcementLearning Learn Play Atari Using DDPG and LSTM.
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/fwang91/residual-attention-network">residual-attention-network</a></td>
<td>Residual Attention Network for Image Classification
</td>
<td><img src="https://media.licdn.com/mpr/mpr/AAEAAQAAAAAAAArgAAAAJGU2YmE4ZTAxLTFmYmQtNGVmOS04ZmFjLWI4YmQ2ZGIzNzJiOA.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/ilija139/vqa-soft">vqa-soft</a></td>
<td>Accompanying code for "A Simple Loss Function for Improving the Convergence and Accuracy of Visual Question Answering Models" CVPR 2017 VQA workshop paper.
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/jayleicn/animeGAN">animeGAN</a></td>
<td>A simple PyTorch Implementation of Generative Adversarial Networks, focusing on anime face drawing.
</td>
<td><img src="https://github.com/jayleicn/animeGAN/raw/master/images/fake_sample.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/taesung89/pytorch-CycleGAN-and-pix2pix">pytorch-CycleGAN-and-pix2pix</a></td>
<td>Image-to-image translation in PyTorch (e.g. horse2zebra, edges2cats, and more)
</td>
<td><img src="https://camo.githubusercontent.com/69cbc0371777fba5d251a564e2f8a8f38d1bf43f/68747470733a2f2f6a756e79616e7a2e6769746875622e696f2f4379636c6547414e2f696d616765732f7465617365725f686967685f7265732e6a7067" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/jiasenlu/faster_rcnn_pytorch">faster_rcnn_pytorch</a></td>
<td>Faster RCNN with PyTorch
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/LuoweiZhou/video-to-text">video-to-text</a></td>
<td>Video to text model based on NeuralTalk2
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/rizar/actor-critic-public">actor-critic-public</a></td>
<td>The source code for "An Actor Critic Algorithm for Structured Prediction"
</td>
<td><img src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-11-08/0d24a0695c9fc669e643bad51d4e14f056329dec/5-Figure1-1.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/woozzu/dong_iccv_2017"> dong_iccv_2017</a></td>
<td>A PyTorch implementation of the paper "Semantic Image Synthesis via Adversarial Learning" in ICCV 2017
</td>
<td><img src="https://github.com/woozzu/dong_iccv_2017/raw/master/images/architecture.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/marvis/pytorch-caffe-darknet-convert">pytorch-caffe-darknet-convert</a></td>
<td>convert between pytorch, caffe prototxt/weights and darknet cfg/weights
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/yijingxiao/Stock-Crawler-Analysis">Stock-Crawler-Analysis</a></td>
<td>This project download stock data and analyze with plots
</td>
<td><img src="https://raw.githubusercontent.com/yijingxiao/Stock-Crawler-Analysis/master/blog-images/20daymovingavg.png" alt="" width="200"></td></tr>
<tr><td><a href="">rl_a3c_pytorch</a></td>
<td>Reinforcement learning A3C LSTM Atari with Pytorch
</td>
<td><img src="https://github.com/dgriff777/rl_a3c_pytorch/raw/master/demo/BeamRider.gif" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/snakeztc/NeuralDialog-CVAE">NeuralDialog-CVAE</a></td>
<td>Tensorflow Implementation of Knowledge-Guided CVAE for dialog generation. It is released by Tiancheng Zhao (Tony) from Dialog Research Center, LTI, CMU
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/robbiebarrat/art-DCGAN">art-DCGAN</a></td>
<td>Modified implementation of DCGAN focused on generative art.
</td>
<td><img src="https://raw.githubusercontent.com/robbiebarrat/art-DCGAN/master/images/landscapenet_waifu2x.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/zsdonghao/im2txt2im">im2txt2im</a></td>
<td>I2T2I: Text-to-Image Synthesis with textual data augmentation
</td>
<td><img src="https://github.com/zsdonghao/im2txt2im/raw/master/img/qualitative.jpeg" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/peteanderson80/bottom-up-attention"> bottom-up-attention</a></td>
<td>Bottom-up attention model for image captioning and VQA, based on Faster R-CNN and Visual Genome
</td>
<td><img src="https://github.com/peteanderson80/bottom-up-attention/raw/master/data/demo/rcnn_example_2.png?raw=true" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/samaonline/Hierarchical-Model-for-Long-term-Video-Prediction">Hierarchical-Model-for-Long-term-Video-Prediction</a></td>
<td>Given the previous frames of the video as input, we want to get the long-term frame prediction.
</td>
<td><img src="https://sites.google.com/a/umich.edu/rubenevillegas/_/rsrc/1491011262926/hierch_vid/High_level_Hierarchical_Architecture.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/delijati/pytorch-siamese">pytorch-siamese</a></td>
<td>Siamese Network implementation using Pytorch
</td>
<td><img src="https://cdn-images-1.medium.com/max/880/1*A22PPyMSQRL5E_6IaJqhdg.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/yjxiong/tsn-pytorch">tsn-pytorch</a></td>
<td>Temporal Segment Networks (TSN) in PyTorch
</td>
<td><img src="http://yjxiong.me/others/tsn/pipeline.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/ypxie/pytorch_gans">pytorch_gans</a></td>
<td>Experiment with different Gans architecture
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/chihyaoma/Activity-Recognition-with-CNN-and-RNN">Activity-Recognition-with-CNN-and-RNN.Torch</a></td>
<td>Temporal Segments LSTM and Temporal-Inception for Activity Recognition
</td>
<td><img src="https://github.com/chihyaoma/Activity-Recognition-with-CNN-and-RNN/raw/master/figures/overview_image.png?raw=true" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/allenai/bi-att-flow">bi-att-flow</a></td>
<td>Bidirectional Attention Flow
</td>
<td><img src="https://allenai.github.io/bi-att-flow/BiDAF.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/Yugnaynehc/ssta-captioning">ssta-captioning.pytorch</a></td>
<td>Repository for paper: Saliency-Based Spatio-Temporal Attention for Video Captioning
</td>
<td><img src="https://github.com/Yugnaynehc/ssta-captioning/raw/master/diagram/framework.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/VisionLearningGroup/caption-guided-saliency">caption-guided-saliency</a></td>
<td>Supplementary material to "Top-down Visual Saliency Guided by Captions" (CVPR 2017)
</td>
<td><img src="https://camo.githubusercontent.com/cc7a141b449b4c08a7ac4a19203e96b9ff9a3371/68747470733a2f2f7777772e64726f70626f782e636f6d2f732f337232783566776461346e6b6174752f766964656f373032332e6769663f7261773d31" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/guillaume-chevalier/seq2seq-signal-prediction"> seq2seq-signal-prediction.TensorFlow</a></td>
<td>Signal prediction with a seq2seq RNN model in TensorFlow
</td>
<td><img src="https://github.com/guillaume-chevalier/seq2seq-signal-prediction/raw/master/images/E5.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/eriche2016/stnm.pytorch">stnm.pytorch</a></td>
<td>Pytorch code for spatial transformer network with mask
</td>
<td><img src="https://github.com/eriche2016/stnm.pytorch/raw/master/images/stnm_figure.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/iqbalu/PoseTrack-CVPR2017">PoseTrack-CVPR2017.caffe</a></td>
<td>PoseTrack: Joint Multi-Person Pose Estimation and Tracking
</td>
<td><img src="https://camo.githubusercontent.com/56ddfad684583f9e3054a3e07701cde204b2bdf5/687474703a2f2f70616765732e6961692e756e692d626f6e6e2e64652f697162616c5f756d61722f506f7365547261636b2f646174612f506f7365547261636b2e676966" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/twitter/meta-learning-lstm">meta-learning-lstm.Torch</a></td>
<td>
This repo contains the code for the following paper: https://openreview.net/pdf?id=rJY0-Kcll
</td>
<td><img src="https://cdn-images-2.medium.com/max/800/1*93MYsBa3DFtBNuvjdWuXkQ.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/ugo-nama-kun/gym_torcs">Gym-TORCS</a></td>
<td>Gym-TORCS is the reinforcement learning (RL) environment in TORCS domain with OpenAI-gym-like interface. TORCS is the open-rource realistic car racing simulator recently used as RL benchmark task in several AI studies.
</td>
<td><img src="https://i.ytimg.com/vi/l5f_TwCLyCA/maxresdefault.jpg" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/ghliu/pytorch-ddpg">DDPG.pytorch</a></td>
<td>Deep Deterministic Policy Gradient on PyTorch
</td>
<td><img src="https://github.com/ghliu/pytorch-ddpg/raw/master/output/Pendulum-v0-run0/validate_reward.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/XingxingZhang/dress">dress</a></td>
<td>Sentence Simplification with Deep Reinforcement Learning
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/JannerM/spatial-reasoning">spatial-reasoning</a></td>
<td>Code for the paper "Representation Learning for Grounded Spatial Reasoning" https://arxiv.org/abs/1707.03938
</td>
<td><img src="https://github.com/JannerM/spatial-reasoning/raw/master/logs/example/predictions.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/ranjaykrishna/densevid_eval">densevid_eval</a></td>
<td>Dense Captioning Events in Video - Evaluation Code
</td>
<td><img src="http://img.blog.csdn.net/20170518104038240?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYm9qYWNraG9zcmVtYW4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/twtygqyy/pytorch-SRResNet">SRResNet.pytorch</a></td>
<td>pytorch implementation for Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network arXiv:1609.04802v2
</td>
<td><img src="https://github.com/twtygqyy/pytorch-SRResNet/raw/master/result/result.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/iqbalu/PoseTrack-CVPR2017">PoseTrack-CVPR2017</a></td>
<td>PoseTrack: Joint Multi-Person Pose Estimation and Tracking
</td>
<td><img src="https://camo.githubusercontent.com/56ddfad684583f9e3054a3e07701cde204b2bdf5/687474703a2f2f70616765732e6961692e756e692d626f6e6e2e64652f697162616c5f756d61722f506f7365547261636b2f646174612f506f7365547261636b2e676966" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/huggingface/neuralcoref">neuralcoref</a></td>
<td>State-of-the-art coreference resolution based on neural nets and spaCy
</td>
<td><img src="https://camo.githubusercontent.com/94157dbf6ab835f0608aa44d8fca92b4ae74eeec/68747470733a2f2f68756767696e67666163652e636f2f636f7265662f6173736574732f7468756d626e61696c2d6c617267652e706e67" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/JannerM/spatial-reasoning">spatial-reasoning.PyTorch</a></td>
<td>Code for the paper "Representation Learning for Grounded Spatial Reasoning" https://arxiv.org/abs/1707.03938
</td>
<td><img src="https://github.com/JannerM/spatial-reasoning/raw/master/logs/example/predictions.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/2014mchidamb/TorchGlove">TorchGlove.PyTorch</a></td>
<td>PyTorch implementation of Global Vectors for Word Representation. http://suriyadeepan.github.io/2016-06-28-easy-seq2seq/
</td>
<td><img src="https://github.com/2014mchidamb/TorchGlove/raw/master/glove.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/Kaixhin/NoisyNet-A3C">NoisyNet-A3C.PyTorch</a></td>
<td>Noisy Networks for Exploration
</td>
<td><img src="https://github.com/Kaixhin/NoisyNet-A3C/raw/master/figures/good-noisynet-a3c.png" alt="" width="200"></td></tr>
<tr>
<td><a href="https://github.com/gujiuxiang/MIL.pytorch">MIL.pytorch</a></td>
<td>PyTorch implementation of Multiple-instance learning.
</td>
<td><img src="https://www.microsoft.com/en-us/research/wp-content/uploads/2015/04/teaser-258x300.jpg" alt="" width="200"></td>
</tr>
<tr>
<td>
<a href="https://github.com/meetshah1995/pytorch-semseg">pytorch-semseg</a>
</td>
<td>
Semantic Segmentation Algorithms Implemented in PyTorch
</td>
<td>
<img src="https://camo.githubusercontent.com/c1ebf30322adb7208363f8660386c177e4c5d8b6/68747470733a2f2f6d65657473686168313939352e6769746875622e696f2f696d616765732f626c6f672f73732f707473656d7365672e706e67" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/lanpa/tensorboard-pytorchg">tensorboard-pytorch</a>
</td>
<td>
Write tensorboard events with simple command.
</td>
<td>
<img src="https://github.com/lanpa/tensorboard-pytorch/raw/master/screenshots/histogram.png" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/lanpa/tensorboard-pytorchg">caffe-ron</a>
</td>
<td>
RON: Reverse Connection with Objectness Prior Networks for Object Detection
</td>
<td>
<img src="http://wx2.sinaimg.cn/mw690/5396ee05ly1fhbnc2v3loj20q60fggyf.jpg" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/facebookresearch/ELF">ELF.PyTorch</a>
</td>
<td>
ELF: An Extensive, Lightweight and Flexible Platform for Game Research
</td>
<td>
<img src="https://github.com/facebookresearch/ELF/raw/master/overview.png" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/GunhoChoi/Kind_PyTorch_Tutorial">Kind_PyTorch_Tutorial</a>
</td>
<td>
Kind PyTorch Tutorial for beginners
</td>
<td>
<img src="https://github.com/GunhoChoi/Kind_PyTorch_Tutorial/raw/master/logo/PyTorch.jpg" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/facebookresearch/SentEval">SentEval.PyTorch</a>
</td>
<td>
SentEval is a library for evaluating the quality of sentence embeddings. We assess their generalization power by using them as features on a broad and diverse set of "transfer" tasks (more details here). Our goal is to ease the study and the development of general-purpose fixed-size sentence representations.
</td>
<td>
<img src="https://pbs.twimg.com/media/DEAndYaXgAIHoTn.jpg" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/facebookresearch/InferSent">InferSent.PyTorch</a>
</td>
<td>
InferSent is a sentence embeddings method that provides semantic sentence representations. It is trained on natural language inference data and generalizes well to many different tasks.
</td>
<td>
<img src="https://camo.githubusercontent.com/eacfae9d9987988db2774e703609473bc10ed311/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f73656e746576616c2f696e66657273656e742f76697375616c697a6174696f6e2e706e67" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/dallascard/TreeLSTM">TreeLSTM.PyTorch</a>
</td>
<td>
An attempt to implement the Constinuency Tree LSTM in "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks"
</td>
<td>
<img src="https://adeshpande3.github.io/assets/NLP28.png" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/zawlin/cvpr17_vtranse">cvpr17_vtranse.caffe</a>
</td>
<td>
This implements "Visual Translation Embedding Network for Visual Relation Detection,Hanwang Zhang, Zawlin Kyaw, Shih-Fu Chang, Tat-Seng Chua (CVPR2017)"
</td>
<td>
<img src="https://image.slidesharecdn.com/05visualtranslationembeddingnetworkforvisualrelationdetection-170320185136/95/visual-translation-embedding-network-for-visual-relation-detection-upc-reading-group-3-638.jpg?cb=1490036010" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/slaypni/fastdtw">FastDTW.Python</a>
</td>
<td>
Python implementation of FastDTW, which is an approximate Dynamic Time Warping (DTW) algorithm that provides optimal or near-optimal alignments with an O(N) time and memory complexity.
</td>
<td>
<img src="http://slideplayer.com/6449709/22/images/3/Dynamic+Time+Warping+%28DTW%29.jpg" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/NickShahML/rnn.wgan">rnn.wgan.TensorFlow</a>
</td>
<td>
Code for training and evaluation of the model from "Language Generation with Recurrent Generative Adversarial Networks without Pre-training". Additional Code for using Fisher GAN in Recurrent Generative Adversarial Networks
</td>
<td>
<img src="https://www.researchgate.net/publication/308324937/figure/fig1/AS:408264924778496@1474349351242/Figure-4-Negative-log-likelihood-performance-with-different-pre-training-epochs-before.png" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/facebookresearch/clevr-iep">clevr-iep.PyTorch</a>
</td>
<td>
This is the code for the paper: Inferring and Executing Programs for Visual Reasoning. Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Judy Hoffman, Fei-Fei Li, Larry Zitnick, Ross Girshick
arXiv 2017
</td>
<td>
<img src="https://github.com/facebookresearch/clevr-iep/raw/master/img/system.png" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/hitvoice/DrQA">DrQA.PyTorch</a>
</td>
<td>
A pytorch implementation of Reading Wikipedia to Answer Open-Domain Questions (DrQA).
</td>
<td>
<img src="https://camo.githubusercontent.com/ee85124dcc6a41d2f59199818471bfc9b57129f3/68747470733a2f2f7261776769742e636f6d2f686974766f6963652f447251412f6d61737465722f696d672f766f6361622e737667" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/deepsense-io/roi-pooling">roi-pooling.TensorFlow</a>
</td>
<td>
This repo contains the implementation of Region of Interest pooling as a custom TensorFlow operation. The CUDA code responsible for the computations was largely taken from the original Caffe implementation by Ross Girshick.
</td>
<td>
<img src="https://github.com/deepsense-io/roi-pooling/raw/master/roi_pooling_animation.gif" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/e-lab/pytorch-CortexNet">CortexNet.PyTorch</a>
</td>
<td>
This repo contains the PyTorch implementation of CortexNet. Check the project website for further information.
</td>
<td>
<img src="https://engineering.purdue.edu/elab/CortexNet/images/model_02-black.svg" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/JianGoForIt/YellowFin_Pytorch">YellowFin.Pytorch</a>
</td>
<td>
YellowFin is an auto-tuning optimizer based on momentum SGD which requires no manual specification of learning rate and momentum. It measures the objective landscape on-the-fly and tunes momentum as well as learning rate using local quadratic approximation.
</td>
<td>
<img src="https://github.com/JianGoForIt/YellowFin_Pytorch/raw/master/plots/resnext_test_acc.png" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/vijayvee/video-captioning">video-captioning.TensorFlow</a>
</td>
<td>
This repository contains my implementation of a video captioning system. This system takes as input a video and generates a caption describing the event.
</td>
<td>
<img src="https://github.com/vijayvee/video-captioning/raw/master/images/Arch_S2VT.png" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/Rangozhang/VideoCaption">VideoCaption.Torch</a>
</td>
<td>
Video captioning using LSTM and CNN following the paper sequence to sequence - video to text. This is the Visual Learning project done by Rui Zhang, Yujia Huang and Yu Zhang. Neuraltalk2 from Karpathy is taken as reference.
</td>
<td>
<img src="https://github.com/Rangozhang/VideoCaption/raw/master/VideoCaption.jpg" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/mrjel/noise-as-targets-tensorflow">noise-as-targets.Tensorflow</a>
</td>
<td>
Noise-as-targets representation learning for cifar10. Implementation based on the arxiv-paper "Unsupervised Learning by Predicting Noise" by Bojanowski and Joulin: https://arxiv.org/abs/1704.05310
</td>
<td>
<img src="https://github.com/mrjel/noise-as-targets-tensorflow/raw/master/neighbors.png?raw=true" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/ttpro1995/treelstm.pytorch">treelstm.pytorch</a>
</td>
<td>
A PyTorch based implementation of Tree-LSTM from Kai Sheng Tai's paper Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks.
</td>
<td>
<img src="" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/RobRomijnders/AE_ts">AE_ts.Tensorflow</a>
</td>
<td>
This repo presents a simple auto encoder for time series. It visualizes the embeddings using both PCA and tSNE. I show this on a dataset of 5000 ECG's. The model doesn't use the labels during training. Yet, the produced clusters visually separate the classes of ECG's.
</td>
<td>
<img src="https://github.com/RobRomijnders/AE_ts/raw/master/im/latent_vectors2.png?raw=true" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/abhshkdz/neural-vqa-attention">neural-vqa-attention.Torch</a>
</td>
<td>
Torch implementation of an attention-based visual question answering model (Stacked Attention Networks for Image Question Answering, Yang et al., CVPR16).
</td>
<td>
<img src="https://camo.githubusercontent.com/828817c970da406d2d83dc9a5c03fb120231e2a2/687474703a2f2f692e696d6775722e636f6d2f56627149525a7a2e706e67" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/YutingZhang/generative-models">generative-models.PyTorch</a>
</td>
<td>
Collection of generative models, e.g. GAN, VAE in Pytorch and Tensorflow.
</td>
<td>
<img src="" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/YutingZhang/caffe-recon-dec">caffe-recon-dec</a>
</td>
<td>
This software is for the following paper: Yuting Zhang, Kibok Lee, Honglak Lee, “Augmenting Supervised Neural Networks with Unsupervised Objectives for Large-scale Image Classification”, The 33rd International Conference on Machine Learning (ICML), 2016.
</td>
<td>
<img src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-11-08/d34e4e73e5e7a506a7315f703d9153a14e62879e/3-Figure3-1.png" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/oeway/pytorch-deform-conv">deform-conv.PyTorch</a>
</td>
<td>
PyTorch implementation of Deformable Convolution
</td>
<td>
<img src="https://github.com/oeway/pytorch-deform-conv/raw/master/deformable-learned-offset-filtered.gif" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/longcw/yolo2-pytorch">yolo2.pytorch</a>
</td>
<td>
This is a PyTorch implementation of YOLOv2. This project is mainly based on darkflow and darknet.
</td>
<td>
<img src="https://pjreddie.com/media/image/model2.png" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/jacobgil/pytorch-pruning">pruning.PyTorch</a>
</td>
<td>
PyTorch implementation of [1611.06440 Pruning Convolutional Neural Networks for Resource Efficient Inference]
</td>
<td>
<img src="https://image.slidesharecdn.com/fvjdgcx1roseyhbxcrql-signature-d774a83ad96ff5ab419f7a2a324ae6152f960a41110707f9c2f12ad17953c6c2-poli-170128022739/95/pruning-convolutional-neural-networks-for-resource-efficient-inference-3-638.jpg?cb=1485570531" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/alexis-jacq/Pytorch-Sketch-RNN">Sketch-RNN.PyTorch</a>
</td>
<td>
A pytorch implementation of https://arxiv.org/abs/1704.03477. In order to draw other things than cats, you will find more drawing data here: https://github.com/googlecreativelab/quickdraw-dataset
</td>
<td>
<img src="https://github.com/alexis-jacq/Pytorch-Sketch-RNN/raw/master/images/1900_output_.jpg" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="">SRGAN.Tensorflow</a>
</td>
<td>
Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network http://tensorlayer.readthedocs.io
</td>
<td>
<img src="https://github.com/zsdonghao/SRGAN/raw/master/img/model.jpeg" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/ArrasL/LRP_for_LSTM">LRP_for_LSTM.Python</a>
</td>
<td>
This code release contains an implementation of two relevance decomposition methods, Layer-wise Relevance Propagation (LRP) and Sensitivity Analysis (SA), for a bidirectional LSTM, as described in the paper Explaining Recurrent Neural Network Predictions in Sentiment Analysis by L. Arras, G. Montavon, K.-R. Müller and W. Samek, 2017
</td>
<td>
<img src="http://karpathy.github.io/assets/rnn/pane1.png" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/zsdonghao/Spatial-Transformer-Nets">Spatial-Transformer-Nets.Tensorflow</a>
</td>
<td>
Spatial Transformer Networks (STN) is a dynamic mechanism that produces transformations of input images (or feature maps)including scaling, cropping, rotations, as well as non-rigid deformations. This enables the network to not only select regions of an image that are most relevant (attention), but also to transform those regions to simplify recognition in the following layers.
</td>
<td>
<img src="https://github.com/zsdonghao/Spatial-Transformer-Nets/raw/master/images/network.jpeg" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/HendrikStrobelt/LSTMVis">LSTMVis.Python</a>
</td>
<td>
Visualization Toolbox for Long Short Term Memory networks (LSTMs)
</td>
<td>
<img src="https://github.com/HendrikStrobelt/LSTMVis/raw/master/docs/img/teaser_V2_small.png" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/OpenNMT/OpenNMT-py">OpenNMT.PyTorch</a>
</td>
<td>
This is a Pytorch port of OpenNMT, an open-source (MIT) neural machine translation system. Full documentation is available here.
</td>
<td>
<img src="https://camo.githubusercontent.com/6340603acc1062d8ec6d274283a48fc7562bc8ba/687474703a2f2f6f70656e6e6d742e6769746875622e696f2f73696d706c652d6174746e2e706e67" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/felixgwu/img_classification_pk_pytorch">img_classification_pk.pytorch</a>
</td>
<td>
Image Classification Project Killer in PyTorch
</td>
<td>
<img src="https://camo.githubusercontent.com/f0b53276e24e963bd7d2d8ab998128070f38faea/687474703a2f2f692e696d6775722e636f6d2f454d4d37516e322e706e6767" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/eladhoffer/seq2seq.pytorch">seq2seq.pytorch</a>
</td>
<td>
This is a complete suite for training sequence-to-sequence models in PyTorch. It consists of several models and code to both train and infer using them.
</td>
<td>
<img src="https://raw.githubusercontent.com/MaximumEntropy/Seq2Seq-PyTorch/master//images/Seq2Seq.png" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/wkentaro/pytorch-fcn">fcn.pytorch</a>
</td>
<td>
Fully Convolutional Networks implemented with PyTorch.
</td>
<td>
<img src="http://meetshah1995.github.io/images/blog/ss/fcn.png" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/Kaixhin/FCN-semantic-segmentation">FCN-semantic-segmentation.PyTorch</a>
</td>
<td>
Simple end-to-end semantic segmentation using fully convolutional networks. Takes a pretrained 34-layer ResNet, removes the fully connected layers, and adds transposed convolution layers with skip connections from lower layers. Initialises upsampling convolutions with bilinear interpolation filters and zeros the final (classification) layer. Uses an independent cross-entropy loss per class.
</td>
<td>
<img src="https://raw.githubusercontent.com/sunshineatnoon/Paper-Collection/master/images/FCN1.png" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/pemami4911/neural-combinatorial-rl-pytorch">neural-combinatorial-rl.pytorch</a>
</td>
<td>
PyTorch implementation of Neural Combinatorial Optimization with Reinforcement Learning.
</td>
<td>
<img src="https://camo.githubusercontent.com/5593b2c8184c4bc08372f919063e826d9bcc2c67/687474703a2f2f7333332e706f7374696d672e6f72672f79747836336b7763762f7768617469735f6167656e746e65745f706e672e706e67" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/locuslab/pytorch_fft">fft.PyTorch</a>
</td>
<td>
A package that provides a PyTorch C extension for performing batches of 2D CuFFT transformations, by Eric Wong
</td>
<td>
<img src="https://sites.google.com/site/zilong308/_/rsrc/1292283454165/cuda/cuda-fft-library-cufft/radix2fft.png" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/matthewfl/nlp-entity-convnet">nlp-entity-convnet.Python</a>
</td>
<td>
Convolutional network for entity linking (Naacl 2016)
</td>
<td>
<img src="" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/mjacar/pytorch-nec">nec.pytorch</a>
</td>
<td>
PyTorch implementation of Neural Episodic Control
</td>
<td>
<img src="https://theintelligenceofinformation.files.wordpress.com/2017/03/necfeatim.jpg?w=772" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/benathi/word2gm">word2gm.Tensorflow</a>
</td>
<td>
This is an implementation of the model in Athiwaratkun and Wilson, Multimodal Word Distributions, 2017, ACL.
</td>
<td>
<img src="" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/jakezhaojb/ARAE">ARAE.PyTorch</a>
</td>
<td>
Code for the paper "Adversarially Regularized Autoencoders for Generating Discrete Structures" by Zhao, Kim, Zhang, Rush and LeCun https://arxiv.org/abs/1706.04223
</td>
<td>
<img src="https://user-images.githubusercontent.com/544269/27250917-22649aea-5376-11e7-8e10-ab873848be57.PNG" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/SeanRosario/Conditional-GAN-Pytorch">Conditional-GAN.Pytorch</a>
</td>
<td>
Implementation of a Conditional GAN in PyTorch for generating movie posters, conditioned on the genre of movie
</td>
<td>
<img src="" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/Guim3/IcGAN">IcGAN.Torch</a>
</td>
<td>
Invertible conditional GANs for image editing
</td>
<td>
<img src="https://github.com/Guim3/IcGAN/raw/master/images/model_overview.png" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/kimhc6028/relational-networks">relational-networks.PyTorch</a>
</td>
<td>
Pytorch implementation of Relational Networks - A simple neural network module for relational reasoning
</td>
<td>
<img src="https://github.com/kimhc6028/relational-networks/raw/master/data/sample.png" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/yunjey/pytorch-CycleGAN-and-pix2pix">CycleGAN-and-pix2pix.pytorch</a>
</td>
<td>
CycleGAN and pix2pix in PyTorch
</td>
<td>
<img src="https://github.com/yunjey/pytorch-CycleGAN-and-pix2pix/raw/master/imgs/horse2zebra.gif" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="">dragan.pytorch</a>
</td>
<td>
PyTorch implementation of DRAGAN (https://arxiv.org/abs/1705.07215)
</td>
<td>
<img src="" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/doubledaibo/drnet">drnet.caffe</a>
</td>
<td>
Code of Detecting Visual Relationships with Deep Relational Networks
</td>
<td>
<img src="https://camo.githubusercontent.com/76446b7c2d3cb7dfda5ebd92a96309f4224bc131/68747470733a2f2f7261772e6769746875622e636f6d2f646f75626c65646169626f2f64726e65742f6d61737465722f696d67732f706169725f66696c7465722e6a7067" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/thnkim/OpenFacePytorch">OpenFace.Pytorch</a>
</td>
<td>
PyTorch module to use OpenFace's nn4.small2.v1.t7 model
</td>
<td>
<img src="" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="">attention-is-all-you-need.pytorch</a>
</td>
<td>
This is a PyTorch implementation of the Transformer model in "Attention is All You Need" (Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin, arxiv, 2017).
</td>
<td>
<img src="https://camo.githubusercontent.com/88e8f36ce61dedfd2491885b8df2f68c4d1f92f5/687474703a2f2f696d6775722e636f6d2f316b72463252362e706e67" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/BoyuanJiang/context_encoder_pytorch">context_encoder.pytorch</a>
</td>
<td>
Context Encoders: Feature Learning by Inpainting
</td>
<td>
<img src="https://github.com/BoyuanJiang/context_encoder_pytorch/raw/master/val_cropped_samples.png" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/alecng94/fully-convolutional-network-semantic-segmentation"> fully-convolutional-network-semantic-segmentation.Caffe</a>
</td>
<td>
This repository is an implementation of the paper Fully Convolutional Networks for Semantic Segmentation for the purpose of segmenting humans from a singular RGB image.
</td>
<td>
<img src="https://github.com/alecng94/fully-convolutional-network-semantic-segmentation/raw/master/results/T3/seg/Robyn-iPhone6-GoodLighting-Tpose_11.png" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/gchrupala/visually-grounded-speech">visually-grounded-speech.python</a>
</td>
<td>
This repository contains code to reproduce the results from: Chrupała, G., Gelderloos, L., & Alishahi, A. (2017). Representations of language in a model of visually grounded speech signal. ACL. arXiv preprint: https://arxiv.org/abs/1702.01991
</td>
<td>
<img src="https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-24-638.jpg?cb=1485543803" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="">Generating Visual Explanations.caffe</a>
</td>
<td>
Code to replicate "Generating Visual Explanations"
</td>
<td>
<img src="https://www.mpi-inf.mpg.de/fileadmin/_processed_/csm_Teaser-model_ac87ff0443.jpg" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/ypxie/pytorch-cramer-Gan">cramer-Gan.Pytorch</a>
</td>
<td>
pytorch implementation of cramer gan https://arxiv.org/abs/1705.10743
</td>
<td>
<img src="https://cdn-ak.f.st-hatena.com/images/fotolife/y/yusuke_ujitoko/20170603/20170603155440.png" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/ajbrock/FreezeOut">FreezeOut.pytorch</a>
</td>
<td>
Accelerate Neural Net Training by Progressively Freezing Layers
</td>
<td>
<img src="https://camo.githubusercontent.com/c98cabfaa64adb8efc4fe5d4f3db5e7d35577667/687474703a2f2f692e696d6775722e636f6d2f794b4539707a472e676966" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/xternalz/WideResNet-pytorch"> WideResNet.pytorch</a>
</td>
<td>
Wide Residual Networks (WideResNets) in PyTorch
</td>
<td>
<img src="http://people.cs.uchicago.edu/~larsson/fractalnet/overview.png" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/jameslyons/python_speech_features">speech_features.Python</a>
</td>
<td>
This library provides common speech features for ASR including MFCCs and filterbank energies.
</td>
<td>
<img src="" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/fullfanta/multimodal_transfer">multimodal_transfer.Tensorflow</a>
</td>
<td>
tensorflow implementation of 'Multimodal Transfer: A Hierarchical Deep Convolutional Neural Network for Fast Artistic Style Transfer'
</td>
<td>
<img src="http://www.cs.ucsb.edu/~xwang/indexpics/transfer_2.png" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/kimhc6028/forward-thinking-pytorch">forward-thinking.pytorch</a>
</td>
<td>
Pytorch implementation of "Forward Thinking: Building and Training Neural Networks One Layer at a Time"
</td>
<td>
<img src="https://static1.squarespace.com/static/56f08c6e4c2f85752c9ca3bd/t/58ae168ae58c62d5649f8201/1487804055805/Learning+Loop" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/xiaohaoChen/rrc_detection"> rrc_detection.c++</a>
</td>
<td>
Accurate Single Stage Detector Using Recurrent Rolling Convolution
</td>
<td>
<img src="" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="">Activation-Visualization-Histogram.Python</a>
</td>
<td>
Compare SELUs (scaled exponential linear units) with other activations on MNIST, CIFAR10, etc.
</td>
<td>
<img src="" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/bermanmaxim/jaccardSegment">jaccardSegment.Pytorch</a>
</td>
<td>
Deeplab-resnet-101 in Pytorch with Jaccard loss
</td>
<td>
<img src="" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/arunmallya/simple-vqa"> simple-vqa.Torch</a>
</td>
<td>
Implements an MLP for VQA
</td>
<td>
<img src="https://avisingh599.github.io/images/vqa/challenge.png" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/ericjang/gumbel-softmax">gumbel-softmax.Tensorflow</a>
</td>
<td>
Gumbel Softmax / Concrete VAE with BayesFlow
</td>
<td>
<img src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-11-08/875211d6527da1a07bd426ae8f77f4ad7f2f265e/2-Figure1-1.png" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/Jungyhuk/Latent-Attention">Latent-Attention.Tensorflow</a>
</td>
<td>
This repo provides the code to replicate the experiments in the paper. Xinyun Chen, Chang Liu, Richard Shin, Dawn Song, Mingcheng Chen, Latent Attention For If-Then Program Synthesis , in Proc. of NIPS 2016
</td>
<td>
<img src="" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/Cadene/vqa.pytorch">vqa.pytorch</a>
</td>
<td>
Visual Question Answering in Pytorch
</td>
<td>
<img src="https://avisingh599.github.io/images/vqa/challenge.png" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/amdegroot/ssd.pytorch">ssd.pytorch</a>
</td>
<td>
A PyTorch Implementation of Single Shot MultiBox Detector
</td>
<td>
<img src="http://joshua881228.webfactional.com/media/uploads/ReadingNote/arXiv_SSD/SSD.png" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/ZeweiChu/nmt-seq2seq">nmt-seq2seq.PyTorch</a>
</td>
<td>
seq2seq model written in Pytorch
</td>
<td>
<img src="" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="">odin.pytorch</a>
</td>
<td>
Principled Detection of Out-of-Distribution Examples in Neural Networks
</td>
<td>
<img src="http://www.cs.cornell.edu/~yli/images/odin-teaser.png" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/arielephrat/vid2speech"> vid2speech.Pytorch</a>
</td>
<td>
This is the code for the paper. Vid2speech: Speech Reconstruction from Silent Video. Ariel Ephrat and Shmuel Peleg to appear at ICASSP 2017
</td>
<td>
<img src="https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-49-638.jpg?cb=1485543803" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/kuangliu/pytorch-cifar">cifar.pytorch</a>
</td>
<td>
95.04% on CIFAR10 with PyTorch
</td>
<td>
<img src="" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="">latent-noise-icnm.caffe</a>
</td>
<td>
Based on the CVPR 2016 Paper - "Seeing through the Human Reporting Bias : Visual Classifiers from Noisy Human-Centric Labels"
</td>
<td>
<img src="" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/kefirski/pytorch_RVAE">RVAE.PyTorch</a>
</td>
<td>
Recurrent Variational Autoencoder that generates sequential data implemented in pytorch
</td>
<td>
<img src="https://image.slidesharecdn.com/160625tokyowebmining2-160624151348/95/vaetype-deep-generative-models-17-638.jpg?cb=1466781926" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/s-gupta/visual-concepts">visual-concepts.caffe</a>
</td>
<td>
From Captions to Visual Concepts and Back
</td>
<td>
<img src="" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/kimiyoung/review_net"> review_net.Torch</a>
</td>
<td>
Review Network for Caption Generation
</td>
<td>
<img src="http://read.html5.qq.com/image?src=forum&amp;q=5&amp;r=0&amp;imgflag=7&amp;imageUrl=http://mmbiz.qpic.cn/mmbiz_png/G3dAicUK7RSKBPn6vHrfjo9tLxnSfS7hia4pcYAWa2grNYCOic3ul7qPxibqPv371CPJ59DpxhfZmWqWUr8wOkKphw/0?wx_fmt=png" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/geek-ai/irgan">irgan</a>
</td>
<td>
This repository hosts the experimental code for SIGIR 2017 paper "IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models".
</td>
<td>
<img src="" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/mjacar/pytorch-trpo">trpo.pytorch</a>
</td>
<td>
PyTorch Implementation of Trust Region Policy Optimization (TRPO)
</td>
<td>
<img src="https://deepdrive.berkeley.edu/sites/default/files/styles/full_width/public/projects/leaderboard-comp_0.jpg?itok=BLNWz0AU" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/lmb-freiburg/flownet2">flownet2.c++</a>
</td>
<td>
FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks
</td>
<td>
<img src="" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/jacobgil/pytorch-explain-black-box">explain-black-box.pytorch</a>
</td>
<td>
PyTorch implementation of Interpretable Explanations of Black Boxes by Meaningful Perturbation
</td>
<td>
<img src="" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/taey16/vision_language">vision_language.Torch</a>
</td>
<td>
vision language
</td>
<td>
<img src="http://mscoco.cloudapp.net/static/images/captions-challenge2015.jpg" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/nightrome/really-awesome-gan">really-awesome-gan</a>
</td>
<td>
A list of papers and other resources on General Adversarial (Neural) Networks. This site is maintained by Holger Caesar. To complement or correct it, please contact me at holger-at-it-caesar.com or visit it-caesar.com. Also checkout really-awesome-semantic-segmentation and our COCO-Stuff dataset.
</td>
<td>
<img src="https://camo.githubusercontent.com/e4976062d0dd6f91e570b0ebe52f633e0894b7a8/687474703a2f2f69742d6361657361722e636f6d2f6769746875622f706f73652d6775696465642d706572736f6e2e706e67" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/eladhoffer/bigBatch">bigBatch.PyTorch</a>
</td>
<td>
This is a code repository used to generate the results appearing in "Train longer, generalize better: closing the generalization gap in large batch training of neural networks" By Elad Hoffer, Itay Hubara and Daniel Soudry.
</td>
<td>
<img src="http://neuralnetworksanddeeplearning.com/images/more_data_comparison.png" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="">BGAN.Theano</a>
</td>
<td>
boundary-seeking generative adversarial networks
</td>
<td>
<img src="" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/sagiebenaim/DistanceGAN">DistanceGAN.PyTorch</a>
</td>
<td>
Pytorch implementation of "One-Sided Unsupervised Domain Mapping"
</td>
<td>
<img src="http://img.blog.csdn.net/20170622223802121?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSHVuZ3J5b2Y=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/talolard/DenseContinuousSentances">DenseContinuousSentances.Tensorflow</a>
</td>
<td>
Working towards implementing Generating Sentences from a Continuous Space but with DenseNet
</td>
<td>
<img src="https://image.slidesharecdn.com/generatingsentencesfromacontinuousspace-160704233328/95/generating-sentences-from-a-continuous-space-2-638.jpg?cb=1467675299" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/Kyubyong/bytenet_translation">bytenet_translation.Tensorflow</a>
</td>
<td>
A TensorFlow Implementation of Machine Translation In Neural Machine Translation in Linear Time
</td>
<td>
<img src="" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/rdevon/BGAN">BGAN.Theano</a>
</td>
<td>
Boundary-seeking generative adversarial networks (BGAN). As featured in the paper: https://arxiv.org/abs/1702.08431v2
</td>
<td>
<img src="https://image.slidesharecdn.com/generativeadversarialnetworks-170111082649/95/generative-adversarial-networks-70-638.jpg?cb=1489908568" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/stas-semeniuta/textvae">textvae.Theano</a>
</td>
<td>
Theano code for experiments in the paper A Hybrid Convolutional Variational Autoencoder for Text Generation.
</td>
<td>
<img src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-11-08/81aee1c76e6bd4b915b016f7a8b70abe42841dd8/3-Figure2-1.png" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/sumehta/siamese_network_vqa">siamese_network_vqa.Torch</a>
</td>
<td>
Siamese network implementation in Torch for binary Visual Question Answering
</td>
<td>
<img src="" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/pytorch/examples">examples.Pytorch</a>
</td>
<td>
A set of examples around pytorch in Vision, Text, Reinforcement Learning, etc.
</td>
<td>
<img src="" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/lipiji/hierarchical-encoder-decoder">hierarchical-encoder-decoder.Theano</a>
</td>
<td>
Hierarchical encoder-decoder framework for sequences of words, sentences, paragraphs and documents using LSTM and GRU in Theano
</td>
<td>
<img src="http://yanran.li/images/attention-3.png" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="">pointGAN.PyTorch</a>
</td>
<td>
point set generative adversarial nets
</td>
<td>
<img src="https://github.com/fxia22/pointGAN/raw/master/misc/output.gif?raw=true" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="">baselines.TensorFlow</a>
</td>
<td>
OpenAI Baselines: high-quality implementations of reinforcement learning algorithms
</td>
<td>
<img src="" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/amujika/Fast-Slow-LSTM">Fast-Slow-LSTM.TensorFlow</a>
</td>
<td>
</td>
<td>
<img src="" alt="" width="200">
</td>
</tr>
<tr>
<td><a href="https://github.com/juefeix/lbcnn.torch">lbcnn.torch</a></td>
<td>
Torch implementation of CVPR'17 - Local Binary Convolutional Neural Networks: http://xujuefei.com/lbcnn.html
</td>
<td><img src="http://vishnu.boddeti.net/images/lbcnn/03_LBCNN_CNN.png" alt="" width="200"></td>
</tr>
<tr>
<td><a href="https://github.com/mosessoh/CNN-LSTM-Caption-Generator"></a>CNN-LSTM-Caption-Generator.TensorFlow</td>
<td>
A Tensorflow implementation of CNN-LSTM image caption generator architecture that achieves close to state-of-the-art results on the MSCOCO dataset.
</td>
<td><img src="" alt="" width="200"></td>
</tr>
<tr>
<td>
<a href="https://github.com/jiasenlu/AdaptiveAttention">AdaptiveAttention.Torch</a>
</td>
<td>
Implementation of "Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning"
</td>
<td>
<img src="https://c2.sfdcstatic.com/content/dam/web/en_us/www/images/einstein/publications/image_caption.svg" alt="" width="200">
</td>
</tr>
<tr>
<td><a href="https://github.com/kimhc6028/pytorch-noreward-rl"></a>noreward-rl.PyTorch</td>
<td>
pytorch implementation of Curiosity-driven Exploration by Self-supervised Prediction
</td>
<td><img src="https://pathak22.github.io/noreward-rl/resources/overview.jpg" alt="" width="200"></td>
</tr>
<tr><td><a href="https://github.com/stormraiser/GAN-weight-norm"></a> GAN-weight-norm.Torch</td>
<td>
Code for "On the Effects of Batch and Weight Normalization in Generative Adversarial Networks"
</td>
<td><img src="https://pbs.twimg.com/media/DAjS6sqXYAA2zBJ.jpg" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/cxhernandez/molencoder">molencoder</a></td>
<td>
Molecular AutoEncoder in PyTorch
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/zhiguowang/BiMPM">BiMPM.Tensorflow</a></td>
<td>BiMPM: Bilateral Multi-Perspective Matching for Natural Language Sentences</td>
<td><img src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-11-08/b9d220520a5da7d302107aacfe875b8e2977fdbe/1-Figure1-1.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/SeanNaren/deepspeech.pytorch">deepspeech.pytorch</a></td>
<td>
Speech Recognition using DeepSpeech2 and the CTC activation function. Edit
</td>
<td><img src="http://simplecore.intel.com/nervana/wp-content/uploads/sites/53/2017/06/Picture2.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/episodeyang/grammar_variational_autoencoder">grammar_variational_autoencoder.PyTorch</a></td>
<td>pytorch implementation of grammar variational autoencoder
</td>
<td><img src="http://video.ch9.ms/ch9/d659/2259c256-40db-46c8-b734-175233b8d659/39001_512.jpg" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/jiweil/Neural-Dialogue-Generation">Neural-Dialogue-Generation.Torch</a></td>
<td>Neural Dialogue Generation
</td>
<td><img src="http://img.blog.csdn.net/20161120084609200" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/andersbll/autoencoding_beyond_pixels">autoencoding_beyond_pixels.Python</a></td>
<td>Generative image model with learned similarity measures
</td>
<td><img src="http://lijiancheng0614.github.io/2016/12/07/2016_12_07_VAEGAN/fig2.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/Stonesjtu/Pytorch-NCE">NCE.Pytorch</a></td>
<td>The Noise Contrastive Estimation for softmax output written in Pytorch
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/kefirski/pytorch_NEG_loss">NEG_loss.PyTorch</a></td>
<td>NEG loss implemented in pytorch
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/PureDiors/pytorch_RFCN">RFCN.PyTorch</a></td>
<td>Region-based FCN implemented with PyTorch
</td>
<td><img src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-11-08/a8f24fcc1eb0354ffd91f0e3031f5c4dc3e02dd6/1-Figure1-1.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/guillitte/pytorch-sentiment-neuron">sentiment-neuron.PyTorch</a></td>
<td>Pytorch version of generating-reviews-discovering-sentiment : https://github.com/openai/generating-reviews-discovering-sentiment
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/yuandong-tian/ICML17_ReLU">ICML17_ReLU.python</a></td>
<td>An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis. Yuandong Tian. ICML, 2017
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/Cloud-CV/diverse-beam-search">diverse-beam-search.Torch</a></td>
<td>
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/dhwajraj/deep-siamese-text-similarity">deep-siamese-text-similarity.Tensorflow</a></td>
<td>Tensorflow based implementation of deep siamese LSTM network to capture phrase/sentence similarity using character embeddings
</td>
<td><img src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-11-08/35b11ac652646c70a559f7ae29295e1d5de09a80/3-Figure1-1.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/LMescheder/AdversarialVariationalBayes">AdversarialVariationalBayes.Tensorflow</a></td>
<td>This repository contains the code to reproduce the core results from the paper "Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks".
</td>
<td><img src="https://i1.wp.com/pbs.twimg.com/media/C3INKdnUoAAARhe.jpg?w=840&amp;ssl=1" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/spro/pytorch-seq2seq-intent-parsing">seq2seq-intent-parsing.PyTorch</a></td>
<td>Intent parsing and slot filling in PyTorch with seq2seq + attention
</td>
<td><img src="https://camo.githubusercontent.com/4125995f183d3158103b46eeb5ffdea4eef0ef52/68747470733a2f2f692e696d6775722e636f6d2f56316c747668492e706e67" alt="" width="200"></td></tr>
<tr><td><a href=""></a></td>
<td>
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/openai/roboschool">roboschool.Python</a></td>
<td>Roboschool is a long-term project to create simulations useful for research.
</td>
<td><img src="https://blog.openai.com/content/images/2017/05/image1.gif" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/PkuRainBow/Hard-Aware-Deeply-Cascaed-Embedding">Hard-Aware-Deeply-Cascaed-Embedding.c++</a></td>
<td>source code for the paper "Hard-Aware-Deeply-Cascaed-Embedding"
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/onlytailei/pytorch-rl">rl.Pytorch</a></td>
<td>Deep Reinforcement Learning with pytorch & visdom (the branch for A3C continuous control)
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/karpathy/neuraltalk2">neuraltalk2.Torch</a></td>
<td>
</td>
<td><img src="https://inst.eecs.berkeley.edu/~cs194-26/fa15/upload/files/projFinalUndergrad/cs194-cb/k1.PNG" alt="" width="200"></td></tr>
<tr><td><a href=""></a></td>
<td>Efficient Image Captioning code in Torch, runs on GPU
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/niangaotuantuan/Publications-of-Deep-Learning-in-NLP">Publications-of-Deep-Learning-in-NLP</a></td>
<td>collect the publications and related resources of Deep Learning in NLP
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/cheng6076/SNLI-attention">SNLI-attention.Torch</a></td>
<td>SNLI with word-word attention by LSTM encoder-decoder
</td>
<td><img src="http://yanran.li/images/attention-3.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/cheng6076/Variational-LSTM-Autoencoder">Variational-LSTM-Autoencoder.Torch</a></td>
<td>Variational Seq2Seq model
</td>
<td><img src="http://yanran.li/images/infoflow_5.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/tokestermw/text-gan-tensorflow">text-gan.tensorflow</a></td>
<td>TensorFlow GAN implementation using Gumbel Softmax
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/daviddao/pytorch-neural-search-optimizer"> neural-search-optimizer.pytorch</a></td>
<td>PyTorch implementation of Neural Optimizer Search's Optimizer_1
</td>
<td><img src="https://github.com/daviddao/pytorch-neural-search-optimizer/raw/master/imgs/optimizer_1.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/caogang/wgan-gp">wgan-gp.PyTorch</a></td>
<td>A pytorch implementation of Paper "Improved Training of Wasserstein GANs"
</td>
<td><img src="https://casmls.github.io/img/GAN/mix-dcgan-dcgan-comparison.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/LuoweiZhou/e2e-gLSTM-sc">e2e-gLSTM-sc.Torch</a></td>
<td>Code for paper "Image Caption Generation with Text-Conditional Semantic Attention"
</td>
<td><img src="http://img.mp.itc.cn/upload/20170121/f7477052ce2c45ff8683433231a01fc5.jpeg" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/jinfagang/pytorch_chatbot">chatbot.PyTorch</a></td>
<td>A Marvelous ChatBot implement using PyTorch.
</td>
<td><img src="https://camo.githubusercontent.com/b2af91d8860ddef7321dbaebb9a7b2ca6fa2ef33/687474703a2f2f6f66777a63756e7a692e626b742e636c6f7564646e2e636f6d2f454974544977717063724173726578712e706e67" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/jingweiz/pytorch-dnc">dnc.PyTorch</a></td>
<td>Neural Turing Machine (NTM) & Differentiable Neural Computer (DNC) with pytorch & visdom
</td>
<td><img src="https://qph.ec.quoracdn.net/main-qimg-08b667f9a7706e0cf2baf70cd00450b4" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/yunjey/mnist-svhn-transfer">mnist-svhn-transfer.PyTorch</a></td>
<td>PyTorch Implementation of CycleGAN and SSGAN for Domain Transfer (Minimal)
</td>
<td><img src="https://i2.wp.com/www.aimechanic.com/wp-content/uploads/2017/05/CycleGAN-TensorFlow-tutorial-results.png?resize=470%2C383" alt="text-to-image.TensorFlow" width="200"></td></tr>
<tr><td><a href="https://github.com/zsdonghao/text-to-image"></a></td>
<td>Generative Adversarial Text to Image Synthesis in TensorFlow and TensorLayer
</td>
<td><img src="https://www.mpi-inf.mpg.de/fileadmin/_processed_/csm_g12_99715581c0.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/facebookresearch/fairseq">fairseq.Torch</a></td>
<td>Facebook AI Research Sequence-to-Sequence Toolkit
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/GKalliatakis/Delving-deep-into-GANs">Delving-deep-into-GANs</a></td>
<td>A curated, quasi-exhaustive list of state-of-the-art publications and resources about Generative Adversarial Networks (GANs) and their applications.
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/andrewliao11/dni.pytorch">dni.pytorch</a></td>
<td>Implement Decoupled Neural Interfaces using Synthetic Gradients in Pytorch
</td>
<td><img src="https://image.slidesharecdn.com/01920160907decoupledneuralinterfacesusingsyntheticgradients-161011140057/95/019-20160907-decoupled-neural-interfaces-using-synthetic-gradients-4-638.jpg?cb=1476194840" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/julian121266/RecurrentHighwayNetworks">RecurrentHighwayNetworks.Torch</a></td>
<td>Recurrent Highway Networks - Implementations for Tensorflow, Torch7, Theano and Brainstorm
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/akolishchak/doom-net-pytorch" doom-net.pytorch=""></a></td>
<td>Reinforcement learning models in ViZDoom environment
</td>
<td><img src="" alt="http://blog.bsu.me/assets/images/per/gameplay_snapshots.jpg" width="200"></td></tr>
<tr><td><a href="https://github.com/transedward/pytorch-dqn">dqn.Pytorch</a></td>
<td>Deep Q-Learning Network in pytorch
</td>
<td><img src="" alt="http://shws.cc.oita-u.ac.jp/shibata/KissSystem.png" width="200"></td></tr>
<tr><td><a href="https://github.com/jingweiz/pytorch-rl">rl.pytorch</a></td>
<td>Deep Reinforcement Learning with pytorch & visdom
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/atgambardella/pytorch-es">es.PyTorch</a></td>
<td>Evolution Strategies in PyTorch
</td>
<td><img src="https://4.bp.blogspot.com/-tm1px1Jz2G4/WNQ9OlLVvUI/AAAAAAAATKE/EOo699MnBU0ThBbAaReLNMcIXoj3YkJaQCLcB/s1600/es.jpg" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/ikostrikov/pytorch-a3c">a3c.PyTorch</a></td>
<td>PyTorch implementation of Asynchronous Advantage Actor Critic (A3C) from "Asynchronous Methods for Deep Reinforcement Learning".
</td>
<td><img src="https://camo.githubusercontent.com/126a8ecce999b884451c9ca090ba4cfb3783195e/687474703a2f2f672e7265636f726469742e636f2f4265697143396c3730422e676966" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/msracver/Deformable-ConvNets">Deformable-ConvNets</a></td>
<td>A third-party improvement of Deformable R-FCN + Soft NMS, best single-model performance on COCO detection
</td>
<td><img src="https://github.com/msracver/Deformable-ConvNets/raw/master/demo/deformable_conv_demo1.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/1adrianb/2D-and-3D-face-alignment">2D-and-3D-face-alignment.Torch</a></td>
<td>This repository implements a demo of the networks described in "How far are we from solving the 2D & 3D Face Alignment problem? (and a dataset of 230,000 3D facial landmarks)" paper.
</td>
<td><img src="https://i.ytimg.com/vi/8FdSHl4oNIM/maxresdefault.jpg" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/ruotianluo/self-critical.pytorch">self-critical.pytorch</a></td>
<td>Unofficial pytorch implementation for Self-critical Sequence Training for Image Captioning
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/erfannoury/order-embedding-disc" order-embedding-disc.theano<="" a=""></a></td>
<td>Implementation of caption-image retrieval from the paper "Order-Embeddings of Images and Language"
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/lmb-freiburg/hand3d">hand3d.Tensorflow</a></td>
<td>Network estimating 3D Handpose from single color images
</td>
<td><img src="https://github.com/lmb-freiburg/hand3d/raw/master/teaser.png" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/maciejkula/glove-python">glove.python</a></td>
<td>Toy Python implementation of http://www-nlp.stanford.edu/projects/glove/
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/mingdachen/gated-attention-reader">gated-attention-reader.PyTorch</a></td>
<td>Tensorflow/Pytorch implementation of Gated Attention Reader
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/jeffdonahue/bigan">bigan.Theano</a></td>
<td>code for "Adversarial Feature Learning"
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/chapternewscu/image-captioning-with-semantic-attention">image-captioning-with-semantic-attention.Torch</a></td>
<td>
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/eladhoffer/TripletNet">TripletNet.Torch</a></td>
<td>Deep metric learning using Triplet network
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/charlesq34/3dcnn.torch">3dcnn.torch</a></td>
<td>Volumetric CNN for feature extraction and object classification on 3D data.
</td>
<td><img src="http://graphics.stanford.edu/projects/3dcnn/teaser.jpg" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/nutszebra/multimodal_word2vec">multimodal_word2vec.python</a></td>
<td>implementation of Combining Language and Vision with a Multimodal Skip-gram Model
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/rotmanmi/glove.torch">glove.torch</a></td>
<td>glove wrapper for torch7
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/Cadene/skip-thoughts.torch">skip-thoughts.torch</a></td>
<td>Porting of Skip-Thoughts pretrained models from Theano to PyTorch & Torch7
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/iamalbert/pytorch-wordemb">wordemb.PyTorch</a></td>
<td>Load pretrained word embeddings (word2vec, glove format) into torch.FloatTensor for PyTorch
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/ganeshjawahar/dl4nlp-made-easy">dl4nlp-made-easy.Torch7</a></td>
<td>Toy codes to kick-start deep learning for NLP !
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/vivanov879/word2vec">word2vec.Torch7</a></td>
<td>Torch implementation of word2vec and sentiment analysis
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/batra-mlp-lab/visdial">visdial.Torch</a></td>
<td>Visual Dialog (CVPR 2017) code in Torch
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/Cloud-CV/visual-chatbot">visual-chatbot.Python</a></td>
<td>Visual Chatbot
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/jcjohnson/densecap"> densecap.Torch</a></td>
<td>Dense image captioning in Torch
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/zhegan27/SCN_for_video_captioning">SCN_for_video_captioning.Theano</a></td>
<td>Using Semantic Compositional Networks for Video Captioning
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/zhegan27/Semantic_Compositional_Nets">Semantic_Compositional_Nets.Theano</a></td>
<td>The Theano code for the CVPR 2017 paper "Semantic Compositional Networks for Visual Captioning"
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/CharlesShang/FastMaskRCNN">FastMaskRCNN.Tensorflow</a></td>
<td>Mask RCNN in TensorFlow
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/richardassar/SampleRNN_torch">SampleRNN.torch</a></td>
<td>Torch implementation of SampleRNN: An Unconditional End-to-End Neural Audio Generation Model
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/felixgwu/mask_rcnn_pytorch">mask_rcnn.pytorch</a></td>
<td>Mask RCNN in PyTorch
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix">CycleGAN-and-pix2pix.pytorch</a></td>
<td>Image-to-image translation in PyTorch (e.g. horse2zebra, edges2cats, and more)
</td>
<td><img src="https://github.com/harvardnlp/seq2seq-attn" alt="" width="200"></td></tr>
<tr><td><a href="">seq2seq-attn.Torch</a></td>
<td>Sequence-to-sequence model with LSTM encoder/decoders and attention
</td>
<td><img src="https://github.com/JonghwanMun/TextguidedATT" alt="" width="200"></td></tr>
<tr><td><a href="">TextguidedATT.Torch</a></td>
<td>The implementation of Text-guided Attention Model for Image Captioning
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/phillipi/pix2pix">pix2pix.Torch</a></td>
<td>Image-to-image translation with conditional adversarial nets
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/JamesChuanggg/pytorch-REINFORCE">REINFORCE.pytorch</a></td>
<td>PyTorch Implementation of REINFORCE for both discrete & continuous control
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/ruotianluo/neuraltalk2.pytorch">neuraltalk2.pytorch</a></td>
<td>image captioning model in pytorch(finetunable cnn in branch "with_finetune")
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/EderSantana/gumbel">gumbel.Keras</a></td>
<td>Gumbel-Softmax Variational Autoencoder with Keras
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/Hanock/generating_images_part_by_part">generating_images_part_by_part.Python</a></td>
<td>Implementation of the paper "Generating images part by part with composite generative adversarial networks"
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/igul222/improved_wgan_training">improved_wgan_training.Pytorch</a></td>
<td>Code for reproducing experiments in "Improved Training of Wasserstein GANs"
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/JonathanTaws/VAE-im2txt">VAE-im2txt.Torch</a></td>
<td>
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/wiseodd/generative-models">generative-models.PyTorch</a></td>
<td>Collection of generative models, e.g. GAN, VAE in Pytorch and Tensorflow.
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/xcyan/eccv16_attr2img">eccv16_attr2img.Torch</a></td>
<td>Torch implementing of ECCV'16 paper: Attribute2Image
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/c0nn3r/pytorch_highway_networks">highway_networks.Pytorch</a></td>
<td>Highway networks implemented in PyTorch.
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/carpedm20/BEGAN-pytorch">BEGAN.pytorch</a></td>
<td>
</td>
<td><img src="" alt="" width="200"></td></tr>
<tr><td><a href="https://github.com/junyanz/CycleGAN">CycleGAN.Torch</a></td>
<td>Software that can generate photos from paintings, turn horses into zebras, perform style transfer, and more.
</td>
<td><img src="" alt="" width="200"></td></tr>
</table>
<h1>Datasets</h1>
<table border="3" style="width:100%">
<caption><em><center>Datasets</center></em></caption>
<th>Name</th><th>Descriptions</th><th>Illustration</th>
<tr>
<td>
<a href="http://visualgenome.org/api/v0/api_readmeV">Visual_Genome</a>
</td>
<td>
Visual Genome is a dataset, a knowledge base, an ongoing effort to connect structured image concepts to language.
</td>
<td>
<img src="http://visualgenome.org/static/images/front-page/interconnected_images.png" alt="" width="200">
</td></tr>
<tr>
<td>
<a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/vision-and-language/visual-turing-challenge/">DAQUAR</a></td>
<td>
It is a dataset for question answering (natural language sentences) based on real world images( which include indoor scenes).
</td>
<td>
<img src="https://camo.githubusercontent.com/53c28e13bd645acbf49c9e71e82a36202d1981bc/687474703a2f2f7333322e706f7374696d672e6f72672f77636a6c7a7a7532742f53637265656e5f53686f745f323031365f30355f30385f61745f325f34325f30375f504d2e706e67" alt="" width="617">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/nightrome/cocostuff">COCO-Stuff_10K</a></td>
<td>
COCO-Stuff augments the popular COCO dataset with pixel-level stuff annotations. These annotations can be used for scene understanding tasks like semantic segmentation, object detection and image captioning.
</td>
<td>
<img src="https://camo.githubusercontent.com/d10b897e15344334e449104a824aff6c29125dc2/687474703a2f2f63616c76696e2e696e662e65642e61632e756b2f77702d636f6e74656e742f75706c6f6164732f646174612f636f636f7374756666646174617365742f636f636f73747566662d6578616d706c65732e706e67" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="http://mscoco.org/dataset/#captions-challenge2015">COCO_caption_dataset</a></td>
<td>
The Microsoft Common Objects in Context (MSCOCO) dataset  contains 91 common object categories with 82 of them having more than 5,000 labeled instances. In total the dataset has 2,500,000 labeled instances in 328,000 images. <a href="http://mscoco.org/dataset/#captions-leaderboard&gt;Leaderboard&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;img src=" http:="" mscoco.cloudapp.net="" static="" images="" captions-challenge2015.jpg"="" alt="" width="200">
</a></td>
</tr>
<tr>
<td>
<a href="http://mscoco.org/dataset/#detections-challenge2016">COCO_detection</a></td>
<td>
The COCO train, validation, and test sets, containing more than 200,000 images and 80 object categories, are available on the download page. All object instances are annotated with a detailed segmentation mask. Annotations on the training and validation sets (with over 500,000 object instances segmented) are publicly available.
</td>
<td>
<img src="http://mscoco.org/static/images/detections-challenge2015.png" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="http://mscoco.org/dataset/#keypoints-challenge2016">COCO_Keypoint</a></td>
<td>
The COCO 2016 Keypoint Challenge requires localization of person keypoints in challenging, uncontrolled conditions. The keypoint challenge involves simultaneously detecting people and localizing their keypoints (person locations are not given at test time). For full details of this task please see the keypoint evaluation page.
</td>
<td>
<img src="http://mscoco.org/static/images/keypoints-challenge2016.png" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/googlecreativelab/quickdraw-dataset">Quick_Draw_Dataset</a></td>
<td>
The Quick Draw Dataset is a collection of 50 million drawings across 345 categories, contributed by players of the game Quick, Draw!. The drawings were captured as timestamped vectors, tagged with metadata including what the player was asked to draw and in which country the player was located. You can browse the recognized drawings on quickdraw.withgoogle.com/data
</td>
<td>
<img src="https://github.com/googlecreativelab/quickdraw-dataset/raw/master/preview.jpg" alt="" width="200">
</td>
</tr>
<tr>
<td>
<a href="https://github.com/hardmaru/sketch-rnn-datasets">Simple_Vector_Drawing_Datasets</a></td>
<td>
In each dataset, each sample is stored as list of coordinate offsets: ∆x, ∆y, and a binary value representing whether the pen is lifted away from the paper. This format, we refer to as stroke-3, is described in Alex Graves' paper on sequence generation. Note that the data format described in the sketch-rnn paper has 5 elements (stroke-5 format), and we need to perform live conversion of the data to the stroke-5 format during mini-batch construction.
</td>
<td>
<img src="https://camo.githubusercontent.com/16c83b69c0fc6ad66533d64de0b1dff75b3fe99a/68747470733a2f2f63646e2e7261776769742e636f6d2f686172646d6172752f736b657463682d726e6e2f6d61737465722f6578616d706c652f646174615f666f726d61742e737667" alt="" width="200">
</td>
</tr>
<tr>
<td>  <a href=""></a></td>
<td>
</td>
<td>
<img src="" alt="" width="200">
</td>
</tr>
</table></body>
</html>
