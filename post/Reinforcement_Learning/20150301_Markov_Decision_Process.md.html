<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="markov-decision-process">Markov Decision Process</h1>
<p>在监督学习中，给定了训练集以及对应的标签 <span class="math display">\[\pmb{y}\]</span>，算法要做的就是令预测输出尽可能地接近<span class="math display">\[\pmb{y}\]</span>。在这种情况下，算法运行过程中对应的是有正确答案的。但有些时候，涉及到机器人的操控的时候，很多事情可能并不是supervised和unsupervised learning能够解决的。在对问题作出决策或者控制时，我们很难提供一个确切的正确答案。比如在四足机器人行走编程中，我们在一开始的时候对才去怎样的行动是“正确的”根本没有概念，我们只知道这是一个足部调节的过程，因此在这里，监督学习算法并不适用。还有比如说andrew ng之前一直提到的自动控制直升飞机，另一个例子就是下棋，有可能<strong>很久之前的一步棋就埋下了后面失败的伏笔</strong>，而机器很难去判断一步棋的好坏。这就是增强学习需要解决的问题。</p>
<p>在自增强学习框架中，算法的核心是<strong>奖励函数</strong>，区分出学习过程中哪些行为是“好”的，哪些行为是“坏”的。对于四足机器人行走问题，当机器人能够向前进时，我们给予积极奖励；当机器人向后退或者跌倒时候，我们给予消极惩罚。这样，有了奖励惩罚机制，在多次训练后，机器人会越走越好。自增强学习成功应用在了无人机飞行、机器人行走、市场决策选择等问题上。为了正式地定义自增强学习，我们先来看马尔科夫决策过程（Markov Decision Process,简写MDP）。</p>
<p>参考： 1. <a href="http://blog.csdn.net/dark_scope/article/details/8252969">机器学习 cs229学习笔记6(增强学习 reinforcement learning,MDP)</a> 2. <a href="http://lanbing510.info/2015/11/17/Master-Reinforcement-Learning-MDP.html">增强学习与马尔科夫决策过程</a> 3. <a href="https://www.zybuluo.com/frank-shaw/note/145153">网易机器学习课16--增强学习与马尔科夫决策过程（MDP</a> 4. <a href="http://blog.csdn.net/songrotek/article/details/50580904">Deep Reinforcement Learning 基础知识（DQN方面）</a> 5. <a href="http://open.163.com/movie/2008/1/2/N/M6SGF6VB4_M6SGKSC2N.html">机器学习公开课-视频-马尔科夫决策过程</a> 6. <a href="">人工智能：一种现代方法</a> 7. <a href="http://www.infoq.com/cn/articles/atari-reinforcement-learning">看DeepMind如何用Reinforcement learning玩游戏</a></p>
<h2 id="马尔可夫模型的几类子模型">马尔可夫模型的几类子模型</h2>
<p>大家应该还记得<a href="https://zh.wikipedia.org/wiki/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE">马尔科夫链(Markov Chain)</a>，了解机器学习的也都知道<a href="https://www.zhihu.com/question/20962240">隐马尔可夫模型(Hidden Markov Model，HMM)</a>。它们具有的一个共同性质就是<strong>马尔可夫性(无后效性)</strong>，也就是<strong>指系统的下个状态只与当前状态信息有关，而与更早之前的状态无关</strong>。MDP也具有马尔可夫性，与上面不同的是MDP考虑了动作，即<strong>系统下个状态</strong><span class="math display">\[\mathbf{s}_{t+1}\]</span>不仅和<strong>当前的状态有关</strong><span class="math display">\[\mathbf{s}_t\]</span>，也和<strong>当前采取的动作有关</strong>。还是举下棋的例子，当我们在某个局面（状态<span class="math display">\[\mathbf{s}\]</span>）走了一步(动作<span class="math display">\[\mathbf{a}\]</span>)，这时对手的选择（导致下个状态<span class="math display">\[\mathbf{s}_{t+1}\]</span>）我们是不能确定的，但是他的选择只和<span class="math display">\[\mathbf{s}\]</span>和<span class="math display">\[\mathbf{a}\]</span>有关，而不用考虑更早之前的状态和动作，即<span class="math display">\[\mathbf{s}_{t+1}\]</span>是根据<span class="math display">\[\mathbf{s}\]</span>和<span class="math display">\[\mathbf{a}\]</span>随机生成的。</p>
<p>我们用一个二维表格表示一下，各种马尔可夫子模型的关系就很清楚了：</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left">不考虑动作</th>
<th align="left">考虑动作</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">状态完全可见</td>
<td align="left">马尔科夫链(MC)</td>
<td align="left">马尔可夫决策过程(MDP)</td>
</tr>
<tr class="even">
<td align="left">状态不完全可见</td>
<td align="left">隐马尔可夫模型(HMM)</td>
<td align="left">不完全可观察马尔可夫决策过程(POMDP)</td>
</tr>
</tbody>
</table>
<h2 id="马尔可夫决策过程">马尔可夫决策过程</h2>
<p>一个马尔科夫决策过程（Markov Decision Processes, MDP）有一个五个关键元素组成<span class="math display">\[\{S,A,\{P_{sa}\},\gamma,R\}\]</span>（当然有一些书籍上用四元组表示）,其中： - <span class="math display">\[S\]</span>：表示 <strong>状态集合</strong>，例如上例中4x3的每个环境<span class="math display">\[\{(i,j)|i=1,2,3,4,j=1,2,3\}\]</span>。自动直升机系统中的所有可能的位置、方向等。 - <span class="math display">\[A\]</span>：表示<strong>一组动作集合</strong>，例如上例中的（上、下、左、右），自动直升机系统中的让飞机向前，向后等。有<span class="math display">\[a \in A\]</span>，<span class="math display">\[a_i\]</span>表示第i步的动作。 - <span class="math display">\[P_{sa}\]</span>：<strong>状态转移概率/状态转换分布</strong>，表示在当前<span class="math display">\[s_t \in S\]</span>状态下，通过执行动作<span class="math display">\[a_t \in A\]</span>后转移到下一个状态<span class="math display">\[s_{t+1}\]</span>(下一个状态出现的概率依赖于前一个状态以及前状态所采取的动作)的概率分布, 且有<span class="math display">\[\sum_{s&#39;}P_{sa}(s&#39;)=1,P_{sa}(s&#39;)\ge 0\]</span>。例如上例中，<span class="math display">\[P_{(1,1)\text{UP}}\]</span>表示Ａgent在状态(1,1)执行向上的动作后转移到状态(1,2)，(2,1)的概率分布。 - <span class="math display">\[\gamma \in [0,1)\]</span>：<strong><em>阻尼系数(discount factor)</em></strong>，表示的是随着时间的推移回报率的折扣。 - <span class="math display">\[R:S \times A \mapsto \mathbb{R}\]</span>：*<strong>回报函数</strong>，R为实数,有时回报函数是只与<span class="math display">\[S\]</span>有关的函数（更多时候与状态S与行为A都有关），<span class="math display">\[R\]</span>重写为<span class="math display">\[R:S \mapsto \mathbb{R}\]</span>。</p>
<h2 id="mdp的动态过程">MDP的动态过程</h2>
<p>Ａgent在状态<span class="math display">\[s_{0}\]</span>选择某个动作<span class="math display">\[a_{0} \in A\]</span>，智能体根据概率<span class="math display">\[P_{s_{0}a_{0}}\]</span>转移到状态<span class="math display">\[s_{1}\]</span>，然后执行动作<span class="math display">\[a_{1}\]</span>，...如此下去我们可以得到这样的过程： <span class="math display">\[
s_{0} \stackrel{a_{0}}{\longrightarrow} s_{1} \stackrel{a_{1}}{\longrightarrow} s_{2} \stackrel{a_{2}}{\longrightarrow} s_{3} \stackrel{a_{3}}{\longrightarrow} ···
\tag{1}
\]</span></p>
<p>经过上面的转移路径，我们可以得到相应状态序列<span class="math display">\[s_0,s_1,...\]</span>的回报函数和如下：</p>
<p><span class="math display">\[
R(s_{0},a_{0})+\gamma R(s_{1},a_{1})+\gamma^{2}R(s_{2},a_{2})+··· \tag{2}
\]</span> 如果回报函数<span class="math display">\[R\]</span>只与<span class="math display">\[S\]</span>有关，我们上式可重新写作: <span class="math display">\[
R(s_{0})+\gamma R(s_{1})+\gamma^{2}R(s_{2})+··· \tag{3}
\]</span></p>
<p>我们的目标是选择一组最佳的动作，使得<strong>全部的回报加权和期望最大</strong>：</p>
<p><span class="math display">\[
\text{Reward}=\max \mathbb{E}[R(s_{0})+\gamma R(s_{1})+\gamma^{2}R(s_{2})+···] \tag{4}
\]</span></p>
<p>从上式可以发现，在<span class="math display">\[t\]</span>时刻的回报值是被打了<span class="math display">\[\gamma^{t}, \gamma\in [0,1)\]</span>倍折扣的，注意到<span class="math display">\[\gamma &lt; 1\]</span>，则越靠后的状态对回报和影响越小。因此采取的行动策略是：积极的奖励尽量在前面（越快出现越好），消极惩罚尽量在后面（越晚出现越好）.</p>
<p>下图是上述内容的一个直观示意, 下一部分将对上述过程进行进一步数学表示，以方便求解。</p>
<div class="figure">
<img src="http://lanbing510.info/public/img/posts/reinforcement-learning02.png" />

</div>
<h2 id="policy的数学表示">Policy的数学表示</h2>
<p>首先我们来定义策略，一个策略<span class="math display">\[\pi\]</span>就是一个从状态到动作的映射函数<span class="math display">\[\pi:S \mapsto A\]</span>。亦即策略<span class="math display">\[a=\pi(s)\]</span>告诉我们在状态<span class="math display">\[s\]</span>下该执行的动作是什么。为了知道如何得出最佳策略的算法，我们还需要定义一些变量：为每一个策略<span class="math display">\[\pi\]</span>我们定义一个相应的值函数（Value Function） <span class="math display">\[
V^{\pi}(s)=\mathbb{E}[R(s_{0})+\gamma R(s_{1})+\gamma^{2}R(s_{2})+···|s_{0}=s,\pi] \tag{5}
\]</span> 由<span class="math display">\[V^{\pi}(s)\]</span>的定义可以看出，<span class="math display">\[V^{\pi}(s)\]</span>就是给定初始状态<span class="math display">\[s_{0}\]</span>,依据策略<span class="math display">\[\pi\]</span>采取行动后的累积折扣回报期望（Expected Sum Of Discounted Rewards）。上式还可以写成：</p>
<p><span class="math display">\[
V^{\pi}(s)=\mathbb{E}[R(s_{0})+\gamma （R(s_{1})+\gamma R(s_{2})+···）|s_{0}=s,\pi] \tag{6}
\]</span> 里面的一项其实就是：</p>
<p><span class="math display">\[
V^{\pi}(s_1)=\mathbb{E}[R(s_{1})+\gamma R(s_{2})+\gamma^{2}R(s_{3})+···|s_{1}=s,\pi] \tag{7}
\]</span> 也就是说，值函数<span class="math display">\[V^{\pi}\]</span>可以表示成：</p>
<p><span class="math display">\[
V^{\pi}(s)=R(s)+\gamma \sum_{s&#39; \in S}P_{s\pi(s)}(s)V^{\pi}(s&#39;) \tag{8}
\]</span> 上式就是贝尔曼公式(Bellman equation). 其中，<span class="math display">\[R(s)\]</span>项可视为我们采取状态<span class="math display">\[s\]</span>作为初始状态的立即回报/即时回报<span class="math display">\[R(s)\]</span>，而第二项可视为将来的回报(未来累积折扣回报期望)<span class="math display">\[\mathbb{E}_{s \sim P_{s\pi(s)}}[V^{\pi}(s)]\]</span>。</p>
<p>利用贝尔曼等式能够有效的解出<span class="math display">\[V^{\pi}\]</span>（给定的策略<span class="math display">\[\pi\]</span>的回报值）。尤其，对于一个有限状态的MDP（<span class="math display">\[|S| &lt; \infty\]</span>），对每一个状态<span class="math display">\[s\]</span>我们都能写出这样的等式<span class="math display">\[V^{\pi}(s)\]</span>，求解变为了解一个<span class="math display">\[|S|\]</span>个方程，<span class="math display">\[|S|\]</span>个未知数的线性方程组。</p>
<p>当然，我们求解<span class="math display">\[V^{\pi}\]</span>的目的是为找到一个当前状态<span class="math display">\[s\]</span>下最优的行动策略<span class="math display">\[\pi\]</span>服务的（最优的策略下得到最优的值函数）。定义最优的值函数为：</p>
<p><span class="math display">\[
V^{*}(s)=\max_{\pi}V^{\pi}(s) \tag{9}
\]</span></p>
<p>从定义可以看出，<span class="math display">\[V^{*}(s)\]</span>其实就是从状态<span class="math display">\[s\]</span>出发，依据任意一个 <span class="math display">\[\pi\]</span> 所能够获得的最大的总回报函数的期望。注意，这里不再限制需要依据某一个固定的策略<span class="math display">\[\pi\]</span>。关于最优值函数的贝尔曼方程可以写成:</p>
<p><span class="math display">\[
V^{*}(s)=R(s)+\max_{a \in A}\gamma\sum_{s&#39; \in S}P_{sa}(s&#39;)V^{*}(s&#39;) \tag{10}
\]</span> 第一项可以视为采取状态<span class="math display">\[s\]</span>作为最初状态的立即回报，第二项即使在状态s上采取行动<span class="math inline">\(a\)</span>时获得的将来回报（由于<span class="math inline">\(a\)</span>有多重选择，选择可以令总回报期望最大的那个<span class="math display">\[a\]</span>）。同样需要注意的是，这里不再限制需要依据某一个固定的<span class="math inline">\(\pi\)</span>作选择</span>。同样的，我们可以通过联立方程求解<span class="math display">\[V^{*}\]</span></p>
<p><span class="math display">\[
V^{*}(s)=R(s)+\max_{a \in A}\gamma\sum_{s \in S}P_{sa}(s)V^{*}(s)
\]</span> 也可表示为增强学习中的Q函数形式：</p>
<p><span class="math display">\[
\begin{cases}V^{*}(s)=\max_{a}Q(s,a)\\
Q(s,a) \equiv R(S)+\gamma \sum_{s&#39; \in S}P_{sa}(s&#39;)V^{*}(s&#39;)\end{cases}\tag{11}
\]</span> 其中<span class="math display">\[Q(s,a)\]</span>表示在<span class="math display">\[s\]</span>状态下执行动作<span class="math display">\[a\]</span>作为第一个动作时的最大累计折扣回报。</p>
<p>对应最优值函数的最优的策略<span class="math display">\[\pi\]</span>为：</p>
<p><span class="math display">\[
\pi^{*}(s)=arg\max_{a \in A}\sum_{s&#39; \in S}P_{sa}(s&#39;)V^{*}(s&#39;)\tag{12}
\]</span> 这是<strong>最优值函数的贝尔曼方程中的第二项</strong>，它回答的问题是：</p>
<p>现在我们有了优化目标的数学表达（最优值函数，最优策略），下一部分讨论两种求解方法。在当前状态<span class="math display">\[s\]</span>下，才采取怎样的行动才是最优的。当然，想求解该表达式，必须先求得<span class="math display">\[V^*(s)\]</span>的具体表示。但是公式（10）并没有给我们一个漂亮的算法去计算它（相比于之前固定的策略<span class="math display">\[\pi\]</span>然后求解<span class="math display">\[n\]</span>个线性方程的解法，这里由于策略<span class="math display">\[\pi\]</span>不固定，可能有的<span class="math display">\[\pi\]</span>组合过多）。(需要注意的是，<span class="math display">\[\pi^{*}\]</span>有一个有趣的特性，即<span class="math display">\[\pi^{*}\]</span>是针对的是所有的状态<span class="math display">\[s\]</span>的，确定了每一个状态<span class="math display">\[s\]</span>的下一个动作<span class="math display">\[a\]</span>，不管初始状态是哪一个状态，通过策略<span class="math display">\[\pi^{*}\]</span>都会取得最大回报。)</p>
<p>这并不意味着我们无法求解<span class="math display">\[\pi\]</span>了，毕竟我们的目标就是找到最优的策略<span class="math display">\[\pi\]</span>来指导我们的行动。求解思路如下：先求解出<span class="math display">\[V^*\]</span>，然后由<span class="math display">\[\pi\]</span>的定义求解出<span class="math display">\[\pi\]</span>，下面有两种算法可以帮助我们实现目标（针对有限状态、有限动作的MDP）：</p>
<h2 id="值迭代方法">值迭代方法</h2>
<p>算法步骤： 1. 对于每一个状态<span class="math display">\[s\]</span>，初始化值函数<span class="math display">\[V(s)=0\]</span> 2. 重复以下步骤直至收敛{ 对于每一个状态<span class="math display">\[s\]</span>， 更新<span class="math display">\[V(s):=R(s)+\max_{a \in A}\gamma\sum_{s’}P_{sa}(s&#39;)V(s‘’)\]</span> }</p>
<p>算法步骤简单，思想也简单但有效：重复贝尔曼公式（10），更新<span class="math display">\[V(s)\]</span>.经过验证，该算法最终能够使得<span class="math display">\[V(s)\rightarrow V^{*}(s)\]</span></p>
<p>值迭代方法里面的内循环又有两种策略：<strong>同步迭代，异步迭代</strong>。 1. 同步迭代就是得到<span class="math display">\[V(s)\]</span>后不立即更新，等所有的状态<span class="math display">\[s\]</span>的<span class="math display">\[V(s)\]</span>都完成计算后统一更新。 2. 异步迭代就是对每个状态<span class="math display">\[s\]</span>得到新的<span class="math display">\[V(s)\]</span>后立即更新。两种都会使得<span class="math display">\[V(s)\]</span>收敛于<span class="math display">\[V^{*}(s)\]</span>。求得最优的<span class="math display">\[V^{*}(s)\]</span>后，可使用公式<span class="math display">\[\pi^{*}(s)=\underset{a \in A}{\arg\max}\sum_{s \in S}P_{sa}(s)V^{*}(s)\]</span>来求出相应的最优策略<span class="math display">\[\pi^{*}\]</span>。</p>
<h2 id="策略迭代方法">策略迭代方法</h2>
<p>于值迭代方法不同，<strong>策略迭代法</strong>之间关注<span class="math display">\[\pi\]</span>，使<span class="math display">\[\pi\]</span>收敛到<span class="math display">\[\pi^{*}\]</span></p>
<p><strong>算法步骤</strong>： 1. 随机初始化策略<span class="math display">\[\pi\]</span> 2. 重复以下步骤直至收敛{ 2.1 令<span class="math display">\[V:=V^{\pi}\]</span>, 其中<span class="math display">\[V^{\pi}\]</span>通过求解贝尔曼方程（多个方程联立）得到 2.2 对每一个状态<span class="math inline">\(s\)</span>,对<span class="math display">\[\pi(s)\]</span>做更新 <span class="math display">\[\pi(s):=\underset{\arg\max}{a \in A}\sum_{s}P_{sa}(s)V(s)\]</span> }</p>
<p>其中2.1步即为上述对于一个给定策略<span class="math display">\[\pi\]</span>利用贝尔曼等式求解<span class="math display">\[V^{\pi}\]</span>的过程（求解<span class="math display">\[|S|\]</span>个方程，<span class="math display">\[|S|\]</span>个未知数的线性方程组）。2.2是根据2.1步的结果，挑选出当前状态<span class="math inline">\(s\)</span>下最优的动作<span class="math display">\[a\]</span>来更新<span class="math display">\[\pi(s)\]</span>。</p>
<h2 id="两者比较">两者比较</h2>
<p>算法步骤依然简洁。在比较两个算法的计算成本时候，因为policy iteration 需要求解n个方程联立的问题（n为状态数），所以可以这样选：在状态数较少的时候，选policy iteration ，反之选value iteration。</p>
<p>下面来着手解决一个可能的情况：假设MDP中的五元组中我不知道<span class="math display">\[P_{sa}\]</span>,如何处理呢？</p>
<p>这种情况是常见的：当试图控制直升机的时候，你一开始并不知道直升机从初始状态过渡到什么状态或者在某一状态下采取的行动是什么，因为直升机整个系统很复杂，往往不清楚会达什么状态。</p>
<p>这个时候，需要做的事情就是尝试从数据中估计出状态转移分布<span class="math display">\[P_{sa}\]</span>.</p>
<h2 id="mdp中的参数估计">MDP中的参数估计</h2>
<p>到目前为止，我们讨论的MDP和MDP求解算法都是在已知状态转移概率<span class="math display">\[P_{sa}\]</span>和回报函数<span class="math display">\[R(s)\]</span>的。在许多实际问题中，状态转移概率和回报函数不能显式的得到，本部分讲如何从数据中估计这些参数（通常<span class="math display">\[S,A,\gamma\]</span>是已知的）。</p>
<p>假设我们已知很多条状态转移路径如下：</p>
<p><span class="math display">\[
\begin{split}
s_{0}^{(1)} \stackrel{a_{0}^{(1)}}{\longrightarrow} s_{1}^{(1)} \stackrel{a_{1}^{(1)}}{\longrightarrow} s_{2}^{(1)} \stackrel{a_{2}^{(1)}}{\longrightarrow} s_{3}^{(1)} \stackrel{a_{3}^{(1)}}{\longrightarrow} ···\\
s_{0}^{(2)} \stackrel{a_{0}^{(2)}}{\longrightarrow} s_{1}^{(2)} \stackrel{a_{1}^{(2)}}{\longrightarrow} s_{2}^{(2)} \stackrel{a_{2}^{(2)}}{\longrightarrow} s_{3}^{(2)} \stackrel{a_{3}^{(2)}}{\longrightarrow} ··· \\
···
\end{split}
\]</span></p>
<p>其中<span class="math display">\[s_{i}^{(j)}\]</span>是<span class="math display">\[i\]</span>时刻第<span class="math display">\[j\]</span>条转移路径对应的状态，<span class="math display">\[a_{i}^{j}\]</span>是<span class="math display">\[s_{i}^{j}\]</span>状态要执行的动作。每条转移路径中的状态数都是有限的，在实际操作中每个转移路径要么进入终结状态，要不达到规定的步数后终结。</p>
<p>当我们获得了很多类似上面的转移路径后（样本），我们可以用最大似然估计来估计状态转移概率。</p>
<p><span class="math display">\[
P_{sa}(s&#39;)=\frac{在状态采取行动时转变到状态的次数}{在状态采取行动时转变转变状态的总次数}，如果\frac{0}{0},则令P_{sa}(s&#39;)=\frac{1}{|s|}
\]</span> 对于未知的回报函数，我们令<span class="math inline">\(R(s)\)</span>为在状态<span class="math inline">\(s\)</span>下观察到的回报均值。得到状态转移概率和回报函数的估值后，就简化为了前面部分讲述的问题，用第三部分将的值迭代或者策略迭代方法即可解决。例如我们将值迭代和参数估计结合到一块：</p>
<p>算法流程如下： 1. 随机初始化话一个<span class="math display">\[S\]</span>到<span class="math display">\[A\]</span>的映射<span class="math display">\[\pi\]</span> 2. 循环直至收敛{ 2.1 在MDP中执行策略<span class="math display">\[\pi\]</span>一定次数 2.2 通过2.1得到的样本估计<span class="math display">\[P_{sa}\]</span>（和<span class="math display">\[R\]</span>，需要的话） 2.3 使用上一节提到的值迭代方法和估计得到的参数来更新<span class="math display">\[V\]</span> 2.4 对于得到的<span class="math inline">\(V\)</span>更新得到更优的策略<span class="math display">\[\pi\]</span> }</p>
<p>其中2.3步，是一个循环迭代的过程。上一节中我们通过将<span class="math display">\[V\]</span>初始化为0然后进行迭代，当嵌套上述过程中后，如果每次都将<span class="math display">\[V\]</span>初始化为0然后迭代更新，速度回很慢。一个加速的方法是将<span class="math display">\[V\]</span>初始化我上次大循环中得到的<span class="math display">\[V\]</span>。</p>
<h2 id="值函数与q函数计算的例子">值函数与Q函数计算的例子</h2>
<p>接下来我们实际计算一下，如图所示是一个格子世界，我们假设agent从左下角的start点出发，右上角为目标位置，称为吸收状态(Absorbing state)，对于进入吸收态的动作，我们给予立即回报100，对其他动作则给予0回报，折合因子<span class="math display">\[\gamma\]</span>的值我们选择0.9。 为了方便描述，记第<span class="math display">\[i\]</span>行，第<span class="math display">\[j\]</span>列的状态为<span class="math display">\[s_{ij}\]</span>, 在每个状态，有四种上下左右四种可选的动作，分别记为<span class="math display">\[a_u,a_d,a_l,a_r\]</span>。（up，down，left，right首字母），并认为状态按动作a选择的方向转移的概率为1。</p>
<p><img src="http://images.cnitblog.com/blog/489049/201401/132330225338.png" /> 1. 由于状态转移概率是1，每组<span class="math display">\[(s,a)\]</span>对应了唯一的<span class="math display">\[s&#39;\]</span>。回报函数<span class="math display">\[R(s&#39;|s,a)\]</span>可以简记为<span class="math display">\[R(s,a)\]</span> 如下所示，每个格子代表一个状态<span class="math display">\[s\]</span>，箭头则代表动作<span class="math display">\[a\]</span>，旁边的数字代表立即回报，可以看到只有进入目标位置的动作获得了回报100，其他动作都获得了0回报。　即<span class="math display">\[R(s_{12},a_r) = R(s_{23},a_u) =100\]</span>。 <img src="http://images.cnitblog.com/blog/489049/201401/132323587361.jpg" /> 2. 一个策略<span class="math display">\[\pi\]</span>如图所示： <img src="http://images.cnitblog.com/blog/489049/201401/161254079863.png" /> 3. 值函数<span class="math display">\[V_{\pi}(s)\]</span>如下所示 <img src="http://images.cnitblog.com/blog/489049/201401/132324064863.jpg" /> 根据<span class="math display">\[V_{\pi}\]</span><span class="math inline">\(的表达式，立即回报，和策略\)</span><span class="math inline">\(\pi\)</span>$，有: <span class="math display">\[V_{\pi}(s_{12}) = R(s_{12},a_{r}) = R(s_{13}|s_{12},ar) = 100\]</span> <span class="math display">\[V_{\pi}(s_{11})= R(s_{11},a_{r})+\gamma\times V_{\pi}(s_{12}) = 0+0.9\times 100 = 90\]</span> <span class="math display">\[V_{\pi}(s_{23})= R(s_{23},a_{u}) = 100\]</span> <span class="math display">\[V_{\pi}(s_{22})= R(s_{22},a_{r})+\gamma\times V_{\pi}(s_{23}) = 90\]</span> <span class="math display">\[V_{\pi}(s_{21})= r(s_{21},a_{r})+\gamma\times V_{\pi}(s_{22}) = 81\]</span> 4. <span class="math display">\[Q(s,a)\]</span>值如下所示 <img src="http://images.cnitblog.com/blog/489049/201401/132345002982.png" /> 有了策略<span class="math display">\[\pi\]</span>和立即回报函数<span class="math display">\[R(s,a)\]</span>, <span class="math display">\[Q_{\pi}(s,a)\]</span>如何得到的呢？ 对<span class="math display">\[s_{11}\]</span>计算Q函数（用到了上面<span class="math display">\[V_{\pi}\]</span>的结果）如下： <span class="math display">\[Q_{\pi}(s_{11},a_r)=R(s_{11},a_r)+ \gamma\times V_{\pi}(s_{12})  =0+0.9\times100 = 90\]</span> <span class="math display">\[Q_{\pi}(s_{11},a_d)=R(s_{11},a_d)+ \gamma\times V_{\pi}(s_{21})  = 72\]</span></p>
<h1 id="小结">小结</h1>
<p>至此我们讨论完了增强学习的数学本质————马尔科夫决策过程（MDP）的数学表示及求解过程（这里的MDP是非确定的MDP，即状态转移函数和回报函数是有概率的,，对于确定性的，求解会更简单些） 。</p>
</body>
</html>
