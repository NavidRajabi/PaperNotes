<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="algorithms-actor-critic">Algorithms-Actor Critic</h1>
<p>RL is a general-purpose framework for artificial intelligence. It is for an agent with the capacity to act, each action influences the agent's future state, and success is measured by a scalar reward signal. The essence of AI is : seek a single agent which can solve any human-level task.</p>
<h2 id="the-relationship-of-agent-and-environment">The relationship of agent and environment</h2>
<div class="figure">
<img src="https://webdocs.cs.ualberta.ca/~sutton/book/ebook/figtmp7.png" alt="Actor critic" />
<p class="caption">Actor critic</p>
</div>
<p>From the above figure, we can see that at each step <span class="math inline">\(t\)</span> , agent receives a state from environment, along with the scalar reward <span class="math inline">\(r_t\)</span>. Then the agent will executes an action <span class="math inline">\(a_t\)</span>. Correspondingly, the environment receives the action from agent, then emits state <span class="math inline">\(s_t\)</span> and scalar reward <span class="math inline">\(r_t\)</span>.</p>
<p>The agent will act under the policy <span class="math inline">\(\pi\)</span>, where <span class="math inline">\(\pi\)</span> is a behavior function selecting actions with given states: $ =(s) $Environment has a value function <span class="math inline">\(Q^{\pi}(\mathbf{s},\mathbf{a})\)</span>, value function is the expected total Reward from state <span class="math inline">\(\mathbf{s}\)</span> and action <span class="math inline">\(\mathbf{a}\)</span> under policy <span class="math inline">\(\pi\)</span>. $ Q^{}(,)=[r_{t+1}+r_{t+2}+^2 r_{t+3}+...|,]  $ ## Approaches to RL</p>
<ol style="list-style-type: decimal">
<li>Policy-based RL: Search directly for the optimal policy <span class="math inline">\(\pi^{\star}\)</span>, this is the policy achieving maximum future reward.</li>
<li>Value-based RL: Estimate the optimal value function <span class="math inline">\(Q^{\pi}(\mathbf{s},\mathbf{a})\)</span>, this is the maximum value achievable under any policy Model-based RL</li>
<li>Model-based RL: Build a transition model of the environment. Plan using model.</li>
</ol>
<h2 id="deep-q-learning">Deep Q-learning</h2>
<p>We can combine deep learning with RL, for example the value function <span class="math inline">\(Q^{\pi}(\mathbf{s},\mathbf{a})\)</span> and policy <span class="math inline">\(\pi(s)\)</span> can be represented by neural network. We also can apply SGD to Deep RL.</p>
<p>The value function can be unrolled recursively with Bellman equation: $ \begin{split} Q^{}(,)&amp;=[r_{t+1}+r_{t+2}+^2 r_{t+3}+...|,] \ &amp;=[r+Q^{}(',')|,] \end{split}  $</p>
<p>Then, the optimal value function <span class="math inline">\(Q^{\star}(\mathbf{s},\mathbf{a})\)</span> can be unrolled recursively: $ Q^{}(,)=_{s'}[r+Q^{}(',')|,]  $</p>
<p>Using Value iteration algorithms solve the Bellman equation: $ Q_{i+1}(,)=_{s'}[r+Q_i(',')|,]  $</p>
<p>For deep Q-learning, we represent the value function with weights <span class="math inline">\(\mathbf{w}\)</span>:</p>
<p>$ \begin{cases} Q(,,) Q^{}(,) \ Q_{i+1}(,)=<em>{s'}[r+Q_i(',')|,] \ \ Q</em>{i+1}(,,)=_{s'}[r+Q_i(',',')|,] \end{cases} $</p>
<p>The following figure is an example of Q-network, input situation <span class="math inline">\(\mathbf{s}\)</span> is the raw pixels from last 4 frames, after several CONV layers and full-connected layers, the Q-network will output the predicted actions <span class="math inline">\(\mathbf{a}\)</span>(joystick/button positions). Reward is change in score for that step. <img src="http://www.nature.com/nature/journal/v518/n7540/carousel/nature14236-f1.jpg" /></p>
<p>Then we can get the objective function(mean squared) error in Q-values: $ ()=[(_{target}-Q(,,))^ï¼’]  $</p>
<p>where <span class="math inline">\(taget\)</span> is the expected value function, and <span class="math inline">\(Q(\mathbf{s},\mathbf{a},\mathbf{w})\)</span> is the actual value. For this objective function, we can use SGD to optimize it end-to-end. $ =  $</p>
<p>However, there is some stability issues with Deep RL, Naive Q-Learning or diverges with neural nets.The reason are as follows: 1. Data is sequential, while successive samples are correlated and non-iid(independently and identically distributed). 2. Policy changes rapidly with slight changes to Q-values, which means policy <span class="math inline">\(\pi\)</span> may oscillate. So the distribution of data can swing from one extreme to another. 3. Scale of rewards and Q-values is unknown. Naive Q-learning gradients can be large unstable when back-propagated.</p>
<p>DQN provides a stable solution to deep value-based RL, it has three characteristics: 1. experience replay. This can break correlations in data, bring us back to IID setting. And can also learn from all past policies. In order to remove correlations in data, they build data-set from agent's own experience. 1. Take action <span class="math inline">\(a_t\)</span> according to <span class="math inline">\(\epsilon\)</span>-greedy policy 2. Store transition <span class="math inline">\((s_t, a_t, r_{t+1}, s_{t+1})\)</span> in replay memory <span class="math inline">\(\mathcal{D}\)</span> 3. Sample random mini-batch of transitions <span class="math inline">\((s,a, r,s&#39;)\)</span> from <span class="math inline">\(\mathcal{D}\)</span> 4. Optimize MSE between Q-network and Q-learning targets, $ ()=<em>{s,a,r,s'D}[((r+Q(',',')-Q(,,))^2)] $2. Freeze target Q-network. This can avoid oscillations, and break correlations between Q-network and target.To avoid oscillations, fix parameters used in Q-learning target. 1. compute Q-learning target w.r.t(With Regard To) old, fixed parameters <span class="math inline">\(\mathbf{w}^{-}\)</span> $ r+Q(',',^{-}) $ 2. Optimize MSE between Q-network and Q-learning targets $ ()=</em>{s,a,r,s'}[((r+Q(',',<sup>{-})-Q(,,))</sup>2)] $ 3. Periodically update fixed parameters <span class="math inline">\(\mathbf{w}^{-}\leftarrow \mathbf{w}\)</span></p>
<ol start="3" style="list-style-type: decimal">
<li>Clip rewards or normalize network adaptively to sensible range. DQN clips the rewards to <span class="math inline">\([-1,1]\)</span>, which prevents Q-values from becoming too large. Also ensures gradients are well-conditioned. But can not tell the difference between small and large rewards.</li>
</ol>
<p>Another improvement of DQN is Normalized DQN. Normalized DQN uses true reward signal. The network outputs a scalar value in &quot;stable&quot; range <span class="math inline">\(U(\mathbf{s},\mathbf{a},\mathbf{w})\in[-1,+1]\)</span>, and output is scaled and translated into Q-values <span class="math inline">\(Q(\mathbf{s},\mathbf{a},\mathbf{w},\delta,\pi)=\sigma U(\mathbf{s},\mathbf{a},\mathbf{w})+\pi\)</span>, where <span class="math inline">\(\pi,\sigma\)</span> are adapted to ensure <span class="math inline">\(U(\mathbf{s},\mathbf{a},\mathbf{w})\in[-1,+1]\)</span>. Network parameters <span class="math inline">\(\mathbf{w}\)</span> are adjusted to keep Q-values constant. $ _1 U(,,_1)+_1=_2 U(,,_2)+_2 $</p>
<h2 id="deep-q-learning-with-experience-replay">Deep Q-learning with Experience Replay</h2>
<p>Algorithm 1 Deep Q-learning with <strong>Experience Replay</strong> 1. Initialize replay memory <span class="math inline">\(\mathcal{D}\)</span> to capacity <span class="math inline">\(N\)</span> 2. Initialize action-value function <span class="math inline">\(Q\)</span> with random weights 3. for episode = 1,M do 3.1 Initialize sequence <span class="math inline">\(s_1=\{x_1\}\)</span> and preprocessed sequenced <span class="math inline">\(\phi_1=\phi(s_1)\)</span> 3.2 for t = 1, T do 3.2.1 With probability <span class="math inline">\(\epsilon\)</span> select a random actoin <span class="math inline">\(a_t\)</span>, otherwise select <span class="math inline">\(a_t=\max_a Q^{*}(s_t),a;\theta\)</span> 3.2.2 Execute action <span class="math inline">\(a_t\)</span> in emulator and observe reward <span class="math inline">\(r_t\)</span> and image <span class="math inline">\(x_{t+1}\)</span> 3.2.3 Set <span class="math inline">\(s_{t+1}=s_t,a_t,x_{t+1}\)</span> and preprocess <span class="math inline">\(\phi_{t+1}=\phi(s_{t+1})\)</span> 3.2.4 Store transition <span class="math inline">\((\phi_t, a_t,r_t, \phi_{t+1})\)</span> in <span class="math inline">\(\mathcal{D}\)</span> 3.2.5 Sample random minibatch of transitions <span class="math inline">\((\phi_t, a_t,r_t, \phi_{t+1})\)</span> from <span class="math inline">\(\mathcal{D}\)</span> 3.2.6 Set <span class="math inline">\(y_j=\begin{cases}r_j, \ \text{for terminal } \phi_{j+1} \\ r_j+\gamma \underset{a&#39;}{\max}Q(\phi_{j+1},\mathbf{a}&#39;;\theta) , \ \text{for non-terminal } \phi_{j+1}\end{cases}\)</span> 3.2.7 Perform a gradient descent step on <span class="math inline">\((y_j-Q(\phi_j,a_j;\theta))^2\)</span> according to equation 3 3.3 end for 4. end for</p>
</body>
</html>
