<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="an-actor-critic-algorithm-for-sequence-prediction"><strong>An Actor-Critic Algorithm for Sequence Prediction</strong></h1>
<!-- <p align="center"><img src="https://dl.dropboxusercontent.com/s/j65qfloub9ml2v5/An-Actor-Critic-Algorithm-for-Sequence-Prediction.png" width="1920" ></p> -->
<h2 id="backgound-knowledge">Backgound knowledge</h2>
<p>http://blog.csdn.net/lanbing510/article/details/49912739</p>
<h2 id="problems">Problems</h2>
<p>Current <font color="red">log-likelihood</font> traning methods are limited by the discrepancy[相差] between their training and testing modes. As models must generate tokens <font color="red">conditioned on their previous guesses rather than the ground-truth tokens</font>. The authors propose to use the <font color="red">Actor Critic framework</font> from Reinforcement Learning for Sequence prediction. They train an actor (policy) network to generate a sequence together with a critic (value) network that estimates the <font color="red">q-value function</font>. <font color="red">Crucially, the actor network does not see the <strong>ground-truth</strong> output, but the critic does</font>. This is <font color="red">different</font> from LL (log likelihood) where errors are likely to cascade. The authors evaluate their framework on an artificial spelling correction and a real-world German-English Machine Translation tasks, beating baselines and competing approaches in both cases.</p>
<h2 id="key-points">Key Points</h2>
<ul>
<li>In <font color="red"><span class="math inline">\(LL\)</span></font> training, the model is conditioned on its own guesses during search, leading to error compounding.</li>
<li>The <font color="red">critic is allowed to see the ground truth</font>, but the actor isn't</li>
<li>The reward is a <font color="red">task-specific score</font>, e.g. BLEU</li>
<li>Use bidirectional RNN for both actor and critic. <font color="red">Actor</font> uses a soft attention mechanism.</li>
<li><strong><font color="red">The reward is partially receives at each intermediate step, not just at the end</font></strong></li>
<li>Framework is analogous to <font color="red">TD-Learning</font> in RL</li>
<li>Trick: Use additional <font color="red">target network</font> to compute <span class="math inline">\(q_t\)</span> (see <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Resources_files/deep_rl.pdf">Deep-Q paper</a>) for stability</li>
<li>Trick: Use delayed actor (as in <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Resources_files/deep_rl.pdf">Deep-Q paper</a>) for stability</li>
<li>Trick: Put <font color="red">constraint</font> on critic to deal with large action spaces (is this analogous to advantage functions?)</li>
<li>Pre-train actor and critic to encourage exploration of the right space</li>
<li>Task 1: Correct corrupt character sequence. AC outperforms LL training. Longer sequences lead to stronger lift.</li>
<li>Task 2: GER-ENG Machine Translation: Beats LL and Reinforce models</li>
<li>Qualitatively, critic assigns high values to words that make sense</li>
<li>BLUE scores during training are lower than those of LL model - Why? Strong regularization? Can't overfit the training data.</li>
</ul>
<h2 id="previous-training-process">Previous training process</h2>
<p>Given an input <span class="math inline">\(\{X,Y=(y_1,...,y_2)\}\)</span> and <span class="math inline">\(\mathcal{A}\)</span> (alphabet of output tokens). Use notation <span class="math inline">\(\hat{Y}_{f...l}\)</span> to refer to subsquences of the form <span class="math inline">\((\hat{y}_f,...,\hat{y}_l)\)</span>. The trained predictor <span class="math inline">\(h\)</span> is evaluated by computing the average task-specific score <span class="math inline">\(R(\hat{Y},Y)\)</span> on the test set, where <span class="math inline">\(\hat{Y}=h(X)\)</span> is the prediction.</p>
<p>The Basic mechanism of RNNs for sequence prediction is <span class="math inline">\(\begin{cases} y_t \sim g(s_{t-1}) \\ s_t=f(s_{t-1},e(y_t)) \\ s_0 = s_0(X) \end{cases}\)</span>, if we add soft attention mechanism <span class="math inline">\(\begin{cases} y_t \sim g(s_{t-1}, c_{t-1}) \\ s_t=f(s_{t-1},c_{t-1},e(y_t)) \\ \alpha_t = \beta(s_t,(h_1,...,h_L)) \\ c_t = \sum_{j=1}^L \alpha_{t,j}h_j \end{cases}\)</span>. The attention weights are computed by MLP (takes as input the current RNN state and each individual vector to focus on). The loss is the sum of the negative log likelihood of thecorrect word at each step as follows: <span class="math inline">\(L(Y,X)=-\sum_{t=1}^T\log p_t(y_t)\)</span>. A conditioned RNN can be trained for sequence prediction by <font color="red">gradient ascent</font> on the log-likelihood <span class="math inline">\(\log p(Y|X)=\sum_{t=0}^T\log p(y_t|X,y_0,...,y_{t-1})\)</span> for the input-output pairs <span class="math inline">\((X,Y)\)</span> from the training set.</p>
<p>To produce a prediction <span class="math inline">\(\hat{Y}\)</span> for a test <span class="math inline">\(X\)</span>, an approximate beam search for the maximum of <span class="math inline">\(p(.|X)\)</span> is usually conducted. During this search the <font color="red">previous probabilities <span class="math inline">\(p(.|\hat{y}_1,...,\hat{y}_{t-1})\)</span></font> are considered, where the previous tokens <span class="math inline">\(\hat{y}_1,...,\hat{y}_{t-1}\)</span> comprise a candidate beginning of the prediction <span class="math inline">\(\hat{Y}\)</span>. <strong><font color="red">Since these tokens come from a different distribution than the one used during training, the probabilities <span class="math inline">\(p(.|\hat{y}_1,...,\hat{y}_{t-1})\)</span> can potentially be wrong.</font></strong> This presents a major issue for maximum likelihood training, together with the concern that the task score <span class="math inline">\(R\)</span> is used only at test time and not during training.</p>
<h2 id="proposed-method">Proposed method</h2>
<p>View the conditioned RNN as a <strong><font color="red">stochastic policy</font></strong> that generates actions and receives the task score as areward. The reward is partially received at the <u>inter-mediate steps</u> of a sequence of actions and given by:<span class="math inline">\(R(\hat{Y},Y) =\sum_{t=1}^Tr_t (\hat{y}_t ; \hat{Y}_{1,...,t-1},Y)\)</span>, <span class="math inline">\(\hat{y}_t\)</span> is the predicted word at time step <span class="math inline">\(t\)</span>.</p>
<p>Given the <u>policy <span class="math inline">\(\pi\)</span></u>, <u>possible actions <span class="math inline">\(y_t\)</span></u> and <u>reward function <span class="math inline">\(r_t\)</span></u>, the value represents the <u><strong>expected future reward</strong></u> as a function of the <strong><u>current state of the system</u></strong>(tokens predicted <span class="math inline">\(\hat{y}_1,...,\hat{y}_{t-1}\)</span>) so far.</p>
<p>For each step <span class="math inline">\(t\)</span>, the <u>reward is given by :</u> $ R(<em>{t})=</em>{=t+1}^Tr_{}(<em>{};</em>{1,...,{-1}})$. Then the <u>Value Function</u> is : <span class="math inline">\(V(\hat{Y}_{1,..,t};X,Y)=\mathbb{E}_{\hat{Y}_{t+1,..,T}\sim p(\hat{Y}_{t+1,..,T}|\hat{Y}_{1,..,t},X)}R(\hat{y}_{t})\)</span> <span class="math inline">\(V(\hat{Y}_{1,..,t};X,Y)=\mathbb{E}_{\hat{Y}_{t+1,..,T}\sim p(\hat{Y}_{t+1,..,T}|\hat{Y}_{1,..,t},X)}\left[r_{t+1}(\hat{y}_{t+1};\hat{Y}_{1,..,t},Y)+r_{t+2}(\hat{y}_{t+2};\hat{Y}_{1,..,t+1},Y)+...+r_{T}(\hat{y}_{T};\hat{Y}_{1,..,T-1},Y)\right]\)</span>. <span class="math inline">\(V(\hat{Y}_{1,..,t};X,Y)=\mathbb{E}_{\hat{Y}_{t+1,..,T}\sim p(\hat{Y}_{t+1,..,T}|\hat{Y}_{1,..,t},X)}R(\hat{y}_{t}) =\sum_{t=1}^T\log p(\hat{y}_t|\hat{Y}_{1,...,t-1})Q(\hat{y}_t;\hat{Y}_{1,...,t-1})=\sum_{t=1}^T\log p(\hat{y}_t|\hat{Y}_{1,...,t-1})\sum_{\tau=t}^T r_{\tau}(\hat{y}_{\tau};\hat{Y}_{1,...,\tau-1})\)</span></p>
<p>Action-Value function is: <span class="math inline">\(Q(a;\hat{Y}_{1,...,t-1};X,Y)=\mathbb{E}_{\hat{Y}_{t+1,...,T}\sim p(\hat{Y}_{t+1,...,T}|\hat{Y}_{1,...,t-1}a,X)}\left[r_{t+1}(a;\hat{Y}_{1,...,t},Y)+\sum_{\tau=t+1}^Tr_{\tau}(\hat{y}_{\tau};\hat{Y}_{1,...,t-1}a,Y)\right]\)</span></p>
<p>Our target is get the Optimal value function: <span class="math inline">\(Q(\hat{y}_t;\hat{Y}_{1,...,t-1})=r_t(\hat{y}_t;\hat{Y}_{1,...,t-1})+\max \sum_{a\in \mathcal{A}}p(a|\hat{Y}_{1,...,t})Q(a;\hat{Y}_{1,...,t};X,Y)\)</span></p>
<h3 id="training-and-testing-the-critic"><strong><font color="red">Training and testing the critic</font></strong></h3>
<p>The training folow chart is as following: <!-- ![](https://dl.dropboxusercontent.com/s/2g9yoh1sj3szs4c/An-Actor-Critic-Algorithm-for-Sequence-Prediction_training.png) --></p>
<!-- #### **<font color="red">Detailed flow</font>** -->
<ul>
<li>Initialize <strong>Critic</strong> <span class="math inline">\(\hat{Q}(a;\hat{Y}_1,Y,\phi_1)\)</span> and <strong><font color="red">Actor</font></strong> <span class="math inline">\(p(a|\hat{Y}_1,X,\theta_1)\)</span></li>
<li><strong><font color="red">Pretrain</font></strong> <strong><font color="red">Actor</font></strong> to predict <span class="math inline">\(\hat{y}_2\)</span> given <span class="math inline">\(\hat{Y}_1\)</span> by maximizing <span class="math inline">\(\log p(y_2|Y_1,X)\)</span></li>
<li><strong><font color="red">Pretrain</font></strong> <strong><font color="red">Critic</font></strong> to estimate <span class="math inline">\(Q\)</span> by running the following algorithm with fixed <strong>Actor&lt;/font</strong>.</li>
<li>Receive a random sample <span class="math inline">\((X,Y)\)</span></li>
<li>Given <span class="math inline">\(X\)</span>, predict <span class="math inline">\((X,\hat{Y})\)</span> from <strong><font color="red">Delayed Actor <span class="math inline">\(p&#39;\)</span></strong></li>
<li><strong><font color="red">Target Critic <span class="math inline">\(\hat{Q}&#39;\)</span></font></strong> compute the target $q_t=r_t(<em>t;</em>{1,...,t-1},Y)+<em>{a}p(a|</em>{1,...,t},X)'(a;_{1,..,t},Y) $ <span class="math inline">\(\begin{cases} q_2=r_2(\hat{y}_2;\hat{Y}_{1,...,1},Y)+\sum_{a\in \mathcal{A}}p(a|\hat{Y}_{1,...,2},X)\hat{Q}&#39;(a;\hat{Y}_{1,..,2},Y)\\ q_3=r_3(\hat{y}_3;\hat{Y}_{1,...,2},Y)+\sum_{a\in \mathcal{A}}p(a|\hat{Y}_{1,...,3},X)\hat{Q}&#39;(a;\hat{Y}_{1,..,3},Y)\\ ...\\ q_t=r_t(\hat{y}_t;\hat{Y}_{1,...,t-1},Y)+\sum_{a\in \mathcal{A}}p(a|\hat{Y}_{1,...,t},X)\hat{Q}&#39;(a;\hat{Y}_{1,..,t},Y)\\ ...\\ q_T=r_T(\hat{y}_T;\hat{Y}_{1,...,T-1},Y)+\sum_{a\in \mathcal{A}}p(a|\hat{Y}_{1,...,T},X)\hat{Q}&#39;(a;\hat{Y}_{1,..,T},Y) \end{cases}\)</span></li>
<li>Update the <strong><font color="red">Target Critic <span class="math inline">\(\hat{Q}&#39;\)</span></font></strong> weights <span class="math inline">\(\phi\)</span> using the gradient $ [_{t=1}<sup>T((<em>t;</em>{1,..,t-1},Y)-q_t)</sup>2] $</li>
<li>Update <strong><font color="red">Actor</font></strong> weights <span class="math inline">\(\theta\)</span> using $ =<em>{t=1}^T</em>{a}(a;_{1,..,t-1},Y) $</li>
<li>Update <strong><font color="red">Delayed Actor <span class="math inline">\(p&#39;\)</span></font></strong> and <strong><font color="red">Delayed Target Critic <span class="math inline">\(Q&#39;\)</span></font></strong></li>
</ul>
</body>
</html>
