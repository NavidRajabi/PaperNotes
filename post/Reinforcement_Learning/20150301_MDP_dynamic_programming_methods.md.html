<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="mdp的动态规划解法">MDP的动态规划解法</h1>
<p>增强学习的目的就是<span style="color:red">求解马尔可夫决策过程(MDP)的最优策略</span>，使其在任意初始状态下，都能获得最大的<span class="math inline">\(V_{\pi}\)</span>值。(本文不考虑非马尔可夫环境和不完全可观测马尔可夫决策过程(POMDP)中的增强学习)。</p>
<p>那么如何求解最优策略呢？<span style="color:red">基本的解法有三种</span>：</p>
<ol style="list-style-type: decimal">
<li><strong><em>动态规划法</em></strong>(dynamic programming methods)</li>
<li><strong><em>蒙特卡罗方法</em></strong>(Monte Carlo methods)</li>
<li><strong><em>时间差分法</em></strong>(temporal difference)。</li>
</ol>
<p>动态规划法是其中最基本的算法，也是理解后续算法的基础，因此本文先介绍动态规划法求解MDP。本文假设拥有MDP模型<span class="math inline">\(M=(S, A, P_{sa},\gamma, R)\)</span>的完整知识。</p>
<h2 id="贝尔曼方程bellman-equation"><strong><em><span style="color:red">1. 贝尔曼方程（Bellman Equation）</span></em></strong></h2>
<p>上一篇我们得到了<span class="math inline">\(V_{\pi}\)</span>和<span class="math inline">\(Q_{\pi}\)</span>的表达式，并且写成了如下的形式:</p>
<p><span class="math display">\[
V^{\pi}(s)=\sum_{s&#39;\in S}P(s&#39;|s,\pi(s))[R(s&#39;|s,\pi(s))+\gamma V^{\pi}(s)]=E_{\pi}[R(s&#39;|s,a)+\gamma V^{\pi}(s&#39;)|_{s_0=s}] \tag{1}
\]</span> <span class="math display">\[
Q^{\pi}(s,a)=\sum_{s&#39;\in S}P(s&#39;|s,\pi(s))[R(s&#39;|s,a)+\gamma V^{\pi}(s)]=E_{\pi}[R(s&#39;|s,a)+\gamma V^{\pi}(s&#39;)|_{s_0=s,a_0=a}] \tag{2}
\]</span></p>
<p>在动态规划中，上面两个式子称为<strong><em><span style="color:red">贝尔曼方程</span></em></strong>，它<strong><em>表明了当前状态的值函数与下个状态的值函数的关系</em></strong>。</p>
<p><strong><em>优化目标</em></strong><span class="math inline">\(\pi^*\)</span>可以表示为：</p>
<p><span class="math display">\[
\pi^{*}(s)=arg\max V^{\pi}(s) \tag{3}
\]</span></p>
<p>分别记<strong><em>最优策略</em></strong><span class="math inline">\(\pi^*\)</span>对应的<strong><em>状态值函数</em></strong>和<strong><em>行为值函数</em></strong>为<span class="math inline">\(V^*(s)\)</span>和<span class="math inline">\(Q^*(s, a)\)</span>，由它们的定义容易知道，<span class="math inline">\(V^*(s)\)</span>和<span class="math inline">\(Q^*(s, a)\)</span>存在如下关系:</p>
<p><span class="math display">\[
V^*(s)=\max_{a}Q^*(s, a) \tag{4}
\]</span></p>
<p>状态值函数<span class="math inline">\(V^*(s)\)</span>和行为值函数<span class="math inline">\(Q^*(s, a)\)</span>分别满足如下<strong><em><span style="color:red">贝尔曼最优性方程(Bellman optimality equation)</span></em></strong>：</p>
<p><span class="math display">\[
\begin{cases}
V^*(s)=\max_{a} E[R(s&#39;|s,a)+\gamma V^*(s&#39;)|s_0=s]=\max_{a\in A(s)}\sum P(s&#39;|s,\pi(s))[R(s&#39;|s,\pi(s))+\gamma V^{\pi}(s&#39;)]\\
Q^*(s)=E[R(s&#39;|s,a)+\gamma_{a&#39;} Q^*(s&#39;,a&#39;)|s_0=s,a_0=a]=\sum P(s&#39;|s,\pi(s))[R(s&#39;|s,\pi(s))+\gamma \max_{a\in A(s)} Q^{*}(s&#39;,a&#39;)]\\
\end{cases} \tag{5}
\]</span></p>
<p>有了<strong><em>贝尔曼方程</em></strong>和<strong><em>贝尔曼最优性方程</em></strong>后，我们就可以用<strong><em>动态规划</em></strong>来求解MDP了。</p>
<h2 id="策略估计policy-evaluation"><strong><em><span style="color:red">2. 策略估计(Policy Evaluation)</span></em></strong></h2>
<h3 id="首先对于任意的策略pi我们如何计算其状态值函数vpis这个问题被称作策略估计">首先，对于任意的策略<span class="math inline">\(\pi\)</span>，我们如何计算其状态值函数<span class="math inline">\(V^{\pi}(s&#39;)\)</span>？这个问题被称作<strong><em>策略估计</em></strong>，</h3>
<p>前面讲到对于确定性策略，值函数<span class="math inline">\(V^{\pi}(s)=\sum_{s&#39;\in S}P(s&#39;|s,\pi(s))[R(s&#39;|s,\pi(s))+\gamma V^{\pi}(s)]\)</span></p>
<p>现在扩展到更一般的情况，如果在某策略<span class="math inline">\(\pi\)</span>下，<span class="math inline">\({\pi}(s)\)</span>对应的动作<span class="math inline">\(a\)</span>有多种可能，每种可能记为<span class="math inline">\({\pi}(a|s)\)</span>，则状态值函数定义如下：</p>
<p><span class="math display">\[V^{\pi}(s)=\sum_{a} \pi(a|s) \sum_{s&#39;\in S} P(s&#39;|s,\pi(s))[R(s&#39;|s,\pi(s))+\gamma V^{\pi}(s)] \tag{6}\]</span></p>
<p>一般采用迭代的方法更新状态值函数，首先将所有<span class="math inline">\(V^{\pi}(s)\)</span>的初值赋为0（其他状态也可以赋为任意值，不过吸收态必须赋0值），然后采用如下式子更新所有状态s的值函数（第k+1次迭代）</p>
<p><span class="math display">\[V_{k+1}(s)=\sum_{a} \pi(a|s) \sum_{s&#39;\in S} P(s&#39;|s,a)[R(s&#39;|s,a)+\gamma V^{\pi}(s)] \tag{7}\]</span></p>
<p>对于<span class="math inline">\(V^{\pi}(s)\)</span>，有两种更新方法，</p>
<ul>
<li>第一种：将第<span class="math inline">\(k\)</span>次迭代的各状态值函数<span class="math inline">\([V_k(s_1),V_k(s_2),V_k(s_3)..]\)</span>保存在一个数组中，第k+1次的<span class="math inline">\(V^{\pi}(s)\)</span>采用第k次的<span class="math inline">\(V^{\pi}(s&#39;)\)</span>来计算，并将结果保存在第二个数组中。</li>
<li>第二种：即仅用一个数组保存各状态值函数，每当得到一个新值，就将旧的值覆盖,形如<span class="math inline">\([V_{k+1}(s_1),V_{k+1}(s_2),V_{k+1}(s_3)..]\)</span>，第k+1次迭代的<span class="math inline">\(V^{\pi}(s)\)</span>可能用到第k+1次迭代得到的<span class="math inline">\(V^{\pi}(s&#39;)\)</span>。</li>
</ul>
<p>通常情况下，我们采用第二种方法更新数据，因为它及时利用了新值，能更快的收敛。整个策略估计算法如下图所示：</p>
<p align="center">
<img src="http://images.cnitblog.com/blog/489049/201401/201019414696.png" width="400" >
</p>
<h2 id="策略改进policy-improvement"><strong><em><span style="color:red">3. 策略改进(Policy Improvement)</span></em></strong></h2>
<p>上一节中进行策略估计的目的，是为了寻找更好的策略，这个过程叫做<strong><em><span style="color:red">策略改进(Policy Improvement</span></em></strong>。</p>
<p>假设我们有一个策略π，并且确定了它的所有状态的值函数<span class="math inline">\(V^{\pi}(s)\)</span>。对于某状态s，有动作<span class="math inline">\(a_0=\pi(s)\)</span>。 那么如果我们在状态s下不采用动作<span class="math inline">\(a_0\)</span>，而采用其他动作<span class="math inline">\(a\ne \pi(s)\)</span>是否会更好呢？要判断好坏就需要我们计算行为值函数<span class="math inline">\(Q^{\pi}(s,a)\)</span>，公式我们前面已经说过：</p>
<p><span class="math display">\[
Q^{\pi}(s,a)=\sum_{s&#39;\in S}P(s&#39;|s,a)[R(s&#39;|s,a)+\gamma V^{\pi}(s&#39;)] \tag{8}
\]</span></p>
<p><strong><em><span style="color:red">评判标准</span></em></strong>是：<span class="math inline">\(Q_{\pi}(s,a)\)</span>是否大于<span class="math inline">\(V^{\pi}(s)\)</span>。如果<span class="math inline">\(Q_{\pi}(s,a)&gt;V^{\pi}(s)\)</span>,那么至少说明新策略【仅在状态s下采用动作a，其他状态下遵循策略<span class="math inline">\(\pi\)</span>】比旧策略【所有状态下都遵循策略<span class="math inline">\(\pi\)</span>】整体上要更好。 <strong><em><span style="color:red">策略改进定理</span></em></strong>(policy improvement theorem)：<span class="math inline">\(\pi\)</span>和<span class="math inline">\(\pi &#39;\)</span>是两个确定的策略，如果对所有状态<span class="math inline">\(s\in S\)</span>有<span class="math inline">\(Q^{\pi}(s,\pi &#39;(s))\ge V^{\pi}(s)\)</span>，那么策略<span class="math inline">\(\pi &#39;必然比策略\)</span><span class="math inline">\(更好，或者至少一样好。其中的不等式等价于\)</span>V<sup>{}(s)V</sup>{}(s)$。</p>
<p>有了在某状态s上改进策略的方法和策略改进定理，我们可以遍历所有状态和所有可能的动作a，并采用贪心策略来获得新策略<span class="math inline">\(\pi &#39;\)</span>。即对所有的s∈S, 采用下式更新策略：</p>
<p><span class="math display">\[
\pi&#39;(s)=arg\max_{a}Q^(\pi)(s,a)=arg\max_{a}E_{\pi}[R(s&#39;|s,a)+\gamma V^{\pi}(s&#39;)]=arg\max_{a} \sum_{s&#39;\in S}P(s&#39;|s,a)[R(s&#39;|s,a)+\gamma V^{\pi}(s&#39;)] \tag{9}
\]</span></p>
<p>这种采用关于值函数的贪心策略获得新策略，改进旧策略的过程，称为策略改进(Policy Improvement)</p>
<p>最后大家可能会疑惑，贪心策略能否收敛到最优策略，这里我们假设策略改进过程已经收敛，即对所有的s，<span class="math inline">\(V^{\pi&#39;}(s)\)</span>等于<span class="math inline">\(V^{\pi}(s)\)</span>。那么根据上面的策略更新的式子，可以知道对于所有的s∈S下式成立：</p>
<p><span class="math display">\[
V^{\pi&#39;}=\max_{a}E_{\pi}[R(s&#39;|s,a)+\gamma V^{\pi}(s&#39;)]=\max_{a}\sum_{s&#39;\in S}P(s&#39;|s,a)[R(s&#39;|s,a)+\gamma V^{\pi}(s&#39;)] \tag{10}
\]</span></p>
<p>可是这个式子正好就是我们在1中所说的Bellman optimality equation，所以<span class="math inline">\(\pi\)</span>和<span class="math inline">\(\pi &#39;\)</span>都必然是最优策略！神奇吧！</p>
<h2 id="策略迭代policy-iteration"><strong><em><span style="color:red">4. 策略迭代(Policy Iteration)</span></em></strong></h2>
<p>策略迭代算法就是上面两节内容的组合。假设我们有一个策略π，那么我们可以用policy evaluation获得它的值函数Vπ(s)，然后根据policy improvement得到更好的策略π'，接着再计算Vπ'(s),再获得更好的策略π''，整个过程顺序进行如下图所示：</p>
<p><span class="math display">\[{\pi}_{0} \stackrel{E}{\longrightarrow} v_{\pi_0} \stackrel{I}{\longrightarrow} {\pi}_{1} \stackrel{E}{\longrightarrow} v_{\pi_1} \stackrel{I}{\longrightarrow} ···
\tag{11}
\]</span></p>
完整的算法如下图所示：
<p align="center">
<img src="http://images.cnitblog.com/blog/489049/201401/201019447506.png" width="400" >
</p>
<h2 id="值迭代value-iteration"><strong><em><span style="color:red">5. 值迭代(Value Iteration)</span></em></strong></h2>
<p>从上面我们可以看到，策略迭代算法包含了一个策略估计的过程，而策略估计则需要扫描(sweep)所有的状态若干次，其中巨大的计算量直接影响了策略迭代算法的效率。我们必须要获得精确的Vπ值吗？事实上不必，有几种方法可以在保证算法收敛的情况下，缩短策略估计的过程。</p>
<p><strong><em>值迭代（Value Iteration）</em></strong>就是其中非常重要的一种。它的每次迭代只扫描(sweep)了每个状态一次。值迭代的<strong><em>每次迭代</em></strong>对所有的<span class="math inline">\(s\in S\)</span>按照下列公式更新：</p>
<p><span class="math display">\[V_{k+1}(s)=\max_{a} E_{\pi}[R(s&#39;|s,a)+\gamma V^{\pi}(s&#39;)]=\max_{a}\sum_{s&#39;\in S}P(s&#39;|s,a)[R(s&#39;|s,a)+\gamma V^{\pi}(s&#39;)] \tag{12}\]</span></p>
<p>即在值迭代的第k+1次迭代时，直接将能获得的最大的<span class="math inline">\(V^{\pi}(s)\)</span>值赋给<span class="math inline">\(V_{k+1}\)</span>。值迭代算法直接用可能转到的下一步s'的V(s')来更新当前的V(s)，算法甚至都不需要存储策略π。而实际上这种更新方式同时却改变了策略πk和V(s)的估值Vk(s)。 直到算法结束后，我们再通过V值来获得最优的<span class="math inline">\(\pi\)</span>。</p>
<p>此外，值迭代还可以理解成是采用迭代的方式逼近1中所示的贝尔曼最优方程。</p>
值迭代完整的算法如图所示：
<p align="center">
<img src="http://images.cnitblog.com/blog/489049/201401/201019462191.png" width="400" >
</p>
<p>由上面的算法可知，值迭代的最后一步，我们才根据<span class="math inline">\(V^*(s)\)</span>，获得最优策略<span class="math inline">\(\pi^*\)</span>。</p>
<p>一般来说值迭代和策略迭代都需要经过无数轮迭代才能精确的收敛到<span class="math inline">\(V^*\)</span>和<span class="math inline">\(\pi^*\)</span>， 而实践中，我们往往设定一个阈值来作为中止条件，即当<span class="math inline">\(V^{\pi}(s)\)</span>值改变很小时，我们就近似的认为获得了最优策略。在折扣回报的有限MDP(discounted finite MDPs)中，进过有限次迭代，两种算法都能收敛到最优策略<span class="math inline">\(\pi^*\)</span>。</p>
<p>至此我们了解了马尔可夫决策过程的动态规划解法，动态规划的优点在于它有很好的数学上的解释，但是动态要求一个完全已知的环境模型，这在现实中是很难做到的。另外，当状态数量较大的时候，动态规划法的效率也将是一个问题。下一篇介绍蒙特卡罗方法，它的优点在于不需要完整的环境模型。</p>
</body>
</html>
