<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="object-recognition---bag-of-words-models">Object Recognition - Bag-of-Words Models</h1>
<p>Bag-of-Words模型源于<font style="color:red">文本分类技术</font>，在信息检索中，它假定对于一个文本，<font style="color:red">忽略其词序和语法、句法</font>。将其仅仅看作是<font style="color:red">一个词集合</font>，或者说是词的一个组合，文本中每个词的出现都是独立的，不依赖于其他词是否出现，或者说<font style="color:red">这篇文章的作者在任意一个位置选择词汇都不受前面句子的影响而独立选择的</font>。</p>
<div class="figure">
<img src="http://img.blog.csdn.net/20131002211936421?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2hsZWxlMDEwNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" />

</div>
<p><font style="color:red">图像可以视为一种文档对象</font>，图像中<font style="color:red">不同的局部区域或其特征可看做构成图像的词汇</font>，其中<font style="color:red">相近的区域或其特征可以视作为一个词</font>。这样，就能够把文本检索及分类的方法用到图像分类及检索中去。</p>
<div class="figure">
<img src="http://img.blog.csdn.net/20131002212016859?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2hsZWxlMDEwNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" />

</div>
<p>Bag-of-Features模型仿照文本检索领域的Bag-of-Words方法，把<font style="color:red">每幅图像描述为一个局部区域/关键点(Patches/Key Points)特征的无序集合</font>。使用<font style="color:red">某种聚类算法(如K-means)将局部特征进行聚类</font>，每个<font style="color:red">聚类中心</font>被看作是词典中的一个<font style="color:red">视觉词汇(Visual Word)</font>，相当于文本检索中的词，视觉词汇由<font style="color:red">聚类中心对应特征形成的码字(code word)</font>来表示（可看当为一种特征量化过程）。所有视觉词汇形成一个视觉词典(Visual Vocabulary)，对应一个<font style="color:red">码书(code book)</font>，即码字的集合，词典中所含词的个数反映了词典的大小。图像中的<font style="color:red">每个特征都将被映射到视觉词典的某个词上</font>，这种映射可以通过计算特征间的距离去实现，然后统计每个视觉词的出现与否或次数，图像可描述为一个维数相同的直方图向量，即Bag-of-Features。</p>
<p>Bag-of-Features更多地是用于<font style="color:red">图像分类或对象识别</font>。在上述思路下对训练集提取Bag-of-Features特征，在某种监督学习（如:SVM）的策略下，对训练集的Bag-of-Features特征向量进行训练，<font style="color:red">获得对象或场景的分类模型</font>；对于待测图像，<font style="color:red">提取局部特征，计算局部特征与词典中每个码字的特征距离</font>，选取<font style="color:red">最近距离的码字代表该特征</font>，建立一个<font style="color:red">统计直方图，统计属于每个码字的特征个数，即为待测图像之Bag-of-Features特征</font>；在分类模型下，对该特征进行预测从实现对待测图像的分类。</p>
<p>早期Bag-of-Words模型主要是纹理识别。纹理识别（texture recognition）中纹理是由图像中一些基本的细小结构组成的，是早期视觉感知中的基本单元。局部的细小结构组合在一起，形成了图像中的纹理，这与BoW的思想有相同的地方。</p>
<ol style="list-style-type: decimal">
<li>局部特征提取：通过分割、密集或随机采集、关键点或稳定区域、显著区域等方式使图像形成不同的patches，并获得各patches处的特征。
<center>
<img src="http://img.blog.csdn.net/20131002212120406?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2hsZWxlMDEwNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="Test Image" width="400"/>
</center>
</br> 其中，SIFT特征较为流行。</br>
<center>
<img src="http://img.blog.csdn.net/20131002212234390?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2hsZWxlMDEwNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="Test Image" width="400"/>
</center>
</br></li>
<li>构建视觉词典：
<center>
<img src="http://img.blog.csdn.net/20131002212311687?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2hsZWxlMDEwNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="Test Image" width="400"/>
</center>
</br> 由聚类中心代表的视觉词汇形成视觉词典：</br>
<center>
<img src="http://img.blog.csdn.net/20131002212412687?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2hsZWxlMDEwNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="Test Image" width="400"/>
</center>
</br></li>
<li>生成码书，即构造Bag-of-Features特征，也即局部特征投影过程：
<center>
<img src="http://img.blog.csdn.net/20131002212441343?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2hsZWxlMDEwNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="Test Image" width="400"/>
</center>
</br></li>
<li>SVM训练BOF特征得分类模型，对待测图像BOF特征预测：
<center>
<img src="http://img.blog.csdn.net/20131002212532765?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2hsZWxlMDEwNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="Test Image" width="400"/>
</center>
</br></li>
</ol>
<p>Bag-of-words在CV中的应用首先出现在Andrew Zisserman[6]中为解决对视频场景的搜索，其提出了使用Bag-of-words关键点投影的方法来表示图像信息。后续更多的研究者归结此方法为Bag-of-Features，并用于图像分类、目标识别和图像检索。在Bag-of-Features方法的基础上，Andrew Zisserman进一步借鉴文本检索中TF-IDF模型(Term Frequency一Inverse Document Frequency)来计算Bag-of-Features特征向量。接下来便可以使用文本搜索引擎中的反向索引技术对图像建立索引，高效的进行图像检索。</p>
<center>
<img src="http://img.blog.csdn.net/20131002212540375?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2hsZWxlMDEwNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="Test Image" width="400"/>
</center>
<p></br></p>
<p>实现检索的过程同分类的过程无本质的差异，更多的是细节处理上的差异： 1. 局部特征提取； 2. 构建视觉词典； 3. 生成原始BOF特征； 4. 引入TF-IDF权值： <font style="color:red">TF-IDF是一种用于信息检索的常用加权技术</font>，在文本检索中，用以评估词语对于一个文件数据库中的其中一份文件的重要程度。词语的重要性随着它在文件中出现的频率成正比增加，但同时会随着它在文件数据库中出现的频率成反比下降。TF的主要思想是:<font style="color:red">如果某个关键词在一篇文章中出现的频率高，说明该词语能够表征文章的内容，该关键词在其它文章中很少出现，则认为此词语具有很好的类别区分度，对分类有很大的贡献。IDF的主要思想是:如果文件数据库中包含词语A的文件越少，则IDF越大，则说明词语A具有很好的类别区分能力</font>。</br> 词频(Term Frequency，TF)指的是一个给定的词语在该文件中出现的次数。如：tf = 0.030 ( 3/100 )表示在包括100个词语的文档中, 词语'A'出现了3次。 逆文档频率(Inverse Document Frequency，IDF)是描述了某一个特定词语的普遍重要性，如果某词语在许多文档中都出现过，表明它对文档的区分力不强，则赋予较小的权重;反之亦然。如:idf = 13.287 ( log (10,000,000/1,000) )表示在总的10,000,000个文档中，有1,000个包含词语'A'。 最终的TF-IDF权值为词频与逆文档频率的乘积。 5. 对查询图像生成同样的带权BOF特征； 6. 查询：初步是通过余弦距离衡量，至于建立索引的方法还未学习到，望看客指点。</p>
<p>Issues 1. 使用k-means聚类，除了其K和初始聚类中心选择的问题外，对于海量数据，<font style="color:red">输入矩阵的巨大将使得内存溢出及效率低下</font>。有方法是在海量图片中抽取部分训练集分类，使用朴素贝叶斯分类的方法对图库中其余图片进行自动分类。另外，由于图片爬虫在不断更新后台图像集，重新聚类的代价显而易见。 2. <font style="color:red">字典大小的选择也是问题，字典过大，单词缺乏一般性，对噪声敏感，计算量大</font>，关键是图象投影后的维数高；字典太小，单词区分性能差，对相似的目标特征无法表示。 3. <font style="color:red">相似性测度函数用来将图象特征分类到单词本的对应单词上</font>，其涉及线型核，塌方距离测度核，直方图交叉核等的选择。 4. 将图像表示成一个<font style="color:red">无序局部特征集的特征包方法，丢掉了所有的关于空间特征布局的信息</font>，在描述性上具有一定的有限性。为此， Schmid[2]提出了基于空间金字塔的Bag-of-Features。 5. Jégou[7]提出VLAD(vector of locally aggregated descriptors)，其方法是如同BOF先建立出含有k个visual word的codebook，而不同于BOF将一个local descriptor用NN分类到最近的visual word中，VLAD所采用的是计算出local descriptor和每个visual word(c­i)在每个分量上的差距，将每个分量的差距形成一个新的向量来代表图片。</p>
<p><strong>LDA</strong>: LDA要干的事情简单来说就是为一堆文档进行聚类（所以是非监督学习），一种topic就是一类，要聚成的topic数目是事先指定的。聚类的结果是一个概率，而不是布尔型的100%属于某个类。</p>
<h2 id="pyramid-mathcing-kernel">Pyramid mathcing kernel</h2>
<p>Histogram intersection这个方法最初来自The Pyramid Match Kernel:Discriminative Classification with Sets of Image Features这篇论文，用来对特征构成的直方图进行相似度匹配，下面介绍下原理。</p>
<p>假设图像或其他数据的特征可以构成直方图，根据直方图间距的不同可以得到多种类型的直方图：<img src="http://img.blog.csdn.net/20140408105114765" /></p>
<p>论文里是这么设置的，假设​H0(x)里每个直方图宽度为a，那么​H1(x)为2a，以此类推。举个例子，假设有某计算机学院男生身高范围在160cm-200cm，H0(x)宽度可以设置为2cm，那H0(x)里会有20个直方图；类推H1(x)宽度则为4cm，H1(x)会有10个直方图。</p>
<p>两个数据集的相似度可以用下式来匹配：<img src="http://img.blog.csdn.net/20140408110016359" /></p>
<p>y和z分别代表不同的数据集，比如给了两个学院男生身高，想看下这两个学院是不是同一个学院（例子不恰当，凑合着用吧)，用上式他们的相似度就好了。其中w代表权重，论文里将<span class="math inline">\(w_i\)</span>设置为<span class="math inline">\(1/(2^i)\)</span>，N代表每两层之间的新匹配的数目，可以通过下式计算：<img src="http://img.blog.csdn.net/20140408110623187" /></p>
<p>上式里面的L可以通过下式计算：<img src="http://img.blog.csdn.net/20140408110652796" /></p>
<p>附图解释什么意思。</p>
<div class="figure">
<img src="http://img.blog.csdn.net/20140408110748640" />

</div>
<p><span class="math inline">\((a)\)</span>里的<span class="math inline">\(y\)</span>和<span class="math inline">\(z\)</span>代表两种数据分布，三幅图代表三层金字塔，每一层里有间距相等的虚线，意思和我之前说的2cm，4cm的宽度一样。可以看到红点蓝点的位置是固定的，但是根据直方图宽度的不同可以划到不同的直方图里，如<span class="math inline">\((b)\)</span>所示。<span class="math inline">\((c)\)</span>图就是L的计算结果，是通过<span class="math inline">\((b)\)</span>里两种直方图取交集得来的，不过直方图的高度忽略不计，只计算交集后的数目，<span class="math inline">\((c)\)</span>图每个图的下方都给出了交集数目，比如<span class="math inline">\(x_0=2,x_1=4,x_2=3\)</span>。</p>
<p>L得到了，就算N就是通过，也就是通过<span class="math inline">\(N_i=L_i-L_{i-1}\)</span>得到（看公式是能取负数的，比如上图里的<span class="math inline">\(N_0=2,N_1=2,N_2=-1\)</span>）。由于<span class="math inline">\(w_i\)</span>之前设置为<span class="math inline">\(1/(2^i)\)</span>了，所以<img src="http://img.blog.csdn.net/20140408112432937" />.</p>
<p>算法就是这样了，但是感觉不适合Extended LBP，因为等价模式取值不多，经不起这么多的直方图宽度变化，倒是比较适合原生的LBP方法。</p>
<h2 id="spm空间金字塔匹配">SPM（空间金字塔匹配）</h2>
<p>SPM 全称是Spatial Pyramid Matching，出现的背景是bag of visual words模型被大量地用在了图像表示（Image representation）中. 传统BOF方法提取图像特征时，首先提取每张图像的SIFT特征描述，之后将所有图像的兴趣点的特征描述进行聚类形成BOW视觉词袋，最后对每张图像统计所有视觉关键词出现的频次。因此BOF是在整张图像中计算特征点的分布特征，进而生成全局直方图，所以会丢失图像的空间分布信息，无法对图像进行精确地识别。</p>
<p>但是BOW模型完全缺失了特征点的<font style="color:red">位置信息</font>。为了克服BOF的这一缺点，提出了空间金字塔方法，它是在不同分辨率上统计图像特征点分布，从而获取图像的空间信息。 图像被划分为金字塔各水平上的逐渐精细的网格序列，从每个网格中导出特征并组合为一个很大的特征向量。</p>
<ol style="list-style-type: decimal">
<li><strong>图像尺度空间</strong> SIFT中的图像尺度空间可以<font style="color:red">理解为用高斯对图像做了卷积，图像的分辨率还是那么大，像素还是那么多，只是细节被平均（平滑）掉了</font>，原因就是高斯了，用周围的信号比较弱的像素和中间那个信号比较强的点做平均，平均值当然比最强信号值小了，这就起到了平滑的作用。尺度可变高斯函数：<span class="math inline">\(G(x,y,\sigma)=\frac{1}{2\pi \sigma^2}\exp\{-(x^2+y^2)/2\sigma^2\}\)</span>, 结果如下图所示：</br> <img src="http://img.blog.csdn.net/20150707103907907?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" /></li>
<li><strong>图像金字塔</strong> 金字塔是图像多尺度表示的主要形式，图像金字塔是以多分辨率来解释图像的一种有效但概念简单的结构。一幅图像的金字塔是一系列以金字塔形状排列的分辨率逐步降低的图像集合。如下图所示。</br> <img src="http://img.blog.csdn.net/20150707104255163?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" /></li>
</ol>
<p><font style="color:red">图像金字塔化一般包括二个步骤：</font></p>
<pre><code>1、利用低通滤波器平滑图像；
2、对平滑图像进行抽样，从而得到一系列尺寸缩小的图像。</code></pre>
<ol start="3" style="list-style-type: decimal">
<li><strong>空间金字塔表示图像</strong> <strong>Discriminative Spatial Pyramid</strong>. 原始方法是<font style="color:red">首先提取原图像的全局特征</font>，然后在<font style="color:red">每个金字塔水平把图像划分为</font><font style="color:red">细网格序列</font>，从每个金字塔水平的每个网格中提取出特征，并把它们连接成一个<font style="color:red">大特征向量</font>。但由于图像中每个局部区域反映的信息量不同，由此提出<b>加权空间金字塔方法</b>，及给每层每网格分配一个权重，按权重把每层每网格特征加权串联在一起。如下图：</br> <img src="http://img.blog.csdn.net/20150707104442417?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" /></li>
</ol>
<p>左边图像是原始方法，右边是加权方法。<span class="math inline">\(f_k^l\)</span>表示第<span class="math inline">\(l\)</span>层第<span class="math inline">\(k\)</span>网格的特征向量，特征用<span class="math inline">\(d\)</span>维向量表示，<span class="math inline">\(c(l)\)</span>表示<span class="math inline">\(l\)</span>层金字塔的网格数。原始方法中，一幅图像的空间金字塔特征向量表示为<span class="math inline">\(f_s^T\)</span>,加权方法表示为<span class="math inline">\(f_W\)</span>，如下：</br> <span class="math display">\[\begin{cases}
f_s^T=({f_1^0}^T,{f_1^1}^T,\cdots, {f_{c(1)}^1}^T,\cdots,{f_{c(L-1)}^{L-1}}^T)\\
f_W=w_1^0f_1^0+w_1^1f_1^1+\cdots+w_{c(1)}^1f_{c(1)}^1+\cdots+w_1^{L-1}f_1^{L-1}+\cdots+w_{c(L-1)}^{L-1}f_{c(L-1)}^{L-1}
\end{cases}\]</span></p>
<p>《Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories》</p>
<p><strong>空间金字塔匹配SPM</strong>:空间金字塔匹配Spatial Pyramid Matching(SPM)，是一种利用空间金字塔进行图像匹配、识别、分类的算法。将level(i)的图像划分为pow(4,i)个cell（bins），然后再每一cell上统计直方图特征，最后将所有level的直方图特征连接起来组成一个vector，作为图形的feature。</p>
<ol start="4" style="list-style-type: decimal">
<li>Summary: Pyramid match kernel</li>
</ol>
<ul>
<li>假设存在2个特征集合<span class="math inline">\(X,Y\)</span>，其中每个特征<span class="math inline">\(X\)</span>的维度是<span class="math inline">\(d\)</span>.将特征空间划分为不同的尺度<span class="math inline">\(0,\cdots,L\)</span>,在尺度<span class="math inline">\(\ell\)</span>下把特征空间的每一维划分出<span class="math inline">\(2^{\ell}\)</span>个bins, 那么<span class="math inline">\(d\)</span>维的特征空间就能划出<span class="math inline">\(D=2^{\ell}\)</span>个bins.</li>
<li>在<span class="math inline">\(level(i)\)</span>中，如果点<span class="math inline">\(x,y\)</span>落入同一个bin就称<span class="math inline">\(x,y\)</span>点match. 每个bin种匹配的点的个数为<span class="math inline">\(\min(X_i,Y_i)\)</span>，其中<span class="math inline">\(X_i,Y_i\)</span>代表相应level下的第i个bin.</li>
<li><span class="math inline">\(H_X^{\ell}(i)\)</span>和<span class="math inline">\(H_Y^{\ell}(i)\)</span>表示<span class="math inline">\(level^{\ell}\)</span>种<span class="math inline">\(X,Y\)</span>落入第i个bin的特征点的个数，那么在level <span class="math inline">\(\ell\)</span>下的点的总数为:<span class="math inline">\(\mathcal{L}(H_X^{\ell},H_Y^{\ell})=\sum_{i=1}^D\min (H_X^{\ell}(i),H_Y^{\ell})(i)\)</span>.</br> <img src="http://img.blog.csdn.net/20150707105258060?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" /></li>
</ul>
<p>上面的黑圆点、方块、十字星代表一副图像上某个pitch属于k-means后词典中的某个词；</br> 1）将图像划分为固定大小的块，如从左到右：1<em>1，2</em>2，4*4， 然后统计每个方块中词中的不同word的个数； 2）从从左到右，统计不同level中各个块内的直方图； 3）最后个将每个level中获得的直方图都串联起来，并且给每个level赋给相应的权重，从左到右权重依次增大； 4）将SPM放入SVM中进行训练和预测；</p>
<p>论文中的实验过程如下：</br> 1）用 strong feature detector即SIFT进行特征检测，patch size=16<em>16，patch每次移动的步长spacing grid=8</em>8。 2）按照BOF相同的方法（即KMeans）构建包含M个words的dictionary。 3）利用图像金字塔把图像划分为多个scales的bins(空间金字塔分层分网格)，然后计算落入每个bins中属于不同类别的word的个数，则图像X、Y最终的匹配度为(M为关键词个数)：(个人对此匹配度核函数的理解是：这个核函数可当作SVM中的核函数，来匹配两幅图像是否为一类) <span class="math inline">\(K^L(X,Y)=\sum_{m=1}^Mk^L(X_m,Y_m)\)</span> 4）把所有level下的直方图特征链接起来组成一个维度为：<span class="math inline">\(M\sum_{\ell=0}^L4^{\ell}=M\frac{1}{3}\times(4^{L+1}-1)\)</span>的feature， 作为分类的特征向量。</p>
<div class="figure">
<img src="http://img.blog.csdn.net/20150707105517413?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" />

</div>
<p><a href="http://vision.stanford.edu/documents/CVPR2007_tutorial_bag_of_words.ppt">Spatial influence through correlogram features</a>: Savarese, Winn and Criminisi, CVPR 2006</p>
<h2 id="不变性问题invariance-issues">不变性问题(Invariance issues)</h2>
<p><font style="color:red">SIFT特征不只具有尺度不变性，即使改变旋转角度，图像亮度或拍摄视角，仍然能够得到好的检测效果。</font></p>
<h2 id="bow的缺点">BoW的缺点</h2>
<p>不包含物体组成的<font style="color:red">严格几何信息,而对我们来说，物体是由很多部分组成的。</font></p>
<h2 id="参考">参考</h2>
<ul>
<li><a href="http://yongyuan.name/blog/CBIR-BoW-for-image-retrieval-and-practice.html">图像检索：BoW图像检索原理与实战</a></li>
</ul>
</body>
</html>
