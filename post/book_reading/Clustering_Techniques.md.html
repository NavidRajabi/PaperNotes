<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="聚类">聚类</h1>
<p>当把聚类（Clustering）和分类（Classification）放到一起时，很容易弄混淆两者的概念，下分别对两个概念进行解释。</p>
<ol style="list-style-type: decimal">
<li><strong>聚类（Clustering）</strong> 将物理或抽象对象的集合分成由<font style="color:red">类似的对象组成的多个类</font>的过程被称为聚类。</br> 聚类分析的一般做法是，<font style="color:red">先确定聚类统计量</font>，然后利用统计量对样品或者变量进行聚类。对N个样品进行聚类的方法称为Q型聚类，<font style="color:red">常用的统计量称为“距离”</font>；对于m个变量进行聚类的方法称为R型聚类，常用的统计量称为“相似系数”。</li>
<li><strong>分类（Classification）</strong> 在已有分类标准下，对新数据进行划分，分类。</br> 常用分类算法：</br>
<ul>
<li><font style="color:red">朴素贝叶斯(Naive Bayes, NB)</font></br> 超级简单，就像做一些数数的工作。如果条件独立假设成立的话，NB将比鉴别模型（如Logistic回归）收敛的更快，所以你只需要少量的训练数据。即使条件独立假设不成立，NB在实际中仍然表现出惊人的好。如果你想做类似半监督学习，或者是既要模型简单又要性能好，NB值得尝试。</br></li>
<li><font style="color:red">Logistic回归(Logistic Regression, LR)</font></br> LR有很多方法来对模型正则化。比起NB的条件独立性假设，LR不需要考虑样本是否是相关的。与决策树与支持向量机（SVM）不同，NB有很好的概率解释，且很容易利用新的训练数据来更新模型（使用在线梯度下降法）。如果你想要一些概率信息（如，为了更容易的调整分类阈值，得到分类的不确定性，得到置信区间），或者希望将来有更多数据时能方便的更新改进模型，LR是值得使用的。</br></li>
<li><font style="color:red">决策树（Decision Tree, DT）</font></br> DT容易理解与解释（对某些人而言——不确定我是否也在他们其中）。DT是非参数的，所以你不需要担心野点（或离群点）和数据是否线性可分的问题（例如，DT可以轻松的处理这种情况：属于A类的样本的特征x取值往往非常小或者非常大，而属于B类的样本的特征x取值在中间范围）。DT的主要缺点是容易过拟合，这也正是随机森林（Random Forest, RF）（或者Boosted树）等集成学习算法被提出来的原因。此外，RF在很多分类问题中经常表现得最好（我个人相信一般比SVM稍好），且速度快可扩展，也不像SVM那样需要调整大量的参数，所以最近RF是一个非常流行的算法。</br></li>
<li><font style="color:red">支持向量机（Support Vector Machine, SVM）</font></br> 很高的分类正确率，对过拟合有很好的理论保证，选取合适的核函数，面对特征线性不可分的问题也可以表现得很好。SVM在维数通常很高的文本分类中非常的流行。由于较大的内存需求和繁琐的调参，我认为RF已经开始威胁其地位了。</br> 回到LR与DT的问题（我更倾向是LR与RF的问题），做个简单的总结：两种方法都很快且可扩展。在正确率方面，RF比LR更优。但是LR可以在线更新且提供有用的概率信息。鉴于你在Square(不确定推断科学家是什么，应该不是有趣的化身)，可能从事欺诈检测：如果你想快速的调整阈值来改变假阳性率与假阴性率，分类结果中包含概率信息将很有帮助。无论你选择什么算法，如果你的各类样本数量是不均衡的（在欺诈检测中经常发生），你需要重新采样各类数据或者调整你的误差度量方法来使各类更均衡。</li>
</ul></li>
</ol>
<p>那么，为什么要聚类？其实<font style="color:red">聚类就是将相同图片分类一类，什么叫相同图片？所谓相同图片就是图片之间的特征很相似</font>。聚类可以通过将特征相似的图片归为一类。聚类充当了一个特征空间组织的方法，所以，一个给定的特征空间可以被看作一个聚类，或者一个聚类集合。</p>
<h2 id="聚类的目的">聚类的目的</h2>
<p>无监督学习（也有人叫非监督学习，反正都差不多）则是另一种研究的比较多的学习方法，它与监督学习的不同之处，在于<font style="color:red">我们事先没有任何训练样本，而需要直接对数据进行建模</font>。无监督学习里典型的例子就是<font style="color:red">聚类</font>了。聚类的目的在于把相似的东西聚在一起，而我们并不关心这一类是什么。因此，一个聚类算法通常只需要知道如何计算相似度就可以开始工作了。</p>
<p>对聚类进行研究是数据挖掘中的一个热门方向，由于以上所介绍的聚类方法都存在着某些缺点，因此近些年对于聚类分析的研究很多都专注于改进现有的聚类方法或者是提出一种新的聚类方法。以下将对传统聚类方法中存在的问题以及人们在这些问题上所做的努力做一个简单的总结： 1. 从以上对传统的聚类分析方法所做的总结来看，不管是k-means方法，还是CURE方法，在<font style="color:red">进行聚类之前都需要用户事先确定要得到的聚类的数目</font>。然而<font style="color:red">在现实数据中，聚类的数目是未知的</font>，通常要经过不断的实验来获得合适的聚类数目，得到较好的聚类结果。</br> 2. 传统的聚类方法一般都是适合于某种情况的聚类，没有一种方法能够满足各种情况下的聚类，比如BIRCH方法对于球状簇有很好的聚类性能，<font style="color:red">但是对于不规则的聚类，则不能很好的工作</font>；K-medoids方法不太受孤立点的影响，但是其计算代价又很大。因此如何解决这个问题成为当前的一个研究热点，有学者提出<font style="color:red">将不同的聚类思想进行融合以形成新的聚类算法，从而综合利用不同聚类算法的优点，在一次聚类过程中综合利用多种聚类方法，能够有效的缓解这个问题</font>。</br> 3. 随着信息时代的到来，对大量的数据进行分析处理是一个很庞大的工作，这就关系到一个计算效率的问题。有文献提出了一种基于<font style="color:red">最小生成树的聚类算法</font>，该算法通过<font style="color:red">逐渐丢弃最长的边来实现聚类结果，当某条边的长度超过了某个阈值，那么更长边就不需要计算而直接丢弃</font>，这样就极大地提高了计算效率，降低了计算成本。</br> 4. 处理大规模数据和高维数据的能力有待于提高。目前许多聚类方法处理小规模数据和低维数据时性能比较好，但是当数据规模增大，维度升高时，性能就会急剧下降，比如k-medoids方法处理小规模数据时性能很好，但是随着数据量增多，效率就逐渐下降，而现实生活中的数据大部分又都属于规模比较大、维度比较高的数据集。有文献提出了一种在高维空间挖掘映射聚类的方法PCKA（Projected Clustering based on the K-Means Algorithm），它从多个维度中选择属性相关的维度，去除不相关的维度，沿着相关维度进行聚类，以此对高维数据进行聚类。</br> 5. 目前的许多算法都只是理论上的，经常处于某种假设之下，比如聚类能很好的被分离，没有突出的孤立点等，但是现实数据通常是很复杂的，噪声很大，因此如何有效的消除噪声的影响，提高处理现实数据的能力还有待进一步的提高。</p>
<h2 id="聚类的一些概念">聚类的一些概念</h2>
<ol style="list-style-type: decimal">
<li><font style="color:red">距离测量</font>
<ul>
<li>在结构性聚类中，关键性的一步就是要选择测量的距离。一个简单的测量就是使用曼<font style="color:red">哈顿距离，它相当于每个变量的绝对差值之和</font>。该名字的由来起源于在纽约市区测量街道之间的距离就是由人步行的步数来确定的。</li>
<li>一个更为常见的测量是<font style="color:red">欧式空间距离，他的算法是找到一个空间，来计算每个空间中点到原点的距离，然后对所有距离进行换算</font>。</li>
<li>常用的几个距离计算方法：欧式距离（2-norm距离）;曼哈顿距离（Manhattan distance, 1-norm距离）;infinity norm;马氏距离;余弦相似性;汉明距离</li>
</ul></li>
<li>一个好的聚类系统应该具备：<font style="color:red">(簇内距离)最小化；(簇间距离)最大化</font></li>
</ol>
<p><font style="color:red">假设我们有<span class="math inline">\(n\)</span>个点，想要分成<span class="math inline">\(k\)</span>类，那么有多少种可能呢？<span class="math inline">\(\frac{k^n}{k!}\)</span>. 但是实际是很难优化的，所以我们一般采用迭代的方法，例如 K-means。</font></p>
<h2 id="k-means-algorithm">K-Means Algorithm</h2>
<p>通常，人们根据样本间的某种距离或者相似性来定义聚类，即把相似的（或距离近的）样本聚为同一类，而把不相似的（或距离远的）样本归在其他类。我们以一个二维的例子来说明下聚类的目的。</p>
<p>如下图左所示，假设我们的n个样本点分布在图中所示的二维空间。从数据点的大致形状可以看出它们大致聚为三个cluster，其中两个紧凑一些，剩下那个松散一些。我们的目的是为这些数据分组，以便能区分出属于不同的簇的数据，如果按照分组给它们标上不同的颜色，就是像下图右边的图那样：</br> <img src="http://img.blog.csdn.net/20131226190225921?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvem91eHkwOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" /></p>
<p>如果人可以看到像上图那样的数据分布，就可以轻松进行聚类。但我们怎么教会计算机按照我们的思维去做同样的事情呢？这里就介绍个集简单和经典于一身的k-means算法。</p>
<p>K-means是一种经典unsupervised的聚类算法。它的基本思想是：<strong>通过迭代寻找k个聚类的一种划分方案，使得用这k个聚类的均值来代表相应各类样本时所得的总体误差最小</strong>。其算法描述如下：</p>
<p>输入：聚类个数-k，N个数据对象 输出：每个数据对象所属的聚类label（满足方差最小）</p>
<p><font style="color:red">k-means算法的基础是最小误差平方和准则。其代价函数是：</font> <span class="math display">\[
J(c,\mu)=\sum_{i=1}^k||x^{(i)}-\mu_c^{(i)}||^2
\]</span> 式中，<span class="math inline">\(\mu_c^{(i)}\)</span>表示第<span class="math inline">\(i\)</span>个聚类的均值。我们希望代价函数最小，直观的来说，各类内的样本越相似，其与该类均值间的误差平方越小，对所有类所得到的误差平方求和，即可验证分为k类时，各聚类是否是最优的。</p>
<p>上式的代价函数无法用解析的方法最小化，只能有迭代的方法。k-means算法是将样本聚类成 <font style="color:red">k个簇（cluster），其中k是用户给定的</font>，其求解过程非常直观简单，<strong>K-means 步骤如下：</strong>:</br> 1. 从N个数据对象中挑选出k个质心。 2. 计算N个数据对象距离不同质心的距离$c^{(i)}=_j ||x^{(i)}-_j||^2 <span class="math inline">\(，并将N个数据对象划分到与之距离最小的质心，形成新的k个聚类。 3. 重新计算步骤2中获取新的k个聚类的质心，计算方法为求取聚类中所有数据对象的均值\)</span>_j=$。 4. 重复2-3步骤，直到每个聚类的数据成员不再发生变化。</p>
<p><strong>K-means主要的缺点如下：</strong> 1. <font style="color:red">运行速度</font>。虽然通常情况下，k-means执行的循环次数要少于数据对象的个数。但是对于worst cases，其执行的时间复杂度将是super-polynomial的。数据库比较大的时候，收敛会比较慢。 2. <font style="color:red">K值的选取</font>。在执行程序前，需要给定K值的大小。然而对于不同的K值，划分的结果当然不同，因此确定最合适的K值非常关键。 3. <font style="color:red">初始化K个centroid</font>。centroid的初始选取对于划分结果亦非常关键。对k个初始质心的选择比较敏感，容易陷入局部最小值。 4. <font style="color:red">K-means对于数据不同的维度“一视同仁”，缺乏轻重之分。</font></p>
<p><strong>k-means聚类的时间复杂度</strong> 1. 计算m个矢量距离，复杂度是<span class="math inline">\(O(m)\)</span> 2. 重新计算，复杂度<span class="math inline">\(O(knm)\)</span> 3. 计算中心，复杂度<span class="math inline">\(O(nm)\)</span> 4. 假设需要I个循环，<span class="math inline">\(O(Ikmn)\)</span></p>
<h2 id="层次聚类">层次聚类</h2>
<p><font style="color:red">层次法（Hierarchical methods）先计算样本之间的距离,每次将距离最近的点合并到同一个类</font>。然后，再计算类与类之间的距离，<font style="color:red">将距离最近的类合并为一个大类</font>。不停的合并，直到合成了一个类。其中类与类的距离的计算方法有：最短距离法，最长距离法，中间距离法，类平均法等。比如最短距离法，将类与类的距离定义为类与类之间样本的最短距离。</p>
<p><font style="color:red">层次聚类算法</font>根据层次分解的顺序分为：自下底向上和自上向下，即凝聚的层次聚类算法和分裂的层次聚类算法（agglomerative和divisive），也可以理解为自下而上法（bottom-up）和自上而下法（top-down）。</p>
<p><font style="color:red">自下而上法就是一开始每个个体（object）都是一个类，然后根据linkage寻找同类，最后形成一个“类”。</font></p>
<p>自上而下法就是反过来，一开始所有个体都属于一个“类”，然后根据linkage排除异己，最后每个个体都成为一个“类”。这两种路方法没有孰优孰劣之分，只是在实际应用的时候要根据数据特点以及你想要的“类”的个数，来考虑是自上而下更快还是自下而上更快。至于根据Linkage判断“类”的方法就是最短距离法、最长距离法、中间距离法、类平均法等等（其中类平均法往往被认为是最常用也最好用的方法，一方面因为其良好的单调性，另一方面因为其空间扩张/浓缩的程度适中）。为弥补分解与合并的不足，层次合并经常要与其它聚类方法相结合，如循环定位。</p>
<p><font style="color:red">Hierarchical methods中比较新的算法</font>有BIRCH（Balanced Iterative Reducingand Clustering Using Hierarchies利用层次方法的平衡迭代规约和聚类）主要是在数据量很大的时候使用，而且数据类型是numerical。首先利用树的结构对对象集进行划分，然后再利用其它聚类方法对这些聚类进行优化；ROCK（A Hierarchical ClusteringAlgorithm for Categorical Attributes）主要用在categorical的数据类型上；Chameleon（A Hierarchical Clustering AlgorithmUsing Dynamic Modeling）里用到的linkage是kNN（k-nearest-neighbor）算法，并以此构建一个graph，Chameleon的聚类效果被认为非常强大，比BIRCH好用，但运算复杂度很高，O(n^2)。</p>
<p><font style="color:red">凝聚型层次聚类(Hierarchical agglomerative Clustering)的策略是先将每个对象作为一个簇，然后合并这些原子簇为越来越大的簇</font>，直到所有对象都在一个簇中，或者某个终结条件被满足。绝大多数层次聚类属于凝聚型层次聚类，它们只是在簇间相似度的定义上有所不同。</p>
<p>这里给出采用<font style="color:red">最小距离的凝聚层次聚类算法流程</font>： - (1) 将每个对象看作一类，计算两两之间的最小距离； - (2) 将距离最小的两个类合并成一个新类； - (3) 重新计算新类与所有类之间的距离； - (4) 重复(2)、(3)，直到所有类最后合并成一类。</p>
<p>该算法的特点有如下几点： 1. 凝聚聚类耗费的存储空间相对于其他几种方法要高。 2. 可排除噪声点的干扰，但有可能噪声点分为一簇。 3. 适合形状不规则，不要求聚类完全的情况。 4. 合并操作不能撤销。 5. 应注意，合并操作必须有一个合并限制比例，否则可能发生过度合并导致所有分类中心聚集，造成聚类失败。</p>
</body>
</html>
