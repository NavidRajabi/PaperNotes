<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h2 id="关于sparse-coding">关于Sparse Coding</h2>
<h2 id="基本概念">基本概念</h2>
<p>基本概念参考了UFLDL。 对于每个输入<span class="math inline">\(x\)</span>，需要找到一组基<span class="math inline">\(\phi\)</span>来进行重构，形式化表示为：<span class="math inline">\(\mathbf{x}=\sum_{i=1}^ka_i\phi_i\)</span></p>
<p>其中<span class="math inline">\(\phi\)</span>的维度<span class="math inline">\(k\)</span>是大于<span class="math inline">\(x\)</span>的维度<span class="math inline">\(n\)</span>的，称为over-complete(超完备)，它的好处是：</p>
<blockquote>
<p>better able to capture structures and patterns inherent in the input data</p>
</blockquote>
<p>然后由于超完备，所以对于固定的<span class="math inline">\(\phi\)</span>，每个输入是不止一组解的，也就是对于同一个<span class="math inline">\(x\)</span>有多个<span class="math inline">\(a\)</span>，所以这里引入sparse的概念，其实sparse应该算是一个正则项，约束了条件，让大部分a都为0。而sparse的motivation是：</p>
<blockquote>
<p>The choice of sparsity as a desired characteristic of our representation of the input data can be motivated by the observation that most sensory data such as natural images may be described as the superposition of a small number of atomic elements such as surfaces or edges.</p>
</blockquote>
<p>于是求解输入x的sparse形式，优化目标可以写成下列形式： <span class="math display">\[
\text{minimize}_{a_i^{(j)},\phi_i}\sum_{j=1}^m||\mathbf{x}^{(j)}-\sum_{i=1}^ka_i^{(j)}\phi_i||^2+\lambda \sum_{i=1}^k S(a_i^{(j)})
\]</span></p>
<p>有两项，第一项称为reconstruction term，第二项S(.)称为sparsity penalty。也就是理解成，在一个稀疏的约束下最小化重构误差的问题。 最原始<span class="math inline">\(S(.)\)</span>是0范式，但是不好求解，所以一般会会用1范式，2范式，或者log形式。 另外由于上面式子可以通过缩写a而增大φ得到类似的解，所以需要给φ增加一个上限C，式子就变成了： <span class="math display">\[
\text{minimize}_{a_i^{(j)},\phi_i}\sum_{j=1}^m||\mathbf{x}^{(j)}-\sum_{i=1}^ka_i^{(j)}\phi_i||^2+\lambda \sum_{i=1}^k S(a_i^{(j)}), \ \text{subject to } ||\phi_i||^2\le C,\forall i=1,...,n
\]</span></p>
<p>看式子的形式，就跟soft margin下的SVM的loss挺像的了，SVM的是这样子的：</p>
<p><span class="math display">\[
\min\frac{1}{2}||w||^2+C\sum_{i=1}^n\xi_i, \ s.t., \ y_i(w^Tx_i+b)\ge 1-\xi_i, i=1,...,n, \ \xi_i\ge 0,i=1,...,n
\]</span></p>
<p>第一项可以看成正则项约束，第二项可以看成是分类误差。</p>
<p>至于怎么求解参数，这份教程简单提了一下，由于需要求解两个未知的参数a和φ，所以需要用到EM-like的方法： 1. 对于每个样本优化a 2. 对于所有样本优化<span class="math inline">\(\phi\)</span></p>
<p>循环上述两步知道收敛。 而测试时候，需要进行第一步，所以sparse coding在测试时候会比较慢。</p>
<h2 id="sparse-coding和sparse-autoencoder的联系">sparse coding和sparse autoencoder的联系</h2>
<p>依然是参考UFLDL： &gt; http://ufldl.stanford.edu/wiki/index.php/Autoencoders_and_Sparsity &gt; http://ufldl.stanford.edu/wiki/index.php/Sparse_Coding:_Autoencoder_Interpretation</p>
<p>简单的理解sparse autoencoder(SAE)，一般的autoencoder的隐藏神经元是比输入输出都要少的，这样能够学习到“压缩”的特征。而SAE刚刚相反，隐藏神经元是大于输入输出的，但是它增加了sparse约束，具体理解跟上面sparse是同一个东西：超正定下但是很多系数为0，也能够学习到输入样本的结构。</p>
<p>而对于SAE的sparse，可以理解成是隐层神经元<a href="">尽量都不要被激活</a>，所以我们可以通过在loss增加一项正则项来限制隐层神经元的平均激活小于某个特定值<span class="math inline">\(\rho\)</span>，形式化表示为：</p>
<p><span class="math display">\[
\sum_{j=1}^{s2}\rho \log\frac{\rho}{\hat{\rho}_j}+(1-\rho)\log\frac{1-\rho}{1-\hat{\rho}_j}
\]</span></p>
<p>回到sparse coding，它跟SAE有什么关系了，教程只写了一句话： &gt; Sparse coding can be seen as a modification of the sparse autoencoder method in which we try to learn the set of features for some data “directly“. Together with an associated basis for transforming the learned features from the feature space to the data space, we can then reconstruct the data from the learned features.</p>
<p>怎么理解呢，关键应该就是这个directly了。对于SAE的特征学习过程，其实算是indirectly的，因为SAE的特征是在隐神经元的，优化目标其实重构输入，而特征算是在该目标下的一种副产品；另外，个人理解，SAE的稀疏性约束也是比较弱的。另一方面，sparse coding（SC）的目的很明确，就是要学习特征，只不过这些特征能够重构出输入，学习特征是主体，也就是重构反而是学习特征的一种约束。</p>
<p>但其实我觉得本质上还是大同小异的，只是突出了不同的主体，SAE的主体是重构，SC的主体是特征学习，其实从loss来说也都是一项重构项加一项稀疏项，主要是看哪个比较重要咯。</p>
<p>另外，这份教程还给出了SC更加紧凑一个优化形式： <span class="math display">\[
\text{minimize }||As-x||_2^2+\lambda||s||_1, \ \ s.t \ A_j^TA_j\le 1 \ \forall j
\]</span></p>
<p>跟上面的式子相比： <span class="math display">\[
\text{minimize}_{a_i^{(j)},\phi_i}\sum_{j=1}^m||\mathbf{x}^{(j)}-\sum_{i=1}^ka_i^{(i)}\phi_i||^2+\lambda \sum_{i=1}^k S(a_i^{(j)}), \ \text{subject to } ||\phi_i||^2\le C,\forall =1,...,k.
\]</span></p>
<p>本质是一样的，这里的A相当于上面的<span class="math inline">\(\phi\)</span>，<span class="math inline">\(s\)</span>相当于上面的a。 为了方便求解，可以把上式的s.t.也统一到式子中： <span class="math display">\[
J(A,s)=||As-x||_2^2+\lambda ||s||_1+\gamma||A||_2^2
\]</span></p>
</body>
</html>
