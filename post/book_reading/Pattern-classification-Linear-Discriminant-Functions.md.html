<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h2 id="线性判别函数"><span style="color:red"><strong><em>线性判别函数</em></strong></span></h2>
<h3 id="从参数估计到线性判别函数"><span style="color:red"><strong><em>从参数估计到线性判别函数</em></strong></span></h3>
<p>对一个决策理论来说，我们主要的想法是看一个样本应该分到某个类. 我们以判别函数来定义它，比如两类 1 和 2 类就有判别函数<span class="math inline">\(g_1(x), g_2(x)\)</span>.</p>
<p>一般就看谁的值大，我们就觉得分到那个类里. <span style="color:red"><strong><em>当我们不知道 <span class="math inline">\(x\)</span>的任何信息时</em></strong></span>， 可以<span style="color:red"><strong><em>直接用常识，比如 1 类和 2 类的全部概率<span class="math inline">\(P(c_1), P(c_2)\)</span></em></strong></span>.</p>
<p>即定义: <span class="math display">\[g_i(x) = P(c_i).\]</span></p>
<p>这样错误率会很高，于是我们考虑用<span class="math inline">\(x\)</span> 的已知信息即它的特征. 应用贝叶斯公式: <span class="math inline">\(g_i(x) = P(c_i \vert x) = \frac{p(x \vert c_i) P(c_i)}{p(x)}\)</span> 对<span class="math inline">\(P(c_i)\)</span>依然是用到的常识，但是新的一项<span class="math inline">\(p(x \vert c_i)\)</span>就似乎 要求我们知道<span class="math inline">\(x\)</span>的真正密度.</p>
<p>但真实的密度我们真不知道. 不过我们预先收集了这个密度中生成的一些样本 即<span class="math inline">\(D=\{x_1, ..., x_k\}\)</span>，并且假设它们是 i.i.d.</p>
<p>不失一般性，这里可以<span style="color:red"><strong><em>假设样本都是同一类的，这样可以省去对应的下标和密度的下标</em></strong></span>.</p>
<p>于是有 <span style="color:red"><strong><em>MLE (最大似然估计)方法：我们假设知道密度的形式，只是某个参数不知道，那么就求参数</em></strong></span>. 我们已经有了一堆样本，某个参数可以让我们<span style="color:red"><strong><em>得到这些样本的”可能性”(似然)最大</em></strong></span>， 就以该参数值作为我们对真实参数的一个估计. <span class="math display">\[\hat \theta_i
= \arg \max_{\theta_i} P(\mathcal D \vert \theta_i)
= \arg \max_{\theta_i} \prod p(x_k \vert \theta_i)\]</span></p>
<blockquote>
<p><span style="color:red"><strong><em>例子</em></strong></span></p>
</blockquote>
<p>我们将MLE估计应用到特定的例子，例如：训练样本是服从多元高斯分布,但是均值<span class="math inline">\(\mathbf{\mu}\)</span>和协方差<span class="math inline">\(\mathbf{\Sigma}\)</span>未知.</p>
<p><span class="math display">\[
p(\mathbf{x}|\theta)=\frac{1}{(2\pi)^{d/2}|\mathbf{\Sigma}|^{1/2}}\text{exp}[-\frac{1}{2}(\mathbf{x}-\mathbf{\mu})^T\mathbf{\Sigma}^{-1}(\mathbf{x}-\mathbf{\mu})]
\]</span> 我们对上面的求对数得到：<span class="math inline">\(\text{ln}p(\mathbf{x}|\mathbf{\theta})=\frac{1}{2}(\mathbf{x}-\mu)\Sigma^{-1}(\mathbf{x}-\mu)^T-\frac{1}{2}\text{ln}(2\pi)^d |\Sigma|\)</span> 其中，<span class="math inline">\(d\)</span>是维度．我们考虑简单的情况，当维度是１的时候．<span class="math inline">\(\mathbf{\theta}\)</span>只有２个参数<span class="math inline">\(\mathbf{\theta}=\begin{bmatrix}\mu\\\sigma^2\end{bmatrix}\)</span></p>
<p>这时候有<span class="math inline">\(\text{ln}p(x_k|\mathbf{\theta})=\frac{1}{2\sigma^2}(x_k-\mu)^2-\frac{1}{2}\text{ln}(2\pi) \sigma^2\)</span></p>
<p>对其求导数：<span class="math display">\[\nabla_{\theta} \text{ln} p(x_k|\theta)=\begin{bmatrix}\frac{1}{\delta^2}(x_k-\mu)\\-\frac{1}{2\delta^2}+\frac{(x_k-\mu)^2}{2\delta^4}\end{bmatrix}\]</span></p>
<p>对多个变量累加求和得到：<span class="math display">\[\begin{cases}
\sum_{k=1}^n\frac{1}{\sigma^2}(x_k-\mu)=0\\
-\sum_{k=1}^n \frac{1}{\delta^2}+\sum_{k=1}^n\frac{(x_k-\mu)}{2\delta^4}=0\end{cases}\Rightarrow
\begin{cases}\hat{\mu}=\frac{1}{n}\sum_{k=1}^nx_k\\
\hat{\sigma}^2=\frac{1}{n}\sum_{k=1}^n(x_k-\hat{\mu})^2
\end{cases}
\]</span></p>
<p>应用到多元：<span class="math display">\[\nabla_{\theta} \text{ln} p(\mathbf{D}|\theta)=\sum_{k=1}^n\nabla_{\theta} \text{ln} p(\mathbf{x_k}|\theta)=\begin{bmatrix}\sum_{k=1}^n\frac{1}{\Sigma}(\mathbf{x_k}-\hat{\mu})\\-\sum_{k=1}^n\frac{1}{2\Sigma}+\sum_{k=1}^n\frac{(\mathbf{x_k}-\hat{\mu})^2}{2\Sigma^2}\end{bmatrix}\Rightarrow
\begin{cases}\hat{\mu}=\frac{1}{n}\sum_{k=1}^n\mathbf{x_k}\\
\hat{\Sigma}=\frac{1}{n}\sum_{k=1}^n(\mathbf{x_k}-\hat{\mu})^T(\mathbf{x_k}-\hat{\mu})
\end{cases}\]</span></p>
<p>以上就是最大似然估计，<span style="color:red"><strong><em>最大似然估计是一种统计方法，它用来求一个样本集的相关概率密度函数的参数</em></strong></span>。</p>
<p><span style="color:red"><strong><em>似然函数<span class="math inline">\(\text{ln} p(\mathbf{D}|\theta)\)</span></em></strong></span>指明了观测的样本作为可能参数值函数的几率有多大。因此，通过最大化似然函数，可以确定最可能产生观测数据的参数.</p>
<p>另有贝叶斯方法: <span style="color:red"><strong><em>我们知道也假设密度的形式在知道某些参数后就确定了</em></strong></span>， 也要估计参数. 但是参数是一个随机变量，也是有分布的.</p>
<p>我们相当于改而求密度的边缘分布，把所有的参数可能性取值积分掉： <span class="math display">\[
p(x \vert c_i) = p(x \vert D[, c_i])
       = \int p(x, \theta \vert D) d\theta
       = \int p(x \vert \theta) p(\theta \vert D) d\theta
       \]</span> <span class="math display">\[
\text{where } p(\theta \vert D)
= \frac{P(D \vert \theta) p(\theta)}{P(D)}
= \frac{P(D \vert \theta) p(\theta)}{\int P(D \vert \theta) p(\theta) d\theta}
= \alpha P(D \vert \theta) p(\theta)
= \alpha \prod_k P(x_k \vert \theta) p(\theta)
\]</span> 而我们已经假设出 <span class="math inline">\(P(x_k \vert \theta)\)</span> 和<span class="math inline">\(p(\theta)\)</span>的具体形式，比如正态， 就又能求得估计了. 重新回到开头的式子：<span class="math inline">\(g_i(x) = \frac{p(x \vert c_i) P(c_i)}{p(x)}\)</span></p>
<p>前面都是假设右边第一项密度是某种已知形式，通过把样本集合引入进来，再估计出参数值. <span style="color:red"><strong><em>也有无参数的方法来得到密度函数， 如 KNN 和 parzen window 等</em></strong></span>，利用已有数据集，<span style="color:red"><strong><em>在特征空间上直接估计某块区域的密度，并没有求某个参数的值，而是直接把所输入的<span class="math inline">\(x\)</span>代进来，去求属于它的概率</em></strong></span>. 一般工程实现上可能需要对样本集建好索引，因为我们没有参数，每次分类都是需要这些样本数据来判断的.</p>
<p>同时如果我们<span style="color:red"><strong><em>直接建模判别函数<span class="math inline">\(g_i(x)\)</span>,而不知道也不关心原本的密度形式是什么，那么 线性判别函数就是一种候选办法</em></strong></span>. 比如直接用线性模型并人工制造一些特征来训练参数. 这里稍微有点混淆，如果线性判别函数恰好也是我们假设的密度函数（甚至还是 <span class="math inline">\(x\)</span> 在真实世界的概率密度），那么我们就可以在相同意义上讨论线性模型. 但如果真实的世界不是线性模型，那除非我们能找到各种扭曲的特征，否则可以预见一个模型的分类能力肯定是有个上限的，并且这个天花板还是小于 100% 的.</p>
<ol style="list-style-type: decimal">
<li><span style="color:red"><strong><em>无参数估计(non parametric estimation)</em></strong></span> <span style="color:red"><strong><em>概率密度函数的估计</em></strong></span>：
<ol style="list-style-type: decimal">
<li><p><span style="color:red"><strong><em>histogram</em></strong></span>: count sample number inside a region</p></li>
<li><span style="color:red"><strong><em>K-nn</em></strong></span>: fix sample count <span class="math inline">\(k\)</span>, volume <span class="math inline">\(V\)</span> varies fix sample count <span class="math inline">\(k\)</span> in a local region, volume <span class="math inline">\(V\)</span> varies:<span class="math inline">\(p_n(x)=\frac{k_n/n}{V_n}, k_n=\sqrt{n}\)</span> 1D example figure: nearest-K samples, the smaller radius gives greater probability</li>
<li><span style="color:red"><strong><em>Parzen window</em></strong></span>: volume <span class="math inline">\(V\)</span> fixed, <span class="math inline">\(k\)</span> may vary window function, ( weight function, or can be a kernel function ) <span class="math inline">\(\phi(u)=\begin{cases} 1, \vert \mu_j \vert \leq 1/2, j=1, ..., d \\ 0, otherwise \end{cases}\)</span> kernel function condition:<span class="math inline">\(\phi(x) \geq 0, \int \phi(u)du=1 ,u=\frac{x - x_i}{h_n}\)</span> common kernels:
<ol style="list-style-type: decimal">
<li>sqare window <span class="math inline">\(\phi(u)=\begin{cases} 1 &amp; \vert \mu_j \vert \leq 1/2, j=1, ..., d \\ 0 &amp; otherwise \end{cases}\)</span></li>
<li>gaussian kernel</li>
<li>exponential window</li>
<li>hypersphere kernel</li>
</ol></li>
</ol></li>
</ol>
<h3 id="线性判别函数与判定面"><span style="color:red"><strong><em>线性判别函数与判定面</em></strong></span></h3>
<p>分类器的设计有两种思路：<span style="color:red"><strong><em>基于判别函数的分类器和基于距离的分类器</em></strong></span>。本节我们开始介绍基于判别函数的分类器中的线性判别函数法。</p>
判别函数法是<span style="color:red"><strong><em>对不同模式进行分类有很多方法中应用比较广的一种</em></strong></span>。 所谓判别函数就是<span style="color:red"><strong><em>指直接用来对模式样本进行分类的准则函数</em></strong></span>。
<p align="center">
<img src="http://img.blog.csdn.net/20140228151228515" width="600" >
</p>
<p>上图给了我们一个直观的理解，<span style="color:red"><strong><em>第一个图中的蓝色直线代表一个线性分类器，而第二个图中的绿色曲线代表一个非线性分类器</em></strong></span>。</p>
<ol style="list-style-type: decimal">
<li><span style="color:red"><strong><em>２类情况</em></strong></span></li>
</ol>
<p>例如在两类别问题中，按最小错误率作决策时，决策规则的一种形式是: Decide : <span class="math inline">\(w_k=\text{argmax}_{w_k}[p(w_i|\mathbf{x})]=\text{argmin}_{w_k}[p(e_i|\mathbf{x})]\)</span></p>
<p>则相应的判别函数就是：<span class="math inline">\(\begin{cases}g_1(\mathbf{x})=p(w_1|\mathbf{x})\\g_2(\mathbf{x})=p(w_2|\mathbf{x})\end{cases}\)</span> 而决策面方程则可写成: <span class="math inline">\(g_1(\mathbf{x})=g_2(\mathbf{x})\)</span> 此时决策规则也可以写成用判别函数表示的形式: 　如果<span class="math inline">\(g_1(\mathbf{x})&gt;g_2(\mathbf{x})\)</span>, 则<span class="math inline">\(\mathbf{x}\in w_1\)</span>, 否则<span class="math inline">\(\mathbf{x}\in w_2\)</span>．</p>
<p>此外，决策面是一种统称，当特征空间只是一维时，一个决策面实际上只是<span style="color:red"><strong><em>一个点</em></strong></span>。在二维特征空间里，决策面是<span style="color:red"><strong><em>一条曲线</em></strong></span>。三维则是<span style="color:red"><strong><em>一曲面</em></strong></span>，超过三维的空间，决策面是一个<span style="color:red"><strong><em>超曲面</em></strong></span>。</p>
<ol start="2" style="list-style-type: decimal">
<li><span style="color:red"><strong><em>多类情况</em></strong></span></li>
</ol>
<p>至于多类别情况，则对应于一种决策规则要定义一组判别函数: <span class="math inline">\(g_i(\mathbf{x}),i=1,2,...,c\)</span>. 而决策规则可表示成: 如果<span class="math inline">\(g_i(\mathbf{x})=\text{argmax}_{w_j}g_j(\mathbf{x})\)</span>,　 则将<span class="math inline">\(\mathbf{x}\)</span>归于<span class="math inline">\(w_i\)</span>类． 多类别情况下的决策面方程比两类问题复杂，并且只有<span style="color:red"><strong><em>在特征空间中具有相邻关系的决策域的边界面才是有意义的决策面</em></strong></span>。</p>
<p>当<span class="math inline">\(w_i\)</span>的决策域与<span class="math inline">\(w_j\)</span>的<span style="color:red"><strong><em>决策域相邻</em></strong></span>时，以下关系决定了相应的决策面<span class="math inline">\(g_i(\mathbf{x})=g_j(\mathbf{x})\)</span></p>
<p><span style="color:red"><strong><em>讨论完决策原理后，我们就可以开始设计</em></strong></span>：</p>
<p>首先，我们知道多维分类器的贝叶斯决策是：最大化：　<span class="math inline">\(p(w_i|\mathbf{x})=p(w_i)p(\mathbf{x}|w_i)p(\mathbf{x})^{-1}\)</span> 对数化后得到<span style="color:red"><strong><em>判别函数</em></strong></span>：<span class="math inline">\(\begin{cases} g_1(\mathbf{x})=\text{ln}p(\mathbf{x}|w_1)+\text{ln}p(w_1) ,\text{ Decision area } R_1\\ g_2(\mathbf{x})=\text{ln}p(\mathbf{x}|w_2)+\text{ln}p(w_2) ,\text{ Decision area } R_2\\ ...\\ g_c(\mathbf{x})=\text{ln}p(\mathbf{x}|w_c)+\text{ln}p(w_c),\text{ Decision area } R_c \end{cases}\)</span> 想要区分2个类，条件： <span style="color:red"><strong><em>在特征空间中具有相邻关系的决策域</em></strong></span>. 例如，我们想要类<span class="math inline">\(w_i\)</span>和<span class="math inline">\(w_j\)</span>的决策边界，我们令<span class="math inline">\(g_1(\mathbf{x})=g_2(\mathbf{x})\)</span>，然后会<span style="color:red"><strong><em>得到一个关于<span class="math inline">\(\mathbf{x}\)</span>的决策方程</em></strong></span>．</p>
<p>最典型的，类条件PDF服从多元高斯分布( <span style="color:red"><strong><em>不同维度具有不同的均值和协方差</em></strong></span>) <span class="math display">\[p(\mathbf{x}|w_i)=N(\mu_i,\Sigma_i)=\frac{1}{(2\pi)^{d/2}|\mathbf{\Sigma}_i|^{1/2}}\text{exp}[-\frac{1}{2}(\mathbf{x}-\mathbf{\mu}_i)^T\mathbf{\Sigma}_i^{-1}(\mathbf{x}-\mathbf{\mu}_i)]\]</span> 那么决策函数就是：<span class="math display">\[
\begin{cases}
g_i(\mathbf{x}_i)=-\frac{1}{2}(\mathbf{x}-\mathbf{\mu}_i)^T\mathbf{\Sigma}_i^{-1}(\mathbf{x}-\mathbf{\mu}_i)+\text{ln}p(w_i)-\frac{1}{2}\text{ln}((2\pi)^d |\Sigma_i|)\\
g_i(\mathbf{x}_i)=-\frac{1}{2}d_{\Sigma_i}(\mathbf{x},\mu_i)+b_i\\
d_{\Sigma_i}(\mathbf{x},\mu_i)=(\mathbf{x}-\mathbf{\mu}_i)^T\mathbf{\Sigma}_i^{-1}(\mathbf{x}-\mathbf{\mu}_i)\\
b_i=\text{ln}p(w_i)-\frac{1}{2}\text{ln}((2\pi)^d |\Sigma_i|)
\end{cases}
\]</span></p>
<p>这里，<span class="math inline">\(d_{\Sigma_i}(\mathbf{x},\mu_i)=(\mathbf{x}-\mathbf{\mu}_i)^T\mathbf{\Sigma}_i^{-1}(\mathbf{x}-\mathbf{\mu}_i)\)</span>叫做马氏距离．</p>
<blockquote>
<p>马氏距离是由印度统计学家马哈拉诺比斯(P. C. Mahalanobis)提出的，表示数据的协方差距离。</p>
</blockquote>
<p>它是一种<span style="color:red"><strong><em>有效的计算两个未知样本集的相似度的方法</em></strong></span>。与欧氏距离不同的是它考虑到各种特性之间的联系（例如：一条关于身高的信息会带来一条关于体重的信息，因为两者是有关联的）并且是尺度无关的(scale-invariant)，即独立于测量尺度。</p>
<p>对于一个均值为<span class="math inline">\(\mu=(\mu_1,\mu_2,...,\mu_m)^T\)</span>,协方差矩阵为<span class="math inline">\(\Sigma\)</span>的多变量向量<span class="math inline">\(\mathbf{X}=(\mathbf{x}_1,\mathbf{x}_2,...,\mathbf{x}_m)\)</span>,,其马氏距离为</p>
<p><span class="math inline">\(d_{\Sigma}=(\mathbf{X}-\mu)^T\Sigma^{-1}(\mathbf{X}-\mu)\)</span></p>
<p>若协方差矩阵是单位矩阵（<span style="color:red"><strong><em>各个样本向量之间独立同分布</em></strong></span>）,则公式就成了： <span class="math inline">\(d_{Eu}(\mathbf{x},\mu_i)=(\mathbf{x}-\mathbf{\mu}_i)^T(\mathbf{x}-\mathbf{\mu}_i)\)</span> 也就是欧氏距离了。即：<span style="color:red"><strong><em>若协方差矩阵是对角矩阵，公式变成了标准化欧氏距离</em></strong></span>。</p>
<p>协方差矩阵逆的作用是什么？本人直观理解是，<span style="color:red"><strong><em>协方差的逆起到了归一化的作用,使用协方差矩阵的逆矩阵去掉单位而使之尺度无关. 比如当协方差是对角阵的时候，当lamuda越大的时候，其逆就越小，一定程度上归一化了不同变量间量纲的影响</em></strong></span>. 例如一维时，欧氏距离是<span class="math inline">\(d_{Eu}(x,\mu_i)=(x-\mu_i)^2\)</span>，　马氏距离是<span class="math inline">\(d_{\Sigma}=(x-\mu_i)^/\delta^2\)</span></p>
<blockquote>
<p><span style="color:red"><strong><em>特例１，所有类具有相同的协方差矩阵<span class="math inline">\(\Sigma_i=\Sigma\)</span></em></strong></span></p>
</blockquote>
<p><span class="math display">\[
g_i(\mathbf{x}_i)=-\frac{1}{2}(\mathbf{x}-\mathbf{\mu}_i)^T\mathbf{\Sigma}_i^{-1}(\mathbf{x}-\mathbf{\mu}_i)+\text{ln}p(w_i)-\frac{1}{2}\text{ln}((2\pi)^d |\Sigma_i|)
\]</span> 简化为： <span class="math display">\[
g_i(\mathbf{x}_i)=-\frac{1}{2}(\mathbf{x}-\mathbf{\mu}_i)^T\mathbf{\Sigma}_i^{-1}(\mathbf{x}-\mathbf{\mu}_i)+\text{ln}p(w_i)-\frac{1}{2}\text{ln}((2\pi)^d |\Sigma|)
\]</span> 由于最后一项都一样，所以可以丢掉，得到： <span class="math display">\[
g_i(\mathbf{x}_i)=-\frac{1}{2}(\mathbf{x}-\mathbf{\mu}_i)^T\mathbf{\Sigma}_i^{-1}(\mathbf{x}-\mathbf{\mu}_i)+\text{ln}p(w_i)
\]</span> 如果所有的类都有相同的先验概率<span class="math inline">\(p(w_i)\)</span>,那么上式可以进一步简化： <span class="math display">\[
g_i(\mathbf{x}_i)=-\frac{1}{2}(\mathbf{x}-\mathbf{\mu}_i)^T\mathbf{\Sigma}_i^{-1}(\mathbf{x}-\mathbf{\mu}_i)=d_{\Sigma}(\mathbf{x},\mu_i)
\]</span> 这就叫<span style="color:red"><strong><em>最小马氏距离分类器</em></strong></span>． 所以优化的决策规则为：计算特征<span class="math inline">\(\mathbf{x}\)</span>到所有类<span class="math inline">\(c\)</span>的均值<span class="math inline">\(\mu_i\)</span>的马氏距离, 将<span class="math inline">\(\mathbf{x}\)</span>归类到最近马氏距离的那个类．</p>
<p>将上式展开得到： <span class="math display">\[
g_i(\mathbf{x}_i)=-\frac{1}{2}(\mathbf{x}-\mathbf{\mu}_i)^T\mathbf{\Sigma}_i^{-1}(\mathbf{x}-\mathbf{\mu}_i)+\text{ln}p(w_i)=-\frac{1}{2}\mathbf{x}^T\Sigma^{-2}\mathbf{x}+\mu_i^T\Sigma^{-1}\mathbf{x}-\frac{1}{2}\mu_i^T\Sigma^{-1}\mu_i++\text{ln}p(w_i)
\]</span> 第一项和类<span class="math inline">\(i\)</span>无关，所以丢掉．我们的得到： <span class="math display">\[
g_i(\mathbf{x}_i)=\mu_i^T\Sigma^{-1}\mathbf{x}-\frac{1}{2}\mu_i^T\Sigma^{-1}\mu_i++\text{ln}p(w_i)=\mu_i^T\Sigma^{-1}\mathbf{x}+w_{i0}=\mathbf{w}_i^T\mathbf{x}+w_{i0}
\]</span></p>
<p><span style="color:red"><strong><em>这是关于<span class="math inline">\(\mathbf{x}\)</span>的一个线性方程，所以２个类的决策边界是一个超平面</em></strong></span>．</p>
<p><span style="color:red"><strong><em>那么如何获得２个类的决策边界呢？</em></strong></span></p>
<p>我们将２个决策函数相等：<span class="math inline">\(g_i(\mathbf{x})=g_j(\mathbf{x})\Rightarrow (\mathbf{w}_i-\mathbf{w}_i)^T\mathbf{x}+w_{i0}-w_{j0}=0\)</span></p>
<p><span class="math display">\[
(\mu_j-\mu_i)^T\Sigma^{-1}\mathbf{x}-\frac{1}{2}(\mu_i-\mu_j)^T(\mu+i+\mu_j)+\text{ln}[p(w_1)/p(w_2)]=0
\]</span></p>
<p>在２维空间<span class="math inline">\(\mathbf{x}\)</span>这是一条直线，在３Ｄ空间，则是一个平面．</p>
<p><span class="math display">\[
(\mu_j-\mu_i)^T\Sigma^{-1}\mathbf{x}=0
\]</span></p>
<p>也说明，<span style="color:red"><strong><em>２个类决策的超平面和连接２个均值直线不正交(orthogonal)． 如果<span class="math inline">\(\Sigma_i=\sigma^2 \mathbf{I}\)</span>, 则正交．</em></strong></span></p>
<blockquote>
<p><span style="color:red"><strong><em>特例2，所有类具有相同的协方差矩阵<span class="math inline">\(\Sigma_i=\Sigma=\sigma^2\mathbf{I}\)</span>, 且为对角阵</em></strong></span></p>
</blockquote>
<p>这种情况发生在<span style="color:red"><strong><em>样本是独立同分布，且有相同方差的情况下</em></strong></span>．<span class="math inline">\(\Sigma_i=\Sigma=\sigma^2\mathbf{I}\Rightarrow \Sigma^{-1}=\mathbf{I}/\sigma^2, |\Sigma_i|=\sigma^{2d}\)</span>, 其中<span class="math inline">\(d\)</span>是维度．</p>
<p><span class="math display">\[
g_i(\mathbf{x}_i)=-\frac{1}{2}(\mathbf{x}-\mathbf{\mu}_i)^T\mathbf{\Sigma}_i^{-1}(\mathbf{x}-\mathbf{\mu}_i)+\text{ln}p(w_i)=-\frac{1}{2\sigma^2}(\mathbf{x}-\mathbf{\mu}_i)^T(\mathbf{x}-\mathbf{\mu}_i)+\text{ln}p(w_i)
\]</span> 计算决策边界<span class="math inline">\(g_i(\mathbf{x}_i)=g_i(\mathbf{x}_j)\)</span>, 得到 <span class="math display">\[
(\mu_j-\mu_i)^T\mathbf{x}=0
\]</span> 也说明，<span style="color:red"><strong><em>２个类决策的超平面和连接２个均值直线正交(orthogonal)．</em></strong></span></p>
<h3 id="广义线性判别函数">广义线性判别函数</h3>
<h3 id="两类线性可分的情况">两类线性可分的情况</h3>
<p>1.　几何解释和术语 2. 梯度下降算法 ### 感知器准则函数最小化 1.　感知器准则函数 2. 单个样本校正的收敛性证明 ### 松弛算法 1.　下降算法 2. 收敛性证明 ### 不可分的情况</p>
<h3 id="最小平方误差方法">最小平方误差方法</h3>
<p>1.　最小平方误差及伪逆 2. 与Fisher线性判别的关系 3. 最优判别的渐进逼近 4. Widrow-Hoff算法或最小均方算法 ### Ho-Kashyap算法 1. 下降算法 2. 收敛性证明 ### 线性规划算法 1.　线性规划 2. 线性可分的情况 3. 极小化感知器准则函数</p>
<h3 id="支持向量机">支持向量机</h3>
<h3 id="推广到多类问题">推广到多类问题</h3>
<p>1.　Kesler构造法 2. 固定增量规则的收敛性 3. ＭＳＥ算法的推广</p>
</body>
</html>
