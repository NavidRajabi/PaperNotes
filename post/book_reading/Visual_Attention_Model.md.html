<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="visual-attention-model"><a href="">Visual Attention Model</a></h1>
<h2 id="早期视觉特征提取">早期视觉特征提取</h2>
<ol style="list-style-type: decimal">
<li><font style="color:red">亮度提取</font> 用r，g，b分别表示图像红、绿、蓝三个通道的值，因此亮度I可以表示为I=（r+g+b）/3，由于输入图片是9个尺度的高斯金字塔图片，所以I也对于9个尺度高斯金子塔亮度图。</li>
<li><font style="color:red">颜色提取</font> r，g，b通道采用I进行归一化以从亮度中解耦色调。产生了四个宽调谐（broadly-tuned）的颜色通道，从这些颜色通道中产生四个高斯金字塔R,G,B,Y。<span class="math inline">\(Y=(r+g)/2-(r-g)/2-b\)</span>;<span class="math inline">\(R=r-(g+b)/2\)</span>;<span class="math inline">\(B=b-(r+g)/2\)</span>;<span class="math inline">\(G=g-(r+b)/2\)</span></li>
<li><font style="color:red">方向特征</font> 方向特征主要是使用Gabor滤波器对图像的亮度特征在0°，45°，90°，135°四个主要方向上进行滤波得到的。因此，Gabor滤波器可以很好地模拟人类视皮层简单细胞的信号处理特点，其结果直接体现了图像局部方向特征的信息。在Gabor滤波器某个方向的滤波结果中，给定区域的能量可以反映该区域灰度图在该方向的特征是否强烈，即反映了区域内的该方向的直线或者规则纹理的朝向特征是否明显。因此方向特征的提取可以直接使用几个方向的Gabor滤波器进行滤波获得。</li>
</ol>
<p><font style="color:red">第一组特征图集</font>：中心细尺度<span class="math inline">\(c\)</span>和周围粗尺度<span class="math inline">\(s\)</span>之间的中心围绕差值产生特征图。模拟灵长类动物的中心围绕机制，所有类型的敏感性是在六个<span class="math inline">\(I(c,s)\)</span>图（六尺度）中同时计算的：<span class="math inline">\(I(c,s)=|I(c)\theta(c)|\)</span>, <span class="math inline">\(c\in(2,3,4)\)</span>, <span class="math inline">\(s=s+\delta,\delta\in(3,4)\)</span>.</p>
<p><font style="color:red">第二组特征图集</font>：模拟皮层中的“颜色双竞争” 系统表示：在接受野中心，神经元被一种颜色激活而禁止另外一种颜色，而在周围区域则是相反的。在主视觉皮层中，红/绿，绿/红，蓝/黄，黄/蓝颜色对都存在这种空间和色彩竞争关系。因此，产生RG(c,s)和BY(c,s)特征图来反映双竞争关系。 RG(c,s)=|(R(c)-G(c))Θ(G(s)-R(s))| BY(c,s)=|(B(c)-Y(c))Θ(Y(s)-B(s))|</p>
<p><font style="color:red">第三组特征图集</font>：采用方向Gabor金字塔得到局部方向信息，采用4角度和6尺度来表示角度和方向。最后得到方向特征图：O(c,s,θ)=|O(c,θ)ΘO(s,θ)|;</p>
<h2 id="center-surround-contrast中央周边差">Center-surround Contrast（中央周边差）</h2>
<p>“Center-surround” difference operations，即<font style="color:red">中央周边差操作，是根据人眼生理结构设计的</font>。人眼感受野对于<font style="color:red">视觉信息输入中反差大的特征反应强烈</font>，例如中央亮周边暗的情况、中央是绿色周边是红色的情况等，这都属于反差较大的视觉信息。</p>
<p>在高斯金字塔中，<font style="color:red">尺度较大的图像细节信息较多，而尺度较小的图像由于高斯平滑和减抽样操作使得其更能反映出局部的图像背景信息</font>，因而将<font style="color:red">尺度较大的图像和尺度较小的图像进行跨尺度减操作（across-scale）</font>，能得到<u>局部中心和周边背景信息的反差信息</u>。</p>
<p><font style="color:red">跨尺度减的具体算法如下：</font> 1. 通过将代表<u>周边背景信息的较小尺度</u>的图像进行<font style="color:red">线性插值</font>，使之与代表中心信息的较大尺度的图像具有相同大小，然后进行点对点的减操作，即<font style="color:red">中央周边差操作</font>，这样的跨尺度减操作使用符号Θ表示。 2. 对每个特征通道的<font style="color:red">高斯金字塔进行中央周边差操作</font>，依次检测图像中的特征不联系性，即中心和周边背景信息反差对比强烈的区域，很好地模仿了视网膜上探测突出目标的生理机制。</br></p>
<p>在模型中，代表中心信息的图像尺度<span class="math inline">\(c\)</span>取<span class="math inline">\(c\in \{2,3,4\}\)</span>, 代表周边背景信息的图像尺度s取s=c+δ，其中δ∈{3,4}，从而在每个特征通道里可以产生6个尺度对，即{2-5,2-6,3-6,3-7,4-7,4-8}，在每个特征通道可以产生6张中央周边差结果图，所以7个通道共有42张中央周边差图，中央周边差结果图在该模型中被称为特征图（Feature Map）。</p>
<h2 id="显著性图">显著性图</h2>
<p>特征图的结合： 1. 将特征图进行归一化到固定区间[0...M] 2. Find the location of the map’s global maximum <span class="math inline">\(M\)</span> and computing the average m of all its other local maxima. 3. Globally multiplying the map by <span class="math inline">\((M-m)^2\)</span></p>
<p>This factor replicates cortical lateral inhibition mechanisms, in which neighbouring similar features inhibit each other.</p>
<h2 id="feature-map-combination">Feature Map Combination</h2>
<ul>
<li>Across Scale Addition: Reduce/Enlarge a map to scale 4 and point-by-point addition. Denoted by <span class="math inline">\(\oplus\)</span></li>
<li>LinearCombination</li>
<li>Combination of Intensity Maps <span class="math inline">\(I=\oplus_{c=2}^4 \oplus_{s=c+3}^{c+4}N(I(c,s))\)</span></li>
<li>Combination of color maps: <span class="math inline">\(C=\oplus_{c=2}^4 \oplus_{s=c+3}^{c+4}[N(RG(c,s))+N(BY(c,s))]\)</span></li>
<li>Combination of Orientation Maps: <span class="math inline">\(O=\sum_{\theta\in \{0,45,90,135\}}N(\oplus_{c=2}^4\oplus_{s=c+3}^{c+4}N(O(c,s,\theta)))\)</span></li>
</ul>
<h2 id="other-combination-method">Other Combination Method</h2>
<ul>
<li>Linear Combination with Learned Weights: Can achieve better performance but with poor generalization.</li>
<li>Local Nonlinear Competition: Suppress spurious attention in individual maps</li>
<li>Global Nonlinear Normalization: The simplified version of local nonlinear competition</li>
</ul>
<h2 id="saliency-map">Saliency Map</h2>
<p>关于显著度的研究是从生物研究发展而来，早期比较重要的工作是C.Koch与S.Ullman做的，时间可以追溯到1985年。显著度的获取方式主要有两种：<font style="color:red">自上而下</font>，从高层语义入手，其实目标检测等等也可以归入这一类中；更普遍的是自下而上，从底层特征入手。直到现在，第二种自下而上的方法仍是主流。<font style="color:red">从CV角度研究显著度问题从1998年开始，开创者是Itti。其最出名的文章是A model of saliency based visual attention for rapid scene analysis，发表在1998年的PAMI上</font>。</p>
<p>这篇文章基本奠定了显著度研究的基本思路，即：<font style="color:red">特征提取—&gt;归一化—&gt;特征综合/显著度计算—&gt;显著性区域划分/兴趣点标定。</font></p>
<p>在最初的文章中主要采取<font style="color:red">启发式的方法提取特征</font>，依据生物学研究，主要提取<font style="color:red">亮度、色彩、旋转一致性三种特征</font>，得到三种feature map。随后这些feature map被<font style="color:red">归一化以便综合，综合方法是简单的相加</font>。从综合后的saliency map上<font style="color:red">提取前N个峰值即为寻求的interest point</font>。</p>
<p>Itti提出的显著图模型是一种模拟生物体视觉注意机制的<font style="color:red">选择性注意模型，比较适合处理自然图像</font>。这里的显著值<font style="color:red">是像素点在颜色、亮度、方向方面与周边背景的对比</font>，所有点的显著值构成一张显著图，算法流程如下：</p>
<center>
<img src="http://ilab.usc.edu/bu/theory/model.gif" alt="Test Image" width="400"/>
</center>
<p></br> 1. <font style="color:red">特征的提取</font> 先把输入图像表示成<font style="color:red">9层的高斯金字塔</font>。其中第0层是输入图像，<font style="color:red">1到8层分别是用5*5的高斯滤波器对输入图像进行滤波和采样形成的</font>，大小分别的输入图像的1/2到1/256。然后对金字塔每一层分别提取<font style="color:red">各种特征:亮度、红色、绿色、蓝色、黄色、方向，形成特征金字塔</font>。Itti算法为了<font style="color:red">模拟感受野的中心—外周拮抗的结构</font>，对各种特征分别在特征金字塔的<font style="color:red">不同尺度间作差</font>。作差得到的是<font style="color:red">中心（尺度c）和外周（尺度s）的特征的对比表示中心和外周的局部方向特征的对比</font>。 2. <font style="color:red">显著图生成</font> 把每一个上述得到的特征图<font style="color:red">归一化到区间[0 1]</font>，以消除和特征相关的幅度差别。为了消除干扰噪声突出显著部分，对每个特征图M分别用二维高斯差函数进行卷积，并把卷积结果叠加回原特征图，使同种特征以侧抑制的方式在空间上竞争。</br> 卷积和迭代过程进行多次，这样可以<font style="color:red">让少数几个最显著的点均匀分布在整个特征图上</font>，从而每个特征图上只保留少数的几个显著点，在叠加多个特征图时能把多种显著特征的点突现出来。<font style="color:red">接下来分别把每一类（亮度、色度、方向）归一化后的特征图逐点求和（采样到第4尺度），得到对应于每一类特征的显著图,综合所有特征的显著性，就得到对应于输入图像的显著图S</font>.</p>
<p>有点晕吧，其实也没那么复杂，就好比白茫茫的雪地上出现一只黑猫，<font style="color:red">那么这团黑色的东西相对人的视觉是显著的，当然这是在颜色上的显著性</font>，还有诸如边缘、纹理的差异造成的显著性.</p>
<p>大家如此热衷地研究图像显著性算法，那么它到底有那些应用呢，总不会只是借以证明“男人是视觉动物”这一公理吧，下面是列举的一些应用： 1. <font style="color:red">图像缩放</font>：传统的图像缩放算法会使得图片中的<font style="color:red">某些目标变形，加入显著值做为能量约束可避免这一缺点</font>，此种缩放又称为“ 液态缩放 ”。 A. R. Achanta and S. Süsstrunk, Saliency Detection for Content-aware Image Resizing,IEEE International Conference on Image Processing, 2009. 2. <font style="color:red">视频编码</font>：同上述的图片缩放，当16:9的视频画面转为4：3的画面时，<font style="color:red">可保持画面中对象的清晰</font>，又称为“ 基于内容感知 的视频缩放”。 3. <font style="color:red">图像分割</font>：结合graphcut/grabcut，将图片中的目标抠出。</p>
</body>
</html>
