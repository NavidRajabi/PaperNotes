<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h2 id="image-fundamentals-and-human-perception">Image Fundamentals and Human Perception</h2>
<p><strong>Image Formation and Representation</strong></p>
<p>Physically[物理地], an image is a two dimensional (2‐D) projection of a three dimensional (3‐D) scene, a visual representation, a vivid or graphic description of an object or scene.</p>
<p align="center">
<img src="https://apps.carleton.edu/reason_package/reason_4.0/www/images/995298.jpg" width="450" >
<figcaption>
Fig. Numbers 1-9: range of the intensity of light.
</figcaption>
</p>
<p>The Visible Light: The visible spectrum is the portion of the electromagnetic spectrum t 390 to 700 nm. In terms of frequency, this corresponds to a band in the vicinity of 430–770 THz.</p>
<p><strong>Luminance and brightness</strong> Light received from an object is <span class="math inline">\(I(\lambda)=\rho(\lambda)L(\lambda)\)</span>, where <span class="math inline">\(\rho(\lambda)\)</span> is the reflectivity or transmissivity of object, <span class="math inline">\(L(\lambda)\)</span> is the spectral energy[光谱能量] distribution of the light source. <span class="math inline">\(\lambda\)</span> is the wavelength in the visible spectrum, 350nm to 780 nm.</p>
<p>Luminance or intensity of a spatially distributed object with light distribution[光分布] <span class="math inline">\(I(x,y,\lambda)\)</span> is defined as <span class="math inline">\(f(x,y)=\int_{0}^{\infty}I(x,y)V(\lambda)d\lambda\)</span>, where <span class="math inline">\(V(\lambda)\)</span> is the relative spectral sensitivity function[相对光谱灵敏度函数] of the visual system.</p>
<p align="center">
<img src="http://www.sxyhzm.com/uploadfile/2015/0818/20150818031916280.jpg" width="350" >
<figcaption>
Fig. <span class="math inline">\(V(\lambda)\)</span> is a bell shaped curve. The luminance[亮度] of an object is independent of the luminance of its surrounding.
</figcaption>
</p>
<p><strong>Digital Image Representation</strong> Mathematically, an Image is a two‐dimensional (2‐D) function f(x,y), a function of the two spatial coordinates. A digital image is a sampled, quantized version of a 2D light‐intensity function generated by optical means.The function is usually sampled in an equally spaced rectangular grid pattern, with its amplitude quantized in equal intervals[间隔]. A digital image is <span class="math inline">\(f(x,y)\)</span> where x, y and f are all finite[有限] and discrete[离散] quantities[量].</p>
Digitization of an image: Spatial sampling[空间采样] or discretization[离散化] and Intensity or gray‐level quantization[量子化].
<p align="center">
<img src="http://what-when-how.com/wp-content/uploads/2012/07/tmp26dc25_thumb2.png" width="350" >
</p>
<p>Denote[描述] an image, a 2‐D light‐intensity function, as <span class="math inline">\(f(x,y)\)</span>. The value or amplitude of f at spatial coordinates <span class="math inline">\((x, y)\)</span> indicates the intensity (brightness) of the image at that point. <span class="math inline">\(f(x,y)\)</span> must be digitized both spatially and in amplitude for computer processing. Digitization of spatial coordinates <span class="math inline">\(f(x,y)\)</span> is referred to as spatial sampling[空间采样] or discretization[离散化]. Digitization of intensity amplitude f(x,y) is referred to as intensity or gray‐level quantization.</p>
<p>The (spatial) resolution of a digital image refers to the size of the mxn array of which the image is sampled. The gray‐level resolution of a digital image refers to the number of gray levels (intensities) <span class="math inline">\(g=2^b\)</span>, where b is the number of bits per sample. There are three different type of cones help perceive colour <a href="http://daily.zhihu.com/story/7103305">zhihu</a>.</p>
<p>Color is a function of wavelength (frequency), Color Primaries[基元]: Red(R), Green(G), Blue(B).</p>
<strong>color space</strong> Additive primaries[加色法原色]: Red(R), Green(G), Blue(B) Subtractive primaries[减法三原色]: Cyan[蓝绿色], Magenta[品红色], Yellow[黄色] A color can be specified in terms of the amounts of three primaries required: <span class="math inline">\(c = a\times p1 + b\times p2 + c\times p3\)</span>, where (p1,p2,p3) is a particular set of primaries[基元]. A color space is a 3D space, defined to describe color in some standard way. Common color spaces: - RGB ‐ hardware oriented, used for monitors, video cameras. <a href="https://en.wikipedia.org/wiki/RGB_color_space">RGB color space</a> - rgb ‐ Normalized RGB. $ r = ; g = ; b = ; r+g+b = 1$ - CMY (Cyan‐Magenta‐Yellow) ‐ used for color printer - YIQ (luminance, in‐phase, quadrature. ) ‐ color TV broadcast. $
\begin{bmatrix} Y \\ I \\ Q \end{bmatrix}
=
\begin{bmatrix}
  0.299 &amp;  0.587 &amp;  0.114 \\
  0.596 &amp; -0.274 &amp; -0.322 \\
  0.211 &amp; -0.523 &amp;  0.312
\end{bmatrix}\begin{bmatrix} R \\ G \\ B \end{bmatrix}
$ - HSI (HSV) (Hue, saturation, intensive/value) ‐ used for color manipulation: <a href="http://blog.csdn.net/wxb1553725576/article/details/45827923">RGB、HSV、HSI颜色空间</a> - CIE‐Luv, CIE‐Lab (lightness, red‐green, yellow‐blue) ‐ used for color differentiation. - sRGB – used for device independent digital image display.
<p align="center">
<img src="http://what-when-how.com/wp-content/uploads/2012/07/tmp26dc25_thumb2.png" width="350" >
</p>
<p><strong>Human Perception</strong> Human eye has many cells shaped like cones[视锥细胞] to perceive[认知] light.</p>
<p><strong>Image histogram</strong> The histogramof a digital image f(x,y) with gray level range [0, L] is a discrete function <span class="math inline">\(p_f(f)=\frac{n_f}{n}\)</span>. where f is the gray level, f = 0, 1, 2, …, L. <span class="math inline">\(n_f\)</span> is the number of pixels with that gray level. n is the total number of pixels in the region of the image being processed. It is clear that the histogram <span class="math inline">\(p_f(f)\)</span> of a digital image is the frequency of occurrence[发生频率] of gray‐level f in the image. Histogram shows the frequency distribution of gray‐level <span class="math inline">\(f\)</span>. Obviously, <span class="math inline">\(p_f(f)\ge 0\)</span>, and <span class="math inline">\(\sum_{f=0}^L p_f(f)=1\)</span>. If we treat the pixel gray‐level of an image as a random number, the histogram <span class="math inline">\(p_f(f)\)</span> is an estimate of the probability of occurrence of gray‐level f.</p>
<p>Understand the histogram: The shape of image histogram provides many clues[提示] as to the characteristics of image. For example,a narrowly distributed histogram indicates a low‐contrast image. A bimodal histogram[直方图] suggests that the image contains an object with a narrow amplitude range[狭窄的振幅范围] against a <em>background</em> of differing[相异] amplitude. <a href="http://www.mathworks.com/help/images/ref/histeq.html">Matlab histeq</a></p>
<p><strong>Image Processing</strong> An digital image is a two dimensional numerical representation (a 2‐D function <span class="math inline">\(f(x,y)\)</span> or a mxn matrix) of a 3D scene or an object. <em>Digital image processing is a series of machine or computer operations leading to some desired results.</em> The operations could be, should be, and are desired to be described by mathematics. Digital image processing, starting with an image or a set of images, produces a modified version of the image(s) or extract more “meaningful” information (features) from the image(s) or understand (recognize) the meaning of the image content.</p>
<p><strong>Why need image processing?</strong> - isualization : Contrast enhancement, noise removal, visual quality improvement, pseudo colouring - Image understanding: Extraction of image properties such as colour, shape, texture, edges, lines, curves, corners. - Automated Guided vehicle. Identifying road, vehicle, pedestrian, traffic signs - Visual servicing. Automated robot control - Security ?Intrusion detection, biometrics - Information retrieval. Image content based search</p>
<h2 id="lsi-systems-and-transformations">LSI Systems and Transformations</h2>
<p><strong>Basic image element</strong> A digital image can be represented by a 2‐D function with two integer arguments, such as <span class="math inline">\(f(x,y)\)</span> where x and y are integers. The basic element image is the impulse <span class="math inline">\(\delta(x,y)\)</span>. Shifted and scaled the impulse <span class="math inline">\(f(x,y)=c\delta(x-i,y-j)\)</span>. It describes any arbitrary[任意的] pixel with all other pixels zero gray vale.</p>
<p><strong>Image decomposition</strong> Any image f(x,y) then can be represented by the sum of a number of shifted and scaled impulses <span class="math inline">\(f(x,y)=\begin{cases}\sum_{j=-\infty}^{\infty}\sum_{i=-\infty}^{\infty}f(i,j)\delta(x-i,y-j) \\ \sum_{j=-m}^{m}\sum_{i=-n}^{n}f(i,j)\delta(x-i,y-j)\end{cases}\)</span></p>
<p>A processing systemrelates any input image f(x,y) to a unique output image g(x,y), given by <span class="math inline">\(g(x,y)=T\{ \sum_{j=-\infty}^{\infty}\sum_{i=-\infty}^{\infty}f(i,j)\delta(x-i,y-j)\}\)</span>, If the processing system is linear, then <span class="math inline">\(g(x,y)= \sum_{j=-\infty}^{\infty}\sum_{i=-\infty}^{\infty}f(i,j)T\{\delta(x-i,y-j)\}\)</span>.</p>
<p>If define the output image of the input impulse image as impulse response of the system, <span class="math inline">\(h(x,y)\triangleq T\{\delta(x,y)\}\)</span>. Then for shift invariant system:<span class="math inline">\(T\{\delta(x-i,y-j)\}=h(x-i,y-j)\)</span>.</p>
<p>Therefore, given an input image f(x,y), a linear and shift‐invariant (LSI) image processing system T produces the output image g(x,y) by <span class="math inline">\(g((x,y)=T\{f(x,y)\})=\sum_{j=-\infty}^{\infty}\sum_{i=-\infty}^{\infty}f(i,j)h(x-i,y-j)\}\)</span>. A LSI system is completely characterized by its impulse response[脉冲响应] h(x,y). ? is the convolution operator. For any LSI image processing system, the output image equals to the input image convolving with the impulse response of the system.</p>
<p><strong>convolution property</strong> Commutative[交换的]: <span class="math inline">\(f(x,y)*h(x,y)=h(x,y)*f(x,y)\)</span> Associative[结合的]: <span class="math inline">\(f(x,y)*h_1(x,y)*h_2(x,y)=h(x,y)*h_2(x,y)*h_1(x,y)\)</span> Distributive[分配的]: <span class="math inline">\(f(x,y)*(h_1(x,y)+h_2(x,y))=h(x,y)*h_2(x,y)+h(x,y)*h_1(x,y)\)</span></p>
<p><strong>what is h(x,y)?</strong> Impulse response h(x,y) of an image processing system is the output image when a impulse image is inputted to the system. Therefore, the impulse response h(x,y) is also an image, often called spatial representation[空间表征] of a filter or filter mask or filter coefficients[滤波系数] or filter parameters.</p>
<p>Although the impulse response h(x,y) is basically an image, to speed up the image process, it is often a small size of image with size such as 3X3, 5X5, …11X11, comparing with a normal image of size 256X256.<span class="math inline">\(g(x,y)=T\{ \sum_{j=-3}^{3}\sum_{i=-3}^{3}f(i,j)h(x-i,y-j)\}\)</span>, if <span class="math inline">\(h(x,y)\ne 0\)</span>, only when <span class="math inline">\(-3&lt;x,y&lt;3\)</span>.</p>
<p><strong>Example</strong></p>
<p>Given an input image <span class="math inline">\(f(x,y)\)</span>, you want to suppress the pixel random noise or smooth the image so that the image looks “soft”. So you do some local average that a pixel in the output image is produced by sum up the corresponding pixel and its 4 neighbor pixels in the input image. What is the mathematical expression of this very simple process? What is the convolution like? ? What is the size of the impulse response? Is the impulse response a constant one within the filter window h(x,y)=1? What is the impulse response analytically? How to represent the impulse response by image, or filter mask or filter coefficient?</p>
<p>A numerical example of convolution:<span class="math inline">\(g(x,y)=\sum_{j=-1}^{1}\sum_{i=-1}^{1}h(i,j)f(x-i,y-j)\)</span>, At each point (x,y) the response of the filter at that point is calculated as a sum of products of the filter coefficients and the corresponding image pixels in the area spanned by the filter mask.</p>
<p><strong>Fourier transform</strong> Let f(x) be a continuous function of a single variable x and F(u) be its Fourier transform, then <span class="math inline">\(\begin{cases}F(w)=\mathscr{F}\{f(x)\}=\int_{-\infty}^{\infty}f(x)e^{-j2\pi wx}dx \\ f(x)=\mathscr{F}^{-1}\{F(w)\}=\int_{-\infty}^{\infty}F(w)e^{j2\pi wx}dw \end{cases}\)</span> <span class="math inline">\(\begin{cases}F(u,v)=\mathscr{F}\{f(x,y)\}=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f(x)e^{-j2\pi (ux+vy)}dxdy \\ f(x,y)=\mathscr{F}^{-1}\{F(u,v)\}=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}F(w)e^{j2\pi (ux+vy)}dudv \end{cases}\)</span> where u and v are 2 frequency variables.</p>
<p>2‐D Fourier transform is separable only if <span class="math inline">\(f(x,y)=f_1(x)f_2(y)\)</span></p>
<p>Obviously, <span class="math inline">\(F(u,v)\)</span> is in general a complex function that can be represented by <span class="math inline">\(F(u,v)=R(u,v)+jI(u,v)=|F(u,v)|e^{j\varphi(u,v)}\)</span>, where: <span class="math inline">\(|f(u,v)|=\sqrt{R^2(u,v)+I^2(u,v)}\)</span>, <span class="math inline">\(\varphi(u,v)=arctan [\frac{I(u,v)}{R(u,v)}]\)</span>, <span class="math inline">\(R(u,v)=|f(u,v)| cos(\varphi(u,v))\)</span>, <span class="math inline">\(I(u,v)=|f(u,v)| sin(\varphi(u,v))\)</span></p>
<p>The discrete Fourier transform (DFT) of a 2‐D discrete function (or image) f(x,y) of size mxn is defined by: <span class="math inline">\(F(u,v)=\frac{1}{mn}\sum_{x=0}^{m-1}\sum_{y=0}^{n-1}f(x,y)e^{-j2\pi(ux/m+vy/n)}\)</span> <span class="math inline">\(f(x,y)=\frac{1}{mn}\sum_{u=0}^{m-1}\sum_{v=0}^{n-1}F(u,v)e^{j2\pi(ux/m+vy/n)}\)</span></p>
<p><strong>properties of DFT</strong> - Periodicity:$X_{k+N} = <em>{n=0}^{N-1} x_n e^{- (k+N) n} <span class="math inline">\(=\)</span> </em>{n=0}^{N-1} x_n e^{- k n} e^{-2 i n} <span class="math inline">\(=\)</span> _{n=0}^{N-1} x_n e^{- k n} = X_k $ - Conjugate symmetry[共轭对称] for real image f(x,y) , <span class="math inline">\(F(u,v)=F^*(-u,-v)\)</span> - Linearity and scaling: <span class="math inline">\(=\mathscr{F}\{\alpha x, \beta y\}=\frac{1}{|\alpha \beta|}F(u/\alpha,v/\beta)\)</span> - Translation: <span class="math inline">\(\begin{cases}f(x-x_0,y-y_0) \leftrightarrow F(u,v)e^{-j2\pi(x_0u/m+y_0v/n)} \\ F(u-u_0,v-v_0) \leftrightarrow f(x,y)e^{-j2\pi(u_0x/m+v_0y/n)}\end{cases}\)</span> - Rotation: <span class="math inline">\(f(r,\theta+\theta_0) \leftrightarrow F(w,\varphi+\theta_0)\)</span> - Rotation Invariant Transform[旋转不变]: <span class="math inline">\(g(u,v)=\frac{1}{\pi}\int_{0}^{2\pi}\int_{0}^1 e^{-j(2\pi ur^2+v\theta)f(r,\theta)}\theta\)</span>, <a href="http://www.yidianzixun.com/083QErjh">灰度图像--频域滤波 傅里叶变换之二维离散傅里叶变换</a>. 频谱与空域具有旋转同步性，即空域旋转多少频域就旋转多少，频域旋转空域就旋转多少。</p>
<p><strong>some basic FT pairs</strong> <a href="http://www.thefouriertransform.com/pairs/fourier.php">Fourier Transform Pairs</a></p>
<p><strong>discrete cosine transform</strong> Discrete Cosine Transform (DCT) is closely related to DFT and is widely used in image and video compression standards. The DCT pair of a 2‐D discrete function (or image) f(x,y) of size NXN can be defined as follows: <span class="math inline">\(F(u,v)=\frac{2}{N}C(u)C(v)\sum_{x=0}^{N-1}\sum_{y=0}^{N-1}f(x,y)cos(\frac{(2x+1)u\pi}{2N})cos(\frac{(2y+1)v\pi}{2N})\)</span> <span class="math inline">\(f(x,y)=\frac{2}{N}\sum_{x=0}^{N-1}\sum_{y=0}^{N-1}C(u)C(v)f(u,v)cos(\frac{(2x+1)u\pi}{2N})cos(\frac{(2y+1)v\pi}{2N})\)</span> where, <span class="math inline">\(C(u),C(v)=\begin{cases}\frac{1}{\sqrt{2}}, u,v=0 \\ 1 , u,v=1,2...,N-1\end{cases}\)</span></p>
<p><strong>image sampling</strong> Given a continuous analogy image <span class="math inline">\(f_c(x,y)\)</span>, it is very simple to get its discrete image fd(m,n) mathematically by <span class="math inline">\(f_d(m,n)=f_c(m\Delta x,n\Delta y)\)</span>.</p>
<p>A 2‐D function fc(x,y) is band‐limited if its Fourier transform Fc(u,v) is zero outside a bounded spatial frequency support; e.g., <span class="math inline">\(F_c(u,v)=0\)</span>, for |u|&gt;U_0, <span class="math inline">\(|v|&gt;V_0\)</span>. In practice, real‐world images can be well approximated by band‐ limited signals. Define a 2‐D sampling function <span class="math inline">\(s(x,y)=\sum_{m=-\infty}^{\infty}\sum_{m=-\infty}^{\infty}\delta(x-m\Delta x, y-\Delta y)\)</span>. It has the Fourier transform of <span class="math inline">\(S(u,v)=\frac{1}{\Delta x \Delta y} \sum_{m=-\infty}^{\infty}\sum_{m=-\infty}^{\infty}\delta(u-m/\Delta x, v-n/\Delta y)\)</span></p>
<p>Multiply the continuous image <span class="math inline">\(f_c(x,y)\)</span> with the sampling image <span class="math inline">\(s(x,y)\)</span> yield <span class="math inline">\(f_d(x,y)=f_c(x,y)s(x,y)=\sum_{m=-\infty}^{\infty}\sum_{m=-\infty}^{\infty} f_c(m\Delta x, n\Delta y)\delta(x-m\Delta x, y- n\Delta y)\)</span>. The Fourier transform of the sampled image fd(x,y) in continuous domain is: <span class="math inline">\(F_d(u,v)=F_c(u,v)*S(u,v)=\frac{1}{\Delta x \Delta y} \sum_{m=-\infty}^{\infty}\sum_{m=-\infty}^{\infty} F_c(u-mf_{xs}.y-nf_{ys})\)</span>. It is a periodic replication of Fc(u,v), on a rectangular grid with spacing <span class="math inline">\((1/\Delta x, 1/\Delta y)\)</span>.</p>
<p>The spectrum of the sampled image <span class="math inline">\(f_d(x,y)\)</span> consists of the spectrum of the continuous <span class="math inline">\(f_c(x,y)\)</span> image (top) infinitely repeated over the frequency plane in a rectangular grid with spacing <span class="math inline">\((1/\Delta x, 1/ \Delta y)\)</span>. It is a periodic replication of <span class="math inline">\(F_c(u,v)\)</span>, on a rectangular grid with spacing <span class="math inline">\((1/\Delta x, 1/ \Delta y)\)</span>.</p>
<p>Sampling Theorem: - A band‐limited image <span class="math inline">\(f_c(x,y)\)</span> with bandwidths (<span class="math inline">\(2U_0, 2V_0\)</span>) sampled uniformly on a rectangular grid with spacing (<span class="math inline">\(\Delta x,\Delta y\)</span>) can be recovered without error from the sampled values <span class="math inline">\(fc(m\Delta x,n\Delta y)= f_d(m,n)\)</span> provided that the sampling rates (<span class="math inline">\(f_{xs}, f_{ys}\)</span>) are greater than the Nyquist rates. - The lower bounds of the required sampling rates, the band width, are known as the Nyquist rates or Nyquist frequencies. Their reciprocals are known as the Nyquist intervals. - Sampling below the Nyquist rates will cause the periodic replications of <span class="math inline">\(F_c(u,v)\)</span> to overlap, resulting in a distorted spectrum[畸变谱] <span class="math inline">\(F_d(u,v)\)</span>, in which <span class="math inline">\(F_c(u,v)\)</span> is irrevocably lost—a phenomenon that is known as aliasing[混淆现象]. - Aliasing can be avoided or reduced by low‐pass filtering the image <span class="math inline">\(f_c(x,y)\)</span> before sampling so that its bandwidth is less than the sampling frequency. (at the expense of what?)</p>
<p><strong>image quantization</strong> An image (scalar) quantizer Q maps a continuous intensity variable f into a discrete variable <span class="math inline">\(f_q\)</span>, which takes values from a finite set {<span class="math inline">\(r_1,...,r_L\)</span>} of L numbers. A common L‐level image quantizer has the following form: <span class="math inline">\(f_q=Q(f)=r_k\)</span>, if <span class="math inline">\(t_k\le f &lt; t_{k+1}\)</span> for <span class="math inline">\(k=1,...,L\)</span>. where {t1, …, tL+1} is a set of increasing values known as decision levels, and <span class="math inline">\(r_k\)</span> is the k-th reconstruction level corresponding to the decision interval <span class="math inline">\([t_k , t_{k+1})\)</span>. As such a quantizer performs many‐ (much)‐to‐1 mapping, it introduces distortions or errors, known as quantization errors.</p>
<p>An image (scalar) quantizer Q maps a continuous intensity variable f into a discrete variable fq, which takes values from a finite set {<span class="math inline">\(r_1,..., r_L\)</span>} of L numbers. The choice of quantized levels should minimize the quantization error between f and <span class="math inline">\(f_q\)</span>, . For example, Lloyd‐Max quantizer[Lloyd-Max量化器] minimizes the mean squared error: <span class="math inline">\(\varepsilon = E\{(f-f_d)^2\}=\int_{t_1}^{t_{L+1}}(f-Q(f))^2 p(f)df =\sum_{k=1}^L \int_{k=1}^{t_{k+1}}(f-r_k)^2p(f)df=\sum_{k=1}^L int_{t_k}^{t_{k+1}} (f-r_k)^2 p(f)df\)</span>.</p>
<p>Differentiating the above equation with respect to <span class="math inline">\(r_k\)</span> and <span class="math inline">\(t_k\)</span>, and setting the results to zero, it can be shown that <span class="math inline">\(t_k=\frac{r_k+r_{k-1}}{2}\)</span>, for <span class="math inline">\(k=2,..,L\)</span>. <span class="math inline">\(r_k=\frac{\int_{t_k}^{t_{k+1}fp(f)df}}{\int_{t_K}^{t_{k+1}p(f)df}}\)</span>, for <span class="math inline">\(k=1,..,L\)</span>. These equations are to be solved simultaneously[同时地]— a non‐trivial[非平凡] problem that usually calls for an iterative scheme such as Newton method.</p>
<p>If p(f) is uniform, the Lloyd‐Max quantizer becomes a linear quantizer with equal intervals between the decision levels and the reconstruction levels, given as <span class="math inline">\(t_k=t_{k-1}+q\)</span>, <span class="math inline">\(r_k=t_k+q/2\)</span>, <span class="math inline">\(q=\frac{t_{L+1}+t_1}{L}\)</span>, and the mean square error can be shown as <span class="math inline">\(\varepsilon = q^2/12\)</span>. It is easy to show that the signal‐to‐noise ratio (SNR) achievable by a B‐bit (<span class="math inline">\(2^B\)</span>‐level) linear quantizer for a uniform distributed function is given by <span class="math inline">\(SNR=10log_{10}\frac{\delta^2}{\varepsilon}=10log_{10}2^{2B}\sim 6BdB\)</span>., where <span class="math inline">\(\delta^2\)</span> is the variance of the f.</p>
<h2 id="image-enhancement">Image Enhancement</h2>
<p><strong>point processing</strong> Image processing operations are designed to enhance image content or features so that they are more suitable for display or analysis. Many image enhancement processes are point and memoryless operations which map input image gray‐level to output gray‐level according to a transformation <span class="math inline">\(g=T(f)\)</span>. For example: - Power Transformation (gamma correction)<a href="https://en.wikipedia.org/wiki/Gamma_correction">伽玛校正</a> : <span class="math inline">\(g=cf^{\gamma}\)</span> - Log Transformation: <span class="math inline">\(g=clog(1+f)\)</span> - Piecewise Linear Transformation : <span class="math inline">\(g=T(f)=\begin{cases}\alpha f, 0\le f&lt; \alpha \\ \beta(f-a)+T(a), a\le f &lt;b \\ \gamma(f-b)+T(b), b\le f &lt; L \end{cases}\)</span> - Histogram equalization</p>
<p><strong>histogram equalization</strong> Histogram equalization aims to obtain a uniform histogram[直方图均衡] for the output image <span class="math inline">\(g(x,y)\)</span> by transforming the gray‐level f of the input image f(x,y) into <span class="math inline">\(g=T(f)\)</span>. Histogram equalization algorithm: <span class="math inline">\(\begin{cases} c(f)=\sum_{t=0}^f p_f(t)=\sum_{t=0}^f \frac{n_t}{n}, f=0,1...,L \\ g=T(f)=round[\frac{c(f)-c_{min}}{1-c_{min}}L], c(f)\ge c_{min}\end{cases}\)</span>. where <span class="math inline">\(t\)</span> is dummy variable[虚拟变量] of the summation.<span class="math inline">\(c_{min}\)</span> is the smallest positive value of all c(f) obtained, rount [] rounds a real number to an integer. g is approximately uniformly distributed in [0, L].</p>
<p>Theoretical analysis of the histogram equalization can only be done for continuous variable. Let f be the continuous gray value normalize to [0,1]. Let the transform g=T(f) single‐valued, monotonically increasing in <span class="math inline">\(0 \le g=T(f )\le 1\)</span>. The inverse transform is <span class="math inline">\(f=T^{-1}(g)\)</span> should also be single‐valued and monotonically increasing. From probability theory, if original gray level pdf <span class="math inline">\(p_f(f)\)</span> and <span class="math inline">\(T(f)\)</span> are known, <span class="math inline">\(T^{-1}(g)\)</span> satisfies the above condition, then the transformed gray level pdf <span class="math inline">\(p_g(g)\)</span> is <span class="math inline">\(p_g(g)=p_f(f)\frac{df}{dg}\)</span>. Consider that <span class="math inline">\(g=T(f)=\int_{0}^fp_f(t)dt\)</span> The above function is the cumulative distribution function[累积分布函数] (cdf) of f. cdf is single‐valued and monotonically increasing. <span class="math inline">\(p_g(g)=p_f(g)\frac{df}{dg}=p_f(f)\frac{1}{p_f(f)}=1\)</span>. Therefore, the transformed gray value has uniform distribution[uniform distribution]. The histogram equalization <span class="math inline">\(c(f)=\sum_{t=0}^fp_f(t)=\sum_{t=0}^f\frac{n_t}{n}\)</span> is the discrete version of <span class="math inline">\(g=T(f)=\int_{0}^fp_f(t)dt\)</span></p>
<p><strong>image smoothing</strong> The basic form of image filtering in the spatial and frequency domain can be given as <span class="math inline">\(g(x,y)=f(x,y)*h(x,y)=\sum_{-\intfy}^{\infty}\sum_{-\intfy}^{\infty}h(i,j)f(x-i,y-j)=\sum_{-3}^{3}\sum_{-3}^{3}h(i,j)f(x-i,y-j)\)</span>, if <span class="math inline">\(h(x,y)=0\)</span>, for <span class="math inline">\(-3&lt;x,y&lt;3\)</span>. At each point (x,y) the response of the filter at that point is calculated as a sum of products of the filter coefficients and the corresponding image pixels in the area spanned by the filter mask centered at (<span class="math inline">\(x,y\)</span>).</p>
<p>Image smoothing filters are used for blurring[模糊] and for noise reduction[噪声消减]. These filters are also known as averaging or low pass filters. - Ideal lowpass filter <span class="math inline">\(H(u,v)=\being{cases}1, if\ D(u,v)\leD_0 \\ 0, if \ D(u,v)&gt;D_0\end{cases}\)</span>, <span class="math inline">\(D(u,v)=\sqrt{u^2+v^2}\)</span>. - Gaussian lowpass filtering (GLPF). <span class="math inline">\(G(u,v)=\frac{1}{2\pi\delta^2}}e^{-\frac{u^2+v^2}{2D_0}}\)</span>.</p>
<p><strong>image sharpening</strong> Highpass filtering <span class="math inline">\(H(u,v)=1-H_{lr}(u,v)\)</span>, Ideal highpass filter <span class="math inline">\(H(u,v)=\begin{cases}0, \ if D(u,v)\le D_0 \\ 1, if \ D(u,v)&gt;D_0 \end{cases}\)</span>, <span class="math inline">\(D(u,v)=\sqrt{u^2+v^2}\)</span>. - Gaussian highpass filter (GHPF) <span class="math inline">\(G(u,v)=1-\frac{1}{2\pi\delta^2}}e^{-\frac{u^2+v^2}{2D_0}}\)</span> - High‐boost filtering <span class="math inline">\(f_{hb}(x,y)=Af(x,y)-f_{lp}(x,y)\)</span>, where <span class="math inline">\(f_{lp}(x,y)\)</span> is a smoothed version of <span class="math inline">\(f(x,y)\)</span> by a lowpass filter.</p>
<p><strong>Problems of Linear Filter</strong> Any linear filter output is a weighted average of the input pixels: <span class="math inline">\(\hat{f}(x,y)=h(x,y)*f(x,y)=\sum_{i=-a}^{a}\sum_{j=-n}^bh(i,j)f(x-i,y-j)\)</span>. What are problems of the average of pixel grey values? <span class="math inline">\(\rightarrow\)</span> image blurring, sharpness details are lost, difficult to smooth strong noise Why? The response is based on ordering (ranking) the pixels contained in the image area encompassed by the filter.</p>
<p><strong>Median Filter</strong> The best‐known example is median filter, which replaces the value of a pixel by the median of the gray levels in the neighborhood of that pixel： <span class="math inline">\(\hat{f}(x,y)=median\{f(s,t)\}\)</span>.</p>
<p align="center">
<img src="http://www.mathworks.com/matlabcentral/mlc-downloads/downloads/submissions/46563/versions/2/screenshot.jpg" width="350" >
</p>
<p>Median filter forces the points with distinct gray levels to be more like their neighbors. Isolated clusters of pixels that are lighter or darker with respect to their neighbors, and whose area is less than <span class="math inline">\(n^2/2\)</span> (one‐half the filter area), are eliminated by an n x n median filter. eliminated[淘汰] = forced to have the value equal the median intensity of the neighbors. Larger clusters are affected considerably less.</p>
<p>Edge is a basic and significant structure of an image.</p>
<p><strong>Mean vs. Median Filter</strong> Consider a uniform 1‐D image with a pulse function. Pulse function corresponds to fine image detail such as lines and curves. - Mean filter ‘blurs’ the image details. - If the pulse is noise, mean filter suppress it only for some extent but spread the noise - Median filter does not ‘blur’ the edge. - If the pulse is noise, 5X5 median filter totally remove such noise. - Mean filter blurs[模糊] the step edge. Median filter preserves[保存] the step edge. - Mean filter further blurs the smooth edge. Median filter preserves the smooth edge. - Mean filter attenuates[衰减] additive Gaussian noise but blurs the edge. Median filter attenuates Gaussian noise and preserves the edge. - Mean filter is ineffective to attenuate impulsive noise and blurs the edge</p>
<p><strong>Med. Filter Properties</strong> Linear filter has established theory to analyze its properties, especially in the frequency domain.However, It is difficult to analyze Median filter and other order‐statistic[排序统计] filters due to their nonlinearity. Repeated applications of median filter to a signal result in an invariant signal called the “root signal”. A root signal is invariant[不变量] to further application of the median filter</p>
<p><strong>Other Order‐stati. Filter</strong> Simple extension of the median filter: - max filter : <span class="math inline">\(f(x,y)=max\{f(s,t)\}\)</span> - min filter : <span class="math inline">\(\hat{f}(x,y)=min\{f(s,t)\}\)</span> - midpoint filter : <span class="math inline">\(\hat{f}(x,y)=\frac{1}{2}[max\{f(s,t)\}+min\{f(s,t)\}]\)</span></p>
<p><strong>Limitation and Solution</strong> - Although Median filter preserves image edges, it removes image details such as corner, thin lines / curves and other fine details. - How to design a rank order filter that can effectively removes impulsive noise[脉冲噪声] and preserves these image details at the same time?</p>
<p>As median filter underperforms mean filter in attenuating short‐ tailed noise, e.g. Gaussian noise, filters that own merits of the both mean and median filters have been developed: <span class="math inline">\(\hat{f}(x,y)= \frac{1}{mn-d}\sumf_r(s,t)\)</span>, where <span class="math inline">\(f_r(s,t)\)</span> are the ramaining mn-d pixels around median.</p>
<h2 id="image-restoration">Image Restoration</h2>
<p>We can only recover the original image well with priori knowledge of the degradation[降级]. This is the task of image restoration. Reconstruct or recover a degraded image using priori knowledge of the degradation. It is an objective process and modeling oriented. Therefore, most restoration techniques assume some knowledge of the degradation process.Different from image enhancement that is a subjective process[主观过程] that takes heuristic[启发式] procedures for human visual system or for easy computer manipulation[处理].</p>
A Model of the Image Degradation / Restoration Process:
<p align="center">
<img src="http://1.bp.blogspot.com/_rIBnkPGB9po/StPetyvo0QI/AAAAAAAAAo8/ytPUEkSQ8tg/s400/fig1.bmp" width="350" >
</p>
<p>here <span class="math inline">\(f(x,y)\)</span> is original image, <span class="math inline">\(H\)</span> is degredation function, <span class="math inline">\(\eta(x,y)\)</span> is additive noise, <span class="math inline">\(g(x,y)\)</span> is degraded (observed) image, <span class="math inline">\(\hat{f}(x,y)\)</span> is estimated (restored) image.</p>
<p><strong>Degradation Models</strong> - A simple degradation model assumes the degradation process to be an LTI / LSI system. - A simple noise model assumes additive uncorrelated noise[无关噪声]. - Assume the impulse response (point spread function) of the degradation process is finite in space[时空有限],</p>
<ol style="list-style-type: decimal">
<li>Motion blur due to relative motion between camera and object (moving camera or moving object) For example, if an image undergoes planar motion[平面运动] in x‐ and y‐directions with <span class="math inline">\(x_0(t)\)</span> and <span class="math inline">\(y_0(t)\)</span> and <span class="math inline">\(T\)</span> is the duration of the exposure, we have <span class="math inline">\(g(x,y)=\int_{0}^T f[x-x_0(t),y-y_0(t)]dt\)</span>. Suppose <span class="math inline">\(x_0(t)=at/T,y_0(t)=bt/T\)</span> We get the degradation function in Fourier domain <span class="math inline">\(H(u,v)=\frac{G(u,v)}{F(u,v)}=\frac{Tsin[\pi(ua+vb)]}{\pi (ua+vb)}e^{-j\pi(ua+vb)}\)</span>, Frequency response is a directional sinc function .</li>
<li>Rectangular aperture[矩孔衍射] of the camera <span class="math inline">\(h(x,y)=\begin{cases}1,-a\lex\le a,-b\le y \le b \\ 0, else \end{cases}\)</span>, a and b depend on the aperture dimensions. For example, out‐of‐focus means large aperture. So the above gives out‐of‐focus blur, and the more out‐of‐focus the image is, the larger the values of a and b. The frequency response will be the product of a horizontal and a vertical sinc function.</li>
<li>Atmospheric turbulence[大气湍流] Atmospheric turbulence due to random variation in the atmosphere / air between the camera and the object (for satellite / aerial / astronomical images) The frequency response is a 2‐D Gaussian function (with circular contours). <span class="math inline">\(H(u,v)=e^{-k(u^2+v^2)^{5/6}}\)</span>, k is the turbulence parameter.</li>
</ol>
<p><strong>Noise Models</strong> Typically, noise <span class="math inline">\(\eta(x,y)\)</span> is modeled as: - zero‐mean : <span class="math inline">\(E\{\eta(x,y)\}=0\)</span> - Independent to original image : <span class="math inline">\(E\{\eta(x,y)f(x+i,y+i)\}=0\)</span> - Gaussian: probability density function (PDF) of <span class="math inline">\(\eta(x,y)\)</span> is Gaussian - White noise: power spectral density(PSD) of <span class="math inline">\(\eta(x,y)\)</span> is flat and constant. This means the autocorrelation is an impulse <span class="math inline">\(E\{\eta(x,y)\eta(x+i,y+j)\}=\delta(i,j)\)</span>. Noise of different pixels is thus uncorrelated. - Some images, however, have a signal‐dependent or periodic noise component.</p>
<p><strong>Inverse Filter</strong> Inverse filter recovers the original image f(x,y) from the observed image g(x,y). <span class="math inline">\(H^I(u,v)=H^{-1}(u,v)\)</span>, <span class="math inline">\(\hat{F}(u,v)=G(u,v)H^{I}(u,v)=[F(u,v)H(u,v)+N(u,v)]H^{-1}(u,v)=F(u,v)+\frac{N(u,v)}{H(u,v)}\)</span> Problem 1: <span class="math inline">\(H^I(u,v)\)</span> will not exist if <span class="math inline">\(H(u,v)\)</span> has any zero; Problem 2: Even otherwise, inverse filters result in noise amplification if <span class="math inline">\(H(u,v)\)</span> is small at certain frequency.</p>
<p>Consider the example of motion blur. The degradation frequency response is a sinc function:Thus, the inverse filter <span class="math inline">\(H^I(u,v)\)</span> becomes infinite at some frequency points: The generalized inverse (or, pseudo‐inverse) filter solves this problem, and is defined as: <span class="math inline">\(H^{-}(u,v)=\begin{cases}\frac{1}{H(u,v) |H|\ne 0 \\ 0, |H|\ne 0}\end{cases}\)</span>, Such a filter may be practically impossible to design due to the sharp transition on either side of zero.</p>
<p>Therefore, in practice, the pseudo‐inverse[广义逆矩阵] filter is constructed as:<span class="math inline">\(H^{-}(u,v)=\begin{cases}\frac{1}{H(u,v) |H|\ge \varepsilon \ \\ 0, |H|\ne \varepsilon}\end{cases}\)</span></p>
<p><strong>Wiener Filter</strong> Inverse and pseudo‐inverse filtering doesn’t perform well in the presence of noise <span class="math inline">\(\hat{F}(u,v)=F(u,v)+\frac{N(u,v)}{H(u,v)}\)</span>. From the above formula we see that if <span class="math inline">\(H(u,v)\)</span> is zero or small at certain frequency, the term N/H at the output will be large, resulting in noise amplification[噪声放大].</p>
<p>To solve noise amplification Problem, note that if H is zero or small at some frequency or frequency range, then FH is also zero or small. since <span class="math inline">\(G(u,v)=F(u,v)H(u,v)+N(u,v)\)</span> at those frequencies the signal component (FH) is smaller than the noise component (N).</p>
<p>If we amplify at those frequencies (because 1/H is large), the noise will become larger. Instead, we should attenuate at those frequencies. This noise amplification problem of the inverse and pseudo‐inverse filtering is solve by Wiener filter which is a minimum mean square error (MMSE) linear filter. The filter mean square error is given by: <span class="math inline">\(e^2=E\{[f(x,y)-\hat{f}(x,y)]^2\}=E\{[f(x,y)-h^w(x,y)*g(x,y)]^2\}\)</span>. To minmize <span class="math inline">\(e^2\)</span>, let its differentiation with respect to <span class="math inline">\(h^w\)</span> zero <span class="math inline">\(\frac{\partial e^2}{\partial h^w(x,y)}=0\)</span> Then we have <span class="math inline">\(H^w(u,v)=\frac{H^*(u,v)S_f(u,v)}{|H(u,v)^2S_f(u,v)+S_{\eta}(u,v)|}\)</span>. The output of the Wiener filter is <span class="math inline">\(\hat{F}(u,v)=H^w(u,v)G(u,v)\)</span></p>
<p><strong>Wiener Filter Interpretation</strong> It is very easy to express the Wiener filter (least mean square error (LMSE) linear filter) as <span class="math inline">\(H^w(u,v)=\frac{1}{H(u,v)}W(u,v)\)</span>. So, the Wiener filter is an inverse filter with a scale factor: <span class="math inline">\(W(u,v)=\frac{|H(u,v)|^2S_f(u,v)}{|H(u,v)|^2S_f(u,v)+S_{\eta}(u,v)}\)</span> If there is no noise, scaling is 1, the Wiener filter reduces to inverse filter. If for some frequency there is no signal, scaling is 0, the Wiener filter becomes 0 there, noise is suppressed. If there is no degradation, H = 1, the Wiener filter reduces to smoothing filter. This scale factor decreases with increasing noise <span class="math inline">\(S_{\eta}(u,v)\)</span> and increases with decreasing noise. This scale factor approaches to zero when the degradation function approaches to zero. Therefore, the Wiener filter solves noise amplification problem and the infinity problem of the inverse filter.</p>
<p>In general, Image restoration is to inverse the degradation process H(u,v). Problem 1) <span class="math inline">\(H^I(u,v)\)</span> will not exist if <span class="math inline">\(H(u,v)\)</span> has any zero. Problem 2) Even otherwise, inverse filters result in noise amplification if <span class="math inline">\(H(u,v)\)</span> is small at certain frequency. Therefore, we modify the inverse filter to Wiener filter.</p>
<p><strong>Frequency Domain Filtering</strong> In some cases, the noise model can be well represented in the frequency domain, such as various periodic noise and directional periodic noise[定向周期性噪声]. To remove such kinds of noise effectively, we design the restoration filter in the frequency domain because the 2‐D Fourier transform provide the direction and frequency information of the image and hence noise. We will discuss band reject / pass filters, notch reject / pass filters and optimum notch filtering. An ideal band reject filter is given by: <span class="math inline">\(H(u,v)=\begin{cases}0 , if \ D_0-W/2\le D(u,v) \le D_0+W/2 \\ 1, otherwise \end{cases}\)</span>, <span class="math inline">\(D(u,v)=\sqrt{u^2+v^2}\)</span> is the distance of point (u,v) from the origin. Butterworth band reject filter: <span class="math inline">\(H(u,v)=\frac{1}{1+[\frac{D(u,v)W}{D^2(u,v)W}]^{2\pi}}\)</span> Gaussian band reject filter: <span class="math inline">\(H(u,v)=1-exp\{-\frac{1}{2}[\frac{D^2(u,v)-D_0^2}{D(u,v)W}]\}\)</span> A band pass filter is given by: <span class="math inline">\(H_{bp}(u,v)=1-H_{br}(u,v)\)</span></p>
<p>An ideal notch reject filter is given by:<span class="math inline">\(H_{bp}(u,v)=1-H_{br}(u,v)\)</span>. It is used to separate noise from the image. An ideal notch reject filter is given by: <span class="math inline">\(H(u,v)=\being{caes}0, if\ D_1(u,v)\;e D_0 \ Or \ D_2(u,v)\le D_0 \\ 1, otherwise \end{cases}\)</span>. where <span class="math inline">\(D_1(u,v)=[(u-u_0)^2+(v-v_0)^2]^{1/2}\)</span> and <span class="math inline">\(D_2(u,v)=[(u+u_0)^2+(v+v_0)^2]^{1/2}\)</span> A Butterworth notch reject filter: <span class="math inline">\(H(u,v)=\frac{1}{1+[\frac{D_0^2}{D_1(u,v)D_2(u,v)}]^n}\)</span>. A Gaussian notch reject filter: <span class="math inline">\(H(u,v)=1-exp{-\frac{1}{2}[\frac{D_1((u,v)D_2(u,v)}{D_0^2}]}\)</span> The notch pass filter is given by: <span class="math inline">\(H_{np}(u,v)=1-H_{nr}(u,v)\)</span></p>
<p>Optimum Notch Filter: 1. Observe the DFT of the image to find the noise spikes and hence design notch pass filter H(u,v) 2. <span class="math inline">\(\eta(x,y)=F^{-1}\{H(u,v)G(u,v)\}\)</span> 3. <span class="math inline">\(w(x,y)=\frac{\bar{g(x,y)\eta(x,y)-\bar{g}(x,y)\bar{\eta}(x,y)}}{\bar{\eta}^2(x,y)-\bar{\eta}^2(x,y)}\)</span> 4. <span class="math inline">\(\hat{f}(x,y)=g(x,y)-w(x,y)\eta(x,y)\)</span> Where the bar operation is mean over the neighborhood pixels of (x, y), S(x,y).</p>
<p>w(x,y) is determined by making the mean square error minimum. Minimize <span class="math inline">\(\delta^2(x,y)\)</span> by Solve: <span class="math inline">\(\frac{\partial \delta^2(x,y)}{\partial w(x,y)}=0\)</span> Obtain: <span class="math inline">\(w(x,y)=\frac{\bar{g(x,y)h(x,y)}-\bar{g}(x,y)\bar{\eta}(x,y)}{\bar{\eta^2}(x,y)-\bar{\eta}^2(x,y)}\)</span> Problem: During acquisition, an image undergoes uniform planar motion in x‐ and y‐directions with xo(t) and yo(t) and T is the duration of the exposure. Assuming that shutter opening and closing time are negligible, give an expression for the blurring function H(u,v). Solution: Solution: Suppose the real image without motion blurring is f(x,y) and the motion blurred image is g(x,y). We have <span class="math inline">\(g(x,y)=\int_{0}^Tf(x-x_0(t),y-y_0(t))dt\)</span> Appling Fourier transform, we get <span class="math inline">\(G(u,v)=\int_{0}^T e^{-j2\pi[ux_0(t)+vy_0(t)]}dt\)</span></p>
<p>Problem: If the image undergoes uniform linear planar motion, i.e. <span class="math inline">\(x_o(t)=\alpha t/T\)</span> and <span class="math inline">\(y_o(t) =bt/T\)</span> . What is the blurring function H(u,v)? Solution: <span class="math inline">\(H(u,v)=\int_{0}^Te^{-j2\pi [ux_0(t)+vy_0(t)]}dt=\frac{Tsin[\pi(ua_vb)]}{\pi (ua+vb)}e^{-j\pi(ua+vb)}\)</span></p>
<h2 id="morphological-image-processing">Morphological Image Processing</h2>
<p>Looking at these images…… What is interesting, important or useful information we care about? Region shape and boundaries of object are important. Form and structure can be represented by object pixel set. Image analysis needs to measure the characteristics of objects in the images.Geometric[几何学的] measurements (e.g., object location, orientation, area, length of perimeter) are important characteristics of objects. These geometric characteristics is often easier to be extracted/measured from binary images.</p>
<p>Visual perception requires transformation of images so as to make explicit particular shape information.Goal: Distinguish meaningful shape information from irrelevant one.The vast majority of shape processing and analysis techniques are based on designing a shape operator which satisfies desirable properties.</p>
<p>Morphology[形态学] deals with form and structure. Mathematical morphology is a tool for extracting image components useful in:representation and description of region shape (e.g. boundaries). pre‐ or post‐processing (filtering, thinning, etc.). Morphological operations are powerful tools in image analysis. They usually operate on binary images and thus often follow a segmentation task or an edge detection task.Based on set theory[集合论] and logic operations[逻辑运算].</p>
<p><strong>set theory</strong> A two dimensional integer space is denoted by <span class="math inline">\(Z^2\)</span>, An element in this space has two components <span class="math inline">\(a=(a_1,a_2)\)</span>. For image representation, <span class="math inline">\(a=(a_1, a_2)\)</span> are the x‐ and y‐ coordinates of a pixel. - Let A be a set in <span class="math inline">\(Z^2\)</span>. If <span class="math inline">\(a=(a_1, a_2)\)</span> is an element of A, we denote <span class="math inline">\(a \in A\)</span> - If not, then <span class="math inline">\(a \notin A\)</span> - <span class="math inline">\(\varnothing\)</span> denotes null(empty) set - An example that specifies a set C: <span class="math inline">\(C=\{w|w=a+d,a\in A\}\)</span>, d=(8,5) - If a set A is a subset of B, we denote: <span class="math inline">\(A \subseteq B\)</span> - Union of A and B: <span class="math inline">\(C=A\cup B\)</span> - Intersection of A and B: <span class="math inline">\(D=A\bigcap B\)</span> - Disjoint[解体] sets: <span class="math inline">\(A \bigcap B = \varnothing\)</span> - Complement[补集] of A: <span class="math inline">\(A^c=\{w|w\notin A\}\)</span> - Difference of A and B: <span class="math inline">\(A-B=\{w|w\in A, w\notin B\}=A\bigcap B^c\)</span> - Translation of A by <span class="math inline">\(z=(z_1,z_2):\)</span> <span class="math inline">\((A)_z=\{c|c=a+z, a\in A\}\)</span> - Reflection[反射] of B : <span class="math inline">\(\hat{B}=\{w|w=-b,b\in B\}\)</span></p>
<p><strong>Morphological Operators</strong> Primary morphological operations are Dilation[膨胀] and Erosion[腐蚀]. More complicated morphological operators such as Opening and Closing can be designed by means of combining erosions and dilations. - Opening generally smoothes the contour[轮廓] of an image and eliminates protrusions[消除了突出] - Closing smoothes sections of contours, but it generally fuses[融合] breaks, holes and gaps</p>
<p><strong>Dilation</strong> Dilation of A by B, denoted by <span class="math inline">\(A\oplus B\)</span> , is defined as: $AB ={z|[(_z A)]} Interpretation[解释]: Obtaining the reflection of <span class="math inline">\(B\)</span> about its origin and then shifting this reflection by <span class="math inline">\(z\)</span>. Dilation of <span class="math inline">\(A\)</span> by <span class="math inline">\(B\)</span> then is the set of all <span class="math inline">\(z\)</span> displacements such that the shifted <span class="math inline">\(\hat{B}\)</span> and <span class="math inline">\(A\)</span> overlap by at least one nonzero element. B is called the structuring element in Dilation.</p>
<p>Dilation of A by B can also be expressed as: <span class="math inline">\(A\oplus B=\{z|[\hat{B}_z \bigcap A]\subseteq A\}\)</span> Further Interpretation: Set B can be viewed as a convolution mask. The basic process of “flipping” B and then successively displace it so that it slides over set (image) A is analogous to the convolution.</p>
<p>The dilation morphological operation generates an output image ‘g’ from an input image ‘f’ using a structuring element ‘h’ where: Place(shift) the center(origin) of ‘h’ at(to) (x, y): <span class="math inline">\(g(x,y)=\begin{cases}1, if \ h \ hints f \\ 0, \  else \end{cases}\)</span>. The effect of dilation with 3 x 3 mask is to add asingle layer of pixels to the outer edge of an object and to decrease by a single layer of pixels to the holes in the object. A 5 x 5 mask will add two layers of pixels which is equivalent to applying a 3 x 3 mask twice. The main application of dilation is to remove small holes from the interior[内部] of an object.</p>
<p><strong>Erosion</strong> - Erosion of A by B, denoted <span class="math inline">\(A \ominus B\)</span>, is defined as : <span class="math inline">\(A \ominus B = \{z|(B)_z \subseteq A\}\)</span> - Erosion of A by B is the set of all points z such that B, translated by z, is contained in A. - Comparing with the Dilation: <span class="math inline">\(A\oplus B=\{z|[\hat{B}_z \bigcap A]\subseteq A\}\)</span> - Dilation and erosion are duals of each other with respect to set complementation and reflection. That is <span class="math inline">\((A \ominus B)^c=A^c \oplus \hat{B}\)</span> - The erosion morphological operation generates an output image ‘g’from an input image ‘f’ using a structuring element ‘h’ where: Place(shift) the center(origin) of ‘h’ at(to) (x, y) <span class="math inline">\(g(x,y)=\begin{cases}1, if \ h \ completly \ falls \ in \ f \\ 0, \ else \end{cases}\)</span> - The effect of an erosion with 3 x 3 mask is to strip a single layer of pixels from the outer edge of an object and to increase by a single layer of pixels to holes in the object. - A 5 x 5 mask will strip off two layers of pixels which is equivalent to applying a 3 x 3 mask twice. - The main application of erosion is to remove small noise artifacts from an image.</p>
<p><strong>Opening</strong> A compound operation is when two or more morphological operations are performed in succession. A common example is opening which is an erosion followed by a dilation: <span class="math inline">\(A \circ B = (A\ominus B)\oplus B\)</span> The opening A by B is obtained by taking the union of all translates of B that fit into A. This can be expressed as a fitting processing such that: <span class="math inline">\(A\circ B=\cap \{(B)_z | (B)_z \subseteq A \}\)</span>, <span class="math inline">\(A \ominus B = \{z|(B)_z \ subseteq A\}\)</span>. Note that the outward pointing corners are rounded, where the inward pointing corners remain unchanged. Opening is often performed to clear an image of noise whilst retaining the original object size. Care must be taken that the operation does not distort the shape size of the object if this is significant.The opening operation tends to flatten the sharp peninsular[半岛] projections on the object.A useful way to see the effects of an opening operation is to look for differences between the original image and the image after opening by projecting these differences onto the original image.</p>
<p><strong>Closing</strong> Closing is the complementary operation[互补运算] of opening, defined as dilation followed by erosion. <span class="math inline">\(A\bullet B = (A\oplus B)\ominus B\)</span> Opening and closing are duals of each other as: $(AB)<sup>c=A</sup>c $ Or : <span class="math inline">\(A\bullet B=(A^c\circ \hat{B})^c\)</span> Note that the inward pointing corners are rounded, where the outward pointing corners remain unchanged.</p>
<p>The classic application of closing is to fill holes in a region whilst retaining the original object size. Dilation fills the holes and erosion restores the original region size. In addition to filling holes the closing operation tends to fill the ‘bays’ on the edge of a region.</p>
<p><strong>Opening and Closing</strong> The opening operation satisfies the following properties: - <span class="math inline">\(A\circ B\)</span> is a subset(subimage) of A - If C is a subset of D, then <span class="math inline">\(C \circ B\)</span> is a subset of <span class="math inline">\(D \circ B\)</span> - <span class="math inline">\((A\circ B)\circ B=A \circ B\)</span></p>
<p>Similarly, the closing operation satisfies the following properties: - A is a subset (subimage) of <span class="math inline">\(A \bullet B\)</span> - If C is a subset of D, then <span class="math inline">\(C\bullet B\)</span> is a subset of <span class="math inline">\(D\bullet B\)</span>. <span class="math inline">\((A\bullet B)\bullet B=A\bullet B\)</span></p>
<p><strong>Algorithms and Applications</strong> Boundary Extraction: The boundary of a set A, denoted by <span class="math inline">\(\beta(A)\)</span>, can be obtained by <span class="math inline">\(\beta(A)=A-(A\ominus B)\)</span> Region Filling: <span class="math inline">\(A^F=X_k\cap A\)</span>. Beginning with a point X0 inside the boundary, the entire region inside the boundary is filled by the above procedure. - An example of Region Filling: <span class="math inline">\(A^F=X_k \cap A\)</span>, <span class="math inline">\(X_k=(X_{k-1}\oplus B)\bigcap A^c\)</span>, k=1,2,3... - Extract connected components: <span class="math inline">\(X_k=(X_{k-1}\oplus B)\bigcap A\)</span></p>
<p>Denoising: <span class="math inline">\((A\circ B)\bullet B\)</span> or <span class="math inline">\((A\bullet B)\circ B\)</span> Can be used to eliminate noise and its effect on the object. Noise pixels outside the object area are removed by opening with B while noise pixels inside the object area are removed by closing with B.</p>
<h2 id="image-analysis-i-segmentation-and-edge-detection">Image Analysis I: Segmentation and Edge Detection</h2>
<p><strong>Segmentation</strong> Segmentation is to subdivide an image into its constituent regions or objects. Segmentation should stop when the objects of interest in an application have been isolated. Segmentation algorithms generally are based on one of 2 basic properties of intensity values: - similarity: to partition an image into regions that are similar according to a set of predefined criteria. (Thresholding) - discontinuity: to partition an image based on abrupt changes in intensity (Point, Line and Edge Detection)</p>
<p><strong>Segmentation by Thresholding</strong> Many images contain some objects of interest of uniform brightness placed against a background of differing brightness. Typical examples include handwritten and typewritten text, microscopic biomedical samples, fingerprints, and airplanes on a runway. A thresholded image g(x, y) is defined as <span class="math inline">\(g(x,y)=\begin{cases}1, if \ f(x,y)&gt;T \\ 0 , \ if \ f(x,y)\le T \end{cases}\)</span>, where T is the threshold given by <span class="math inline">\(T=T[x,y,p(x,y),f(x,y)]\)</span> - Global threshold: T depends on gray‐level values f(x, y)of the whole image alone - Local threshold: T depends on both f(x, y)and its local neighbors property p(x, y) - Adaptive threshold: T depends on x and y coordinates</p>
<p>Basic Global Thresholding: Use T midway between the max and min gray levels generate binary image</p>
<p>Heuristic approach to get global threshold T: - Select an initial estimate for T. - Segment the image using T. This will produce two groups of pixels: <span class="math inline">\(G_1\)</span> consisting of all pixels with gray level values &gt; T and <span class="math inline">\(G2_\)</span> consisting of pixels with gray level values <span class="math inline">\(\le T\)</span> - Compute the average gray level values <span class="math inline">\(\mu_1\)</span> and <span class="math inline">\(\mu_2\)</span> for the pixels in regions <span class="math inline">\(G_1\)</span> and <span class="math inline">\(G_2\)</span> - Compute a new threshold value <span class="math inline">\(T=0.5(\mu_1+\mu_2)\)</span> - Repeat steps 2 through 4 until the difference in T in successive iterations is smaller than a predefined parameter <span class="math inline">\(T_0\)</span>.</p>
<p>If Object and background are separated in the grey value, easily use global thresholding If the grey value of object and background are overlapped, Difficult to segment using global thresholding, Solution: Adaptive local thresholding</p>
<p>Adaptive local thresholding: - Subdivide original image into small areas. - Utilize a different threshold to segment different sub‐ images. - Since the threshold used for each pixel depends on the location of the pixel in terms of the sub‐images, this type of thresholding is adaptive.</p>
<p>Further subdivision: - properly and improperly segmented subimages from previous example - corresponding histograms - further subdivision of the improperly segmented subimage. - histogram of small subimage at top - result of adaptively segmenting</p>
<p><strong>Optimal Thresholding</strong> Objective: Minimize the average error in making decisions that a given pixel belongs to an object or the background Assumptions: Image contains only 2 gray‐level regions. <span class="math inline">\(p_1(z)\)</span> and <span class="math inline">\(p_2(z)\)</span> are the probability density functions of grey level <span class="math inline">\(z\)</span> for region 1 (object) and 2 (background) respectively.</p>
<p>Probability of error in classifying a background point as an object:<span class="math inline">\(E_1(T)=\int_{-\infty}^{T}p_2(z)dz\)</span> Probability of error in classifying an object point as background: <span class="math inline">\(E_2(T)=\int_{T}^{\infty}p_1(z)dz\)</span> Mixture pdf of the overall image: <span class="math inline">\(p(z)=P_1p_1(z)+P_2p_2(z)\)</span> - Assume any pixel belongs to either object or background: <span class="math inline">\(P_1+P_2=1\)</span> - Overall probability of error: <span class="math inline">\(E(T)=P_2E_1(T)+P_1E_2(T)\)</span></p>
<p>To minimize the error, differentiate E(T) with respect to T and let the result equal to 0: <span class="math inline">\(\frac{dE(T)}{dT}=0\)</span>, then find T which makes <span class="math inline">\(P_1p_1(T)=P_2p_2(T)\)</span>. If <span class="math inline">\(P_1 = P_2\)</span>, the optimum threshold is where the curve <span class="math inline">\(p_1(z)\)</span> and <span class="math inline">\(p_2(z)\)</span> intersect.</p>
<p>Assuming both <span class="math inline">\(p_1(z)\)</span> and <span class="math inline">\(p_2(z)\)</span> follow Gaussian distribution: <span class="math inline">\(p(z)=\frac{P_1}{\sqrt{2\pi}\detla_1}e^{-\frac{(z-\mu_1)2}{2\delta_1^2}}+\frac{P_2}{\sqrt{2\pi}\detla_2}e^{-\frac{(z-\mu_2)2}{2\delta_2^2}}\)</span> where <span class="math inline">\(\mu_1\)</span> and <span class="math inline">\(\delta_1^2\)</span> are the mean and variance of the Gaussian density of one object. <span class="math inline">\(\mu_2\)</span> and <span class="math inline">\(\delta_2^2\)</span> are the mean and variance of the Gaussian density of the other object.</p>
<p>The optimum T is obtained by solve: <span class="math inline">\(P_1p_1(T)=P_2p_2(T) \rightarrow \frac{P_1}{\sqrt{2\pi}\delta_1}e^{-\frac{(T-\mu_1)^2}{2\delta_1^2}}=\frac{P_2}{\sqrt{2\pi}\delta_2}e^{-\frac{(T-\mu_2)^2}{2\delta_2^2}}\)</span> The result in a quadratic equation: <span class="math inline">\(AT^2+BT+C=0\)</span>, where <span class="math inline">\(A=\delta_1^2-\delta_2^2\)</span>, <span class="math inline">\(B=2(\mu_1\delta_2^2-\mu_2\delta_1^2)\)</span>, <span class="math inline">\(C=\delta_1^2\mu_2^2-\delta_2^2\mu_1^2+2\delta_1^2\delta_2^2ln(\delta_2P_1/\delta_1P_2)\)</span></p>
<p>If <span class="math inline">\(\delta_1 = \delta_2 = \delta\)</span>,the optimum threshold is simply obtained by <span class="math inline">\(T=\frac{\mu_1_\mu_2}{2}+\frac{\delta^2}{\mu_1-\mu_2}ln(\frac{P_2}{P_1})\)</span>. if <span class="math inline">\(P_1 = P_2\)</span>, then the optimal threshold is the average of the two means <span class="math inline">\(T=\frac{\mu_1+\mu_2}{2}\)</span>.</p>
<p><strong>Detection of Discontinuities[检测不连续性]</strong> Detect the three basic types of gray‐level discontinuities (abrupt changes in intensity) - Point detection - Line detection - Edge detection</p>
<p>The simple way is to run a mask through the image <span class="math inline">\(R=\sum_{i=1}^9 w_i z_i\)</span>, The formulation measures the weighted difference between the center point and its neighbors.Apoint has been detected at the location on which the mask is centered if <span class="math inline">\(|R|\ge T\)</span></p>
<p><strong>Point Detection example</strong> Point in larger scales– blob and corner detection: Multiscale interesting point detection with scale selection: refer to Horizontal mask will result in maximum response when a line passed through the middle row of the mask with a constant background. The similar idea is used with other masks. note: The preferred direction of each mask is weighted with a larger coefficient (i.e.,2) than other possible directions.</p>
<p><strong>Line Detection</strong> Apply every mask on the image, let R1, R2, R3,R4 denotes the response of the horizontal, +45 degree, vertical and ‐45 degree masks, respectively. if, at a certain point in the image <span class="math inline">\(|R_i|&gt;|R_j|\)</span>. for all <span class="math inline">\(j\ne i\)</span>, that point is said to be more likely associated with a line in the direction of mask i. To detect all lines in an image in the direction defined by a given mask, we simply run the mask through the image and threshold the absolute value of the result. Points left are the strongest responses, which, correspond closest to the direction defined by the mask.</p>
<p><strong>Edge Detection</strong> How can an algorithm extract relevant information from an image to recognize objects? Most important information for the interpretation of an image (for both technical and biological systems) is the contour of objects. Contours are indicated by abrupt changes in brightness. We can use edge detection filters to extract contour information from an image.</p>
<p>Edge detection is the most common approach for detecting meaningful discontinuities in gray level.We will discuss approaches of - first‐order derivative (Gradient operator) - second‐order derivative (Laplacian operator)</p>
<p>Intuitively, an edge is a set of connected pixels that lie on the boundary between two regions. Changes or discontinuities in image amplitude provide an indication of physical extent of object.</p>
<p><strong>Edge First &amp;Second Derivatives</strong> Noise free edge and its derivatives, The signs of the derivatives would be reversed for an edge that transitions from light to dark.</p>
<p><strong>Noisy Edge Derivatives</strong> First column: images and gray‐level profiles of a ramp edge corrupted by random Gaussian noise of mean 0 and ? = 0.0, 0.1, 1.0 and 10.0, respectively. Second column: first‐ derivative images and gray‐ level profiles. Third column : second‐ derivative images and gray‐ level profiles..</p>
<p>Fairly[公平地] little noise can have a significant impact on the two key derivatives used for edge detection in images. Image smoothing should be serious consideration prior to the use of derivatives in applications where noise is likely to be present. To determine a point as an edge point: - The transition in grey level associated with the point has to be significantly stronger than the background at that point. - Use threshold to determine whether a value is “significant” or not. - The point’s two‐dimensional first‐order derivative must be greater than a specified threshold.</p>
<p><strong>Image First Derivative: Gradient</strong> The first‐order derivative of image called gradient is a two‐ dimensional vector, which consists of x‐ and y‐differentials. <span class="math inline">\(\triangledown f(x,y)=[\begin{cases}G_x(x,y) \\ G_y(x,y)\end{cases}]=\begin{cases}\frac{\partial f(x,y)}{\partial x} \\ \frac{\partial f(x,y)}{\partial y}\end{cases}\)</span> The strength of the differentials is proportional[比例项] to the degree of discontinuity of the image. Thus, image differentiation: enhances edges and other discontinuities (noise); deemphasizes[使不重要] area with slowly varying gray‐level values.</p>
<p>A image gradient vector has magnitude <span class="math inline">\(|\triangledown f(x,y)|=[G_x^2(x,y)+G_y^2(x,y)]^{1/2}\)</span>, and direction <span class="math inline">\(\phi(x,y)=tan^{-1}(\frac{G_y(x,y)}{G_x(x,y)})\)</span> Gradient vector points in the direction of maximum rate of change of f at coordinate (x, y). The directions (orientation) of an edge at (x, y) is perpendicular to the direction of the gradient vector at that point. In this orientation, the rate of change of f at coordinate (x, y) is minimum.</p>
<p>Therefore, certain smoothing is desirable prior to application of differentiation. <span class="math inline">\(\triangledown [h(x,y)*f(x,y)]=[\triangle h(x,y)]*f(x,y)\)</span>. Due to linearity of differentiation, differentiate the image convolved (smoothed) with h is same as convolving an image with <span class="math inline">\(\triangledown h(x,y)\)</span>. So we have gradient operator (Mask) <span class="math inline">\(\triangledown h(x,y)\)</span>. Different design of the smooth filter h(x,y) leads to various different gradient operators (Masks) <span class="math inline">\(\triangledown h(x,y)\)</span>.</p>
<p>Gradient operator (mask) should have noise suppression characteristics, which is important for robust edge detection. However, gradient operator may cause biased gradient direction due to the discrete image and the smoothing filter. Accurate gradient direction is desirable for some applications.</p>
<p><strong>Histogram of Gradient or Unsigned Gradient?</strong> Second‐order derivative is also called Laplacian operator defined by <span class="math inline">\(\triangledown^2f = \frac{\partial^2f(x,y)}{\partial x^2}+\frac{\partial^2f(x,y)}{\partial y^2}\)</span> Approximation in discrete domain: <span class="math inline">\(\frac{\partial^2f}{\partial x^2}=f(x+1,y)+f(x-1,y)-2f(x,y)\)</span>, <span class="math inline">\(\frac{\partial^2f}{\partial y^2}=f(x,y+1)+f(x,y-1)-2f(x,y)\)</span> This yield : <span class="math inline">\(\triangledown^2f = f(x+1,y)+f(x-1,y)+f(x,y+1)+f(x,y-1)-4f(x,y)\)</span>.</p>
<p><strong>Laplacian of Gaussian (LoG)</strong> Laplacian is sensitive to noise, Therefore, certain smoothing is desirable prior to application of Laplacian. Solution: Employ Gaussian‐shaped smoothing <span class="math inline">\(h(x,y)=-e^{-\frac{x^2+y^2}{2\delta^2}}=-e^{-\frac{r^2}{2\delta^2}}\)</span>. Due to linearity of second derivative, taking the Laplacian of the image convolved (smoothed) with h is same as convolving an image with <span class="math inline">\(\triangledown^2 h\)</span>. <span class="math inline">\(\trianledown^2(h*f)=(\triangledown^2 h )*f\)</span>.</p>
<strong>Example of LoG</strong>
<p align="center">
<img src="http://softwarebydefault.files.wordpress.com/2013/05/laplacian_5x5_of_gaussian_5x5_type1.jpg" width="350" >
</p>
<p><strong>Edge Linking &amp; Boundary Detection</strong> Edge detection algorithm are followed by linking procedures to assemble edge pixels into meaningful edges. Basic approaches Local: Local Processing; Global Processing via the Hough Transform.</p>
<p><strong>Local Processing</strong> Analyze the characteristics of pixels in a small neighborhood (say, 3x3, 5x5) about every edge pixels (x,y) in an image. All points that are similar according to a set of predefined criteria are linked, forming an edge of pixels that share those criteria. 1. The strength of the gradient vector: An edge pixel with coordinates (<span class="math inline">\(x_0,y_0\)</span>) in a predefined neighborhood of (x,y) is similar in magnitude to the pixel at (x,y). if <span class="math inline">\(|\triangledown f(x,y)-\triangledown f(x_0,y_0)|\le E\)</span>. 2. The direction of the gradient vector: An edge pixel with coordinates (x0,y0) in a predefined neighborhood of (x,y) is similar in angle to the pixel at (x,y) if <span class="math inline">\(|\theta(x,y)-\theta(x_0,y_0)|&lt;A\)</span>. A point in the predefined neighborhood of (<span class="math inline">\(x_0,y_0\)</span>) is linked to the pixel at (x,y) if both magnitude and direction criteria are satisfied.</p>
<p><strong>Hough Transform</strong> Hough transform is a technique that can be used to detect (link) regular curves such as lines, circles, and ellipses in an image, Line segment in spatial space:<span class="math inline">\(y_i=ax_i+b\)</span>. If the line passes through a point (pixel) (<span class="math inline">\(x_i, y_i\)</span>), we obtain: <span class="math inline">\(y_i=ax_i+b\)</span>, Rewrite it in ab‐plane or parametric space: <span class="math inline">\(b=y_i-ax_i\)</span></p>
<p>A line in xy‐plane is a set of points (x, y) that satisfy equation: <span class="math inline">\(y=a&#39;x+b&#39;\)</span> which is mapped into a point in ab-plane ( <span class="math inline">\(a&#39;, b&#39;\)</span>). All points (xi, yi) on asame line in the image must fall into a same point (ai,bi)in the parametric space. Hough transform: - Ddivision of parameter space into accumulator cells (a, b). - All cells are initialized to zero, <span class="math inline">\(A(a,b)=0\)</span> - For each detected point (<span class="math inline">\(x_i, y_i\)</span>) in the image: <span class="math inline">\(A(a,b)+1 \rightarrow A(a,b)\)</span> for all a and b satisfying <span class="math inline">\(b=y_i-ax_i\)</span>. At the end of the procedure, value A(a, b) corresponds to the number of points in image lying on the line y = ax+b.</p>
<p>Problem of using y=ax+b is that a is infinite for a vertical line. To avoid the problem, use equation <span class="math inline">\(x cos\theta + ysin \theta = \uprho\)</span> to represent a line instead. Vertical line has <span class="math inline">\(\theta = 90^{\circ}\)</span> with <span class="math inline">\(\uprho\)</span> equals to the positive y‐intercept or <span class="math inline">\(\theta= ‐90\)</span> with <span class="math inline">\(\uprho\)</span> equals to the negative y‐intercept.</p>
<p>Generalized Hough transform can be used for any function of the form <span class="math inline">\(g(v,c)=0\)</span>, v is a vector of coordinates, c is a vector of coefficients. For example a circle is represented by equation: <span class="math inline">\((x-c_1)^2+(y-c_2)^2=c_2^2\)</span>, three parameters (c1, c2, c3), cube like cells, accumulators of the form A(c1, c2, c3), For each point in the image, update the value of A(c1, c2, c3) {A(c_1, c_2, c_3)+1A(c_1,c_2,c_3)} that satisfies the equation <span class="math inline">\((x-c_1)^2+(y-c_2)^2=c_3^2\)</span>.</p>
<ul>
<li>Compute the gradient of an image and threshold it to obtain a binary image.</li>
<li>Specify subdivisions in the <span class="math inline">\(\theta\)</span>‐plane.</li>
<li>Examine the counts of the accumulator cells for high pixel concentrations.</li>
<li>Examine the relation (principally for continuity) between pixels in a chosen cell.</li>
<li>A gap at any point is linked if the distance between that point and its closet neighbor below a certain threshold.</li>
</ul>
<p><strong>Local Dominant Orientation</strong> Fingerprint classification: for example, arch, whorl and loop: Local orientations are intrinsic features for such task. Orientations denoted by short lines. Local orientations <span class="math inline">\(\phi(x,y)\)</span>, <span class="math inline">\(0^o&lt;\phi(x,y)&lt;180^o\)</span> of edges and lines are important image features. The gradient vectors are very noisy and a same orientation may be represented by gradients with opposite directions. How to extract stable and robust dominant (main) orientation information of a local area of image? What is the square of a vector?</p>
<p>Should we smooth the image first then take gradient or smooth the gradient image? The output of any linear filter is a weighted average of all inputs. What is the problem of average the gradient vector? Problem is the directions of gradient <span class="math inline">\(\theta(x,y), 0^o&lt;\theta(x,y)\le 360^o\)</span> but the orientations of edges or lines <span class="math inline">\(\phi(x,y), 0^o&lt;\phi(x,y)\le 180^o\)</span>. One solution is to smooth or average the squared gradient vectors. The vector direction angle will be doubled.</p>
<ul>
<li>What is the minimum and maximum values of coh(x,y) ?</li>
<li>In what cases coh(x,y) reaches to the minimum or maximum value?</li>
<li>How to choose the appropriate size of the average window S(x,y) ?</li>
<li>What factor will affects the selection of the S(x,y) ?</li>
<li>What problems of this technique may have?</li>
</ul>
<h2 id="image-analysis-ii-hough-transform-orientation-analysis">Image Analysis II: Hough Transform &amp; Orientation Analysis</h2>
<h2 id="understand-pattern-recognition-decision-theory">Understand Pattern Recognition &amp; Decision Theory</h2>
<p>What is on earth the Pattern Recognition? Pattern recognition is to perceive a pattern, extract the relevant information, understand the content of the information and make decision automatically by machine or computer.</p>
<p>As you decide <span class="math inline">\(w_k\)</span>, base on your observed value <span class="math inline">\(x\)</span>, the probability that you make a correct decision is hence <span class="math inline">\(p(w_k|x)\)</span>. Therefore, the probability that you make a wrong decision or error is:<span class="math inline">\(p(e_k|x)=1-p(w_k|x)\)</span>. If your decision is <span class="math inline">\(w_k\)</span>, because <span class="math inline">\(p(w_k|x)\)</span> is maximum, consequently, the probability of the decision error will be minimum. Therefore, this decision rule is optimal in the sense of minimizing the probability of the decision error.</p>
<p>This decision rule is called the maximum a posterior (MAP) rule, Decide: <span class="math inline">\(w_k = arg max[ p(w_i | x)] = arg min[ p(e_i | x)]\)</span>. It minimizes the probability of the decision error. More generally, If there are c possible classes, obviously <span class="math inline">\(\sum_{i=1}^cp(w_i|x)=1\)</span>, for <span class="math inline">\(\forall x\)</span>. Therefore <span class="math inline">\(p(e_k|x)=1-p(w_k|x)=\sum_{i=1,i\ne k}^c p(w_i|x)\)</span></p>
<p>To compute <span class="math inline">\(p(w_k|x)\)</span>, we just need a very basic formula in the probability theory(joint probability), <span class="math inline">\(p(x,w_i)=p(x)p(w_i|x)=p(w_i)p(x|w_i)\)</span>. so we have <span class="math inline">\(p(w_i|x)=\frac{p(w_i)p(x|w_i)}{p(x)}\)</span>, in addition <span class="math inline">\(p(x)=\sum_{i=1}^c p(x|w_i)p(w_i)\)</span></p>
<p>To fully automatically recognize a person’s gender for all value of x, you need the prior probability <span class="math inline">\(p(w_i)\)</span> of all classes and the class conditional probability <span class="math inline">\(p(x|w_i)\)</span>, of all possible value of <span class="math inline">\(x\)</span>. The decision rule will be the same as before:Decide<span class="math inline">\(w_k=argmax[p(w_i|x)]\)</span>.</p>
<p>But how good is the system? Or what is the performance of the system? Or how to evaluate your designed system? We have understand the formula to compute the error probability for a specific value of <span class="math inline">\(x\)</span>. <span class="math inline">\(p(e_k|x)=1-p(w_k|x)=\sum_{i=1,i\ne k}^c p(w_i|x)\)</span>. It is not difficult to understand that the performance of a pattern recognition system can be measured by the average of <span class="math inline">\(p(e_k|x)\)</span> over all possible value of x.</p>
<p>How to average of <span class="math inline">\(p(e_k|x)\)</span> over all possible value of x? The right way for a random variable should be:<span class="math inline">\(p(e)=\sum_{x=-\infty}{\infty}p(e_k|x)p(x)\)</span> for discrete x,<span class="math inline">\(p(e)=\int_{-\infty}^{\infty}p(e_k|x)p(x)dx\)</span>, for continuous x</p>
<p>Let’s take the continuous x for further working:<span class="math inline">\(p(e)=\int_{-\infty}^{\infty}p(e_k|x)p(x)dx=\int_{-\infty}^{\infty}[1-\frac{p(w_k)p(x|w_k)}{p(x)}]p(x)dx=1-\int_{-\infty}^{\infty}p(w_k)p(x|w_k)dx\)</span>. Note that for different region of x, the pattern recognition system has different decision ωk. Pattern recognition is to partition the whole space of x into c decision regions Ri. Therefore,<span class="math inline">\(p(e)=1-\sum_{i=1}^c\int_{R_i}p(w_i)p(x|w_i)dx=1-\sum_{i=1}^cp(w)\int_{R_i}p(x|w_i)dx\)</span></p>
<p>Obviously, the probability of the correct decision is <span class="math inline">\(p(correct)=1-p(e)=\sum_{i=1}^cp(w_i)\int_{R_i}p(x|w_i)dx\)</span>. for the 2-class problem <span class="math inline">\(p(e)=p(w_1)\int_{R_2}p(x|w_1)dx+p(w_2)\int_{R_1}p(x|w_2)dx\)</span></p>
<p>Now if you need classify an object into one <span class="math inline">\(w_k\)</span> of the <span class="math inline">\(c\)</span> classes <span class="math inline">\(w_i\)</span> after receiving information (in a column vector <span class="math inline">\(x\)</span>) of this particular object, it is the same straightforward to choose the class with highest probability. Mathematically it is now: Decide <span class="math inline">\(w_k=argmax[p(w_i|x)]\)</span>. This decision rule is called the maximum a posterior (MAP) rule, and the probability of making mistake <span class="math inline">\(p(e_k|x)=1-p(w_k|x)=\sum_{i=1,i\ne k}^c p(w_i|x)\)</span> is then minimum.</p>
<p>Allowing the use of more than one feature merely requires replacing the scalar <span class="math inline">\(x\)</span> by the feature vector <span class="math inline">\(x\)</span>, where <span class="math inline">\(x\)</span> is a point in a <span class="math inline">\(d\)</span>-dimensional Euclidean space <span class="math inline">\(R\)</span>-d , called the feature space.</p>
<p>Note that: <span class="math inline">\(\sum_{i=1}^c p(w_i|x)=1\)</span>, <span class="math inline">\(p(w_k|x)=1-\sum_{i=1,i\ne k}^c p(w_i|x)\)</span>. Therefore the decision rule can also be expressed as : Decide: <span class="math inline">\(w_k=argmin[\sum_{i=1,i\ne k}^cp(w_i|X)]=argmin[p(e_k|x)]\)</span>. It is therefore pretty clear that this decision rule minimizes the probability of the mistake or error. So it is also called the minimum error rate classifier. we see that the error is the sum of c-1 terms: <span class="math inline">\(p(e_k|x)=1-p(w_k|X)=\sum_{i=1,i\ne k}^c p(w_i|x)\)</span>. It is the sum of the probabilities of all other classes.</p>
<p>In some application, wrongly classifying one class may not be at the same cost or same risk as wrongly classifying another class. Let’s <span class="math inline">\(\lambda_{ki}\)</span> denote the cost or risk of wrongly classifying the object of class <span class="math inline">\(w_i\)</span> into class <span class="math inline">\(w_k\)</span>. The cost or risk of the decision <span class="math inline">\(w_k\)</span> is then <span class="math inline">\(R_k(x)=\sum_{i=1,i\ne k}^c \lambda_{ki}p(w_i|x)\)</span>. Therefore the decision rule that minimizes the risk or cost becomes: Decide :<span class="math inline">\(w_k=argmin[R_i(x)]=argmin[\sum_{j=1,j\ne i}^c\lambda_{ij}p(w_j|x)]\)</span>. This is the famous Bayesian decision rule. Cost functions allow us to treat situations where some kinds of classification mistakes are more costly than others, although we often discuss the simplest case where all errors are equally costly.</p>
<p>In some applications, the correct decision for different class may have different cost <span class="math inline">\(\lambda_{kk}\ne 0\)</span>. By including the cost of correct decision, the cost or risk of a decision is then <span class="math inline">\(R_k(x)=\sum_{i=1}^c\lambda_{ki}p(w_i|x)\)</span>. Therefore, the famous Bayesian decision rule is generalized as: Decide: <span class="math inline">\(w_k=argmin[R_i(x)]=argmin[\sum_{j=1}^c \lambda_{ij}p(w_j|x)]=argmin[\sum_{j=1}^c \lambda_{ij}p(w_j)p(x|w_j)]\)</span>.</p>
<p><span class="math inline">\(R_k(x)=\sum_{i=1}^c \lambda_{ki}p(w_i|x)\)</span> is the risk or cost for the decision <span class="math inline">\(w_k\)</span> at a specific value of x, called the conditional risk. How good is a decision rule is evaluated by the average or overall cost or risk of a pattern recognition system: <span class="math inline">\(R=\int_{R_x}R_k(x)p(x)dx=\int_{R_x}\sum_{i=1}^c p(w_i)p(x|w_i)dx=\sum_{k=1}^c\int_{R_{xk}}\lambda_{ki}p(w_i)p(x|w_i)dx\)</span>. This overall cost is minimized by the Bayesian decision rule. It delivers the best performance that can be achieved.</p>
<h2 id="statistical-estimation-machine-learning">Statistical Estimation &amp; Machine Learning</h2>
<p>We understand that the best decision or optimal classification is: Decide <span class="math inline">\(w_k=argmin[p(e_i|x)]=argmin[1-p(w_i|x)]=argmax[p(w_i|x)]=argmax[p(w_i)p(x|w_i)]\)</span> To design an automatic pattern recognition system, we need know the probability distribution/density function (PDF) for all values of x so that the system can make decision for any value of received data x. In practice, the PDF is often unknown and can only be estimated by collected examples/samples <span class="math inline">\(\{x_i\}=[x1, x2,..., xn]\)</span> for the design of the pattern recognition system.Determining some rules or some deterministic values on a random variable x based on a set of training samples D={xi} is the task of statistical estimation or machine learning.</p>
<p>The probability distribution/density function (PDF) is the complete information about a random variable.It is difficult to estimate the posterior probability <span class="math inline">\(p(w_k|x)\)</span> function directly. So we learn the prior probability <span class="math inline">\(p(w_k)\)</span> and the class-conditional probability function <span class="math inline">\(p(x|w_k)\)</span>. The c prior probabilities can be easily estimated by <span class="math inline">\(\hat{p}(w_k)=n_k/n\)</span>. where <span class="math inline">\(n_k\)</span> and n are the number of training samples of class <span class="math inline">\(w_k\)</span> and the total number of training samples. As the estimation method is the same for all classes, we simplifying the notation of <span class="math inline">\(p(x|w_k)\)</span> to <span class="math inline">\(p(x)\)</span> and suppose we have n samples <span class="math inline">\(\{x_i\}=[x_1, x_2,..., x_n]\)</span> drawn independently and identically distributed (i.i.d.) according to the probability law p(x).</p>
<p>According to the definition of probability density function, the probability P that x falls in a region R is: <span class="math inline">\(P(x)=\int_{R_x}p(t)dt = p(x)V\)</span>, where V is the volume of the region <span class="math inline">\(R_x\)</span>. If out of all n training samples of this class, there are k samples fall in the region R, the estimate of P is then. <span class="math inline">\(\hat{P}(x)=\frac{k}{n}\)</span>. Therefore, the estimate of the PDF p(x) is <span class="math inline">\(\hat{P}(x)=\frac{k}{nV}\)</span>. This is called nonparametric approach to the estimation of the probability density function because no assumption of the model is applied.</p>
<p>The procedure to estimate the PDF based on n training samples is: Given a value of x, select a region/cell of size (volume) V centered at x, count the number of samples k fall in the region/cell. The probability density p(x) at x is then estimated as : <span class="math inline">\(\hat{P}(x)=\frac{k}{nV}\)</span>. Obviously, different shape and size of the region/cell lead to different estimate of PDF.In general, x is multiple dimensional x=[x1, x2, …, xd]. If we choose a d-dimensional hypercube of the side length h as the region, a sample xi=[xi1, xi2, …, xid] will fall into the hypercube if <span class="math inline">\(\frac{|x_j-x_{ij}|}{h}&lt;\frac{1}{2}\)</span>, for <span class="math inline">\(\forall j=1,...,d\)</span>.</p>
<p>Therefore, we can express the number of samples falling into the cell k mathematically as <span class="math inline">\(k=\sum_{i=1}^n K(\frac{x-x_i}{h})\)</span>, Where the kernel function called Parzen-window.</p>
<p>The rectangular kernel function produces unsmoothed PDF due to the unsmooth rectangular kernel function. In fact, we can choose any function as the kernel so long as: <span class="math inline">\(K(u)\ge 0\)</span> Therefore, any DPF function can be served as the kernel function. This approach is called Parzen-window approach, it in fact interpolates the discrete points {xi}=[x1, x2, …, xn] into a continuous function PDF p(x).</p>
<p>To estimate the PDF by <span class="math inline">\(\hat{P}(x)=\frac{k}{nV}\)</span>, the Parzen-window approach selects a region/cell of fixed size (volume) V centered at x and counts the number of samples k fall in the region/cell for the estimation of PDF. We can also select the fixed number of samples k and compute the size/volume that just encloses k samples to estimate the PDF by <span class="math inline">\(\hat{P}(x)=\frac{k}{nV}\)</span>. This approach is called k–nearest-neighbor kNN estimation.</p>
<p>The KNN technique can also be used for estimation of a posteriori probabilities <span class="math inline">\(P(w_i|x)\)</span> from a set of n labeled samples(compare to PNNs and Parzen window estimates). Suppose that we place a cell of volume <span class="math inline">\(V\)</span> arond <span class="math inline">\(x\)</span> and capture <span class="math inline">\(k\)</span> samples, <span class="math inline">\(k_i\)</span> of which turn out to be labeled <span class="math inline">\(w_i\)</span>. An estimate for joint probability is <span class="math inline">\(p_n(x,w_i)=\frac{k_i}{nV}\)</span>. Hence, we can estimate <span class="math inline">\(P(w_i|x)\)</span> by <span class="math inline">\(\hat{p}(w_i|x)=\hat{p}(w_i)\hat{p}(x|w_i)/\hat{p}(x)=\frac{n_i}{n}\frac{k_i}{n_iV(k)}\frac{nV(k)}{k}=\frac{k_i}{k}\)</span>.</p>
<p>Denote by <span class="math inline">\(D^n ={x_1,..,x_n}\)</span> a set of labeled prototypes or training samples. let <span class="math inline">\(x*\)</span> be the prototype nearest to <span class="math inline">\(x\)</span>. Then the nearest-nerighbor(NN) rule for classifiying <span class="math inline">\(x\)</span> is to assign it the label associated with <span class="math inline">\(x*\)</span>. More formally if we have a set of labeled training samples <span class="math inline">\(\{(x_1,\theta_1), ..., (x_n,\theta_n)\}\)</span>, where each <span class="math inline">\(\theta_i\)</span> is one of the babels <span class="math inline">\(w_1, ...,w_c\)</span>, then the NN decision rule is <span class="math inline">\(\alpha_{nn}=\theta_k:k=argmin||x-x_i||\)</span>.</p>
<p>The error rate of NN classifier P for very large number of training samples are bounded as <span class="math inline">\(P*\le P \le P*(2-\frac{c}{c-1}P*)\)</span>.</p>
<p>Generalizaiton of the NN rule. The <span class="math inline">\(k_n\)</span> nearest neighbour rule: Given a set of training samples <span class="math inline">\(\{x_1,..,x_n\}\)</span> and a test point x, find k training points closest to <span class="math inline">\(x,x_1^*, ...,x_k^*\)</span>. collect the labels associated <span class="math inline">\(\theta_1^*, ..., \theta_k^*\)</span> and clasify x to the class which has the greatest number of representatives in $_1^*, ...,_k^* $. In other words, the classification is performed by taking the majority vote among k nearest neighbors of x.</p>
<p>We see that the learned conditional PDF from training samples could greatly deviate from the true PDF of the population, especially in case of small number of training samples.If our general knowledge about the problem permits us to model the conditional PDF, i.e. using a mathematical analytical function to represent the PDF with unknown parameter. The severity of these problems can be reduced significantly. Here we parameterize the conditional PDF, which is called parametric method.Suppose, for example, we can reasonably assume that <span class="math inline">\(p(x|w_k)\)</span> is a Gaussian density with mean <span class="math inline">\(\mu_k\)</span> and covariance matrix Σk. Although we do not know their values, this knowledge simplifies the problem from estimating an unknown function <span class="math inline">\(p(x|w_k)\)</span> to estimating the unknown parameters <span class="math inline">\(\mu_k\)</span> and Σk only.</p>
<p>Now we will use a set of training samples D={xi}=[x1, x2, …, xn] drawn independently from the probability density p(x|θ) to estimate the unknown parameter vector θ. Lets see an example to generate idea how to estimate the parameter of a given probability density p(x|θ) reasonably based on the training data D.The graph shows several training points in one dimension, known or assumed to be drawn from a Gaussian of a particular variance, but unknown mean. Four PDF with 4 different means are shown in dashed lines.Which PDF you should choose?</p>
<p>Now we formulate our idea mathematically.Obviously, the probability that a sample xk occurs is p(xk|θ). As all samples in the training set are independently collected (occur), the probability that all samples occur is <span class="math inline">\(p(D|\theta)=\prod_{k=1}^nP(x_k|\theta)\)</span>. Intuitively, we should select the parameter so that the probability density p(x|θ) best supports the actually observed training samples, i.e. to make the probability of all training data occur p(D|θ) maximal. Note that p(D|θ) is called the likelihood of θ with respect to the set of samples D. Thus, this method is called the maximum likelihood (ML) estimation. <span class="math inline">\(\hat{\theta}=argmax p(D|\theta)=argmax \prod_{k=1}^n p(x_k|\theta)\)</span>.</p>
<p>It is often not easy to get an analytical solution of <span class="math inline">\(\hat{\theta}=argmax p(D|\theta)=argmax \prod_{k=1}^n p(x_k|\theta)\)</span> due to the multiplication of the functions of θ and p(xk|θ) is often nonlinear function of θ. Since the logarithm is monotonically increasing, maximizing the logarithm of a function also maximizes the function itself. The logarithm has nice property that converts the multiplication into summation and simplifies the exponential function.Thus, we maximize the log-likelihood instead of maximizing the likelihood <span class="math inline">\(\hat{\theta}=argmax ln p(D|\theta)=argmax \prod_{k=1}^n ln p(x_k|\theta)\)</span></p>
<p>The solution can be found by the standard methods of differential calculus: Solving the equation that the gradient is zero. <span class="math inline">\(\Delta_{\theta} ln p(D|\theta)=0\)</span>, <span class="math inline">\(\sum_{k=1}^n \nabla_{\theta} ln p(x_k|\theta)=0\)</span>. If the number of parameters to be estimated is q, then θ is a q-component vector θ=(θ1, θ2, …, θq)T. The gradient is a vector that contains partial differentiation against all components of θ.</p>
<p>To see how maximum likelihood methods results apply to a specific case, suppose that the samples are drawn from a multivariate Gaussian population with unknown mean µ and covariance matrix Σ.</p>
<p><span class="math inline">\(p(x|\theta)=\frac{1}{(2\pi)^{d/2}|\sum|^{1/2}}exp[-\frac{1}{2}(x-\mu)^T\sum^{-1}(x-\mu)]\)</span> The log-likelihood of a single sample is．　 <span class="math inline">\(lnp(x_k|\theta)=-\frac{1}{2}ln[(2\pi)^d|\sum|]-\frac{1}{2}(x_k-\mu)^T\sum^{-1}(x_k-\mu)\)</span> Consider first the univariate case with <span class="math inline">\(\theta=(\theta_1,\theta_2)^T=(\mu,\delta^2)^T\)</span>. p(x|)=exp[-^2]</p>
<p>Here the log-likelihood of a single sample is simplified as． <span class="math inline">\(lnp(x_k|\theta)=-\frac{1}{2}ln[2\pi\delta^2]-\frac{1}{2\delta^2}(x_k-\mu)^2\)</span> Its derivative is <span class="math inline">\(\nabla_{\theta}lnp(x_k|\theta)=\begin{pmatrix}\frac{1}{\delta^2}(x_k-\mu)\\-\frac{1}{2\delta^2}+\frac{(x_k-\mu)^2}{2\delta^4}\\ \end{pmatrix}\)</span>.　Applying ML<span class="math inline">\(\nabla_{\theta}lnp(D|\theta)=\sum_{k=1}^n \nabla_{\theta}lnp(x_k|\theta)=0\)</span>．　We have <span class="math inline">\(\sum_{k=1}^n\frac{1}{\delta^2}(x_k-\mu)=0\)</span> <span class="math inline">\(-\sum_{k=1}^n\frac{1}{\delta^2}+\sum_{k=1}^n\frac{(x_k-\mu)^2}{2\delta^4}=0\)</span> Solve these two equations we have the following maximum likelihood estimates <span class="math inline">\(\hat{\mu}=\frac{1}{n}\sum_{k=1}^nx_k\)</span>,<span class="math inline">\(\hat{\delta}^2=\frac{1}{n}\sum_{k=1}^n(x_k-\hat{\mu})^2\)</span>．</p>
<p>While the analysis of the multivariate case is basically very similar, considerably more manipulations are involved. The result is that the maximum likelihood estimates for mean vector µ and covariance matrix Σ of multivariate Gaussian PDF．　 <span class="math inline">\(p(x|\theta)=\frac{1}{(2\pi)^{d/2}|\sum|^{1/2}}exp[-\frac{1}{2}(x-\mu)^T\sum^{-1}(x-\mu)]\)</span> are given by <span class="math inline">\(\hat{\mu}=\frac{1}{n}\sum_{k=1}^n x_k\)</span>, <span class="math inline">\(\hat{\sum}=\frac{1}{n}\sum_{k=1}^n(x_k-\hat{\mu})(x_k-\hat{\mu})^T\)</span></p>
<h2 id="discriminant-functions-and-classifiers">Discriminant Functions and Classifiers</h2>
<p>We understand that the Bayesian (optimal) classification with the 0/1 loss function maximizes the a posterior probability and hence minimizes the classification error by: Decide: <span class="math inline">\(W_k=argmax[p(w_i|x)=argmin[p(e_i|x)]]\)</span>. Since <span class="math inline">\(p(w_i|x)=p(w_i)p(x|w_i)p^{-1}(x)\)</span> and <span class="math inline">\(p(x)\)</span> is not a function of <span class="math inline">\(w_i\)</span>, the Bayesian(optimal) classification is to evaluate the called discriminant functions that can be defined as <span class="math inline">\(g_i(x)=lnp(x|w_i)p(w_i)\)</span> and find the class <span class="math inline">\(w_i\)</span> that has the maximum value of the discriminant function for a given pattern x. Here, a natural logarithm ln is applied as it is a monotonically increasing function that does not affect the decision result but will simplify its evaluation if p(x |ωi) is an exponential function.</p>
<p>The decision boundary or classification boundary in the space spanned by x between class ωi and class ωj is determined by:<span class="math inline">\(g_i(x)=g_j(x)\)</span>. If the class conditional PDF is multivariate Gaussian of <span class="math inline">\(p(x|w_i)=\frac{1}{(2\pi)^{d/2}|\sum i|^{1/2}}exp[-\frac{1}{2}(x-\mu_i)^T\sum_i^{-1}(x-\mu_i)]\)</span>.The discriminant function becomes: <span class="math inline">\(g_i(x)=-\frac{1}{2}d_{\sum}(x,\mu_i)+b_i\)</span>. where <span class="math inline">\(d_{\sum_i}(x-\mu_i)=(x-\mu_i)^T\sum_i^{-1}(x-\mu_i)\)</span>, <span class="math inline">\(b_i=ln p(w_i)-\frac{1}{2}ln|\sum_i|\)</span></p>
<p>Recall the discriminant function: <span class="math inline">\(g_i(x)=-\frac{1}{2\delta^2}(x-\mu_i)^T+lnp(w_i)\)</span>. If the prior probabilities are the same for all classes, then<span class="math inline">\(g_i(x)=-(x-\mu_i)^T(x-\mu_i)=-d_{Eu}(x,\mu_i)=||x-\mu_i||\)</span></p>
<p>When this happens, the optimum decision rule can be stated very simply: to classify a feature vector x, measure the Euclidean distance from x to each of the c mean vectors, and assign x to the category of the nearest mean. Such a classifier is called a minimum distance classifier. If each mean vector is thought of as being an ideal prototype or template for patterns in its class, then this is essentially a template matching procedure.</p>
<p>Example:We have derived several classifiers under Gaussian assumption. Gaussian PDF is the most natural and most common distribution. For Non-Gaussian distribution, it is very difficult if not impossible to derive a theoretical optimal classifier. One way to solve non-Gaussian data distribution is to perform some proper feature transform to convert it into Gaussian PDF.</p>
<h2 id="classifiers-by-sparse-representation">Classifiers by Sparse Representation</h2>
<p>Query image <span class="math inline">\(y\)</span> can be well represented by a linear combination of its training images:<span class="math inline">\(A_i = [a_{i,1}, ..., a_{i,j}, ...,a_{i,ni}]\)</span>: training images of i-th class: <span class="math inline">\(y=A_i x_i + e_i\)</span>. <span class="math inline">\(A=[A_1,...,A_i,...,A_c]\)</span>:training samples of all c subjects:<span class="math inline">\(y=Ax+e\)</span>, here <span class="math inline">\(x=[x_1;...;x_i;...,x_c]\)</span></p>
<p>If <span class="math inline">\(x=[0;...,0;x_i;0,...,;0]\)</span>, we can identify the class i of query image y.Given y and A: find sparse coefficients x. - y can be well represented by a linear combination of its training images y contains all pixels of the image. - y can be well represented by a linear combination of its training images. What is its training images? - <span class="math inline">\(y=d+b+s\)</span> d: the class-specific/identity component; b: the non-class-specific/intra-class variation component; s: the sparse noise or corruption.</p>
<h2 id="information-decomposition-of-image-dictionary">Information Decomposition of Image Dictionary</h2>
<ul>
<li>The proposed supervised low-rank decomposition (SLR):</li>
<li>y can be well represented by its training images, what is its training images.</li>
</ul>
<p><strong>Unsupervised Learning and Clustering</strong> Previously, all our training samples were labeled: these samples were said “supervised”.We now investigate a number of “unsupervised” procedures which use unlabeled samples because collecting and labeling a large set of sample patterns can be costly or in some cases not possible based on human knowledge. We can use unsupervised methods to identify features that will then be useful for categorization. We gain some insight into the nature (or structure) of the data.</p>
<p>Clustering that partitions samples into a number of groups without knowing the class membership of the samples belong to unsupervised learning. Clustering is a process of partitioning a set of data (or objects) in a set of meaningful sub-classes, called clusters. – Helps users understand the natural grouping or structure in a data set.</p>
<p>Cluster: a collection of data objects that are “similar” to one another and thus can be treated collectively as one group.</p>
<p>Given a database D={t1,t2,...,tn} of n samples and an integer value k, the Clustering Problem is to define a mapping <span class="math inline">\(f: D-&gt;{1,..,k}\)</span> where each <span class="math inline">\(t_i\)</span> is assigned to one cluster <span class="math inline">\(K_j\)</span>, <span class="math inline">\(1 &lt; j &lt; k\)</span>.</p>
<p>A Cluster, <span class="math inline">\(K_j\)</span>, contains those samples mapped to it. Unlike classification problem, clusters are not known a priori. - To group data into un-predefined classes - To make data within the same cluster have high similarity,while data points in different clusters have low similarity. A good method will produce high quality clusters in which:the intra-cluster similarity is high.the inter-cluster similarity is low.The quality of a clustering result also depends on both the similarity measure used by the method and its implementation. The quality of a clustering method is also measured by its ability to discover the hidden patterns.</p>
<p>Major Categories of Algorithms: 1. Partitioning : Construct a partition of a database D of n objects into a set of k clusters that optimizes the chosen partitioning criterion. - K-means algorithm 2. Hierarchy: Construct a hierarchical partition of data using some criterion - Agglomerative - Divisive 3. Model-based : A model is hypothesized for each of the clusters and the idea is to find the best fit of that model to each other. - Gaussian Mixture model 4. Partitioning method: Construct a partition of a database - <span class="math inline">\(D = {x_1 , x_2 ,.., x_n }\)</span> of n objects into a set of k clusters - <span class="math inline">\(D_j,j=1,2,..,k\)</span> that optimizes the chosen partitioning criterion.</p>
<p>k-means algorithm principle: - Each cluster is represented by the center(mean) of the cluster: - <span class="math inline">\(\mu_j=\frac{1}{D_j}\sum_{x_i \in D_j}x_i\)</span> - Estimate the unknown k cluster centers (means) <span class="math inline">\(m=\{\mu_1, \mu_2, ..., \mu_k\}\)</span> to minimize the sum of error squares. $e(m)=_{j=1}^k $</p>
<p>k-means algorithm--implementation: 1. Initialize k clusters, e.g. randomly pick samples. <span class="math inline">\(m=\{\mu_1, \mu_2, ..., \mu_k\}=\{x_{r1},x_{r2},...,x_{rk}\}\)</span> 2. Classify n samples into k clusters according to nearest to µj. 3. Re-compute all cluster centers (means).<span class="math inline">\(\mu_j = \frac{1}{D_j}\sum_{x_i\in D_j}x_i\)</span>,j=1,2,...,k 4. Go back step 2 until no change in <span class="math inline">\(M=\{\mu_1,\mu_2,...,\mu_k\}\)</span>. The computational complexity of the algorithm is O(ndkT ) where d the number of features and T the number of iterations. In practice, the number of iterations is generally much less than the number of samples.</p>
<p>Hierarchical clustering can be divided in divisive and agglomerative.Divisive (top down, splitting): start with all of the samples in one cluster and form the sequence by successively splitting clusters. Agglomerative (bottom up, clumping): start with n singleton cluster and form the sequence by merging clusters: - The first is a partition into n cluster, each one containing exactly one sample - The second is a partition into n-1 clusters, the third into n-2, and so on, until the n-th in which there is only one cluster containing all of the samples - At the level k in the sequence, c = n-k+1.</p>
<p>Hierarchical agglomerative clustering:Given any two samples x and x’, they will be grouped together at some level, and if they are grouped a level k, they remain grouped for all higher levels. A tree representation, called dendrogram, represents the sequence of clusters produced by an agglomerative algorithm.</p>
<p>Example of agglomerative clustering: Let <span class="math inline">\(D=\{x_i, i=1,..,5\}\)</span>, with <span class="math inline">\(x_1=[1,1]^T\)</span>,</p>
<p>Model-based : A model is hypothesized for each of the clusters and the idea is to find the best fit of that model to each other. - Assumption -- the functional forms for the underlying probability densities are known and that the only thing that must be learned is the value of an unknown parameter vector. - Further assumptions: - The samples come from a known number of classes c - The prior probabilities <span class="math inline">\(P(w_j)\)</span> for each class are known, (j = 1, …,c) - <span class="math inline">\(P(x|w_j,\theta_j),(j=1,...,c)\)</span> are known. - The values of the c parameter vectors <span class="math inline">\(\theta_1, \theta_2, ..., \theta_c\)</span> are unknown The category labels are unknown <span class="math inline">\(p(x|\theta)=\sum_{j=1}^c p(x|w_j, \theta_j).P(w_j)\)</span>, where <span class="math inline">\(\theta=(\theta_1, \theta_2, ..., \theta_c)^T\)</span>. This density function is called a mixture density. Our goal will be to use samples drawn from this mixture density to estimate the unknown parameter vector θ.Once θ is known, we can decompose the mixture into its components and use a MAP classifier on the derived densities. <span class="math inline">\(p(x|w_j,\theta_j)~N(\mu_j, \sum_j)\)</span>-&gt;Gaussian Mixture Model (GMM). A popular technique to solve this problem is expectation- maximization (EM) algorithm.</p>
<p>Apply clustering approaches to image segmentation: - Each pixel is a sample - Select a value of k - Select a feature vector for every pixel (color, texture, position, or combination of these etc.) - Define a similarity measure between feature vectors (usually Euclidean distance) or assume a mixture of Gaussians distribution. - Apply K-means or EM algorithm to find the k clusters. - Apply Connected Components Algorithm - Merge any components of size less than some threshold to an adjacent component that is most similar to it.</p>
<p><strong>Feature Extraction/Dimension Reduction v. Machine Learning</strong></p>
<p>Pattern recognition is a series of processes that reduce the dimensionality and the variation of samples for the same class and keep their discrimination for different classes.Pattern recognition in general is to classify an observed data into one of the classes.The observed data often have multiple components and the data of a same class are in general not exact same and have some variation. We thus represent the observed data by a random vector.The probability distribution fully characterizes a random vector.In most cases, the probability distribution is unknown.Pattern Recognition via Machine Learning is to estimate the probability distribution directly or indirectly from the known data/samples.</p>
<p>In some applications such as visual object detection and recognition, bioinformatics and data mining, high data dimensionality imposes great burdens on the robust and accurate recognition due to insufficient knowledge about the data population and limited number of training samples.Dimensionality reduction thus becomes a separate and maybe the most critical module of such recognition systems.Extracting the discriminative and reliable features or dimensions is always the key step for image recognition and computer vision.</p>
<p>Feature extraction and dimensionality reduction can be based on: Human expert knowledge; Image local structures; Image global structure; Machine learning from training database.</p>
<p>Feature Extraction Based on Human Knowledge: - Fingerprint classification: for example, arch, whorl and loop - Local orientations are intrinsic features for such task.(Human knowledge) - Orientation field consisting of fingerprint local orientations represented by short lines.</p>
<p>Feature Extraction Based on Human Knowledge:Fingerprint identification: Human experts teach us, the reliable and discriminative features are minutia points: where ridge terminates or bifurcates.Good knowledge about the features doesn’t mean the computer can reliably extract them due to poor image quality and noise.</p>
<p>Feature Extraction Based on Image Local Structures:Corners and blobs called key points or interesting points have locations, scales and shapes.</p>
<p>Feature Extraction Based on Image Global Structures: Transform image <span class="math inline">\(f(x, y)\)</span> to feature <span class="math inline">\(g(u,v)\)</span></p>
<p><span class="math inline">\(g(u,v)=T\{f(x,y)\}=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}w(u,v,x,y)f(x,y)dxdy\)</span> linear transform. <span class="math inline">\(\Rightarrow \sum_{1}^{h}\sum_{1}^w w(u,v,x,y)f(x,y)\)</span> for digital image; <span class="math inline">\(Rightarrow \sum_{1}^{h}\sum_{1}^w e^{-2\pi j(ux+vy)}f(x,y)\)</span> Fourier transform; <span class="math inline">\(\Rightarrow \sum_{1}^{h}\sum_{1}^w x^u y^v f(x,y)\)</span> moments computing; <span class="math inline">\(\Rightarrow f=W^Tx\)</span> vector-matrix representation.</p>
<p>Feature Extraction Based on Image Global Structures:Polar Complex Exponential Transform (PCET) <span class="math inline">\(g(u,v)=\frac{1}{\pi}\int_{0}^{2\pi}\int_{0}^1 e^{-j(2]pi u r^2+v\theta)}f(r,\theta)drd{\theta}\)</span>. Shows good property in the image reconstruction.</p>
<p>Problems:Poor or even no human knowledge about the effective features.Image patterns are so complex that it is impossible to incorporate all different variations into the deterministic computer program.</p>
<p>Solution: Take all pixels of the whole image as initial features and derive the effective features based on machine learning.</p>
<p>The transform parameter W or Φ that weight the features and reduce the feature dimensionality is determined by machine (computer) learning (training) from a image database.</p>
<p>PCA: Principal Component Analysis Given a data set of q n-dimensional training samples <span class="math inline">\(x_1,...,x_q\)</span>. Each sample represented by a column vector is a point in an n- dimensional space. How to use one point,x0 to best represent all training data?</p>
<p>i.e. <span class="math inline">\(\eta^2=\sum_{i=1}^q||x_0-x_i||^2=\sum_{i=1}^q x_i\)</span>. Just for symbolic simplicity, we centralize training samples and define a training data matrix by: <span class="math inline">\(\hat{x}_i=x_i-\mu\)</span>, <span class="math inline">\(X=[\hat{x}_1,...,\hat{x}_q]\)</span> Now we want to just use one dimension <span class="math inline">\(\phi, ||\phi||^2=\phi^T\phi=1\)</span> to best represent all samples, <span class="math inline">\(\hat{x}_i\)</span>.</p>
<p>The n-dimensional data <span class="math inline">\(\hat{x}_i\)</span> are reduced to one-dimensional data <span class="math inline">\(a_i=\phi \hat{x}_i\)</span>. The best dimension <span class="math inline">\(\phi\)</span> makes reconstruction error minimum. <span class="math inline">\(\varepsilon^2 = \sum_{i=1}^q||\hat{x}_i-a_i \phi||^2 \Rightarrow minimum\)</span></p>
<p>It is easy to have: <span class="math inline">\(\varepsilon^2 = \sum_{i=1}^q||\hat{x}_i-a_i \phi||^2 = \sum_{i=1}^q(\hat{x}_i-a_i \phi)^T(\hat{x}_i-a_i \phi)= \sum_{i=1}^q( ||\hat{x}_i||^2-\sum_{i=1}^q a_i^2 = \sum_{i=1}^q( ||\hat{x}_i||^2-\phi^T \sum_{i=1}^q \hat{x}_i\hat{x}_i^T\phi\)</span> The sample covariance matrix of all training data,<span class="math inline">\(S^t=\frac{1}{q}\sum_{i=1}^q||\hat{x}_i||^2-q\phi^T S^t \phi\)</span> is to maximize <span class="math inline">\(\phi^T S^t \phi\)</span> with the constraint of <span class="math inline">\(||\phi||^2=\phi^T\phi=1\)</span>. Use Lagrange optimization method <span class="math inline">\(f(\phi, \lambda)=\phi^TS^t\phi-\lambda(\phi^T\phi-1)\Rightarrow\)</span>maximum. <span class="math inline">\(\frac{\partial f}{\partial \phi}=2 S^t\phi - 2\lambda \phi=0\)</span>, then we get <span class="math inline">\(S^t\phi=\lambda \phi\)</span>. The solution is eigenvalue and eigenvector of matrix <span class="math inline">\(S^t\)</span>.</p>
<h2 id="pca-principal-component-analysis">PCA: Principal Component Analysis</h2>
<p>Given a data set of q n-dimensional training samples <span class="math inline">\(x_1,x_2,..,x_q\)</span>, Each sample represented by a column vector is a point in an n- dimensional space. How to use one point x0 to best represent all training data? It is very easy to prove that the solution is the sample mean <span class="math inline">\(x_0=\mu=\frac{1}{q}\sum_{i=1}^qx_i\)</span> Just for symbolic simplicity, we centralize training samples and define a training data matrix by <span class="math inline">\(\bar{x}_i=x_i-\mu\)</span>, <span class="math inline">\(X=[x_1,x_2,...,x_q]\)</span>. Now we want to just use one dimension <span class="math inline">\(\Phi=1\)</span> to best represent all samples. The n-dimensional data are reduced to one-dimensional data. <span class="math inline">\(a_i=\Phi^T \bar{x}_i\)</span>. The best dimension <span class="math inline">\(\Phi\)</span> makes reconstruction error minimum. <span class="math inline">\(\varepsilon^2=\sum_{i}^q||\bar{x}_i-a_i\Phi||^2\)</span> -&gt; minimum.</p>
<p><strong>LDA: Linear Discriminant Analysis</strong></p>
<p><strong>Major Efforts in LDA Variants</strong></p>
<div class="figure">
<img src="./image/EE7403_155.png" />

</div>
<p>PCA extracts the most representative information in the sense of the least square error. LDA extracts the most discriminative information in the linear constrain. As it is widely believe that the discriminative information is the most important for pattern recognition, the vast majority of researchers prefer LDA to PCA for pattern recognition. Numerous efforts were made in the past two decades to make LDA workable or propose even more discriminative approaches than LDA. Although they are suboptimal (GLDA, dural space), or have lost some discriminative information before LDA (PCA+LDA, DLDA, NDA), numerous LDA variants makes LDA workable. No doubt, LDA is better than PCA in extracting discriminative information. The dimensionality reduction can greatly reduce the computation complexity of the subsequent classification</p>
<p>However, most approaches (if not all) apply a simple classifier after PCA/LDA. The DR + classification needs even more computational resource than classification in the original high dimensional space. Thus, the claim that DR reduces the computational complexity of a pattern recognition system may not be valid. What is other purpose of DR? Can DR enhances or improves the recognition accuracy and robustness? Any DR loses information. Extracting the most discriminative information just means losing least information. If higher accuracy can be achieved with less information, why the criterion of DR is set to extract the most discriminative information? Superficial study will cause misunderstanding and mistakes and hence insignificant (trivial) research.</p>
<p><strong>Objectives of FE/DR for PR</strong></p>
<p>It is theoretically proven: the probability of misclassification decreases or at least does not increase as the data dimensionality increases, as long as the decision is based on the knowledge about the whole data population. However, it is also well known that high dimensionality often degrades the classification performance in practice for a fixed number of training data (curse of dimensionality). This paradox can be resolved by distinguishing the discriminative information about the data population from that on the training data set. Although the ultimate objective of all modules of a pattern recognition system is to extract the most discriminative information, it is the most discriminative information about the whole data population, not on a specific training set.</p>
<div class="figure">
<img src="./image/EE7403_160.png" />

</div>
<p>All modules of the system, though having different physical procedures, have the same objective: extract the discriminative information on population (DP), or remove the redundant information on population (RP). A proper classifier extracts discriminative information on training data (DT),</p>
<p>Recognition accuracy decreases with decreasing dimensionality Information = discriminative information + redundant information. Minimizing the lost doesn’t mean we can get any gain!</p>
<p>Some general phenomena are well known in the pattern recognition community, such as the curse of dimensionality, small sample size problem, noise removal effect of dimensionality reduction and better generalization in a lower dimensional space. However, these have not indicated what dimensions should be extracted or what else should be removed for a more robust classification. We cannot develop an effective dimensionality reduction technique to maximize the classification accuracy just based on these general phenomena. For example, it is now clear that extracting maximal discriminative information just means that minimal discriminative information is removed. It is straightforward that minimum loss does not mean any gain. Therefore, it is unlikely that the dimensionality reduction by minimizing the loss of discriminative information can boost the classification accuracy.</p>
<p>A classifier is trained to capture the most discriminative information on the training samples. If some statistics estimated on the training data deviate from those of the data population, misclassification rate on the novel data increases. This is always the case in practice. Question is only how severe it is. Therefore, to boost the classification accuracy, the dimensionality reduction should be targeted at removing the dimensions unreliable or harmful for the classification.</p>
<p><strong>Problems of classification in high dimension</strong></p>
<p>We have learned that if data of all classes obey Gaussian distribution, the Bayes optimal decision becomes to evaluate: <span class="math inline">\(g_i(x)=-\frac{1}{2}(x-\mu_i)^T\sum_i^{-1}(x-\mu_i)+b_i\)</span>. The first part is Mahalanobis distance between X and <span class="math inline">\(M_i\)</span>, where bi is a threshold for user to control the error rate of a class at a price of other classes.</p>
<p>Problem is that human knowledge cannot provide the class mean and covariance matrix of the data population, which can only be estimated or learned by machine from the available training samples. If some estimates largely deviate from those of the data population, we will face large misclassification rate. The discriminant function is very sensitive to the covariance matrix because the data vector is multiplied by its inverse.</p>
<p>However, the inverse of the n by n covariance matrix is a great burden for us to clearly see the problems and find solutions. It is very difficult to study the problems of the covariance matrix directly as it carries two different kinds of information by n2 estimates: data variations and correlations, which could have millions of parameters estimated by a limited number of training samples.</p>
<p>Eigen-decomposition provides an effective tool to simplify the problem. It provides us a powerful tools to study, analyze and find problems of the high dimensional matrix. As the covariance matrix is symmetric, its eigenvectors provide an orthogonal basis for n-space.</p>
<p><strong>Problems of Small Eigenvalues</strong></p>
<p>Problems of the dimensions corresponding to the unreliable small eigenvalues.</p>
<p><strong>Solution1: adding a small constant</strong></p>
<p>One solution is to regularize the covariance matrix. A common practice in classification and data regression is to add a constant to its diagonal elements. Although this method was originally proposed to circumvent the singularity of Σi and the numerical instability of its inverse, numerous algorithms for classification, data regression, dimensionality reduction and manifold learning adopt this classical technique. Why this technique can improve the classification accuracy? The underlying principle can been seen by its equivalence to adding the constant to all eigenvalues</p>
<p><strong>Solution2 &amp; 3: probab. subspace learning</strong></p>
<p>These two regularization techniques have some role of dimensionality reduction as it is not necessary to project the data to the eigenvectors for k &gt; m. Euclidian distance between two vectors in the eigen-space is identical to that in the data space. Thus, we only need nxm eigenvector matrix for classification. However, the n-dimensional class mean vectors are still required. I call it semi-dimensionality reduction.</p>
<p><strong>Solution4: Dimensionality Reduction</strong></p>
<p>Why PCA, though an unsupervised method that minimizes the reconstruction error rather than maximizes the discrimination of classes, can improve the classification accuracy? The underline principle behind PCA for classification is to remove dimensions corresponding to the unreliable small eigenvalues of the class-conditional covariance matrices.</p>
<p><strong>Solution5: Eigenspectrum Model</strong></p>
<p><strong>Solution6: Supervised/Asymmetric PCA</strong></p>
<p>Is the unsupervised PCA that minimizes the data reconstruction error optimal in the dimensionality reduction for classification? What is the problem of PCA for classification? Classes with more training samples are heavier weighted so more dimensions unreliable for theses classes are removed. How to modify PCA from an unsupervised method to a supervised one for an even better classification? How to modify PCA to handle unbalanced data and asymmetric classes?</p>
<p><strong>Summary: DR for Accuracy Enhancement</strong></p>
<p>It is the regularization technique or the dimensionality reduction by the supervised/asymmetric principal component analysis (SPCA, APCA) that plays the most vital role in boosting the classification accuracy, the most important objective of the dimensionality reduction. The underlying principle behind the regularization is that it corrects the unreliable statistics that are harmful for classification. Similarly, SPCA/APCA removes dimensions that are harmful for the classification. However, these techniques may not be able to greatly reduce the dimension for a fast classification. The discriminative method can greatly reduce the dimension with the minimum loss of the discriminative information, i.e. it removes redundant dimensions. It can be used for reducing the classification complexity, another objective of the dimensionality reduction. However, the vast majority of researchers so far prefer discriminative approaches and the most published approaches stem from the most discriminative criterion. To make the above conclusions more convincing, let’s restudy the discriminative method.</p>
<p><strong>Restudy the Insights of LDA</strong></p>
<p>In the identification applications, we often have a large number of classes with only a few samples per class for training so that each individual class covariance matrix is extremely unreliable. One solution to regularize them is to pool them together to form a common covariance matrix. Recall that under Gaussian assumption with the same covariance matrix of all classes, the Bayes optimal decision becomes to evaluate the discriminant function, which becomes a linear function of x:</p>
<p><strong>Asymmetric Discriminant Analysis, ADA</strong></p>
<p>The underlying principle of ADA is that the discriminative information is not only carried by the distinction of the two class means but also by the distinction of the two class variances.</p>
<p><strong>Comments and Conclusion of LDA</strong></p>
<p>The discriminative analysis extracts the most discriminative dimensions on the training data. it in principle just duplicates the classification process. Therefore, the most discriminative criterion may not help boost the classification accuracy and robustness. It is the regularization technique or the dimensionality reduction by the supervised principal component analysis that plays the most vital role in boosting the classification accuracy while the discriminative method can greatly reduce the dimensionality for a fast classification with the minimum loss of the accuracy. To boost the classification accuracy, we need remove the harmful dimensions or modify the unreliable statistics of the training data in these dimensions. To speed up the classification, we need remove the redundant dimensions, i.e, extract the most discriminative dimensions</p>
<p>Question may arise as to why NLDA can work well in some applications if the smallest and zero eigenvalues are the most unreliable. The reason behind it is that the classification of the NLDA features does not use the variance due to zero eigenvalues in all dimensions of the null space. Thus, it implicitly circumvents the problem of the unreliable small eigenvalues to a certain extent by evenly weighting all features. Another question is why some approaches using LDA alone can also work well on some data sets. The underlying causes include the avoidance of feature scaling by the variance in the classification and the linearity of LDA but the nonlinearity of the classifier.</p>
<p>These approaches, though applying LDA for feature extraction, do not apply its origin as classifier. Most of them apply the nearest- neighbor classifier (NNC) with Euclidian distance. While the simple Euclidian distance ignores the data variance and hence circumvents the problem of the unreliable small eigenvalues to a certain extent, the complex data distribution is captured by NNC that computes all distances from a novel pattern to all training samples. The NNC, though very simple, is highly nonlinear, can form arbitrary complex, nonlinear decision boundary and classifies all training samples without error. LDA restricts such highly nonlinear classifier to a subspace, which is, though the most discriminative, only in a linear sense. This restriction has similar role to the regularization.</p>
<p>Therefore, the improvement of the classification accuracy by LDA is most likely contributed by its linearity constraint rather than its most discriminative nature.</p>
<p><strong>Feature Extraction/Dimension Reduction v. Machine Learning</strong></p>
<p>Dimensionality reduction and feature extraction has two objectives. One is to reduce the computational complexity of the subsequent classification with the minimum loss of information needed for classification and the other is to circumvent the generalization (prediction) problem of the subsequent classification and hence enhance its accuracy and robustness. For machine learning from training samples based dimensionality reduction and feature extraction: There is no doubt that various discriminant analyses can effectively achieve the first objective. The second objective of the dimensionality reduction is much more important than the first one in most applications with the rapid growth of the computation power. However, the second objective of the dimensionality reduction is far from straightforward</p>
<p>Although the ultimate objective of a pattern recognition system is to extract the most discriminative information, it is the most discriminative information about the whole data population, not on a specific training set. A classifier is trained to capture the most discriminative information on the training samples. If some statistics estimated on the training data deviate from those of the data population, misclassification rate on the novel data increases. The most discriminative criterion cannot solve this problem because it in general just repeats the classification process. Redundant information does not mean harmful for classification. Therefore, to boost the classification accuracy, the dimensionality reduction by machine learning should be targeted at removing the dimensions unreliable (harmful) for the classification or regularizing (correcting) the unreliable statistics in these dimensions.</p>
<h2 id="ann">ANN</h2>
<p><strong>Connectionist Approaches &amp; Neural Networks</strong> What are the connectionist approaches and neural networks? Connectionist approaches model the pattern recognition systems as interconnected networks of simple units. Artificial neural networks (ANN), or simply, neural networks are typical connectionist models. Why is it called (artificial) neural network? It has certain characteristics in common with biological neuron networks: (1) Information processing occurs at many same simple elements called neurons; (2) Information are processed in parallel; (3) Nonlinear information processing; (4) Learning capability.</p>
<p><strong>Motivation</strong> Why do people create and study artificial neural networks? Traditional, sequential, logic-based computers excel in arithmetic, but is less effective than human brains in many fields. A good example is the processing of visual information. A one-year-old baby is much better and faster at recognizing objects, faces and so on than even the most advanced artificial intelligence (AI) systems running on the fastest supercomputer. As modern computers become ever more powerful, scientists continue to be challenged to use machines effectively for some tasks that are relatively simple to humans. Thus, people try to build a machine information processing system to simulate human brains. However, after over 3 decades of hot research, ANN seems not as promising as people expected at beginning. Nevertheless, it provides one alternative approaches to learning and recognition.</p>
<p><strong>Neuron Model</strong> The major task of a neural network is to learn a model of the world based on the known knowledge of the world. Knowledge of the world consists of two kinds of information: (1) The known world state, represented by facts about what is and what has been known. This form of knowledge is often referred to as prior information. (2) Observations (measurements) of the world, obtained by sensors designed to probe the world. The observations so obtained provide the pool of information from which neural networks learn. These observations are often referred to as examples. The examples (samples) can be labeled or unlabeled</p>
<p><strong>Supervised Learning</strong> During the training session of a neural network, an input is applied to the network, and a response of the network is obtained. The response is compared with an a priori target response. If the actual response differs from the target response, the neural network generates an error signal, which is then used to compute the adjustment that should be made to the network's synaptic weights so that the actual response matches the target output. In other words, the error is minimized, possibly to zero. Since the minimization process requires a teacher (supervisor), this kind of training is named supervised learning. The notion of teacher comes from biological observations. For example, when learning a language, we hear the sound of a word from a teacher. The sound is stored in the memory banks of our brain, and we try to reproduce the sound. When we hear our own sound, we mentally compare it (actual response) with the stored sound (desired response) and note the error. If the error is large, we try again and again until it becomes significantly small.</p>
<p><strong>Unsupervised Learning</strong> In contrast to supervised learning, unsupervised learning does not require a teacher, i.e. there is no target response. During the training stage, the neural network receives its input patterns and it arbitrarily organizes the pattern into categories. When an input is later applied, the neural network provides an output response to indicate the class to which the input pattern belongs. For example, show a person a set of different objects. Then ask him to separate them into different groups, such that objects in a group have one or more common features that distinguish them from other groups. When this (training) is done, show the same person an object that is unseen and ask him to place the object in one of the groups, he would put it in the group with which the object has most common features.</p>
<p><strong>Error-Correction Learning</strong> To illustrate the error-correction learning rule, let's consider the simple case that the output layer of a feed-forward neural network consists of only one neuron, say neuron k, as shown below:</p>
<p><strong>Multilayer Perceptron</strong> These results are of greater theoretical interest than practical, since the construction of such a network requires the nonlinear functions and the weight values which are unknown! How to find the nonlinear functions based on data is the central problem in network-based pattern recognition. Our goal now is to set the interconnection weights based on the training patterns and the desired outputs It is a straightforward matter to understand how the output, and thus the error, depend on the hidden-to-output layer weights The power of backpropagation is that it enables us to compute an effective error for each hidden unit, and thus derive a learning rule for the weights.</p>
<p>Such network is also called multilayer perceptron (MLP) having two modes of operation: Feedforward: The feedforward operations consists of presenting a pattern to the input units and passing (or feeding) the signals through the network in order to get outputs units (no cycles!) Learning: The supervised learning consists of presenting an input pattern and modifying the network parameters (weights) to reduce distances between the computed output and the desired output.</p>
<p>This learning algorithm is called backpropagation: The following figure shows a portion of the MLP. Two kinds of signals are identified in this network</p>
<p><strong>Multilayer Perceptron/backpropagation</strong>* (1) Initialization. Assuming that no prior information is available, pick the synaptic weights from a uniform distribution whose mean is zero and whose variance is chosen to make the standard deviation of the activation signals lie at the transition between linear and saturated parts of the sigmoidal activation function. (2) Presentation of training samples Present the network with an epoch of training samples. For each sample in the set, perform the sequence of forward and backward computations. (3) Forward computation Let a training samples in the epoch be denoted by {x(i),t(i)}, with the x(i) applied to the input layer and the desired response t(i) presented to the output layer. Compute the activation signals and function signals of the network by proceeding through the network, layer by layer. (4) Backward computation Compute the local gradient of the network, and adjust the synaptic weights of network. (5) Iteration Iterate the forward and backward computations in steps (3)- (4) by representing new epochs of training samples to the network until the stopping criterion is met.</p>
<p><strong>Problem of Local Minima</strong> Note, the order of presentation of training samples should be randomized from epoch to epoch. The stopping criterion can be the number of iterations, or the rate of change of the average error small enough.</p>
<p><strong>Learning Curve</strong> - Before training starts, the error on the training set is high; through the learning process, the error becomes smaller - The error per pattern depends on the amount of training data and the expressive power (such as the number of weights) in the network - The average error on an independent test set is always higher than on the training set, and it can decrease as well as increase. (Over-fitting/generalization problem) - A validation set is used in order to decide when to stop training ; we do not want to over fit the network and decrease the power of the classifier generalization - “We stop training at a minimum of the error on the validation set”</p>
<p><strong>Radial Basis Function (RBF) NN</strong> In the nervous system of biological organisms, there is evidence of neurons whose response characteristics are &quot;local&quot; or 'tuned' to some region of input space. An example is the orientation sensitive cells of the visual cortex, whose response is sensitive to local region in the retina. In this section, we will investigate a network structure related to the multi-layer feed-forward network, known as the radial basis function (RBF) neural network. The RBF neural network has a feed-forward structure with a modified hidden layer and training algorithms.</p>
<p>As mentioned, RBF networks emulate the behavior of certain biological networks. Basically, the hidden layer consists of the locally tuned or locally sensitive neurons, and the output layer consists of linear units. In hidden-layer neurons, the neuron response (output) is localized and decreases as function of the distance of inputs from the neuron's receptive field center.</p>
<p><strong>RBF Neuron Characteristics</strong> Input layer: The same as the input layer of feed-forward network, the input layer neurons do not perform any computation and just distribute the input variables to the hidden layer. But note the weights between input layer neurons and hidden layer neurons in the RBF network are all set to 1. Hidden layer neurons: a general form for hidden layer neurons of the RBF neural network may be described by:</p>
<p><strong>Training RBF NN</strong> An important point here is that the different layers of an RBF network perform different tasks, and it is reasonable to separate the optimization of the hidden layer and the output layer by using different techniques. Based on this idea, the training of the RBF is usually done using a two-step procedure: (1) In the first step, the RBF centers and widths are determined. (2) In the second step, the weights are estimated using some algorithm, with the goal of minimizing the difference between the desired output and the actual output of the RBF neural network.</p>
<p><strong>Strategies for neuron center selection</strong> Depending on how RBF neuron centers are determined, there are a few strategies that we can follow in the design of an RBF network, such as Random selection from training samples k-means clustering algorithm Other clustering algorithm</p>
<p><strong>Understand Under- and Over-fitting</strong> Problems of local minima (under- fitting the training data) and poor generalization (over-fitting the training data)</p>
<p><strong>Conclusions of Neural Networks</strong> Neural networks are Universal Approximators It has been shown that any nonlinear continuous function can be approximated arbitrarily close, both, by a two layer perceptron, with sigmoid activations, and an RBF network, provided a large enough number of nodes are used. However, these results are of greater theoretical interest than practical, since the construction of such a network requires the nonlinear functions and the weight values which are unknown! It is still an open question how to find the nonlinear functions based on that training data. Problems of local minima (under-fitting the training data) and poor generalization (over-fitting the training data) are central issues of pattern recognition and machine learning.</p>
</body>
</html>
