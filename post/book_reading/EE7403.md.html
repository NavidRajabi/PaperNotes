<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h2 id="图像处理">图像处理</h2>
<h3 id="图像基础和人类感知">图像基础和人类感知</h3>
<p>首先谈谈什么是<strong>图像信息和感知</strong>，从物理上讲，一个图像是一个3D场景的2D投影，是一个视觉的感知，一个生动的物体描述。。。。 谈到图像首先要谈谈可视光，可视光谱是电磁谱在390到700 nm之间的一部分。从频域看，则对应频率范围430–770 THz之间的区域。</p>
<p>光线在物体反射后有<strong><em>亮度和辉度</em></strong>2种表示方式，我们假设从一个物体收到的光线为<span class="math inline">\(I(\lambda)=\rho(\lambda)L(\lambda)\)</span>， 其中<span class="math inline">\(\rho(\lambda)\)</span>是物体的反射率和透射率， <span class="math inline">\(L(\lambda)\)</span>则是光源的光谱能量，<span class="math inline">\(\lambda\)</span>是可见光的波长(350nm到780 nm)。 空间物体的亮度或辉度定义为<span class="math inline">\(f(x,y)=\int_{0}^{\infty}I(x,y)V(\lambda)d\lambda\)</span>，其中<span class="math inline">\(I(x,y,\lambda)\)</span>是亮度分布，<span class="math inline">\(V(\lambda)\)</span>是视觉系统的相对光谱灵敏度函数。<span class="math inline">\(V(\lambda)\)</span>是一个钟形曲线。物体的亮度是和其周围环境辉度独立的。</p>
<p>从数学上看，一幅图像是一个二维函数<span class="math inline">\((x,y)\)</span>。 而数字图像则是一个采样、量化的2D矩阵。采样和量化通常是等间隔矩形网格。所以离散的图像<span class="math inline">\(f(x,y)\)</span>中<span class="math inline">\(x,y\)</span>是有限、离散的数字量。 一幅图像首先需要进行空间上的采样，然后在进行灰度量化。而一幅图像的灰度级分辨率指的是灰度级别<span class="math inline">\(g=2^b\)</span>，其中<span class="math inline">\(b\)</span>是表示灰度所采用的比特数。</p>
<p>那么我们人类如何感知光线和图像呢，眼睛里面有视锥细胞，可以通过它们来感知光线。</p>
<h3 id="图像空间">图像空间</h3>
<p>我们常见的图像空间表示有： RGB表示，也就是一幅2D图像的每个点有3个表示：Red(R), Green(G), Blue(B)，每个分量都是0-255的数值； CMY表示也叫做减法三原色，它是由Cyan[蓝绿色], Magenta[品红色], Yellow[黄色]三种颜色来表示的。 YIQ表示（luminance, in‐phase, quadrature），Y是提供黑白电视及彩色电视的亮度信号（Luminance），即亮度（Brightness），I代表In-phase，色彩从橙色到青色，Q代表Quadrature-phase，色彩从紫色到黄绿色。这是一种电视系统标准。 HSI（HSV）(Hue, saturation, intensive/value) 是2种方式，前面2个分别是色调，饱和度。HSI的I是亮度I（Intensity）：对应成像亮度和图像灰度，是颜色的明亮程度。HSV的V是彩的明亮程度，范围从0到1。有一点要注意：它和光强度之间并没有直接的联系。 CIE‐Luv, CIE‐Lab (lightness, red‐green, yellow‐blue) 主要用于颜色区分 sRGB是惠普与微软于1996年一起开发的用于显示器、打印机以及因特网的一种标准RGB色彩空间。</p>
<h3 id="图像直方图">图像直方图</h3>
<p>直方图是一幅数组图像<span class="math inline">\(f(x,y)\)</span>灰度级别的概率分布，我们假设灰度级别的分布为<span class="math inline">\([0, L]\)</span>，那么得每个灰度分布的统计概率<span class="math inline">\(p_f(f)=\frac{n_f}{n}\)</span>，其中<span class="math inline">\(f\)</span>是灰度级别，<span class="math inline">\(n_f\)</span>是当前灰度级别下的像素数量，<span class="math inline">\(n\)</span>是当前区域所有的像素点，如果没有特别指定，一般都是指的整幅图像。一幅图像的直方图给出了这幅图像的很多线索，例如，一个很狭窄的振幅范围的直方图分布说明这是一个低对比度的图像，反之直方图分布很宽则说明对比度较高。若物体与背景的灰度有明显差别,则其灰度直方图统计将呈双峰状态。</p>
<h3 id="图像处理的意义">图像处理的意义</h3>
<p>一个数字图像是2D、3D的数组，图像处理的目的是如何从这些数据中获得我们感情线的信息或特征，以方便后续处理。典型的应用有：可视化处理（对比度增强，去噪，图像质量增强，伪彩色）；目标识别；生物信息识别；图像检索。 而从图像处理系统分，可以简单的分：线性和非线性处理系统。</p>
<h3 id="线性时不变系统">线性时不变系统</h3>
<p>什么是线性时不变系统？所谓线性时不变系统，是指系统既是线性的，又是时不变的。线性好理解，时不变则是不随时间改变。</p>
<p>一个数字图像可以表示为 <span class="math inline">\(f(x,y)\)</span> ，其中<span class="math inline">\(x,y\)</span>都是整数。对于图像中的每个元素，我们首先引入脉冲函数：<span class="math inline">\(\delta(x,y)\)</span>，该函数只有<span class="math inline">\(x,y\)</span>都在零点时才为1，其他点都是0。那么图像中每个点可以表示为<span class="math inline">\(f(x,y)=c\delta(x-i,y-j)\)</span>，同理可以拓展到所有的点 <span class="math inline">\(f(x,y)=\sum\sum f(i,j)\delta(x-i)(y-j)\)</span>.</p>
<p>在图像处理中，卷积是一个非常重要的概念。卷积有交换性：<span class="math inline">\(f(x,y)*h(x,y)=h(x,y)*f(x,y)\)</span>，联合性：<span class="math inline">\(f(x,y)*h_1(x,y)*h_2(x,y)=h(x,y)*h_2(x,y)*h_1(x,y)\)</span>，分布性：<span class="math inline">\(f(x,y)*(h_1(x,y)+h_2(x,y))=h(x,y)*h_2(x,y)+h(x,y)*h_1(x,y)\)</span>。</p>
<p>一幅图像可以表示成一系列脉冲平移和比例的总和：<span class="math inline">\(f(x,y)=\begin{cases}\sum_{j=-\infty}^{\infty}\sum_{i=-\infty}^{\infty}f(i,j)\delta(x-i,y-j) \\ \sum_{j=-m}^{m}\sum_{i=-n}^{n}f(i,j)\delta(x-i,y-j)\end{cases}\)</span></p>
<p>关于输入图像<span class="math inline">\(f(x,y)\)</span>的图像处理，我们得到特定的图像输出<span class="math inline">\(g(x,y)=T\{ \sum_{j=-\infty}^{\infty}\sum_{i=-\infty}^{\infty}f(i,j)\delta(x-i,y-j)\}\)</span>。 如果处理系统是线性的，则有<span class="math inline">\(g(x,y)= \sum_{j=-\infty}^{\infty}\sum_{i=-\infty}^{\infty}f(i,j)T\{\delta(x-i,y-j)\}\)</span>。</p>
<p>从上面看出，这种方式定义了输出图像是输入图像关于系统的脉冲响应，可以定义系统的脉冲响应函数为<span class="math inline">\(h(x,y)\triangleq T\{\delta(x,y)\}\)</span>， 那么对于LSI系统，则有<span class="math inline">\(T\{\delta(x-i,y-j)\}=h(x-i,y-j)\)</span>. 也就是，给定一幅图像<span class="math inline">\(f(x,y)\)</span>，一个LSI系统的输出是<span class="math inline">\(g((x,y)=T\{f(x,y)\})=\sum_{j=-\infty}^{\infty}\sum_{i=-\infty}^{\infty}f(i,j)h(x-i,y-j)\}\)</span>. 而该系统完全可以由<span class="math inline">\(h(x,y)\)</span>来描述。</p>
<p><strong><em>那么<span class="math inline">\(h(x,y)\)</span>是什么？</em></strong> <span class="math inline">\(h(x,y)\)</span>就是当输入图像是脉冲信号是，系统的输出。所以这样看，<span class="math inline">\(h(x,y)\)</span>其实也是个图像，它叫做空间表征或滤波MASK或滤波参数。 虽然<span class="math inline">\(h(x,y)\)</span>可以用来处理图像，但一般不会这么办，因为如果图像太大，我们不好定义这样一个和图像大小一样的<span class="math inline">\(h(x,y)\)</span>。 所以一般的做法是采用较小的大小，例如3X3, 5X5, …11X11。</p>
<h3 id="傅里叶变换">傅里叶变换</h3>
<p>傅里叶变换的基本公式为：<span class="math inline">\(\begin{cases}F(w)=\mathscr{F}\{f(x)\}=\int_{-\infty}^{\infty}f(x)e^{-j2\pi wx}dx \\ f(x)=\mathscr{F}^{-1}\{F(w)\}=\int_{-\infty}^{\infty}F(w)e^{j2\pi wx}dw \end{cases}\)</span> <span class="math inline">\(\begin{cases}F(u,v)=\mathscr{F}\{f(x,y)\}=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f(x)e^{-j2\pi (ux+vy)}dxdy \\ f(x,y)=\mathscr{F}^{-1}\{F(u,v)\}=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}F(w)e^{j2\pi (ux+vy)}dudv \end{cases}\)</span></p>
<p>显然，傅里叶频率 <span class="math inline">\(F(u,v)\)</span> 是一个复数函数，<span class="math inline">\(F(u,v)=R(u,v)+jI(u,v)=|F(u,v)|e^{j\varphi(u,v)}\)</span>，其幅度<span class="math inline">\(|F(u,v)|=\sqrt{R^2(u,v)+I^2(u,v)}\)</span>，相角<span class="math inline">\(\varphi(u,v)=arctan [\frac{I(u,v)}{R(u,v)}]\)</span>，实数分量<span class="math inline">\(R(u,v)=|f(u,v)| cos(\varphi(u,v))\)</span>，虚数分量<span class="math inline">\(I(u,v)=|f(u,v)| sin(\varphi(u,v))\)</span>。</p>
<p>以上是连续表达，离散表达则是<span class="math inline">\(F(u,v)=\frac{1}{mn}\sum_{x=0}^{m-1}\sum_{y=0}^{n-1}f(x,y)e^{-j2\pi(ux/m+vy/n)}\)</span> <span class="math inline">\(f(x,y)=\frac{1}{mn}\sum_{u=0}^{m-1}\sum_{v=0}^{n-1}F(u,v)e^{j2\pi(ux/m+vy/n)}\)</span></p>
<p>DFT的性质有： - 周期性:$X_{k+N} = <em>{n=0}^{N-1} x_n e^{- (k+N) n} <span class="math inline">\(=\)</span> </em>{n=0}^{N-1} x_n e^{- k n} e^{-2 i n} <span class="math inline">\(=\)</span> _{n=0}^{N-1} x_n e^{- k n} = X_k $ - 共轭对称： <span class="math inline">\(F(u,v)=F^*(-u,-v)\)</span> - 线性和尺度性: <span class="math inline">\(=\mathscr{F}\{\alpha x, \beta y\}=\frac{1}{|\alpha \beta|}F(u/\alpha,v/\beta)\)</span> - 平移: <span class="math inline">\(\begin{cases}f(x-x_0,y-y_0) \leftrightarrow F(u,v)e^{-j2\pi(x_0u/m+y_0v/n)} \\ F(u-u_0,v-v_0) \leftrightarrow f(x,y)e^{-j2\pi(u_0x/m+v_0y/n)}\end{cases}\)</span> - 旋转: <span class="math inline">\(f(r,\theta+\theta_0) \leftrightarrow F(w,\varphi+\theta_0)\)</span> - 旋转不变: <span class="math inline">\(g(u,v)=\frac{1}{\pi}\int_{0}^{2\pi}\int_{0}^1 e^{-j(2\pi ur^2+v\theta)f(r,\theta)}\theta\)</span>, <a href="http://www.yidianzixun.com/083QErjh">灰度图像--频域滤波 傅里叶变换之二维离散傅里叶变换</a>. 频谱与空域具有旋转同步性，即空域旋转多少频域就旋转多少，频域旋转空域就旋转多少。</p>
<h3 id="离散余弦变换dctdiscrete-cosine-transform">离散余弦变换（DCT，discrete cosine transform）</h3>
<p>这是和DFT很类似的，主要用在了图像和视频压缩领域。 DCT变换公式为：<span class="math inline">\(F(u,v)=\frac{2}{N}C(u)C(v)\sum_{x=0}^{N-1}\sum_{y=0}^{N-1}f(x,y)cos(\frac{(2x+1)u\pi}{2N})cos(\frac{(2y+1)v\pi}{2N})\)</span> <span class="math inline">\(f(x,y)=\frac{2}{N}\sum_{x=0}^{N-1}\sum_{y=0}^{N-1}C(u)C(v)f(u,v)cos(\frac{(2x+1)u\pi}{2N})cos(\frac{(2y+1)v\pi}{2N})\)</span> 这里, <span class="math inline">\(C(u),C(v)=\begin{cases}\frac{1}{\sqrt{2}}, u,v=0 \\ 1 , u,v=1,2...,N-1\end{cases}\)</span></p>
<h3 id="图像采样和量化">图像采样和量化</h3>
<p>上面提到的都是图像处理，那么图像如何表示成我们需要的矩阵呢，这就是<strong>图像采用</strong>技术。</p>
<p>给定一个连续图像（当然实际我们看到的都是离散的），我们很容易的得到离散图像<span class="math inline">\(f_d(m,n)=f_c(m\Delta x,n\Delta y)\)</span>. <span class="math inline">\(\Delta\)</span>就是取样间隔。</p>
<p>我们学过采样定理，如果想恢复原图像，至少要满足最高频率2倍的采样率来采样。所以假设一个带限二维信号<span class="math inline">\(F_c(u,v)=0\)</span>, 对于<span class="math inline">\(|u|&gt;U_0\)</span>, <span class="math inline">\(|v|&gt;V_0\)</span>. 我们定义2D的采样函数<span class="math inline">\(s(x,y)=\sum_{m=-\infty}^{\infty}\sum_{m=-\infty}^{\infty}\delta(x-m\Delta x, y-\Delta y)\)</span>， 采样函数的DFT是：<span class="math inline">\(S(u,v)=\frac{1}{\Delta x \Delta y} \sum_{m=-\infty}^{\infty}\sum_{m=-\infty}^{\infty}\delta(u-m/\Delta x, v-n/\Delta y)\)</span>。</p>
<p>所以采样得到的频谱为：<span class="math inline">\(G(u,v)=F(u,v)*S(u,v)\)</span>，也就是将原频谱在平移后累加，平移的<strong><em>单位步伐</em></strong>是<span class="math inline">\((1/\Delta x, 1/ \Delta y)\)</span>.</p>
<p>推导过程为： 1. 首先对连续图像采样，时域表示为<span class="math inline">\(f_d(x,y)=f_c(x,y)s(x,y)=\sum_{m=-\infty}^{\infty}\sum_{m=-\infty}^{\infty} f_c(m\Delta x, n\Delta y)\delta(x-m\Delta x, y- n\Delta y)\)</span> 2. 对离散图像进行DFT，得到<span class="math inline">\(F_d(u,v)=F_c(u,v)*S(u,v)=\frac{1}{\Delta x \Delta y} \sum_{m=-\infty}^{\infty}\sum_{m=-\infty}^{\infty} F_c(u-mf_{xs}.y-nf_{ys})\)</span>，显然这是一个<span class="math inline">\(Fc(u,v)\)</span>的周期函数，方块间隔为<span class="math inline">\((1/\Delta x, 1/\Delta y)\)</span>。</p>
<p>而根据提到的<strong><em>采样定理</em></strong>: 如果一个带限图像<span class="math inline">\(f_c(x,y)\)</span>的带宽为(<span class="math inline">\(2U_0, 2V_0\)</span>)， 而采样间隔是(<span class="math inline">\(\Delta x,\Delta y\)</span>)。那么频率至少是最高频率2倍，也就是<span class="math inline">\(f_{xs}=\frac{1}{\Delta x}\ge 2U_0\)</span>, <span class="math inline">\(f_{ys}=\frac{1}{\Delta y}\ge 2V_0\)</span>. 那么<span class="math inline">\(F_c(u,v)\)</span>可以从<span class="math inline">\(F_d(u,v)\)</span>中恢复出来。</p>
<p>只需要将<span class="math inline">\(F_d(u,v)\)</span>通过一个低通滤波器<span class="math inline">\(H(u,v)=\begin{cases}\Delta x \Delta y , (u,v)\in R \\ 0, \text{otherwise}\end{cases}\)</span>, 得到<span class="math inline">\(F_c(u,v)=F_d(u,v)H(u,v)\)</span>. 但是如果采样频率低于2倍，则会出现畸变谱，引起混淆现象。</p>
<p>图像采样完后需要进行<strong>图像量化</strong>， 量化器是将连续的变量<span class="math inline">\(f\)</span>量化为离散的变量<span class="math inline">\(f_q\)</span>， 离散变量属于离散灰度序列中{<span class="math inline">\(r_1,...,r_L\)</span>}。所以，一个灰度级别为L的图像量化器为：<span class="math inline">\(f_q=Q(f)=r_k\)</span>。 量化不可避免带来误差，所以要避免量化误差。有很多设计在避免，例如Lloyd-Max量化器， 该量化器通过最小化均方差：<span class="math inline">\(\varepsilon = E\{(f-f_d)^2\}=\int_{t_1}^{t_{L+1}}(f-Q(f))^2 p(f)df =\sum_{k=1}^L \int_{k=1}^{t_{k+1}}(f-r_k)^2p(f)df=\sum_{k=1}^L int_{t_k}^{t_{k+1}} (f-r_k)^2 p(f)df\)</span>. 对这个均方差求差分（也即是求导）并置零后得到：<span class="math inline">\(t_k=\frac{r_k+r_{k-1}}{2}\)</span>, for <span class="math inline">\(k=2,..,L\)</span>. <span class="math inline">\(r_k=\frac{\int_{t_k}^{t_{k+1}fp(f)df}}{\int_{t_K}^{t_{k+1}p(f)df}}\)</span>, for <span class="math inline">\(k=1,..,L\)</span>. 显然这是一个迭代求解过程，可以通过牛顿法来求解。</p>
<p>如果<span class="math inline">\(p(f)\)</span>是一个一致的，那么Lloyd‐Max量化器变成线性量化器，且有<span class="math inline">\(t_k=t_{k-1}+q\)</span>, <span class="math inline">\(r_k=t_k+q/2\)</span>, <span class="math inline">\(q=\frac{t_{L+1}+t_1}{L}\)</span>, 然后均方差可以表示为<span class="math inline">\(\varepsilon = q^2/12\)</span>. 很容易得到信噪<span class="math inline">\(SNR=10log_{10}\frac{\delta^2}{\varepsilon}=10log_{10}2^{2B}\sim 6BdB\)</span>., 这里是<span class="math inline">\(f\)</span>的方差 <span class="math inline">\(\delta^2\)</span>.</p>
<h3 id="图像增强">图像增强</h3>
<p>点处理也叫灰度处理，很多图像增强都是点处理和无记忆的，简单的就是根据转换函数<span class="math inline">\(g=T(f)\)</span>.将输入图像灰度级映射到输出图像灰度级上。例如：伽玛校正<span class="math inline">\(g=cf^{\gamma}\)</span>，对数变换<span class="math inline">\(g=clog(1+f)\)</span>，分段灰度变换 <span class="math inline">\(g=T(f)=\begin{cases}\alpha f, 0\le f&lt; \alpha \\ \beta(f-a)+T(a), a\le f &lt;b \\ \gamma(f-b)+T(b), b\le f &lt; L \end{cases}\)</span>， 直方图均衡等。</p>
<p>直方图均衡是非常重要的一种处理，由前面知道，直方图其实是图像灰度级别的一种概率分布。而直方图均衡则是对灰度级别分布的一种转换，也就是将输入图像<span class="math inline">\(f(x,y)\)</span>通过函数转换得到输出<span class="math inline">\(g=T(f)\)</span>。 具体形式为：<span class="math inline">\(\begin{cases} c(f)=\sum_{t=0}^f p_f(t)=\sum_{t=0}^f \frac{n_t}{n}, f=0,1...,L \\ g=T(f)=round[\frac{c(f)-c_{min}}{1-c_{min}}L], c(f)\ge c_{min}\end{cases}\)</span>. 其中，<span class="math inline">\(c_{min}\)</span>是所有<span class="math inline">\(c(f)\)</span>最小正值， <span class="math inline">\(g\)</span>是[0,L]的灰度分布近似。这样就得到一个新的直方图分布。</p>
<p>直方图的理论分析只能从连续变量上推导。假设<span class="math inline">\(f\)</span>是一个在<span class="math inline">\([0,L]\)</span>内的连续灰度值，并假设<span class="math inline">\(g=T(f)\)</span>是单映射，范围从0到1. 那么逆变换<span class="math inline">\(f=T^{-1}(g)\)</span>也应该是一个一一对应的映射。从概率论角度，如果原始图像灰度概率分布函数<span class="math inline">\(p_f(f)\)</span>知道，而且转换函数<span class="math inline">\(T(f)\)</span>也知道，且<span class="math inline">\(T^{-1}(g)\)</span>满足上面的条件。那么转换后的灰度概率密度分布是<span class="math inline">\(p_g(g)=p_f(f)\frac{df}{dg}\)</span>。</p>
<p>函数 <span class="math inline">\(g=T(f)=\int_{0}^fp_f(t)dt\)</span> 表示灰度<span class="math inline">\(f\)</span>的累计分布函数CDF,对其求导，得到<span class="math inline">\(\frac{dg}{df}=p_f(f)\)</span>, 于是有<span class="math inline">\(p_g(g)=p_f(g)\frac{df}{dg}=p_f(f)\frac{1}{p_f(f)}=1\)</span>. 也就是转换后的灰度分布是均匀概率密度函数。</p>
<p>图像处理中时域和频域滤波可以表示为： <span class="math inline">\(g(x,y)=f(x,y)*h(x,y)=\sum_{-\infty}^{\infty}\sum_{-\intfy}^{\infty}h(i,j)f(x-i,y-j)=\sum_{-3}^{3}\sum_{-3}^{3}h(i,j)f(x-i,y-j)\)</span>, if <span class="math inline">\(h(x,y)=0\)</span>, for <span class="math inline">\(-3&lt;x,y&lt;3\)</span>. 对于每个点<span class="math inline">\((x,y)\)</span>, 滤波器的响应可以计算成滤波器系数和以点<span class="math inline">\((x,y)\)</span>为中心的滤波器模板对应图像像素的乘积和。</p>
<p>图像的平滑主要用在了去模糊和去噪，这些滤波器典型的有均值和低通滤波器。 - 理想低通滤波器：<span class="math inline">\(H(u,v)=\begin{cases}1, if\ D(u,v)\le D_0 \\ 0, if \ D(u,v)&gt;D_0\end{cases}\)</span>, <span class="math inline">\(D(u,v)=\sqrt{u^2+v^2}\)</span>. - 高斯低通滤波器(GLPF). <span class="math inline">\(G(u,v)=\frac{1}{2\pi\delta^2}e^{-\frac{u^2+v^2}{2D_0}}\)</span>.</p>
<p>与图像平滑对应，图像锐化则利用高通滤波器，保留高频分量。<strong><em>高通滤波器</em></strong>表示为<span class="math inline">\(H(u,v)=1-H_{lr}(u,v)\)</span>。</p>
<p>理想高通滤波器：<span class="math inline">\(H(u,v)=\begin{cases}0, \ if D(u,v)\le D_0 \\ 1, if \ D(u,v)&gt;D_0 \end{cases}\)</span>, <span class="math inline">\(D(u,v)=\sqrt{u^2+v^2}\)</span>. 高斯高通滤波器(GHPF) <span class="math inline">\(G(u,v)=1-\frac{1}{2\pi\delta^2}e^{-\frac{u^2+v^2}{2D_0}}\)</span> High-Boost 滤波器：<span class="math inline">\(f_{hb}(x,y)=Af(x,y)-f_{lp}(x,y)\)</span>, 其中<span class="math inline">\(f_{lp}(x,y)\)</span>是利用低通滤波器平滑后的<span class="math inline">\(f(x,y)\)</span></p>
<p>线性滤波器的缺点：我们知道任何线性滤波器都可以表示为：<span class="math inline">\(\hat{f}(x,y)=h(x,y)*f(x,y)=\sum_{i=-a}^{a}\sum_{j=-n}^bh(i,j)f(x-i,y-j)\)</span>. 但是线性滤波器会带来图像模糊，边缘锐度丢失，而且很难去除很强噪声。因为以滤波器为中心的图像也被滤波器处理了。</p>
<p>所以下面讨论非线性滤波器：中值滤波。中值滤波用像素点邻近点的像素的中间值来代替该像素点的值：<span class="math inline">\(\hat{f}(x,y)=median\{f(s,t)\}\)</span>. 中值滤波强迫该点的离散灰度值更接近邻域，所以对那些独立的灰度要么就更浅或更深，对于那些小于<span class="math inline">\(n^2/2\)</span>（也就是一半滤波器区域），会被中值滤波忽视。</p>
<p><em>中值滤波和均值滤波的比较</em>： 给定一个脉冲函数的1D图形。 - 均值滤波模糊了图形细节 - 如果脉冲是噪声，那么均值滤波虽然压制了噪声，但是也展开了噪声； - 而中值滤波不会模糊边缘；而均值滤波会模糊边缘。 - 如果脉冲是噪声，那么合适的中值滤波会完全消除这些噪声； - 均值滤波模糊阶跃，而中值滤波保留阶跃 - 均值滤波进一步模糊平缓边界，而中值滤波保存边界 - 均值滤波衰减高斯噪声，然是会模糊边界；中值滤波衰减高斯噪声也保留边界； - 均值滤波对于衰减脉冲噪声和边缘是无效的</p>
<p>中值滤波还有一些扩展，例如：最大滤波 <span class="math inline">\(f(x,y)=max\{f(s,t)\}\)</span>；最小滤波 <span class="math inline">\(\hat{f}(x,y)=min\{f(s,t)\}\)</span>； 中点滤波： <span class="math inline">\(\hat{f}(x,y)=\frac{1}{2}[max\{f(s,t)\}+min\{f(s,t)\}]\)</span>。</p>
<p>当然中值滤波也不是万能的，虽然保留了边缘信息，但是也会去除图像的细节（例如拐角，细线、细曲线以及其他微小细节）。所以如何设计一个二维列序滤波器能有效的去除脉冲噪声，同时保留这些图像细节？</p>
<h2 id="图像恢复">图像恢复</h2>
图像恢复只有知道退化的先验才可以，否则不可能凭空恢复的。这就是图像恢复的本质，必须有先验信息。这是一个客观处理和模型导向的。所以，大部分图像恢复技术都假设退化的先验。这是和图像增强不一样，图像增强是一个主观过程。 图像退化和恢复处理过程是：
<p align="center">
<img src="http://1.bp.blogspot.com/_rIBnkPGB9po/StPetyvo0QI/AAAAAAAAAo8/ytPUEkSQ8tg/s400/fig1.bmp" width="350" >
</p>
<p>这里<span class="math inline">\(f(x,y)\)</span>是原图像，<span class="math inline">\(H\)</span>是退化函数，<span class="math inline">\(\eta(x,y)\)</span>是加性噪声，<span class="math inline">\(g(x,y)\)</span>是降级的图像（或者称观察到的图像），<span class="math inline">\(\hat{f}(x,y)\)</span>是恢复的图像。</p>
<p>首先，我们看如何建立图像的退化模型。简单的退化模型假设退化过程是一个LST系统，且噪声模型假设是加性非相关噪声。并且假设退化过程的脉冲响应是时空有限的。</p>
<p>通常来说，一个图像有可能的退化有：相机噪声、运动模糊、对焦模糊、传输干扰等等。</p>
<p>首先，我们看运动模糊。运动模糊由于相机和物体之间的相对运动引起的。例如，如果一幅图像在<span class="math inline">\(x\)</span>和<span class="math inline">\(y\)</span>方向以<span class="math inline">\(x_0(t)\)</span>和<span class="math inline">\(y_0(t)\)</span>进行平面运动，并且<span class="math inline">\(T\)</span>是曝光的持续时间，那么我们有<span class="math inline">\(g(x,y)=\int_{0}^T f[x-x_0(t),y-y_0(t)]dt\)</span>，假设<span class="math inline">\(x_0(t)=at/T,y_0(t)=bt/T\)</span>。对应傅里叶域表达，<span class="math inline">\(H(u,v)=\frac{G(u,v)}{F(u,v)}=\frac{Tsin[\pi(ua+vb)]}{\pi (ua+vb)}e^{-j\pi(ua+vb)}\)</span>，这个频率响应是一个方向性的sinc函数。 第二种噪声是，对角模糊。我们知道相机是有固定尺寸的，例如：<span class="math inline">\(h(x,y)=\begin{cases}1,-a\le x\le a,-b\le y \le b \\ 0, else \end{cases}\)</span>， <span class="math inline">\(a,b\)</span>取决于孔径尺寸。对焦模糊意味着大孔径，也就是大于<span class="math inline">\(a,b\)</span>的值。 第三种是大气湍流噪声，这种噪声借助于大气湍流的概念，大气湍流是由于相机和物体之间的空气运动造成的，这个主要在航拍和卫星中见得多。这个的频率响应可以用一个2D高斯函数表示<span class="math inline">\(H(u,v)=e^{-k(u^2+v^2)^{5/6}}\)</span>， <span class="math inline">\(k\)</span>是湍流系数。</p>
<p>讲完了退化，下面接着说说噪声模型。</p>
<p>典型的，噪声<span class="math inline">\(\eta(x,y)\)</span>建模为： - 零均值 : <span class="math inline">\(E\{\eta(x,y)\}=0\)</span> - 和原始图像独立 : <span class="math inline">\(E\{\eta(x,y)f(x+i,y+i)\}=0\)</span> - 高斯特性: <span class="math inline">\(\eta(x,y)\)</span>的PDF是高斯函数 - 白噪声: 功率谱密度(PSD) of <span class="math inline">\(\eta(x,y)\)</span> 是平坦的常量，这意味着自相关是一个脉冲<span class="math inline">\(E\{\eta(x,y)\eta(x+i,y+j)\}=\delta(i,j)\)</span>。不同像素的噪声是不相关的 - 有些图像有周期性噪声</p>
<p>下面介绍4中典型噪声： 1. 指数噪声 <span class="math inline">\(p(z)=\begin{cases}ae^{-az}, \ for \ z\ge 0 \\ 0, \ \ for \ z&lt; 0 \end{cases}\)</span>, 均值为<span class="math inline">\(\mu=1/a\)</span>, 方差:<span class="math inline">\(\delta^2=1/a^2\)</span> 2. 高斯噪声 <span class="math inline">\(p(z)=\frac{1}{\sqrt{2\pi}\delta}e^{-\frac{(z-\mu)^2}{2\delta^2}}\)</span>, 均值为<span class="math inline">\(E\{z\}=\mu\)</span>, 方差:<span class="math inline">\(\delta^2\)</span> 3. 均匀噪声 <span class="math inline">\(p(z)=\begin{cases}\frac{1}{b-a}, \ for \ \le z a\le b \\ 0, \ \ for \ otherwise \end{cases}\)</span>, 均值为<span class="math inline">\(\mu=(b-a)/2\)</span>, 方差:<span class="math inline">\(\delta^2=(b-a)^2/12\)</span> 4. 椒盐噪声 <span class="math inline">\(p(z)=\begin{cases}P_a, \ for \ z=a \\ P_b, \ for \ z=b\\ 0, \ otherwise \ \end{cases}\)</span></p>
<p>逆滤波器从观测到图像<span class="math inline">\(g(x,y)\)</span>回复原始图像<span class="math inline">\(f(x,y)\)</span>，<span class="math inline">\(H^I(u,v)=H^{-1}(u,v)\)</span>, <span class="math inline">\(\hat{F}(u,v)=G(u,v)H^{I}(u,v)=[F(u,v)H(u,v)+N(u,v)]H^{-1}(u,v)=F(u,v)+\frac{N(u,v)}{H(u,v)}\)</span> 但是，这种表达方式有问题。如果<span class="math inline">\(H(u,v)\)</span>为0，则<span class="math inline">\(H^I(u,v)\)</span> 不存在。而且如果<span class="math inline">\(H(u,v)\)</span>是一个很小的频率，那么逆滤波会导致噪声方法。</p>
<p>考虑运动模糊的那个例子，我们知道退化函数是一个sinc函数类型，所以逆滤波<span class="math inline">\(H^I(u,v)\)</span>在某些频率点是无穷大的。</p>
<p>一般的逆滤波器定义为：<span class="math inline">\(H^{-}(u,v)=\begin{cases}\frac{1}{H(u,v)} |H| \ne 0 \\ 0, |H| \ne 0\end{cases}\)</span>,这样一个滤波器实际上是不可能设计出来的，因为那些陡峭的边缘。所以，在实际中，用伪逆滤波来进行，伪逆滤波保留了逆滤波的优点，但是滤波结果有畸变。</p>
<p>所以，实际上，伪逆滤波器定义为：<span class="math inline">\(H^{-}(u,v)=\begin{cases}\frac{1}{H(u,v)} |H|\ge \varepsilon \ \\ 0, |H|\ne \varepsilon\end{cases}\)</span></p>
<p>逆滤波器和伪逆滤波器在有噪声的情况下，表现并不好。例如，<span class="math inline">\(\hat{F}(u,v)=F(u,v)+\frac{N(u,v)}{H(u,v)}\)</span>.如果<span class="math inline">\(H(u,v)\)</span>是0或者很小，那么N/H项会很大，导致噪声放大。 为了解决噪声方法问题，我们介绍维纳滤波器。</p>
<p>维纳滤波器是一种最小均方误差线性滤波器，滤波器的均方差为<span class="math inline">\(e^2=E\{[f(x,y)-\hat{f}(x,y)]^2\}=E\{[f(x,y)-h^w(x,y)*g(x,y)]^2\}\)</span>。为了最小化<span class="math inline">\(e^2\)</span>，我们对其求关于<span class="math inline">\(e^2\)</span>的偏导，得到：<span class="math inline">\(\frac{\partial e^2}{\partial h^w(x,y)}=0\)</span>，因为<span class="math inline">\(g(x,y)=h(x,y)*f(x,y)+\eta(x,y)\)</span>, 所以我们得到维纳滤波器的表达： <span class="math inline">\(H^w(u,v)=\frac{H^*(u,v)S_f(u,v)}{|H(u,v)^2S_f(u,v)+S_{\eta}(u,v)|}\)</span>。 其中，<span class="math inline">\(H(u,v)\)</span>是退化函数，<span class="math inline">\(H^{*}(u,v)\)</span>是<span class="math inline">\(H(u,v)\)</span>的共轭。<span class="math inline">\(|H(u,v)|=H^{*}(u,v)H(u,v)\)</span>, <span class="math inline">\(S_f(u,v)=|F(u,v)|^2\)</span>是原始图像的功率谱，<span class="math inline">\(S_{\eta}(u,v)=|N(u,v)|^2\)</span>是噪声的功率谱。 维纳滤波器的输出是：<span class="math inline">\(\hat{F}(u,v)=H^{w}(u,v)G(u,v)\)</span></p>
<p>很容易表示维纳滤波器为<span class="math inline">\(H^w(u,v)=\frac{1}{H(u,v)}W(u,v)=\frac{1}{H(u,v)}[\frac{|H(u,v)|^2S_f(u,v)}{|H(u,v)|^2S_f(u,v)+S_{\eta}(u,v)}]\)</span>, 所以维纳滤波器是一个逆滤波器，比例系数是<span class="math inline">\(W(u,v)=|H(u,v)|^2S_f(u,v){|H(u,v)|^2S_f(u,v)+S_{\eta}(u,v)}\)</span>.</p>
<p>如果没有噪声，那么比例是1，维纳滤波器等于逆滤波器。对于那些频率不存在信号，比例系数是0，在那些频率处，维纳滤波器是0，噪声也被压制。如果没有退化，H=1, 维纳滤波器变成了平滑滤波器。 比例系数随着噪声<span class="math inline">\(S_{\eta}(u,v)\)</span>的增加而减小。当退化函数接近0时，比例系数也接近0.所以维纳滤波器解决了噪声方法的问题，以及逆滤波器的无穷大问题。</p>
<p>在一些例子中，噪声模型可以表示成频域。例如定向周期性噪声，为了有效的去除这类噪声，我们设计了频域的恢复滤波器，因为2D傅里叶提供了方向和图像的频率域信息。 接下来，我们讨论：带阻滤波器、带通滤波器、陷波滤波器等。</p>
<p>首先，看带阻滤波器。<span class="math inline">\(H(u,v)=\begin{cases}0 , if \ D_0-W/2\le D(u,v) \le D_0+W/2 \\ 1, otherwise \end{cases}\)</span>,其中<span class="math inline">\(D(u,v)=\sqrt{u^2+v^2}\)</span>是原点到点<span class="math inline">\(u,v\)</span>的距离。 这里介绍2种带阻，一种是巴特沃思带阻滤波器 <span class="math inline">\(H(u,v)=\frac{1}{1+[\frac{D(u,v)W}{D^2(u,v)W}]^{2\pi}}\)</span>，还有一种是高斯带阻滤波器 <span class="math inline">\(H(u,v)=1-exp\{-\frac{1}{2}[\frac{D^2(u,v)-D_0^2}{D(u,v)W}]\}\)</span>。 与带阻对应，带通则是 <span class="math inline">\(H_{bp}(u,v)=1-H_{br}(u,v)\)</span>。</p>
<p>然后，我们介绍陷波滤波器，一个理想陷波滤波器表示为：<span class="math inline">\(H(u,v)=\begin{cases}0, if\ D_1(u,v)\le D_0 \ Or \ D_2(u,v)\le D_0 \\ 1, otherwise \end{cases}\)</span>. 其中<span class="math inline">\(D_1(u,v)=[(u-u_0)^2+(v-v_0)^2]^{1/2}\)</span> 和 <span class="math inline">\(D_2(u,v)=[(u+u_0)^2+(v+v_0)^2]^{1/2}\)</span>，同样这也有2类，Butterworth陷波滤波器 <span class="math inline">\(H(u,v)=\frac{1}{1+[\frac{D_0^2}{D_1(u,v)D_2(u,v)}]^n}\)</span>. 高斯陷波滤波器 <span class="math inline">\(H(u,v)=1-exp{-\frac{1}{2}[\frac{D_1((u,v)D_2(u,v)}{D_0^2}]}\)</span>。 那么，如何获得最佳陷波滤波器？ - 首先，观察图像的DFT，找到噪声脉冲，然后设计陷波滤波器<span class="math inline">\(H(u,v)\)</span> - <span class="math inline">\(\eta(x,y)=F^{-1}\{H(u,v)G(u,v)\}\)</span> - <span class="math inline">\(w(x,y)=\frac{\bar{g(x,y)\eta(x,y)-\bar{g}(x,y)\bar{\eta}(x,y)}}{\bar{\eta}^2(x,y)-\bar{\eta}^2(x,y)}\)</span> - <span class="math inline">\(\hat{f}(x,y)=g(x,y)-w(x,y)\eta(x,y)\)</span> <span class="math inline">\(w(x,y)\)</span>是有最小均方误差决定的，所以我们通过<span class="math inline">\(\frac{\partial \delta^2(x,y)}{\partial w(x,y)}=0\)</span>来得到最小化的<span class="math inline">\(\delta^2(x,y)\)</span>。 得到<span class="math inline">\(w(x,y)=\frac{\bar{g(x,y)h(x,y)}-\bar{g}(x,y)\bar{\eta}(x,y)}{\bar{\eta^2}(x,y)-\bar{\eta}^2(x,y)}\)</span>。</p>
<h3 id="形态学处理">形态学处理</h3>
<p>形态学基于集合论和逻辑运算。 首先，我们回顾下集合理论： - 假设A是<span class="math inline">\(Z^2\)</span>中的一个集合，如果<span class="math inline">\(a=(a_1, a_2)\)</span>是A的一个元素, 那么有<span class="math inline">\(a \in A\)</span> - 如果不是, 则<span class="math inline">\(a \notin A\)</span> - <span class="math inline">\(\varnothing\)</span> 表示 null(empty) 集合 - 如果集合A是B的子集, 那么有<span class="math inline">\(A \subseteq B\)</span> - A和B联合: <span class="math inline">\(C=A\cup B\)</span> - A和B的交集: <span class="math inline">\(D=A\bigcap B\)</span> - A和B不相交: <span class="math inline">\(A \bigcap B = \varnothing\)</span> - A的补集: <span class="math inline">\(A^c=\{w|w\notin A\}\)</span> - A和B的差集: <span class="math inline">\(A-B=\{w|w\in A, w\notin B\}=A\bigcap B^c\)</span> - 集合A的平移：<span class="math inline">\(z=(z_1,z_2)\)</span> <span class="math inline">\(A_z=\{c|c=a+z, a\in A\}\)</span> - 集合B的反射: <span class="math inline">\(\hat{B}=\{w|w=-b,b\in B\}\)</span></p>
<p>在图像处理中，主要的形态学操作有膨胀和腐蚀。其他复杂的形态学操作例如开闭操作都可以通过腐蚀和膨胀组合实现。</p>
<p>膨胀操作定义为：<span class="math inline">\(A\oplus B =\{z|[(\hat{B}_z \bigcap A)]\ne \varnothing\}\)</span> 腐蚀操作定义为：<span class="math inline">\(A\oplus B=\{z|[\hat{B}_z \bigcap A]\subseteq A\}\)</span> 膨胀和腐蚀有关系： <span class="math inline">\(A\oplus B=\{z|[\hat{B}_z \bigcap A]\subseteq A\}\)</span> 开操作：<span class="math inline">\(A \circ B = (A\ominus B)\oplus B\)</span> 闭操作：<span class="math inline">\(A\bullet B = (A\oplus B)\ominus B\)</span> 开和闭操作的关系：<span class="math inline">\((A\bullet B)^c=A^c \circ \hat{B}\)</span> Or : <span class="math inline">\(A\bullet B=(A^c \circ \hat{B})^c\)</span></p>
<p>形态学的应用有：边界提取、区域填充、分离连接的部件、去噪。 边缘提取：<span class="math inline">\(\beta(A)=A-(A\ominus B)\)</span> 区域填充：<span class="math inline">\(A^F=X_k\cap A\)</span>，<span class="math inline">\(X_k=(X_{k-1}\oplus B)\bigcap A\)</span> 分离链接的部件：<span class="math inline">\(X_k=(X_{k-1}\oplus B)\bigcap A\)</span> 去噪：<span class="math inline">\((A\circ B)\bullet B\)</span> or <span class="math inline">\((A\bullet B)\circ B\)</span></p>
<h3 id="分割和边缘提取">分割和边缘提取</h3>
<p>图像分割是图像处理中很重要的部分，最典型的处理就是通过前景和背景的像素间亮度来区分，一个阈值化的图像表示为<span class="math inline">\(g(x,y)=\begin{cases}1, if \ f(x,y)&gt;T \\ 0 , \ if \ f(x,y)\le T \end{cases}\)</span>, 其中<span class="math inline">\(T\)</span>是由<span class="math inline">\(T=T[x,y,p(x,y),f(x,y)]\)</span>计算出的阈值。 阈值一般有全局、布局和自适应3种： - 全局阈值就是应用整个图像 - 局部阈值就是对一个区域应用阈值，不同区域不一样。 - 而自适应阈值<span class="math inline">\(T\)</span>则取决于x 和 y 的坐标。</p>
<p>首先，我们看看基本的全局阈值，我们可以得到一幅图像直方图的最大灰度和最小灰度分布值，然后取中间值可以。 另一种启发式的方式寻找全局阈值<span class="math inline">\(T\)</span>: - 首先估计一个阈值<span class="math inline">\(T\)</span>. - 用这个阈值<span class="math inline">\(T\)</span>来分割图像，则会产生2组像素点: <span class="math inline">\(G_1\)</span>包含了灰度值大于阈值的像素，<span class="math inline">\(G_2\)</span> 则包含灰度值小于阈值<span class="math inline">\(T\)</span>的像素。 - 计算2组像素的灰度值均值<span class="math inline">\(\mu_1\)</span> 和 <span class="math inline">\(\mu_2\)</span> - 然后得到一个新的阈值<span class="math inline">\(T=0.5(\mu_1+\mu_2)\)</span> - 重复步骤2 - 4 直到 连续<span class="math inline">\(T\)</span>之间的间隔小于预先定义的<span class="math inline">\(T_0\)</span>.</p>
<p>自适应局部阈值： - 将图像划分成数个小区域 - 对每个子区域使用不同的阈值去分割； - 由于每个区域的阈值不一样，所以这叫自适应阈值分割。</p>
<p>那么我们需要知道什么是最优阈值。我们的目标是使得将给定点划分到前景或背景的平均误差最小。假设图像只包含2个灰度级，<span class="math inline">\(p_1(z)\)</span>和<span class="math inline">\(p_2(z)\)</span>是区域1（前景）和区域2（背景）相对应的概率密度函数。 将背景错分为前景的错误概率:<span class="math inline">\(E_1(T)=\int_{-\infty}^{T}p_2(z)dz\)</span>；将前景错分为背景的错误概率: <span class="math inline">\(E_2(T)=\int_{T}^{\infty}p_1(z)dz\)</span>。 将这2种概率混合得到：<span class="math inline">\(p(z)=P_1p_1(z)+P_2p_2(z)\)</span>，并且我们假设每个像素要么是前景要么就是背景，所以<span class="math inline">\(P_1+P_2=1\)</span>。总错误概率为：<span class="math inline">\(E(T)=P_2E_1(T)+P_1E_2(T)\)</span> 为了最小化错误概率，将<span class="math inline">\(E(T)\)</span>对<span class="math inline">\(T\)</span>求导数，并令其等于0.得到： <span class="math inline">\(\frac{dE(T)}{dT}=0\)</span>, 然后找到<span class="math inline">\(T\)</span>，使得 <span class="math inline">\(P_1p_1(T)=P_2p_2(T)\)</span>. 如果 <span class="math inline">\(P_1 = P_2\)</span>, 那么最佳阈值则是<span class="math inline">\(p_1(z)\)</span> 和 <span class="math inline">\(p_2(z)\)</span> 相交的店。</p>
<p>我们假设<span class="math inline">\(p_1(z)\)</span> 和 <span class="math inline">\(p_2(z)\)</span>都服从高斯分布：<span class="math inline">\(p(z)=\frac{P_1}{\sqrt{2\pi}\delta_1}e^{-\frac{(z-\mu_1)2}{2\delta_1^2}}+\frac{P_2}{\sqrt{2\pi}\delta_2}e^{-\frac{(z-\mu_2)2}{2\delta_2^2}}\)</span> 令<span class="math inline">\(P_1p_1(T)=P_2p_2(T)\)</span>, 得到：<span class="math inline">\(P_1p_1(T)=P_2p_2(T) \rightarrow \frac{P_1}{\sqrt{2\pi}\delta_1}e^{-\frac{(T-\mu_1)^2}{2\delta_1^2}}=\frac{P_2}{\sqrt{2\pi}\delta_2}e^{-\frac{(T-\mu_2)^2}{2\delta_2^2}}\)</span> 最后结果是一个二次等式: <span class="math inline">\(AT^2+BT+C=0\)</span>, 其中 <span class="math inline">\(A=\delta_1^2-\delta_2^2\)</span>, <span class="math inline">\(B=2(\mu_1\delta_2^2-\mu_2\delta_1^2)\)</span>, <span class="math inline">\(C=\delta_1^2\mu_2^2-\delta_2^2\mu_1^2+2\delta_1^2\delta_2^2ln(\delta_2P_1/\delta_1P_2)\)</span> 如果<span class="math inline">\(\delta_1 = \delta_2 = \delta\)</span>, 那么最优阈值很简单的就得到： <span class="math inline">\(T=\frac{\mu_1-\mu_2}{2}+\frac{\delta^2}{\mu_1-\mu_2}ln(\frac{P_2}{P_1})\)</span>. 如果 <span class="math inline">\(P_1 = P_2\)</span>, 那么最优阈值则是平均 <span class="math inline">\(T=\frac{\mu_1+\mu_2}{2}\)</span>.</p>
<p>不连续性的检测是图像分割的一个方向，这个方向有3中类型的不连续：点检测、线检测、边沿检测。</p>
<p>点检测很容易，用模板计算一个值 <span class="math inline">\(R=\sum_{i=1}^9 w_i z_i\)</span>, 然后判断是否<span class="math inline">\(|R|\ge T\)</span>。 线条检测，应用每个模板到图像，假设R1, R2, R3,R4是水平，+45度，垂直，-45度的响应。如果在特定的点处，对所有的<span class="math inline">\(j\ne i\)</span>，都有<span class="math inline">\(|R_i|&gt;|R_j|\)</span>.那么该点很有可能属于一个在mask i方向的直线。 为了检测给定mask后图像中所有的线，我们只需要将mask通过图像，然后对结果进行阈值判断。指向左边的则具有最大响应。</p>
<p>边沿检测是检测图像有用信息的最常见方式。有2种方式：一阶导数（梯度算子），二阶导数（拉普拉斯算子）。</p>
<p>首先，我们看梯度算子。一个二维图像的梯度计算为：<span class="math inline">\(\triangledown f(x,y)=[\begin{cases}G_x(x,y) \\ G_y(x,y)\end{cases}]=\begin{cases}\frac{\partial f(x,y)}{\partial x} \\ \frac{\partial f(x,y)}{\partial y}\end{cases}\)</span>， 梯度的强度和图像不连续的方向成正比。所以，图像差分增强了边界和其他不连续点（噪声），使得那些不重要的区域具有缓慢变化的灰度值。 一个图像梯度的幅值计算为：<span class="math inline">\(|\triangledown f(x,y)|=[G_x^2(x,y)+G_y^2(x,y)]^{1/2}\)</span>, 且有方向 <span class="math inline">\(\phi(x,y)=tan^{-1}(\frac{G_y(x,y)}{G_x(x,y)})\)</span>。梯度矢量指向的是坐标<span class="math inline">\((x,y)\)</span>具有最快变化率的方向。</p>
<p>为了更好的理解边沿、线方向和频率。 我们定义一个图像可以模型表示为：<span class="math inline">\(f(x,y)=h(ax+by)\)</span>，<span class="math inline">\(ax+by=t\)</span>是代表<span class="math inline">\(f(x,y)\)</span>方向的直线表示。 有<span class="math inline">\(y=-\frac{a}{b}+t=tan^{-1}\theta x+t\)</span>, 有<span class="math inline">\(\nabla f(x,y)=\begin{bmatrix}G_x(x,y)\\G_y(x,y)\end{bmatrix}=\begin{bmatrix}\frac{\partial f}{\partial x}\\frac{\partial f}{\partial y}\end{bmatrix}=\begin{bmatrix}ah&#39;(t)\\bh&#39;(t)\end{bmatrix}\)</span> 得到方向<span class="math inline">\(\phi(x,y)=tan^{-1}(\frac{G_y(x,y)}{G_x(x,y)})=tan^{-1}(\frac{b}{a})=-tan^{-1}\frac{a}{b}\mp \frac{\pi}{2}=\theta\mp \frac{\pi}{2}\)</span></p>
<p>对偏导的近似有：<span class="math inline">\(\begin{cases}G_x(x,y)=f(x+1,y)-f(x-1,y)\\G_y(x,y)=f(x,y+1)-f(x,y-1)\end{cases}\)</span> 这等价于用2个简单的<span class="math inline">\(3\times 3\)</span>模板通过图像：<span class="math inline">\(\begin{cases}G_x(x,y)=h_x(x,y)*f(x,y)\\G_y(x,y)=h_y(x,y)*f(x,y)\end{cases}\)</span>，所以这对噪声是很敏感的。</p>
<p>因此在求导数之前，先对图像进行平滑 <span class="math inline">\(\nabla[h(x,y)*f(x,y)]=[\nabla h(x,y)]^{*}f(x,y)\)</span></p>
<p>由于导数的线性特性，对经过平滑的图像进行求导等效于将图像和<span class="math inline">\(\nabla h(x,y)\)</span>进行卷积，所以我们有梯度算子<span class="math inline">\(\nabla h(x,y)\)</span>.</p>
<p>我们希望梯度算子有噪声压制特性，这样可以能鲁棒的检测边缘。然而，梯度操作也可能因为平滑滤波器和离散图像的原因而导致偏移的梯度方向。</p>
<p>例子，如果一幅数字图像可以模型表示为：<span class="math inline">\(f(x,y)=cos(ax+by)\)</span> 那么<span class="math inline">\(\bar{\nabla}h(x,y)=f(x=1,y)-f(x-1,y)+j[f(x,y+1)-f(x,y-1)]\)</span> <span class="math inline">\(\phi(x,y)=arctan \frac{Im\{\bar{\nabla f(x,y)}\}}{Re\{\bar{\nabla f(x,y)}\}} \ne arctan \frac{b}{a}\)</span></p>
<p>另外一种是拉普拉斯算子，利用了二阶导性质。 拉普拉斯算子定义为：<span class="math inline">\(\triangledown^2f = \frac{\partial^2f(x,y)}{\partial x^2}+\frac{\partial^2f(x,y)}{\partial y^2}\)</span> 离散函数逼近为：<span class="math inline">\(\frac{\partial^2f}{\partial x^2}=f(x+1,y)+f(x-1,y)-2f(x,y)\)</span>, <span class="math inline">\(\frac{\partial^2f}{\partial y^2}=f(x,y+1)+f(x,y-1)-2f(x,y)\)</span> 得到：<span class="math inline">\(\triangledown^2f = f(x+1,y)+f(x-1,y)+f(x,y+1)+f(x,y-1)-4f(x,y)\)</span>.</p>
<p>由于拉普拉斯算子对噪声敏感。所以，在应用拉普拉斯算子之前，需要进行特定的平滑。也就是：<span class="math inline">\(h(x,y)=-e^{-\frac{x^2+y^2}{2\delta^2}}=-e^{-\frac{r^2}{2\delta^2}}\)</span>. 由于二阶导数连续性，我们有 <span class="math inline">\(\nabla^2 h\)</span>. <span class="math inline">\(\nabla^2(h*f)=(\nabla^2 h )*f\)</span>.</p>
<h3 id="局部处理">局部处理</h3>
<p>Hough变换是用来<strong><em>检测图像中线、圈的技术</em></strong>。直线可以表示为：<span class="math inline">\(y_i=ax_i+b\)</span>. 那么如果一个像素(<span class="math inline">\(x_i, y_i\)</span>)通过直线，则有：<span class="math inline">\(y_i=ax_i+b\)</span>， 将这个表达式重新写成a－b面或参数空间得到：<span class="math inline">\(b=y_i-ax_i\)</span>。 所以，在直线xy-面中第一条线满足<span class="math inline">\(y=a&#39;x+b&#39;\)</span>，对应ab-plane ( <span class="math inline">\(a&#39;, b&#39;\)</span>). xy域中所有一条直线上的点都必须在参数空间的同一个点处。</p>
<p>Hough变换的过程： - 首先将参数空间分成连续的多个单元格(a, b). - 所有的单元格都以0为中心，也就是<span class="math inline">\(A(a,b)=0\)</span> - 对图像中检测到的每个点(<span class="math inline">\(x_i, y_i\)</span>) ，有 <span class="math inline">\(A(a,b)+1 \rightarrow A(a,b)\)</span> 对所有有a和b 满足 <span class="math inline">\(b=y_i-ax_i\)</span>. 在步骤的最后，A(a, b)对应落在<span class="math inline">\(y=ax+b\)</span>直线上的点数目。</p>
<p>用<span class="math inline">\(y=ax+b\)</span>带来的问题是，如果a是无限的，也就是对一个垂直线来说<span class="math inline">\(a\)</span>是无限的。为了避免这个问题，使用公式 <span class="math inline">\(x cos \theta + y sin \theta = \rho\)</span>来代替表示直线。 下面是一般化的Hough变换，<span class="math inline">\(g(v,c)=0\)</span>，其中<span class="math inline">\(v,c\)</span>是坐标的矢量. 例如，一个环可以表示为：<span class="math inline">\((x-c_1)^2+(y-c_2)^2=c_2^2\)</span>，有3个参数(c1, c2, c3)。</p>
<p>前面讲了如果检测线，那么如何检测边缘呢。</p>
<ul>
<li>首先计算图像的梯度，并且根据阈值得到二值图像</li>
<li>然后定义在<span class="math inline">\(\theta\)</span>‐plane的子区域</li>
<li>检查那些高像素浓度在累计单元格的统计数量</li>
<li>检查不同像素直接的关系</li>
<li>如果2点之间的距离或者在一定阈值下接近领域，那么将点之间的间隙链接。</li>
</ul>
<p><strong>Local Dominant Orientation</strong> 例如在指纹识别中，弓形、螺纹和环形。局部的方向性是固有的特性，方向可以用较短的线来表示。那些边缘或线的方向<span class="math inline">\(\phi(x,y)\)</span>, <span class="math inline">\(0^o&lt;\phi(x,y)&lt;180^o\)</span>对图像来说非常重要。那么如何去获得稳定且鲁棒的方向信息呢》</p>
<p>我们可以将梯度矢量表示为复数形式：<span class="math inline">\(\nabla f(x,y)=G_x(x,y)+jG_y(x,y)=|\nabla f(x,y)|e^{j\theta (x,y)}\)</span> 然后有，<span class="math inline">\(|\nabla f(x,y)|^2=G_x^2(x,y)-G_y^2(x,y)+2jG_x(x,y)G_y(x,y)=|\nabla f(x,y)|^2 e^{j\theta (x,y)}\)</span> 所以，方向：<span class="math inline">\(\theta = \frac{1}{2}than^{-1}(\frac{2G_x(x,y)G_y(x,y)}{G_x^2(x,y)-G_y^2(x,y)})\)</span></p>
<p>现在，对梯度平方求平均得到：<span class="math inline">\(\bar{|\nabla f(x,y)|^2}=\sum_{(s,t)\in S_{xy}}G_x^2(s,t)-G_y^2(s,t)+2jG_x(s,t)G_y(s,t)=A-B+j2C\)</span> 所以，显式的方向计算为：<span class="math inline">\(\bar{\theta (x,y)}=\frac{1}{2}tan^{-1}(\frac{2C}{A-B})\)</span>, 其中<span class="math inline">\(A=\sum_{(s,t)\in S_{xy}}G_x^2(s,t)\)</span>, <span class="math inline">\(B=\sum_{(s,t)\in S_{xy}}G_y^2(s,t)\)</span>,<span class="math inline">\(C=\sum_{(s,t)\in S_{xy}}G_x(s,t)G_y(s,t)\)</span></p>
<p>显然，平均梯度幅值是：<span class="math inline">\(|\bar{\nabla f(x,y)^2}|=\sqrt{(A-B)^2+4C^2}\)</span> 以及<span class="math inline">\(\bar{|\nabla f(x,y)^2|}=A+B\)</span>. 接下来得到<span class="math inline">\(coh(x,y)=\frac{\sqrt{(A-B)^2+4C^2}}{A+B}\)</span></p>
<h2 id="模式识别和决策理论"><span style="color:red"><strong><em>模式识别和决策理论</em></strong></span></h2>
<h3 id="概要"><span style="color:blue"><strong><em>概要</em></strong></span></h3>
<ol style="list-style-type: decimal">
<li>理解 <span style="color:red"><strong><em>什么是模式识别</em></strong></span></li>
<li>理解 <span style="color:red"><strong><em>模式识别的处理过程</em></strong></span></li>
<li><span style="color:red"><strong><em>理解决策理论是如何建立的</em></strong></span></li>
<li>最优化决策规则: <span style="color:red"><strong><em>MAP 和 Bayesian decision rules</em></strong></span></li>
<li>如何评价一个模式识别系统: <span style="color:red"><strong><em>识别准确率 or 错误率</em></strong></span></li>
</ol>
<h3 id="什么是模式识别"><span style="color:blue"><strong><em>什么是模式识别?</em></strong></span></h3>
<p>模式识别就是感知一个类型，然后从该类型中解析相关信息，得到这些信息后，理解并用计算机做出相应的决策。比如：假设你在护士学校遇到一个人，并且根据先验知识，了解到这个学校女生95%,男生5%。 我们假设<span class="math inline">\(w_2\)</span>是女生，<span class="math inline">\(w_1\)</span>是男生。显然<span class="math inline">\(p(w_1)=0.05\)</span>, <span class="math inline">\(p(w_2)=0.95\)</span>。 根据这些信息，我们基本可以认为是女生，而不考虑身高的因素。</p>
<p>上面只给出2个信息<span class="math inline">\(p(w_1)=0.05\)</span>, <span class="math inline">\(p(w_2)=0.95\)</span>，让我们去决策是男生和女生，这信息太少。实际情况不会这样。</p>
<p>这里介绍<span style="color:blue"><strong><em>类条件概率分布函数(Class-conditional probability distribution function) <span class="math inline">\(p(x|w)\)</span></em></strong></span>--类别状态为<span class="math inline">\(w\)</span>时的<span class="math inline">\(x\)</span>的概率密度函数。</p>
<p>i回到上面的问题，当得知身高信息为1.72m时，那么可以更好的帮助我们做出决定。从数学上看，判别就成了类别为男性<span class="math inline">\(w_1\)</span>时，身高是<span class="math inline">\(1.72\)</span>的概率，也就是<span class="math inline">\(x=1.72\)</span>的概率<span class="math inline">\(p(w_1|x)\)</span>。以及类别<span class="math inline">\(x\)</span>是女性的概率<span class="math inline">\(p(w_2|x)\)</span>，这2个概率也叫作<strong><em>后验概率</em></strong>。</p>
<p>概率知识：联合概率 <span class="math inline">\(p(x,w_i)=p(x)p(w_i|x)=p(w_i)(w_i|x)\)</span>;后验概率/条件概率<span class="math inline">\(p(w_i|x)=\frac{p(w_i)p(x|w_i)}{p(x)}\)</span>; 先验概率<span class="math inline">\(p(x)=\sum_{i=1}^cp(x|w_i)p(w_i)\)</span></p>
<h3 id="决策规则"><span style="color:blue"><strong><em>决策规则</em></strong></span></h3>
<p>有了“身高”我们可以建立决策规则：<span class="math inline">\(\begin{cases}\text{Decide: }w_k: \text{if } p(w_k|x)&gt;p(w_i|x),i\ne k\\ w_k=\text{argmax}_{w_i}[p(w_i|x)] \end{cases}\)</span>。 这个等式得到的<span class="math inline">\(w_k\)</span>是<span class="math inline">\(w_1,w_2\)</span>中的一个，取决于2个后验概率的大小。</p>
<p>上面是我们基于观测到的<span class="math inline">\(x\)</span>做出决定<span class="math inline">\(w_k\)</span>的概率，也可以叫正确概率。那么错误概率呢？我们可以用1减去正确概率得到：<span class="math inline">\(p(e_k|x)=1-p(w_k|x)\)</span>。这个解释就是，我们在观测到<span class="math inline">\(x\)</span>的情况下，还判断错的概率。</p>
<p>显然，为了做出最好的决定。我们要么最大化正确概率，要么最小化错误概率。这个规则叫做<span style="color:blue"><strong><em>Maximum a posterior (MAP) rule</em></strong></span>： <span class="math inline">\(\text{Decide: }w_k=\text{argmax}_{w_i}[p(w_i|x)]=\text{argmin}_{w_i}[p(e_i|x)]\)</span></p>
<p>上面的例子只有2个类型（男生，女生）。推广到一般性，有<span class="math inline">\(c\)</span>个类型，无论<span class="math inline">\(x\)</span>是什么，总有约束<span class="math inline">\(\sum_{i=1}^cp(w_i|x)=1\)</span>。所以有:<span class="math inline">\(1-p(w_k|x)=\sum_{i=1,i\ne k}^c p(w_i|x)\)</span>.</p>
<p>根据MAP原则，我们得到:<span class="math inline">\(w_k=\text{argmin}_{w_i}p(e_k|x)=1-\text{argmax}_{w_i}p(w_k|x)=\text{argmin}_{w_i}\sum_{i=1,i\ne k}^c p(w_i|x)\)</span>.</p>
<h3 id="决策规则的评价"><span style="color:blue"><strong><em>决策规则的评价</em></strong></span></h3>
<p>以上<span class="math inline">\(p(w_1|x)\)</span>和<span class="math inline">\(p(w_2|x)\)</span>需要我们做出2次观测，但是实际情况是我们不一定每次都观测一样的值，这种规则下，可以将平均误差概率最小化吗？答案是肯定的，所有可能观测值<span class="math inline">\(x\)</span>的平均误差可以表示为：<span class="math inline">\(\begin{cases}p(e)=\sum_{x=-\infty}^{\infty}p(e_k|x)p(x)\\ p(e)=\int_{-\infty}^{\infty}p(e_k|x)p(x)dx \end{cases}\)</span>。</p>
<p>对连续误差进一步展开得到：<span class="math inline">\(p(e)=\int_{-\infty}^{\infty}p(e_k|x)p(x)dx=\int_{-\infty}^{\infty}[1-p(w_k|x)]p(x)dx=1-\int_{-\infty}^{\infty}p(w_k)p(x|w_k)dx\)</span></p>
<p>注意，有时候由于先验空间<span class="math inline">\(x\)</span>很大，也就是样本总量很大，我们一般选取不同的区域<span class="math inline">\(R_i\)</span>， 也就是将整个样本<span class="math inline">\(x\)</span>的空间划分成<span class="math inline">\(c\)</span>个决策区域。 对于不同区域<span class="math inline">\(R_i\)</span>观测到的<span class="math inline">\(x\)</span>我们有不同的决策<span class="math inline">\(w_k\)</span>。 那么我们的积分就变为:<span class="math inline">\(p(e)=1-\sum_{i=1}^c \int_{R_i}p(w_i)p(x|w_i)dx=1-\sum_{i=1}^c p(w_i)\int_{R_i}p(x|w_i)dx\)</span>. 同样，错误的概率也响应的改变：<span class="math inline">\(p(correct)=1-p(e)=\sum_{i=1}^c p(w_i)\int_{R_i}p(x|w_i)dx\)</span></p>
<p>我们不止观察到一个值，也许多个。随着我们观察到的特征越来越多，那么决策正确的概率会越来越高，对应错误概率也越来越低。</p>
<ol style="list-style-type: decimal">
<li>如果实现什么都不知道，还要你将一个物体分类到<span class="math inline">\(c\)</span>个类别(<span class="math inline">\(w_i\)</span>)中的一个类别<span class="math inline">\(w_k\)</span>中，这时候只能选择概率最高的那个了。所以决策就是 <span class="math inline">\(\text{Decide: }w_k=\text{argmax}_{w_i}[p(w_i)]\)</span>。 对应错误决策的概率是： <span class="math inline">\(p(e_k)=1-p(w_k)=\sum_{i=1,i\ne k}^cp(w_i)\)</span></li>
<li>条件宽松些，如果你已经知道了目标的属性或特征向量<span class="math inline">\(\mathbf{x}\)</span>(一个列向量)，这时候将目标分类的决策就变程了后验概率了，因为我们已经知道特征。决策变为：<span class="math inline">\(\text{Decide: }w_k=\text{argmax}_{w_i}[p(w_i|\mathbf{x})]\)</span>, 对应决策错误的概率：<span class="math inline">\(p(e_k|\mathbf{x})=1-p(w_k|\mathbf{x})=\sum_{i=1,i\ne k}^cp(w_i|\mathbf{x})\)</span>. 相应的目标也变成最小化错误概率 <span class="math inline">\(\text{Decide: }w_k=\text{argmin}_{w_i}[\sum_{i=1,i\ne k}^c p(w_i|\mathbf{x})]=\text{argmin}_{w_i}[p(e_k|\mathbf{x})]\)</span>, 这也叫最小化错误概率分类器。</li>
</ol>
<h3 id="贝叶斯决策论"><span style="color:blue"><strong><em>贝叶斯决策论</em></strong></span></h3>
<p>在一些应用中，对某一个类分类错的概率并不能代表其他类分错的概率。我们假设<span class="math inline">\(\lambda_{ki}\)</span>是将实际类别是<span class="math inline">\(w_i\)</span>分错的损失(Loss)(假定实际应该是<span class="math inline">\(w_k\)</span>)。那么决策<span class="math inline">\(w_i\)</span>的损失则为：<span class="math inline">\(R_k(\mathbf{x})=\sum_{i=1,i\ne k}^c \lambda_{ki} p(w_i|\mathbf{x})\)</span>. 因此，根据最下化错误概率原则有：<span class="math inline">\(\text{Decide: }w_k:\text{argmin}_{w_i}[R_i(\mathbf{x})]=\text{argmin}_{w_i}[\sum_{i=1,i\ne k}^c \lambda_{ki} p(w_i|\mathbf{x})]\)</span>---这就是贝叶斯决策论。该损失函数帮助我们去处理那些分类误差比其他类大的类。</p>
<p>在一些应用中，不同类的正确决策也许有不同的损失<span class="math inline">\(\lambda_{kk}\ne 0\)</span>。如果包含正确决策的损失，那么一个决策（包含错误或正确）的损失是<span class="math inline">\(R_k(\mathbf{x})=\sum_{i=1}^c \lambda_{ki} p(w_i|\mathbf{x})\)</span>. 这个决定就是给定特定<span class="math inline">\(x\)</span>的情况下，决策<span class="math inline">\(w_k\)</span>的损失函数，也叫条件风险。</p>
<p>所以上面的贝叶斯决策规则可以改为：<span class="math inline">\(\text{Decide: }w_k:\text{argmin}_{w_i}[R_i(\mathbf{x})]=\text{argmin}_{w_i}[\sum_{j=1}^c \lambda_{ij} p(w_j|\mathbf{x})]=\text{argmin}_{w_i}[\sum_{j=1}^c \lambda_{ij} p(w_j)p(\mathbf{x})|w_j]\)</span></p>
<h3 id="贝叶斯决策规则的评价"><span style="color:blue"><strong><em>贝叶斯决策规则的评价</em></strong></span></h3>
<p>同样我们也有关于贝叶斯决策规则的评价。我们通过所有风险的评平均来定义一个决策规则的好与坏：</p>
<p><span class="math inline">\(R=\int_{R_{\mathbf{x}}}R_k(\mathbf{x})p(\mathbf{x})d{\mathbf{x}}=\int_{R_{\mathbf{x}}} \sum_{i=1}^c \lambda_{ki} p(w_i|\mathbf{x})p(\mathbf{x})d{\mathbf{x}}=\sum_{k=1}^c\int_{R_{xk}}\sum_{i=1}^c \lambda_{ki} p(w_i)p(\mathbf{x}|w_i)d{\mathbf{x}}\)</span></p>
<p>所以我们最终的目标就最小化平均损失。</p>
<h3 id="分类问题"><span style="color:blue"><strong><em>2分类问题</em></strong></span></h3>
<p>假设我们有2个可能的类<span class="math inline">\(w_1,w_2\)</span>，并且定义2种行为：行为<span class="math inline">\(\alpha_1\)</span>对于将类别判决为<span class="math inline">\(w_1\)</span>，<span class="math inline">\(\alpha_2\)</span>对应于将列别判决为<span class="math inline">\(w_2\)</span>。同时，我们定义<span class="math inline">\(\lambda_{ij}=\lambda(\alpha_i|w_j)\)</span>表示当实际类别为<span class="math inline">\(w_j\)</span>时误判为<span class="math inline">\(w_i\)</span>所引起的损失。 所以我们有2种行为的条件风险：<span class="math inline">\(\begin{cases}R_1(\alpha_1|\mathbf{x})=\lambda_{11}P(w_1|\mathbf{x})+\lambda_{12}P(w_2|\mathbf{x})\\ R_2(\alpha_2|\mathbf{x})=\lambda_{21}P(w_1|\mathbf{x})+\lambda_{22}P(w_2|\mathbf{x})\end{cases}\)</span> 有大量方式来描述最小风险决策规则。基本规则就是如果<span class="math inline">\(R_1(\alpha_1|\mathbf{x})&lt;R_2(\alpha_2|\mathbf{x})\)</span>，则判决为<span class="math inline">\(w_1\)</span>，否则判决为<span class="math inline">\(w_2\)</span>。 利用贝叶斯公式，我们可以用先验概率和条件概率来表示后验概率：如果<span class="math inline">\((\lambda_{21}-\lambda_{11})p(w_1|\mathbf{x})&gt;(\lambda_{12}-\lambda_{22})p(w_2|\mathbf{x})\)</span>， 则判决为<span class="math inline">\(w_1\)</span>,否则判决为<span class="math inline">\(w_2\)</span>.</p>
<p>另一种方式是：如果<span class="math inline">\(\frac{p(\mathbf{x}|w_1)}{p(\mathbf{x}|w_2)}&gt;\frac{(\lambda_{12}-\lambda_{22})}{(\lambda_{21}-\lambda_{11})}\frac{P(w_2)}{P(w_1)}\)</span>，则判决为<span class="math inline">\(w_1\)</span>。 其中，<span class="math inline">\(\frac{p(\mathbf{x}|w_1)}{p(\mathbf{x}|w_2)}\)</span>叫做“似然比”。 所以贝叶斯决策理论可以解释为如果似然比不超过某个不依赖于观测值<span class="math inline">\(\mathbf{x}\)</span>的阈值，则可判决为<span class="math inline">\(w_1\)</span></p>
<p>这里引入0-1损失的概念：如果采取行为<span class="math inline">\(\alpha_i\)</span>而实际类别是<span class="math inline">\(w_j\)</span>, 那么在<span class="math inline">\(i=j\)</span>的情况下是正确的，如果<span class="math inline">\(i\ne j\)</span>，则误判。这种情况下的损失函数就是所谓的”0-1“ loss: <span class="math inline">\(\lambda_{ij}=\begin{cases}0, \text{ if } i =j\\1, \text{ if } i\ne j\end{cases}\)</span> 在多类型问题中，可以将0-1 loss写成矩阵形式：<span class="math inline">\(\begin{bmatrix}0&amp;1&amp;1&amp;...&amp;1\\1&amp;0&amp;1&amp;...&amp;1\\...&amp;...&amp;&amp;&amp;...\\1&amp;1&amp;...&amp;&amp;0\end{bmatrix}\)</span> 与这个损失函数对应的就是平均误差概率，条件风险为:<span class="math inline">\(R(\alpha_i|\mathbf{x})=\sum_{j=1}^c\lambda(\alpha_j|w_j)p(w_j|\mathbf{x})=\sum_{j\ne i}p(w_j|\mathbf{x})=1-p(w_i|\mathbf{x})\)</span>. 这种最小化风险的贝叶斯决策规则要求了MAP，也就是最小化错误概率。</p>
<p><span style="color:blue"><strong><em>Example</em></strong></span> 假设A和B有相同数量。A物体服从<span class="math inline">\(N(2,1)\)</span>的类别条件概率密度，B服从<span class="math inline">\(N(8,1)\)</span>的条件概率密度。找个一个规则，使用似然比测试来区分2个物体（对于所有的错误决定，采取同样的惩罚）。如果A是B的2倍，那么又如何确定？</p>
<p>当A=B时，假设<span class="math inline">\(w_1\)</span>是类型A，<span class="math inline">\(w_2\)</span>是类型B。A和B的数量都是<span class="math inline">\(x\)</span>。且有如下规则： 类型 A: <span class="math inline">\(p(x|w_1)\sim N(2,1)\Rightarrow p(x|w_1)=\frac{1}{2\pi}exp[-\frac{1}{2}(x-2)^2]\)</span> 类型 B: <span class="math inline">\(p(x|w_2)\sim N(8,1)\Rightarrow p(x|w_2)=\frac{1}{2\pi}exp[-\frac{1}{2}(x-8)^2]\)</span> 根据A和B的数量相等，我们有<span class="math inline">\(P(w_2)=P(w_1)\)</span>。且他们的惩罚相等，所以：<span class="math inline">\(\lambda_{11}=\lambda_{22}=0\)</span>,<span class="math inline">\(\lambda_{12}=\lambda_{21}=k\)</span>. 似然比测试规则为:<span class="math inline">\(\frac{p(x|w_1)}{p(x|w_2)}&gt;\frac{(\lambda_{12}-\lambda_{22})}{(\lambda_{21}-\lambda_{11})}\frac{P(w_2)}{P(w_1)}\Rightarrow \frac{exp[-\frac{1}{2}(x-2)^2]}{exp[-\frac{1}{2}(x-8)^2]}&gt;1\)</span> 两边去对数，得到：<span class="math inline">\(-\frac{1}{2}[(x-2)^2-(x-8)^2]&gt;0\Rightarrow (x-2)^2-(x08)^2&lt;0\Rightarrow x&lt;5\)</span> 所以最终的规则是：如果<span class="math inline">\(x&lt;5\)</span>，则判定为<span class="math inline">\(w_1\)</span>,否则为<span class="math inline">\(w_2\)</span>.</p>
<p>当A=2B是，有<span class="math inline">\(\frac{P(w_1)}{P(w_2)}=2\Rightarrow P(w_1)=2P(w_2)\)</span> 所以有似然比测试：<span class="math inline">\(\frac{p(x|w_1)}{p(x|w_2)}&gt;\frac{(\lambda_{12}-\lambda_{22})}{(\lambda_{21}-\lambda_{11})}\frac{P(w_2)}{P(w_1)}\Rightarrow \frac{exp[-\frac{1}{2}(x-2)^2]}{exp[-\frac{1}{2}(x-8)^2]}&gt;\frac{1}{2}\Rightarrow x&lt;5.16\)</span> 得到：如果<span class="math inline">\(x&lt;5.16\)</span>，则判定为<span class="math inline">\(w_1\)</span>,否则为<span class="math inline">\(w_2\)</span>.</p>
<h2 id="统计估计和机器学习"><span style="color:red"><strong><em>统计估计和机器学习</em></strong></span></h2>
<h3 id="学习框架"><span style="color:blue"><strong><em>学习框架</em></strong></span></h3>
<ul>
<li><span style="color:blue"><strong><em>非参数概率密度估计问题</em></strong></span>
<ul>
<li>Parzen窗</li>
<li>K近邻</li>
</ul></li>
<li><span style="color:blue"><strong><em>参数化概率密度估计</em></strong></span>
<ul>
<li>最大似然估计</li>
<li>多元高斯概率分布密度</li>
</ul></li>
</ul>
<h3 id="背景"><span style="color:blue"><strong><em>背景</em></strong></span></h3>
<p>有前面我们知道最优决策或最优化分类是:<span class="math inline">\(\text{Decide: }w_k=\text{argmin}_{w_k}[p(e_i|\mathbf{x})]=\text{argmin}_{w_k}[1-p(w_i|\mathbf{x})]=\text{argmax}_{w_k}[p(w_i|\mathbf{x})]=\text{argmax}_{w_k}[p(w_i)p(\mathbf{x}|w_i)]\)</span> 但是如果我们想设计一个自动的模式识别系统，我们需要知道<span class="math inline">\(\mathbf{x}\)</span>的概率分布密度PDF，这样我们才能得到<span class="math inline">\(p(\mathbf{x})\)</span>. 而在实际情况中，PDF总是未知的，我们只能从样本<span class="math inline">\(\{\mathbf{x_i}=[\mathbf{x_1},\mathbf{x_2},...,\mathbf{x_n}]\}\)</span>中估计。</p>
<p>基于训练样本<span class="math inline">\(D=\{\mathbf{x}_i\}\)</span>去确定决策的任务叫做统计估计或者机器学习。PDF是一个随机变量的分布信息。</p>
<p>在实际情况下，想要规估计后验概率<span class="math inline">\(p(w_k|\mathbf{x})\)</span>是很困难的，所以我们可以获得先验概率<span class="math inline">\(p(w_k)\)</span>和关于类别的条件概率<span class="math inline">\(p(\mathbf{x}|w_k)\)</span>。类别<span class="math inline">\(k\)</span>的先验概率估计为：<span class="math inline">\(\hat{p}(w_k)=n_k/n\)</span>，其中<span class="math inline">\(n_k\)</span>是训练样本中类别<span class="math inline">\(w_k\)</span>的数量，<span class="math inline">\(n\)</span>是训练样本的总数。</p>
<p>因为对所有类，<span class="math inline">\(p(\mathbf{x}|w_k)\)</span>的估计方法是一样的，所以简化<span class="math inline">\(p(\mathbf{x}|w_k)\)</span>的符号为<span class="math inline">\(p(\mathbf{x})\)</span>, 并且假设我们有<span class="math inline">\(n\)</span>的样本<span class="math inline">\(\{\mathbf{x}_i\}=[\mathbf{x}_1, \mathbf{x}_2, ..., \mathbf{x}_n]\)</span>, 每个样本是关于<span class="math inline">\(p(\mathbf{x})\)</span>独立同分布的。</p>
<p>并且根据概率密度函数定义，<span class="math inline">\(\mathbf{x}\)</span>掉在区域<span class="math inline">\(R\)</span>的概率为<span class="math inline">\(P(\mathbf{x})=\int_{R_{\mathbf{x}}}p(t)dt\simeq p(x)V\)</span>，其中<span class="math inline">\(V\)</span>区域<span class="math inline">\(R_x\)</span>的体积。如果在所有该类别的<span class="math inline">\(n\)</span>个训练中，有<span class="math inline">\(k\)</span>个样本掉在区域<span class="math inline">\(R\)</span>中，那么<span class="math inline">\(P\)</span>的估计为<span class="math inline">\(\hat{P}(\mathbf{x})=\frac{k}{n}\)</span>，同理概率密度函数<span class="math inline">\(p(\mathbf{x})\)</span>估计为<span class="math inline">\(\hat{p}(\mathbf{x})=\frac{k}{nV}\)</span>。显然，区域的形状<span class="math inline">\(V\)</span>不同，也会有不同的概率密度分布估计。</p>
<p>这叫做非参数化概率密度估计，因为并没有假设任何模型。</p>
<h3 id="parzen-窗概率密度估计非参数"><span style="color:blue"><strong><em>Parzen-窗概率密度估计（非参数）</em></strong></span></h3>
<p>通常，<span class="math inline">\(\mathbf{x}\)</span>是多维的<span class="math inline">\(\{\mathbf{x}\}=[\mathbf{x}_1, \mathbf{x}_2, ..., \mathbf{x}_d]\)</span>（假设有d维）。假设我们选择一个d维的超立方体作为区域，如果对于所有的<span class="math inline">\(j=1,2..,d\)</span>都有<span class="math inline">\(\frac{|x_j-x_{ij}|}{h}&lt;\frac{1}{2}\)</span>，则该样本<span class="math inline">\(\{\mathbf{x}_i\}=[x_{i1}, x_{i2}, ..., x_{id}]\)</span>掉在了这个超立方体中。</p>
<p>我们可以将样本是否掉在区域<span class="math inline">\(k\)</span>这一过程表示为：<span class="math inline">\(k=\sum_{i=1}^nK(\frac{\mathbf{X}-\mathbf{X}_i}{h})\)</span>，这个核函数叫做”Parzen-窗”。并且有：<span class="math inline">\(K(\mathbf{u})=\begin{cases}1, if \ |u_j|&lt;1/2 \ for \ \forall j=1,2,...,d\\0, \ otherwise\end{cases}\)</span></p>
<p>所以，我们可以估计<span class="math inline">\(\mathbf{x}\)</span>的概率密度<span class="math inline">\(p(\mathbf{x})\)</span>为：<span class="math inline">\(\hat{p}(\mathbf{x})=\frac{\sum_{i=1}^nK(\frac{\mathbf{x}-\mathbf{x}_i}{h})}{nh^d}\)</span></p>
<p>矩形核函数会产生非平滑的概率密度函数。我们也可以选择其他的核函数，只要满足条件：<span class="math inline">\(\begin{cases}K(\mathbf{u})\ge 0\\\int_{-\infty}^{\infty}K(\mathbf{u})d\mathbf{u}\end{cases}\)</span></p>
<p>将离散的点<span class="math inline">\(\{\mathbf{x}_i\}=[\mathbf{x}_1, \mathbf{x}_2, ..., \mathbf{x}_n]\)</span>插值以后也就得到连续的概率密度函数<span class="math inline">\(p(\mathbf{x})\)</span>.</p>
<h3 id="knn-估计"><span style="color:blue"><strong><em>kNN 估计</em></strong></span></h3>
<p>Parzen-窗概率密度估计是选择固定大小的体积<span class="math inline">\(V\)</span>（以<span class="math inline">\(\mathbf{x}\)</span>为中心），然后统计样本<span class="math inline">\(k\)</span>掉在区域的数量来作为PDF估计。</p>
<p>我们也可以选择固定样本数量<span class="math inline">\(k\)</span>， 然后计算包含<span class="math inline">\(k\)</span>个样本的体积来估计概率密度函数：<span class="math inline">\(\hat{p}(\mathbf{x})=\frac{k}{nV(k)}\)</span>, <span class="math inline">\(V(k)\)</span>是<span class="math inline">\(k\)</span>的函数。该方法叫做KNN估计。 . KNN可以用来从<span class="math inline">\(n\)</span>的标注样本中估计一个后验概率<span class="math inline">\(P(w_i|\mathbf{x})\)</span>. 假设我们在样本<span class="math inline">\(x\)</span>周围放置一个区域<span class="math inline">\(V\)</span>，然后获得<span class="math inline">\(k\)</span>个样本，在<span class="math inline">\(k\)</span>个样本中有<span class="math inline">\(k_i\)</span>个样本属于类型<span class="math inline">\(w_i\)</span>。</p>
<p>所以联合概率<span class="math inline">\(p_n(x,w_i)=\frac{k_i}{nV}\)</span>, 后验概率为<span class="math inline">\(\hat{p}(w_i|x)=\hat{p}(w_i)\hat{p}(x|w_i)/\hat{p}(x)=\frac{n_i}{n}\frac{k_i}{n_iV(k)}\frac{nV(k)}{k}=\frac{k_i}{k}\)</span>. 其中：先验概率<span class="math inline">\(\hat{p}(x)=\frac{k}{nV(k)}\)</span>,类别条件概率<span class="math inline">\(\hat{p}(x|w_i)=\frac{k_i}{n_iV(k)}\)</span>, 先验概率<span class="math inline">\(\hat{p}(w_i)=\frac{n_i}{n}\)</span>.</p>
<p>KNN的决策规则表示如下： 给定标注的训练样本<span class="math inline">\(D^n=\{x_1,..,x_n\}\)</span>, 假设<span class="math inline">\(x^{*}\)</span>是最靠近<span class="math inline">\(x\)</span>的原型。那么最近邻规则将<span class="math inline">\(x\)</span>分类成<span class="math inline">\(x^{*}\)</span>的类别标签。</p>
<p>一般的，我们有一系列标注的样本，<span class="math inline">\(\{(x_1,\theta_1),...,(x_n,\theta_n)\}\)</span>，其中<span class="math inline">\(\theta_i\)</span>是类别<span class="math inline">\(w_1,..,w_c\)</span>中的一个标签。那么NN规则为：<span class="math inline">\(\alpha_{nn}(x)=\theta_k,k=\text{argmin}_i||x-x_i||\)</span>，也就是距离<span class="math inline">\(x\)</span>最近的那个标签。</p>
<p>极大样本训练数据时最近邻估计的错误率为：<span class="math inline">\(P^{*}\le P \le ^{*}(2-\frac{c}{c-1}P^{*})\)</span></p>
<p>更一般的，我们假设有训练样本<span class="math inline">\(\{x_1,..,x_n\}\)</span>，以及一个测试点<span class="math inline">\(x\)</span>. 找到<span class="math inline">\(k\)</span>个最靠近<span class="math inline">\(x\)</span>的训练样本点<span class="math inline">\(x_1^{*},...,x_k^{*}\)</span>，然后得到他们的标签<span class="math inline">\(\theta_1^{*},\theta_2^{*},...,\theta_k^{*}\)</span>，并且将<span class="math inline">\(x\)</span>归类到在<span class="math inline">\(\theta_1^{*},\theta_2^{*},...,\theta_k^{*}\)</span>中有最大表示的那一类汇中。</p>
<h3 id="参数化方法"><span style="color:blue"><strong><em>参数化方法</em></strong></span></h3>
<p>我们可以看出学习出的条件概率密度函数可以从概率密度中推导出，特别是有小的训练样本时。 如果可以用一个有未知参数的数学函数去表示PDF，那么这个问题可以变得很容易，这就是参数化方法。</p>
<p>假设<span class="math inline">\(p(x|w_k)\)</span>是一个均值<span class="math inline">\(\mu_k\)</span>协方差矩阵<span class="math inline">\(\sum_k\)</span>的高斯分布。尽管我们不知道他们的参数值，但是现在问题已经简化了，我们只需要估计2个参数。</p>
<h3 id="最大似然估计"><span style="color:blue"><strong><em>最大似然估计</em></strong></span></h3>
<p>现在我们用一系列从概率密度函数<span class="math inline">\(p(x|\theta)\)</span>中独立获得的训练样本<span class="math inline">\(D=\{x_i\}=[x_1, x_2, ..., x_n]\)</span>来估计位置参数矢量<span class="math inline">\(\theta\)</span>. 下面我们举例看看如何基于训练数据<span class="math inline">\(D\)</span>来估计给定概率分布密度函数<span class="math inline">\(p(x|\theta)\)</span>的参数<span class="math inline">\(\theta\)</span></p>
<p>显然，样本<span class="math inline">\(x_k\)</span>发生的概率是<span class="math inline">\(p(x_k|\theta)\)</span>， 由于训练集中所有样本的发生是独立的，因此可以获知所有样本发生的概率：<span class="math inline">\(p(D|\theta)=\prod_{k=1}^np(x_k|\theta)\)</span>。</p>
<p>直观表示，我们应该选择那些参数<span class="math inline">\(\theta\)</span>，使得概率密度<span class="math inline">\(p(x|\theta)\)</span>尽可能满足实际观测的训练样本的。例如，使得所有训练数据发生的概率<span class="math inline">\(p(D|\theta)\)</span>最大化。注意<span class="math inline">\(p(D|\theta)\)</span>也叫做<span class="math inline">\(\theta\)</span>关于训练样本<span class="math inline">\(D\)</span>的似然。因此，该方法也叫作最大似然估计。</p>
<p>公式表示为：$ =<em>{}p(D|)=</em>{}_{k=1}^np(x_k|) $</p>
<p>要获得上面的解决不是容易的，因为<span class="math inline">\(p(x_k|\theta)\)</span>通常是一个关于<span class="math inline">\(\theta\)</span>的非线性函数。但是我们可以将其转换到对数域，因为在对数域下，该方程变得单调。 所以，我们最大化对数似然，而不是最大化似然。得到： $ =<em>{}p(D|)=</em>{}_{k=1}^n  p(x_k|) $</p>
<p>要求解这个方程的最大值，可以通过微分学的方法，计算偏导数为0处的参数：$\begin{cases} <em>{}p(D|)=0\ </em>{k=1}^n _{}  p(x_k|)=0\end{cases} $ 我们假设总共有<span class="math inline">\(q\)</span>个系统参数，也就是<span class="math inline">\(\theta=(\theta_1,\theta_2,...,\theta_q)^T\)</span>。 那么对该向量的梯度就是对<span class="math inline">\(\theta\)</span>所有元素的偏微分：<span class="math inline">\(\nabla_{\theta}f(\theta)=\begin{bmatrix}\partial f(\theta)/\partial \theta_1 \\\partial f(\theta)/\partial \theta_2 \\...\\\partial f(\theta)/\partial \theta_q \end{bmatrix}\)</span></p>
<p>为了更好的看最大似然方法如何应用，我们假设样本都是基于多元高斯分布（位置参数<span class="math inline">\(\mu\)</span>和协方差矩阵<span class="math inline">\(\sum\)</span>）. <span class="math inline">\(p(x|\theta)=\frac{1}{(2\pi)^{d/2}|\sum|^{1/2}}exp[-\frac{1}{2}(x-\mu)^T\sum^{-1}(x-\mu)]\)</span> 对上面的等式求对数得到：<span class="math inline">\(ln(p(x_k|\theta)=-\frac{1}{2}ln[(2\pi)^d|\sum|]-\frac{1}{2}(x_k-\mu)^T\sum^{-1}(x_k-\mu)\)</span> 首先需要考虑单变量例子：<span class="math inline">\(\theta=(\theta_1,\theta_2)^T=(\mu,\delta^2)^T\)</span>，<span class="math inline">\(d=1,|\sum|^{1/2}=\delta\)</span> 得到:<span class="math inline">\(p(x|\theta)=\frac{1}{\sqrt{2\pi}\delta}exp[-\frac{1}{2}(\frac{x-\mu}{\delta})^2]\)</span> 然后取对数得到：<span class="math inline">\(ln(p(x_k|\theta))=-\frac{1}{2}ln[2\pi\delta^2]-\frac{1}{2\delta^2}(x_k-\mu)^2\)</span> 导数为：<span class="math inline">\(\nabla_{\theta}ln p(x_k|\theta)=\begin{bmatrix}\frac{1}{\delta^2}(x_k-\mu)\\-\frac{1}{2\delta^2}+\frac{(x_k-\mu)^2}{2\delta^4}\end{bmatrix}\)</span></p>
<p>将其应用到整个训练数据中:<span class="math inline">\(\nabla_{\theta}p(D|\theta)=\sum_{k=1}^n \nabla_{\theta}ln(\mathbf{x_k}|\theta)\)</span> 我们有：<span class="math inline">\(\begin{cases}\sum_{k=1}^n\frac{1}{\delta^2}(x_k-\mu)=0\\ -\sum_{k=1}^n\frac{1}{\delta^2}+\sum_{k=1}^n \frac{(x_k-\mu)^2}{2\delta^4}=0\end{cases}\)</span> 求解上面的等式，得到：<span class="math inline">\(\begin{cases}\hat{\mu}=\frac{1}{n}\sum_{k=1}^nx_k\\hat{\delta}^2=\frac{1}{n}\sum_{k=1}^n (x_k-\hat{\mu})^2\end{cases}\)</span></p>
<h2 id="判别式函数和分类器">判别式函数和分类器</h2>
<p>前面我们知道贝叶斯分类器是最大化后验概率，Decide: <span class="math inline">\(W_k=argmax[p(w_i|x)=argmin[p(e_i|x)]]\)</span>. 因为<span class="math inline">\(p(w_i|x)=p(w_i)p(x|w_i)p^{-1}(x)\)</span>和<span class="math inline">\(p(x)\)</span>都不是<span class="math inline">\(w_i\)</span>的函数，所以改进贝叶斯分类器可以定义为：<span class="math inline">\(g_i(x)=lnp(x|w_i)p(w_i)\)</span>。那么目标就变成了在给定类型<span class="math inline">\(x\)</span>下，寻找类<span class="math inline">\(w_i\)</span> 以最大化判别式函数。 在空间内<span class="math inline">\(w_1\)</span>和<span class="math inline">\(w_2\)</span>的决策边界可以由：<span class="math inline">\(g_i(x)=g_j(x)\)</span>决定。</p>
<p>如果类的条件概率密度函数是一个多维高斯分布：<span class="math inline">\(p(x|w_i)=\frac{1}{(2\pi)^{d/2}|\sum i|^{1/2}}exp[-\frac{1}{2}(x-\mu_i)^T\sum_i^{-1}(x-\mu_i)]\)</span>. 那么判别式函数变成：<span class="math inline">\(g_i(x)=-\frac{1}{2}d_{\sum}(x,\mu_i)+b_i\)</span>. 其中，<span class="math inline">\(d_{\sum_i}(x-\mu_i)=(x-\mu_i)^T\sum_i^{-1}(x-\mu_i)\)</span>, <span class="math inline">\(b_i=ln p(w_i)-\frac{1}{2}ln|\sum_i|\)</span></p>
<p>其中<span class="math inline">\(d_{\sum_i}(x,\mu_i)=(x-\mu_i)^T\sum_i^{-1}(x-\mu_i)\)</span>叫做<span class="math inline">\(x\)</span>和<span class="math inline">\(\mu\)</span>的Mahalanobis 距离。 和欧式距离比较：<span class="math inline">\(d_{Eu}(x-\mu_i)=(x-\mu_i)^T(x-\mu_i)\)</span>. 以一维情况为例：<span class="math inline">\(d_{\sum_i}(x,\mu_i)=(x-\mu_i)^2\)</span> 回顾判别式函数：<span class="math inline">\(g_i(x)=-\frac{1}{2\delta^2}(x-\mu_i)^T+lnp(w_i)\)</span>. 如果所有类的先验概率都是相等的，那么<span class="math inline">\(g_i(x)=-(x-\mu_i)^T(x-\mu_i)=-d_{Eu}(x,\mu_i)=||x-\mu_i||\)</span></p>
<h3 id="稀疏表示的分类器">稀疏表示的分类器</h3>
<p>一个图形<span class="math inline">\(y\)</span> 可以有其他训练数据线性组合得到<span class="math inline">\(A_i = [a_{i,1}, ..., a_{i,j}, ...,a_{i,ni}]\)</span>；y=A_i x_i + e_i<span class="math inline">\(.\)</span>A=[A_1,...,A_i,...,A_c]$ 所有c个目标的训练样本：<span class="math inline">\(x=[x_1;...;x_i;...,x_c]\)</span></p>
<p>如果<span class="math inline">\(x=[0;...,0;x_i;0,...,;0]\)</span>, 那么我们可以将图形<span class="math inline">\(y\)</span>划分为类<span class="math inline">\(i\)</span>. 给定<span class="math inline">\(y\)</span>和<span class="math inline">\(A\)</span>,我们的目标是找到系数系数<span class="math inline">\(x\)</span>,使得<span class="math inline">\(y\)</span>可以由训练样本的线性组合表示。</p>
<p>下面，我们讨论下无监督学习和聚类，之前的数据都是有标记的，我们可以将测试数据分类。但是如果没有标记呢，这时候只能做聚类了。</p>
<p>这里讲讲k-means聚类实现，k-means算法的原则是： - 每个聚类都可以由聚类中心表示：<span class="math inline">\(\mu_j=\frac{1}{D_j}\sum_{x_i \in D_j}x_i\)</span> - 先估计k个聚类中心的均值<span class="math inline">\(m=\{\mu_1, \mu_2, ..., \mu_k\}\)</span>，然后最小化均方误差：$e(m)=_{j=1}^k $</p>
<p>k-means 算法实现: 1. 初始化 k 类别, e.g. 随机挑选样本. <span class="math inline">\(m=\{\mu_1, \mu_2, ..., \mu_k\}=\{x_{r1},x_{r2},...,x_{rk}\}\)</span> 2. 将n个样本根据和均值<span class="math inline">\(\mu_j\)</span>的接近程度分类到k类； 3. 重新计算所有类的均值，得到<span class="math inline">\(\mu_j = \frac{1}{D_j}\sum_{x_i\in D_j}x_i\)</span>,j=1,2,...,k 4. 回到第二步，知道均值没有改变<span class="math inline">\(M=\{\mu_1,\mu_2,...,\mu_k\}\)</span>.</p>
<p>不管是GMM，还是k-means，都面临一个问题，就是k的个数如何选取？比如在bag-of-words模型中，用k-means训练码书，那么应该选取多少个码字呢？为了不在这个参数的选取上花费太多时间，可以考虑层次聚类。</p>
<p>假设有N个待聚类的样本，对于层次聚类来说，基本步骤就是： 1. （初始化）把每个样本归为一类，计算每两个类之间的距离，也就是样本与样本之间的相似度； 2. 寻找各个类之间最近的两个类，把他们归为一类（这样类的总数就少了一个）； 3. 重新计算新生成的这个类与各个旧类之间的相似度； 4. 重复2和3直到所有样本点都归为一类，结束。</p>
<p>另外一种方法是基于模型的聚类，对每个聚类假设一个模型，然后找最合适的模型。 1. 首先假设概率密度已知，我们需要知道的是模型的参数 2. 进一步假设，样本都是来自一个已知的类c 3. 每个类别的先验概率<span class="math inline">\(p(w_j)，j=1,...,c\)</span>已知 4. c个参数<span class="math inline">\(\theta_1,\theta_2,...,\theta_c\)</span>未知 5. 类别标签未知：<span class="math inline">\(p(x|\theta)=\sum_{j=1}^c p(x|w_j,\theta_j).P(w_j)\)</span>, 其中<span class="math inline">\(\theta=(\theta_1,\theta\,...,\theta_c)^T\)</span> 6. 上面的方程叫混合概率密度，我们的目标是用根据这个混合概率密度得到的样本来估计未知参数<span class="math inline">\(\theta\)</span> 7. 一旦<span class="math inline">\(\theta\)</span>知道了，我们就可以将混合概率密度分解到每个物体，然后用MAP分类器 8. 比较流行的是指数混合模型</p>
<h3 id="特征提取和维数降低">特征提取和维数降低</h3>
<p>将图像准换成特征：<span class="math inline">\(g(u,v)=T\{f(x,y)\}=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}w(u,v,x,y)f(x,y)dxdy\)</span> linear transform. <span class="math inline">\(\Rightarrow \sum_{1}^{h}\sum_{1}^w w(u,v,x,y)f(x,y)\)</span> for digital image; <span class="math inline">\(Rightarrow \sum_{1}^{h}\sum_{1}^w e^{-2\pi j(ux+vy)}f(x,y)\)</span> Fourier transform; <span class="math inline">\(\Rightarrow \sum_{1}^{h}\sum_{1}^w x^u y^v f(x,y)\)</span> moments computing; <span class="math inline">\(\Rightarrow f=W^Tx\)</span> vector-matrix representation.</p>
<p>特征维数太多会导致参数增加，所以要做维数降低。给定q维训练数据<span class="math inline">\(x_1,...,x_q\)</span>，每个样本是n维的。怎用一个数据<span class="math inline">\(x_0\)</span>来表示所有数据？ 最小化：<span class="math inline">\(\eta^2=\sum_{i=1}^q||x_0-x_i||^2=\sum_{i=1}^q (x_0-x_i)^T(x_0-x_i)\)</span>，并且对训练样本均值化处理：<span class="math inline">\(\hat{x}_i=x_i-\mu\)</span>, <span class="math inline">\(X=[\hat{x}_1,...,\hat{x}_q]\)</span> 现在我们只想用1维 <span class="math inline">\(\phi, ||\phi||^2=\phi^T\phi=1\)</span> 来表示所有样本<span class="math inline">\(\hat{x}_i\)</span>. n维数据<span class="math inline">\(\hat{x}_i\)</span>被压缩到一维数据<span class="math inline">\(a_i=\phi \hat{x}_i\)</span>. 最好的维度<span class="math inline">\(\phi\)</span>使得重构误差最小：<span class="math inline">\(\varepsilon^2 = \sum_{i=1}^q||\hat{x}_i-a_i \phi||^2 \Rightarrow minimum\)</span></p>
<p>很容易得到：<span class="math inline">\(\varepsilon^2 = \sum_{i=1}^q||\hat{x}_i-a_i \phi||^2 = \sum_{i=1}^q(\hat{x}_i-a_i \phi)^T(\hat{x}_i-a_i \phi)= \sum_{i=1}^q( ||\hat{x}_i||^2-\sum_{i=1}^q a_i^2 = \sum_{i=1}^q( ||\hat{x}_i||^2-\phi^T \sum_{i=1}^q \hat{x}_i\hat{x}_i^T\phi\)</span> 所有训练样本协方差矩阵：<span class="math inline">\(S^t=\frac{1}{q}(x_i-\mu)(x_i-\mu)^T=\frac{1}{q}\sum_{i=1}^q \bat{x_i}\bar{x_i}^T\)</span> 最小化：<span class="math inline">\(\varepsilon^2=\frac{1}{q}\sum_{i=1}^q||\hat{x}_i||^2-q\phi^T S^t \phi\)</span> 也就是最小化 <span class="math inline">\(\phi^T S^t \phi\)</span>，其中<span class="math inline">\(||\phi||^2=\phi^T\phi=1\)</span>。</p>
<p>使用拉格朗日最优化方法：<span class="math inline">\(f(\phi, \lambda)=\phi^TS^t\phi-\lambda(\phi^T\phi-1)\Rightarrow\)</span>maximum. <span class="math inline">\(\frac{\partial f}{\partial \phi}=2 S^t\phi - 2\lambda \phi=0\)</span>, 然后我们有 <span class="math inline">\(S^t\phi=\lambda \phi\)</span>. 最后的方案是特征值和特征向量<span class="math inline">\(S^t\)</span>.</p>
<h3 id="主成分分析">主成分分析</h3>
<p>现在我们如果要降到<span class="math inline">\(n\)</span>维，那么则有<span class="math inline">\(\varepsilon^2=\sum_{i=1}^q ||\bat{x}_i-a_i\phi||^2\)</span> 很容易有：<span class="math inline">\(\varepsilon^2=\sum_{i=1}^q ||\bat{x}_i-a_i\phi||^2\)</span>=<em>{j=1}<sup>q||||</sup>2-^T(</em>{i=1}^q {x}_i_i^T)$</p>
<p>我们可以看出，<span class="math inline">\(\phi\)</span>是协方差矩阵<span class="math inline">\(S&#39;\)</span>的的特征向量，<span class="math inline">\(\phi^T S^t \phi\)</span>具有最大值，所以重构误差最小。 <span class="math inline">\(\phi^TS^t\phi=\frac{1}{q}\sum_{i=1}^q\phi^T(x_i-\mu)[\phi^T(x_i-\mu)]^T=\frac{1}{q}\sum_{i=1}^q a_i^2\)</span> 因为：<span class="math inline">\(S^t\phi=\lambda \phi \Rightarrow \phi^TS^t\phi=\phi^T \lambda \phi = \lambda \phiT^ \phi=\lambda\)</span></p>
<p>将n维数据<span class="math inline">\(x_i\)</span>降到m维数据可以<span class="math inline">\(y_i\)</span>使用:<span class="math inline">\(y_i=\Phi^T(x_i-\mu)\)</span>,其中<span class="math inline">\(\Phi=[\phi_1,...,\phi_m]\)</span>, 重构误差<span class="math inline">\(\varepsilon^2=\sum_{i=1}^q||x_i-\hat{x_i}||^2\)</span></p>
<p>注意：<span class="math inline">\(S^t=\frac{1}{q}\sum_{i=1}^q(x_i-\mu)(x_i-\mu)^T=\frac{1}{q}\sum_{i=1}^q\bar{x}_i\bat{x_i}^T=\frac{1}{1}XX^T\)</span></p>
</body>
</html>
