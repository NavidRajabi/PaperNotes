<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h2 id="非参数技术">非参数技术</h2>
<p><span style="color:red"><strong><em>为什么要用非参数估计的方法</em></strong></span></p>
<p>对于某些数据，我们可以采用已知的模型对数据样本进行匹配，例如利用高斯模型对图像中的像素进行估计，由于我们预估图像的像素的分布是又由一个或者多个不同的高斯函数叠加而成，因此我们只需要对高斯函数的参数确定，便可以得到相应的模型，此时我们的目的是利用数据估计参数。 当我们不确定数据的分布情况时，我们无法通过将观测数据转换成参数，为了得到一些未知点的贝叶斯概率，我们仍然需要判断总体分布的情况，因此提出了某些直接用样本来估计总体分布的方法，称其为估计分布的非参数法。</p>
<p>最直观的用样本去估计概率密度的方法是直方图估计。就是把样本的分布按照某个区间划分为直方图表示，然后用这个表示概率密度分布。这个在数据比较稀疏、连续时往往不是很好表示，特别是针对高维数据，有点不是很好弄。（对了，决策树是不是就是可以这么来看呢？）</p>
<p>简单来说(不是严格定义)对某些样本例如{<span class="math inline">\(x,y,z\)</span>}进行估计时，对于数据<span class="math inline">\(x\)</span>，我们以<span class="math inline">\(x\)</span>为中心，将宽度为<span class="math inline">\(h\)</span>内的所有数据都包含到，例如里中心点<span class="math inline">\(x\)</span>越大的数据贡献越大，越远贡献越小，将这些贡献相加后，我们将此值视为此点的密度<span class="math inline">\(p(x)\)</span>;，在对所有点进行计算后，我们可以得到总体的概率密度分布。</p>
<p><span style="color:red"><strong><em>注：对于来自单个类的样本，最终估计的值为类条件概率密度；若样本来自于多个类，其估计的值为混合密度。</em></strong></span></p>
<h3 id="parzen窗方法"><span style="color:red"><strong><em>Parzen窗方法</em></strong></span></h3>
<p><em>Parzen窗是概率密度函数估计的非参数方法。高斯函数实质上是一个基，每个以样本为均值的高斯函数构成了无数个基底。parzen窗本质是函数逼近的思想，用一组正交基无限逼近一个分布。另外parzen窗可选择另外的窗函数 而不仅是高斯函数。窗的宽度影响较大。</em></p>
<p>parzen窗是用来做概率密度估计的。在一个小区域<span class="math inline">\(R\)</span>内，设体积为<span class="math inline">\(V\)</span>，在总样本数为<span class="math inline">\(n\)</span>的情况下有<span class="math inline">\(k\)</span>个样本落在该小区域R内，我们可以认为落在该区域内的样本的概率为<span class="math inline">\(P = \frac{k}{n}\)</span>(<span style="color:red"><strong><em>经验分布</em></strong></span>)，由于其体积为<span class="math inline">\(V\)</span>, 则概率密度为<span class="math inline">\(p(x) = \frac{P}{V} = \frac{k}{V \cdot n}\)</span>.</p>
<p>接下来就需要明确，<span style="color:red"><strong><em>小区域如何确定、体积如何表示、如何确定在区域内的点个数</em></strong></span>。</p>
用均匀窗（或者说是窗函数）会比较好解释这些问题。我们把区域设置为<span class="math inline">\(d\)</span>维的超立方体，将待求的的点（<span class="math inline">\(d\)</span>维空间下的向量）作为中心，边长为<span class="math inline">\(h_n\)</span>. 设置<span class="math inline">\(\delta\)</span>函数，定义为$ (x) =
\begin{cases}
1 , &amp;\left | x_j \right | \le \frac{1}{2} , j=1 , 2, \cdots , d \\
0 , &amp; other
\end{cases}
<p>$</p>
<p>即每一维到原点的距离小于边长（即1）的一半。可以在图上</p>
<p>该函数表示为是以原点为中心，边长为1的超方体窗下，<span class="math inline">\(d\)</span>维向量表示的点是否在该窗体内部。我们可以把其他非原点为中心、边长不为1的情况归一化到该原点窗的条件下。例如，对以<span class="math inline">\(\vec x\)</span>为原点，<span class="math inline">\(h_n\)</span>为边长的超立方体，点<span class="math inline">\(\vec x^i\)</span>通过如下转换</p>
<p><span class="math display">\[
x&#39; = \frac{x - x^i}{h_n}
\]</span></p>
<p>可以变为在原点、长度为1的情况，就可以使用<span class="math inline">\(\sigma\)</span>函数来判断该点是否在以<span class="math inline">\(\vec x\)</span>为原点、长度为<span class="math inline">\(h_n\)</span>窗体内部了。</p>
<p>有了上面的准备，对于n个d维向量表示的样本集<span class="math inline">\(\{x^1 , x^2 , \cdots , x^n\}\)</span> ， 把它们作为一个分布的部分已知信息，我们就可以据此来估计该分布下任意一个位置的概率密度函数。</p>
<p>由最开始推算的公式，我们选择边长为<span class="math inline">\(h_n\)</span>的超方体，其体积为<span class="math inline">\(V = h_n^d\)</span> ， 该超方体内部拥有的点数量<span class="math inline">\(k\)</span>可以通过<span class="math inline">\(\sigma\)</span>算出，即<span class="math inline">\(k = \sum_{i=1}^n{\sigma \left( \frac{x^i - x}{h_n}\right)}\)</span> ， 由此落在在超方体内点的概率为<span class="math inline">\(P = \frac{k}{n}\)</span>，由<span class="math inline">\(p(x ) = \frac{P}{V}\)</span>，带入上面的各表示式，有：<span class="math display">\[p(x) = \frac{1}{ n h_n^d}\sum_{i=1}^{n}{\sigma \left ( \frac{x^i - x}{h_n}\right )}\]</span></p>
<p>其中<span class="math inline">\(h_n\)</span>的选择往往很关键。通常用<span class="math inline">\(h_n = h_0 * \frac{1}{\sqrt{n}}\)</span>来确定。但是<span class="math inline">\(h_0\)</span>又等于多少呢？ 上述表示的概率密度函数，当然应该还要满足概率密度的意义。至少包含两条： 1) 任何位置大于等于0. 这个考察p(x)，很直观看出是满足的，不可能存在负数 2) 从负无穷积分到正无穷，和为1. $ <em>{-}^{} </em>{i=1}^{n}{( )} dx =  <em>{i=1}^{n} </em>{-}^{}{ ()} dx = <em>{-}^{}(u) du = </em>{-}^{} 1 du= 1$ 其中由<span class="math inline">\(\int_{-\infty}^{\infty}{\frac{1}{h_n^d} \sigma(\frac{x^i -x}{h_n})} dx\)</span>变为<span class="math inline">\(\int_{-\infty}^{\infty}\sigma(u) du\)</span>应该是用了变量替换，但是怎么就把<span class="math inline">\(h_n^d\)</span>消去了不是很确定。</p>
<p>除了上面的窗函数<span class="math inline">\(\sigma\)</span>，还可以选择其他的核函数——就是使用核函数替换掉窗函数<span class="math inline">\(\sigma\)</span>。</p>
<p>比如通常会使用高斯函数（标准正态分布函数）<span class="math inline">\(k(x) = \frac{1}{\sqrt{2 \pi}} \exp(- \frac{x^2}{2})\)</span>来作为核函数。</p>
<p><span style="color:red"><strong><em>注意注意！parzen窗方法也叫作Kernel Density Estimation</em></strong></span> ， 不同领域叫法不同。</p>
<h3 id="k_n近邻估计"><span style="color:red"><strong><em><span class="math inline">\(k_n\)</span>近邻估计</em></strong></span></h3>
<p><span style="color:red"><strong><em>工作原理</em></strong></span> 存在一个训练样本集，样本集中每个数据都存在标签（即分类）。 输入的测试数据（没有标签）的每个特征与样本集中的每个特征进行比较， 然后算法提取样本集中特征最相似数据（最邻近）的分类标签。 通常，我们只选择数据集样本集中前k个最相似的数据。 最后，选择k个最相似数据中出现次数最多的分类，作为新数据的分类。</p>
<ol style="list-style-type: decimal">
<li><p><span style="color:red"><strong><em><span class="math inline">\(k_n\)</span>近邻估计和Parzen窗估计</em></strong></span></p></li>
<li><p><span style="color:red"><strong><em>后验概率的估计</em></strong></span></p></li>
</ol>
<h3 id="最近邻规则"><span style="color:red"><strong><em>最近邻规则</em></strong></span></h3>
<ol style="list-style-type: decimal">
<li><span style="color:red"><strong><em>最近邻规则的收敛性</em></strong></span></li>
<li><span style="color:red"><strong><em>最近邻规则的误差率</em></strong></span></li>
<li><span style="color:red"><strong><em>误差界</em></strong></span></li>
<li><span style="color:red"><strong><em>k－近邻规则</em></strong></span></li>
</ol>
<p><span style="color:red"><strong><em>k-近邻算法的一般流程</em></strong></span>: 1) 计算已知类别数据集中的点与当前点之间的距离; 2) 按照距离递增次序排序; 3) 选取与当前点距离最小的k个点; 4) 确定k个点所在类别的出现概率; 5) 返回前k个点出现概率最高的类别作为当前点的预测分类。</p>
<ol start="5" style="list-style-type: decimal">
<li><span style="color:red"><strong><em>k-近邻规则的计算复杂度</em></strong></span></li>
</ol>
<p><span style="color:red"><strong><em>优点：</em></strong></span> 分类数据的最简单最有效的方法; 精度高、对异常值不敏感、无数据输入假定. <span style="color:red"><strong><em>缺点：</em></strong></span> 计算复杂的高、空间复杂的高; 无法给出任何数据的基础结构信息，因此无法知晓平均实例样本和典型实例样本具有什么特征. <span style="color:red"><strong><em>适用数据范围：</em></strong></span> 数值型和标称型 .</p>
<h3 id="距离度量和最近邻分类">距离度量和最近邻分类</h3>
<p>1.　度量的性质 2.　切空间距离</p>
<h3 id="最小错误率准则贝叶斯分类器"><span style="color:red"><strong><em>最小错误率准则贝叶斯分类器</em></strong></span></h3>
<p><span style="color:red"><strong><em>贝叶斯分类器根据每个类别后验概率<span class="math inline">\(p(y_i|x)\)</span>的大小来判别待识别样本<span class="math inline">\(x\)</span>的类别属性，按照这种方式进行分类的同时，也可以使错误率最小。</em></strong></span></p>
<p>分类器只能讲<span class="math inline">\(x\)</span>被判为<span class="math inline">\(y_1, y_2,...y_c\)</span>中的某一类，假设将其判别为<span class="math inline">\(y_i\)</span>类，这样的判断可能发生错误的概率是多少？ <span class="math display">\[
p_i(e) = \sum^c_{j =1, j \neq i} p(y_j|x) = \sum^c_{j = 1} p(y_j|x) - p(y_i|x) = 1 - p(y_i|x)
\]</span></p>
<p><span style="color:red"><strong><em>分类器的错误率</em></strong></span>是除了<span class="math inline">\(y_i\)</span>类之外，其他所有类别在<span class="math inline">\(x\)</span>发生条件下的后验概率之和，如果希望分类器识别的错误率最小，就应该将<span class="math inline">\(x\)</span>判别为后验概率最大的一个类别，即 <span class="math display">\[
i = argmax p(y_j|x)
\]</span> 则判别$x y_i $.</p>
<p>前面也提到过，使用贝叶斯公式，通过先验概率和类条件概率间接的计算后验概率，另外还需要注意的是上式中只需要比较各个类别后验概率的大小，并不需要知道确切的概率值，所以<span class="math inline">\(p(x)\)</span>可以去掉，因而得到如下的最小错误率贝叶斯分类准则 <span class="math display">\[
i = argmax g_i(x) = p(x|y_i)p(y_i)
\]</span> 则判别<span class="math inline">\(x \in y_i\)</span></p>
<p><span style="color:red"><strong><em>对于二分类问题，两个类别的先验概率与类条件概率的乘积如图所示：</em></strong></span></p>
<div class="figure">
<img src="http://i.imgur.com/0FNX3NJ.png" alt="贝叶斯" />
<p class="caption">贝叶斯</p>
</div>
<p><span style="color:red"><strong><em>根据最小错误率贝叶斯分类准则：</em></strong></span> $x &lt; t $ 则有 <span class="math inline">\(p(y_1|x) &gt; p(y_2|x)\)</span>，那么<span class="math inline">\(x \in y_1\)</span> $x &gt; t $ 则有 <span class="math inline">\(p(y_1|x) &lt; p(y_2|x)\)</span>，那么<span class="math inline">\(x \in y_2\)</span> <span style="color:red"><strong><em>那么发生分类错误的概率：</em></strong></span> $x &lt; t $ 判为<span class="math inline">\(x \in y_2\)</span> 则有 <span class="math inline">\(p(y_2|x)\)</span> $x &gt; t $ 判为<span class="math inline">\(x \in y_2\)</span> 则有 <span class="math inline">\(p(y_1|x)\)</span> <span style="color:red"><strong><em>所以错误的概率为：</em></strong></span> <span class="math display">\[
p(e) = p(e_1) + p(e_2) = \int_{- \infty }^t + \int^{\infty}_t
\]</span> <span style="color:red"><strong><em>这两部分恰好为图中两个阴影的面积。</em></strong></span></p>
<h3 id="最小平均风险准则贝叶斯分类器"><span style="color:red"><strong><em>最小平均风险准则贝叶斯分类器</em></strong></span></h3>
<p>对于某些分类问题，不仅需要考虑识别结果是否正确，同时还需要考虑做出没一次判断所带来的后果，需要承担风险。 因此，需要根据实际情况引入一组『风险值』来评估将某类样本误识为另个类别所要付出的代价。 令<span class="math inline">\(r_{ij}\)</span>为将<span class="math inline">\(y_i\)</span>类的样本判别为<span class="math inline">\(y_j\)</span>类的代价，如果分类器将待识别的样本<span class="math inline">\(x\)</span>识别为<span class="math inline">\(y_j\)</span>类，而<span class="math inline">\(x\)</span>的真是类别可能是<span class="math inline">\(y_1, y_2,...y_c\)</span>中的任何一个，做出这个判别的评价风险<span class="math inline">\(\epsilon_j(x)\)</span>是每个类别的样本被分类为<span class="math inline">\(y_j\)</span>所带来的风险经由后验概率加权求和之后的结果： $ <em>j(x) = </em>{i=1}^c p(y_i|x) $ 注意上式中有一项为 <span class="math inline">\(r_{jj}p(y_j|x)\)</span>，其中<span class="math inline">\(r_{jj}\)</span>是将<span class="math inline">\(y_j\)</span>类判别为<span class="math inline">\(y_j\)</span>类的代价，对于这种判别正确的情况，可以设置风险值为0：<span class="math inline">\(r_{jj} = 0\)</span> 根据平均风险建立贝叶斯分类器， $ k=argmin <em>j(x) = </em>{i = 1}^c r_{ij} p(y_i|x) $ 转化为: $ k = argmax g_i(x) = -<em>{i = 1}^c r</em>{ij} p(x|y_i)p(y_i) $ 则判别<span class="math inline">\(x \in y_k\)</span> 。 ### <span style="color:red"><strong><em>高斯分布贝叶斯分类器</em></strong></span> 贝叶斯分类器器中，作为类条件概率密度的概率模型有很多形式，高斯分布就是其中一种形式</p>
<h4 id="高斯分布的判别函数"><span style="color:red"><strong><em>高斯分布的判别函数</em></strong></span></h4>
<p>将多元高斯密度函数带入式4，并取对数： <span class="math display">\[
g_i(x) = -{1\over2}(x-u_i)^T \Sigma_i^{-1} (x-u_i) -{d\over2} ln(2\pi) -{1\over 2} ln|\Sigma_i| +lnp(y_i)
\]</span> 下面分情况讨论： 1. $= ^2 I $ $p(y_i) = {1c} $ 带入上式，忽略无关项 <span class="math display">\[
g_i(x) = -{{||x-u_I||^2} \over {2\sigma^2}}
\]</span> <span class="math inline">\(1-NN\)</span> 2. <span class="math inline">\(\Sigma_i =\Sigma\)</span> 同样带入上式并忽略无关项 <span class="math display">\[
g_i(x) = u_i^T \Sigma^{-1} x - {1\over 2} u_i^T u_i + lnp(y_i)
\]</span> 令<span class="math inline">\(w_i = \Sigma^{-1} u_i\)</span>和 <span class="math inline">\(w_0 = - {1\over 2} u_i^T u_i + lnp(y_i)\)</span> <span class="math display">\[
g_i(x) = w_i^T x + w_{i0}
\]</span> 线性分类器。</p>
<h3 id="朴素贝叶斯"><span style="color:red"><strong><em>朴素贝叶斯</em></strong></span></h3>
<p><span style="color:red"><strong><em>当特征的维数较高时，每个类别的训练样本数量较少时，对模型参数估计较为困难。</em></strong></span> 例如<span class="math inline">\(d\)</span>维特征的高斯分布中，均值矢量和协方差矩阵一共有<span class="math inline">\((d^2 + 3d)\over 2\)</span>个参数需要估计。 解决多特征，<span style="color:red"><strong><em>少样本贝叶斯分类问题</em></strong></span>。</p>
<p><span style="color:red"><strong><em>基本假设：所有特征在类别已知的条件下是相互独立的</em></strong></span>，即 <span class="math display">\[
p(x | y_i) = p(x_1, x_2, ..., x_n|y_i) = \prod_j^n p(x_j|y_i)
\]</span> 也就是说，在构建分类器的时候需要逐个估计出每个类别的训练样本在每一维特征上的分布，就可以得到每个类别的条件概率。对于类条件概率是高斯分布的朴素贝叶斯，其判别函数： <span class="math display">\[
g_i(x)=lnp(y_i) - \sum_{i=1}^d ln\sigma_{ij} - \sum_{j = 1}^d {{(x_j - u_{ij})2} \over {2\sigma^2_{ij}}}
\]</span></p>
<p><span style="color:red"><strong><em>简介</em></strong></span></p>
<p>贝叶斯定理是18世纪英国数学家托马斯·贝叶斯（Thomas Bayes）提出得重要概率论理论。以下摘一段 wikipedia 上的简介：</p>
<p><em>所谓的贝叶斯定理源于他生前为解决一个“逆概”问题写的一篇文章，而这篇文章是在他死后才由他的一位朋友发表出来的。在贝叶斯写这篇文章之前，人们已经能够计算“正向概率”，如“假设袋子里面有 N 个白球，M 个黑球，你伸手进去摸一把，摸出黑球的概率是多大”。而一个自然而然的问题是反过来：“如果我们事先并不知道袋子里面黑白球的比例，而是闭着眼睛摸出一个（或好几个）球，观察这些取出来的球的颜色之后，那么我们可以就此对袋子里面的黑白球的比例作出什么样的推测”。这个问题，就是所谓的逆向概率问题。</em></p>
<p>对于分类而言，使用概率有时要比使用硬规则更为有效。贝叶斯概率及贝叶斯准则提供了一种利用已知值来估计未知概率的有效方法。</p>
<p>可以通过特征之间的条件独立性假设，降低对数据量的需求。<span style="color:red"><strong><em>独立性假设是指一个词的出现概率并不依赖于文档中的其他词</em></strong></span>。当然我们也知道这个假设过于简单。<span style="color:red"><strong><em>这就是之所以称为朴素贝叶斯的原因</em></strong></span>。尽管条件独立性假设不正确，但是朴素贝叶斯仍然是一种有效的分类器。</p>
<p>与kNN算法一样，朴素贝叶斯算法也是数据挖掘十大算法之一。我们介绍kNN算法时，并没有讨论kNN算法的优缺点，这边首先看看这个问题。</p>
<p>从构造kNN算法的过程可以看到，这个<span style="color:red"><strong><em>分类算法的精度很高</em></strong></span>。因为这个算法计算了<span style="color:red"><strong><em>所有点与待分类点之间的相似度</em></strong></span>，然后去确定带分类点的类别。由此可见，这个算法对异常值<span style="color:red"><strong><em>并不敏感</em></strong></span>。但是正是因为它需要计算所有点之间的距离，所以其<span style="color:red"><strong><em>复杂度是很高的</em></strong></span>，换句话就是，如果数据量大的话，这个算法会很费时，并不高效。</p>
<ul>
<li><span style="color:red"><strong><em>优点</em></strong></span>：在数据较少的情况下任然有效，可以处理多类别问题。</li>
<li><span style="color:red"><strong><em>缺点</em></strong></span>：对于输入数据的准备方式较为敏感。</li>
<li><span style="color:red"><strong><em>适用数据类型</em></strong></span>：标称型数据</li>
</ul>
<p><span style="color:red"><strong><em>示例一：应当根据新情况更新先验概率</em></strong></span> <em>假设有两个各装了100个球的箱子，甲箱子中有70个红球，30个绿球，乙箱子中有30个红球，70个绿球。假设随机选择其中一个箱子，从中拿出一个球记下球色再放回原箱子，如此重复12次，记录得到8次红球，4次绿球。问题来了，你认为被选择的箱子是甲箱子的概率有多大？</em></p>
<p>调查结果显示，大部分人都低估了选择的是甲箱子的概率。根据贝叶斯定理，正确答案是96.7%。下面容我来详细分析解答。</p>
<p>刚开始选择甲乙两箱子的先验概率都是50%，因为是随机二选一（这是贝叶斯定理二选一的特殊形式）。即有：P(甲) = 0.5， P(乙) = 1 - P(甲)； 这时在拿出一个球是红球的情况下，我们就应该根据这个信息来更新选择的是甲箱子的先验概率: P(甲|红球1) = P(红球|甲) × P(甲) / (P(红球|甲) × P(甲) + (P(红球|乙) × P(乙))) P(红球|甲)：甲箱子中拿到红球的概率 P(红球|乙)：乙箱子中拿到红球的概率 因此在出现一个红球的情况下，选择的是甲箱子的先验概率就可被修正为： P(甲|红球1) = 0.7 × 0.5 / (0.7 × 0.5 + 0.3 × 0.5) = 0.7 即在出现一个红球之后，甲乙箱子被选中的先验概率就被修正为： P(甲) = 0.7， P(乙) = 1 - P(甲) = 0.3； 如此重复，直到经历8次红球修正（概率增加），4此绿球修正（概率减少）之后，选择的是甲箱子的概率为：96.7%。</p>
<ol style="list-style-type: decimal">
<li><p><span style="color:red"><strong><em>贝叶斯决策</em></strong></span></p>
数据的特征矢量为<span class="math inline">\(x\)</span>, 分类器的目的是把它分类为<span class="math inline">\(y_1, y_2,...,y_c\)</span>中的一个类别。
<ol style="list-style-type: decimal">
<li><span style="color:red"><strong><em>类先验概率<span class="math inline">\(p(y_i)\)</span></em></strong></span> 类先验概率是类别<span class="math inline">\(y_i\)</span>发生的概率$ _{i = 1}^c p(y_i) = 1 $</li>
<li><span style="color:red"><strong><em>后验概率 <span class="math inline">\(p(y_i|x)\)</span></em></strong></span> 已知特征向量<span class="math inline">\(x\)</span>的条件下，各类别发生概率的大小，即<span class="math inline">\(x\)</span>属于某一个类别的可能性大小</li>
<li><span style="color:red"><strong><em>类条件概率<span class="math inline">\(p(x|y_i)\)</span></em></strong></span> 分类器需要按照后验概率的大小进行分类，然而实际情况是后验概率往往无法直接得到，能够得到的是<span style="color:red"><strong><em>每一个类别的先验概率<span class="math inline">\(p(y_i)\)</span>和每个类别的类条件概率<span class="math inline">\(p(y_i|x)\)</span>.</em></strong></span> <span style="color:red"><strong><em>类条件概率</em></strong></span>描述的是每一个类别样本的分布情况。<span style="color:red"><strong><em>类条件概率</em></strong></span>可以用属于每一个类别的训练样本来估计。 <span style="color:red"><strong><em>类先验概率</em></strong></span>可以根据训练样本集合各类样本所占比例进行估计。 以上三种概率组合在一起就是贝叶斯公式： $ p(y_i|x) = {{p(x|y_i) p(y_i)} p(x)} $</li>
</ol></li>
</ol>
<p>贝叶斯分类器是根据输入<span class="math inline">\(x\)</span>的后验概率大小分类的，将其判别为后验概率最大的类别，在实际情况中，根据贝叶斯公式将后验概率转化为先验概率和类条件概率的乘积。</p>
<p><span style="color:red"><strong><em>贝叶斯定理给出了条件概率之间的关系，是一个非常重要的定理</em></strong></span>。这里直接给出贝叶斯定理的结论：<span class="math inline">\(P(B \mid A) = \frac{P(A \mid B)P(B) }{P(A)}\)</span></p>
<p>理通常用于解释某一特定现象的证据<span class="math inline">\(A\)</span>如何影响假设<span class="math inline">\(B\)</span>的概率。其中，<span class="math inline">\(P(B)\)</span>称为先验概率,之所以称为“先验”是因为它不考虑任何 <span class="math inline">\(A\)</span> 方面的因素。<span class="math inline">\(P(B \mid A)\)</span>称为后验概率, 也由于得自 A 的取值而被称作 B 的后验概率。<span class="math inline">\(P(A \mid B)\)</span>是证据的似然值, 也由于得自 B 的取值而被称作 A 的后验概率。<span class="math inline">\(P(A)\)</span>是<span class="math inline">\(A\)</span>的先验概率，也作标淮化常量（normalizing constant）。</p>
<p>另外，比例P(A|B)/P(A)也有时被称作标淮相似度（standardised likelihood），Bayes定理可表述为：<span style="color:red"><strong><em>后验概率 = 标淮相似度 </em> 先验概率</strong>*</span></p>
<p>在贝叶斯决策理论里面，要判断点<span class="math inline">\(x\)</span>是否属于<span class="math inline">\(C_i\)</span>类，只要验证是否存在：<span class="math inline">\(p(C_i \mid x) = \max_{j} \{ P(C_j \mid x) \}\)</span> 即是，<span style="color:red"><strong><em>使得<span class="math inline">\(p(C_i \mid x)\)</span>达到最大的那个<span class="math inline">\(C_i\)</span>就是<span class="math inline">\(x\)</span>所属的类别</em></strong></span>。</p>
<ol start="2" style="list-style-type: decimal">
<li><span style="color:red"><strong><em>具体流程</em></strong></span></li>
</ol>
<p>要计算<span class="math inline">\(P(C_i \mid x)\)</span>, 那么就需要计算<span class="math inline">\(\frac{P(x \mid C_i)P(C_i )}{P(x)}\)</span>, 我们知道对于每一个<span class="math inline">\(P(C_i \mid x)\)</span>, 其计算公式中的分母都是<span class="math inline">\(P(x)\)</span>, 所以有：<span class="math inline">\(P(C_i \mid x) \propto P(x \mid C_i)P(C_i )\)</span>.</p>
<p>所以我们实际计算时，<span style="color:red"><strong><em>只需要考虑上式右侧的大小即可</em></strong></span>。首先我们来确定一些符号的意思： - <span class="math inline">\(x = [x_1, x_2, ..., x_p]\)</span>为一个带分类的项，<span class="math inline">\(x_i\)</span>为其特征； - data表示一个已知分类的数据集(<span class="math inline">\(n \times p\)</span>的矩阵)，其每一行代表一个观测，每一列代表一个特征； - label表示data中每一个数据对应的类别标签(<span class="math inline">\(n \times 1\)</span>的矩阵)，比如data的第一行观测的类别就是label中的第一个取值； - <span class="math inline">\(C = [ C_1, C_2, ..., C_m ]\)</span>C=[C1,C2,…,Cm] 为一个类别集合，一般来说<span class="math inline">\(m &lt; p\)</span></p>
<p>步骤： 1) 在已知分类的数据集data中统计：<span class="math inline">\(\begin{cases}P(C_i), i = 1, 2, ..., m\\P(x_j \mid C_i), i = 1, 2, ..., m; j = 1, 2, ... p\end{cases}\)</span> 2) 计算<span class="math inline">\(P(x \mid C_i)P(C_i ) = P(C_i) \prod_{j=1}^{p} {P(x_j \mid C_i)}, i = 1, 2, ..., m\)</span> 3) 若<span class="math inline">\(P(x \mid C_k)P(C_k) = \max\{ P(x \mid C_i)P(C_i ) \}\)</span>,则<span class="math inline">\(x \in C_k\)</span>.</p>
<ol start="3" style="list-style-type: decimal">
<li><span style="color:red"><strong><em>一些存在的问题</em></strong></span></li>
</ol>
<ol style="list-style-type: decimal">
<li>当步骤的第2步中, <span class="math inline">\(P(x \mid C_i)P(C_i ) = P(C_i) \prod_{j=1}^{p} {P(x_j \mid C_i)}, i = 1, 2, ..., m\)</span>,中 <span class="math inline">\(P(x_j \mid C_i)\)</span>可能在样本较小时取值出现0，那么就会影响乘积；</li>
<li>计算机计算时会出现精度问题，比如，如果<span class="math inline">\(P(x_j \mid C_i)\)</span>的值有很多都是非常小的（像0.000001），那么计算机在计算是会将其四舍五入成0。</li>
</ol>
<p>出现上面的情况应该怎么办呢？</p>
<p>学数学的应该都清楚，遇到这种问题有一个很简单的处理方式，那就是取个对数。虽说取对数后会改变值的大小，但是取对数不会改变原本数据趋势，即原来大的数，取对数后还是大的。</p>
<p>此外，对于<span class="math inline">\(P(x_j \mid C_i)\)</span>可能在样本较小时取值出现0的情况，处理也很简单，那就是将每一个<span class="math inline">\(x_j\)</span>的初值都设置成1，所有特征的基数都从1开始，不会影响结果。（这时需要注意，所有特征的初值都是1，对应的总数初值也会发生变化。）</p>
<ol start="4" style="list-style-type: decimal">
<li><span style="color:red"><strong><em>例子：训练算法：从词向量计算概率</em></strong></span></li>
</ol>
<p>词向量<span class="math inline">\(w\)</span>定义为文档中每个单词在单词列表中的位置向量（出现为1,不出现为0）。那么，根据贝叶斯理论，该文档属于分类ii的条件概率为：</p>
<p><span class="math display">\[p(c_i|w) = \frac{p(w|c_i)p(c_i)}{p(w)}\]</span> 如果将<span class="math inline">\(w\)</span>展开为一个个独立特征，那么：<span class="math inline">\(p(w|c_i) = p(w_0,w_1,w_2,...,w_N|c_i)\)</span> 假设所有的词都独立（条件独立性假设），那么，<span class="math inline">\(P(w|c_i)\)</span>简化为：<span class="math inline">\(p(w_0|c_i)p(w_1|c_i)...p(w_N|c_i)\)</span> 这就是朴素贝叶斯的朴素所在。</p>
<p><span style="color:red"><strong><em>算法修正</em></strong></span> 1. 计算<span class="math inline">\(p(w_0|c_i)p(w_1|c_i)...p(w_N|c_i)\)</span>时，如果其中一个概率值为0,那么最后乘积值也为0。为了降低这种影响，可以将所有词的出现数初始化为1. 2. 计算<span class="math inline">\(p(w_0|c_i)p(w_1|c_i)...p(w_N|c_i)\)</span>时，由于大部分因子都比较小，所以会出现下益或得不到正确答案。解决方法是对乘积取自然对数：<span class="math inline">\(\ln(a*b) = ln(a) + ln(b)\)</span></p>
<h3 id="模糊分类">模糊分类</h3>
<h3 id="ｒｃｅ网络">ＲＣＥ网络</h3>
<h3 id="级数展开逼近">级数展开逼近</h3>
</body>
</html>
