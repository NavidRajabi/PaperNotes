<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h2 id="实验数据与曲线拟合">实验数据与曲线拟合</h2>
<p>曲线拟合(Curve Fitting)的数学定义是指用连续曲线近似地刻画或比拟平面上一组离散点所表示的坐标之间的函数关系，是一种用解析表达式逼近离散数据的方法。曲线拟合通俗的说法就是“拉曲线”，也就是将现有数据透过数学方法来代入一条数学方程式的表示方法。科学和工程遇到的很多问题，往往只能通过诸如采样、实验等方法获得若干离散的数据，根据这些数据，如果能够找到一个连续的函数（也就是曲线）或者更加密集的离散方程，使得实验数据与方程的曲线能够在最大程度上近似吻合，就可以根据曲线方程对数据进行数学计算，对实验结果进行理论分析，甚至对某些不具备测量条件的位置的结果进行估算。</p>
<p><span style="color:red"><strong><em>简单线性数据拟合的例子</em></strong></span></p>
<p>回想一下中学物理课的“速度与加速度”实验：假设某物体正在做加速运动，加速度未知，某实验人员从时间t0 = 3秒时刻开始，以1秒时间间隔对这个物体连续进行了12次测速，得到一组速度和时间的离散数据，请根据实验结果推算该物体的加速度。</p>
<p>在选择了合适的坐标刻度之后，我们就可以在坐标纸上画出这些点。排除偏差明显偏大的测量值后，可以看出测量结果呈现典型的线性特征。沿着该线性特征画一条直线，使尽量多的测量点能够位于直线上，或与直线的偏差尽量小，这条直线就是我们根据测量结果拟合的速度与时间的函数关系。最后在坐标纸上测量出直线的斜率K，K就是被测物体的加速度。</p>
<p><span style="color:red"><strong><em>最小二乘法曲线拟合</em></strong></span></p>
<p>最小二乘目的是推导出一条曲线，使得数据点和曲线的误差最小。所以英文翻译成:least-squares regression</p>
<p>例子:给定一组观测值: <span class="math inline">\((x_1,y_1),...,(x_n,y_n)\)</span> 来拟合一个函数，这个函数可以是线性和非线性。</p>
<p>我们首先讨论线性回归问题。</p>
<p>假设直线的数学表达式是: <span class="math inline">\(y=a_0+a_1x+e\)</span>， <span class="math inline">\(a_0,a_1\)</span>是系数，分别表示截距和斜率，<span class="math inline">\(e\)</span>为模型与观测值之间的误差，也叫做残差。</p>
我们的目的是似的残差和最小：<span class="math inline">\(\text{Min :}\sum_{i=1}^n |e_i|=\sum_{i=1}^n |y_i-a_0-a_1x_i|\)</span>, 但是这个<strong><em>准则</em></strong>不够充分。例如下面的图，任何一条介于2虚线的直线都是满足的：
<p align="center">
<img src="../images/posts/2016-04-16/linear_regression.png" width="200" >
</p>
<p>改进的方法是利用残差平方和最小的概念：<span class="math inline">\(\text{Min : } \sum_{i=1}^ne_i^2=\sum_{i=1}^n (y_i-a_0-a_1x_i)^2\)</span> 为了确定系数<span class="math inline">\(a_0,a_1\)</span>，对上式偏微分 <span class="math inline">\(\Rightarrow \begin{cases}\frac{\partial S_r}{\partial a_0}=-2\sum[(y_i-a_0-a_1x_i)] \\ \frac{\partial S_r}{\partial a_a}=-2\sum[(y_i-a_0-a_1x_i).x_i]\end{cases}\)</span> 令偏导为0,得到 <span class="math inline">\(\begin{cases}\sum[(y_i-a_0-a_1x_i)]=0 \\ \sum[(y_i-a_0-a_1x_i).x_i]=0\end{cases} \Rightarrow \begin{cases}a_1=\frac{n\sum x_i y_i-\sum x_i\sum y_i}{n\sum x_i^2-(\sum x_i)^2} \\ a_0=\bar{y}-a_1\bar{x}\end{cases}\)</span></p>
<p><span style="color:red"><strong><em>线性最小二乘拟合的统计假设</em></strong></span> (a) 每个<span class="math inline">\(x\)</span>都有一个固定的值，不是随机的; (b) <span class="math inline">\(y\)</span>值是独立的随机变量，并且具有相同的方差; (c) 对于给定的<span class="math inline">\(x,y\)</span>必须服从正态分布</p>
<p>实际的数据肯定不能通过线性回归来描述，往往是高阶的函数表达： <span class="math inline">\(y=a_0+a_1x+a_2x^2+...+a_mx^m\)</span>. 求解这个函数系数也就是求解m+1个联立的线性方程组。 这时候标准差计算为:<span class="math inline">\(\sqrt{S_r/(n-(m+1))}\)</span></p>
<blockquote>
<p>例子</p>
</blockquote>
<p><span class="math inline">\(y=a_0+a_1x+a_2x^2+e\)</span>, 给定n对样本数据。</p>
<p>计算残差<span class="math inline">\(S_r=\sum(y_i-a_0-a_1x_i-a_2x_i^2)^2\)</span>, 对每个系数求偏导:<span class="math inline">\(\begin{cases}\frac{\partial S_r}{\partial a_0}=-s\sum(y_i-a_0-a_1x_i-a_2x_i^2) \\ \frac{\partial S_r}{\partial a_0}=-s\sum x_i(y_i-a_0-a_1x_i-a_2x_i^2) \\ \frac{\partial S_r}{\partial a_0}=-s\sum x_i^2(y_i-a_0-a_1x_i-a_2x_i^2) \end{cases}\)</span> 然后整理得到方程组: <span class="math inline">\(\begin{bmatrix}n &amp; \sum x_i &amp; \sum x_i^2 \\ \sum x_i &amp; \sum x_i^2 &amp; \sum x_i^3 \\ \sum x_i^2 &amp; \sum x_i^3 &amp; \sum x_i^4 \end{bmatrix} \begin{bmatrix}a_0 \\ a_1 \\ a_2 \end{bmatrix}=\begin{bmatrix}\sum y_i \\ \sum x_i y_i \\ \sum x_i^2y_i\end{bmatrix}\)</span>, 然后就可以得到系数<span class="math inline">\(a_0,a_1,a_2\)</span> 。</p>
<p>进而可以推广到<span style="color:red"><strong><em>多元线性回归</em></strong></span>：</p>
<p>给定函数: <span class="math inline">\(y+a_0+a_1x_1+a_2x_2+...+a_mx_m+e\)</span> 残差计算: <span class="math inline">\(S_r=\sum(y_i-f(x_i))^2\)</span>, <span class="math inline">\(S_{y/x}=\sqrt{\frac{S_r}{n-(m+1)}}\)</span> 对系数求偏导: <span class="math inline">\(\begin{bmatrix}\frac{\partial S_r}{\partial a_0}=0 \\ \frac{\partial S_r}{\partial a_1}=0 \\ ... \\ \frac{\partial S_r}{\partial a_n}=0 \end{bmatrix}\)</span> <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(\begin{bmatrix}a_0 \\ a_1 \\ ... \\ a_n\end{bmatrix}\)</span></p>
<p>以上的回归可以归纳为：<span style="color:red"><strong><em>一般线性最小二乘法</em></strong></span> 一般形式的线性最小二乘模型可以写成: <span class="math inline">\(y=a_0z_0+a_1z_1+...+a_nz_n\)</span>, 其中<span class="math inline">\(z_i\)</span>为基函数(basis function)。 (1) <span class="math inline">\(z_0=1,z_1=x_1,z_2=...=z_m=0\)</span>, 简单线性回归; (2) <span class="math inline">\(z_0=x^0,z_1=x^1,z_2=x^2,...,z_m=x^m\)</span>, 多项式回归; (3) <span class="math inline">\(z_0=1,z_1=x_1,z_2=x_2,...,z_m=x_m\)</span>, 多元线性回归。</p>
<p><span class="math inline">\(Z=\begin{bmatrix}z_{01}&amp;z_{11}&amp;..&amp;z_{m1} \\ z_{02}&amp;z_{12}&amp;..&amp;z_{m2} \\ ...&amp;...&amp;..&amp;... \\ z_{0n}&amp;z_{1n}&amp;..&amp;z_{mn} \end{bmatrix}\)</span>, <span class="math inline">\(Y=\begin{bmatrix}y_1\\y_2\\...\\y_n\end{bmatrix}\)</span>, <span class="math inline">\(A=\begin{bmatrix}a_1\\a_2\\...\\a_m\end{bmatrix}\)</span>, <span class="math inline">\(E=\begin{bmatrix}e_1\\e_2\\...\\e_n\end{bmatrix}\)</span></p>
<p><span class="math inline">\(y=\sum a_i z_i +e \Rightarrow [Y]=[Z]\{A\}+\{E\} \Rightarrow Z^TZA=Z^TY \Rightarrow A=[Z^TZ]^{-1}Z^TY\)</span></p>
<blockquote>
<p>非线性回归</p>
</blockquote>
<p><span style="color:red"><strong><em>非线性回归</em></strong></span>在实际的工程中应用更为普遍：</p>
<p>例如： <span class="math inline">\(f(x)=a_0(1-e^{-a_1x})+e\)</span></p>
<p>和线性最小二乘一样，非线性回归也是基于残差平方和最小的准则来确定参数，但是对非线性，参数的求解是<strong><em>迭代</em></strong>进行的。</p>
<p>例如一个非线性方程表示为: <span class="math inline">\(y_i=f(x_i;a_0,a_1,..,a_m)+e_i\)</span>. 其中，<span class="math inline">\(y_i\)</span>是因变量的观测值，<span class="math inline">\(f(x_i;a_0,a_1,..,a_m)\)</span>是观测方程，它是自变量<span class="math inline">\(x_i\)</span>的函数，也是参数<span class="math inline">\(a_0,a_1,...,a_m\)</span>和随机误差<span class="math inline">\(e_i\)</span>的非线性函数。</p>
<p>对于有m个参数的表达式，将非线性模型围绕参数值以泰勒级数展开，并省略一阶导数后面的项，得到： <span class="math inline">\(f(x_i)_{j+1}=f(x_i)_j+\frac{\partial f(x_i)_j}{\partial a_0}\Delta a_0++\frac{\partial f(x_i)_j}{\partial a_1}\Delta a_1+...+\frac{\partial f(x_i)_j}{\partial a_m}\Delta a_m\)</span>, <span class="math inline">\(\Delta a_0=a_{0,j+1}-a_{0,j}\)</span>, <span class="math inline">\(\Delta a_1=a_{1,j+1}-a_{1,j}\)</span>. 然后得到: <span class="math inline">\(y_i -f(x_i)_j=\frac{\partial f(x_i)_j}{\partial a_0}\Delta a_0++\frac{\partial f(x_i)_j}{\partial a_1}\Delta a_1+...+\frac{\partial f(x_i)_j}{\partial a_m}\Delta a_m \Rightarrow D=Z_j\Delta A+E\)</span></p>
<p><span class="math inline">\(Z_j=\begin{bmatrix}\partial f(x_0)_j/\partial a_0 &amp; \partial f(x_0)_j/\partial a_1 &amp; ...&amp;\partial f(x_0)_j/\partial a_m \\ \partial f(x_1)_j/\partial a_0 &amp; \partial f(x_1)_j/\partial a_1 &amp; ...&amp;\partial f(x_1)_j/\partial a_m \\ ... \\ \partial f(x_n)_j/\partial a_0 &amp; \partial f(x_n)_j/\partial a_1 &amp; ...&amp;\partial f(x_n)_j/\partial a_m \end{bmatrix}\)</span>, <span class="math inline">\(D=\begin{bmatrix} y_0-f(x_0)_j \\ y_1-f(x_1)_j \\ ... \\ y_n-f(x_n)_j \end{bmatrix}\)</span>, <span class="math inline">\(A=\begin{bmatrix} a_{0,j+1}-a_{0,j} \\ a_{1,j+1}-a_{1,j} \\ ... \\ a_{m,j+1}-a_{m,j} \end{bmatrix}\)</span></p>
<p>为了求解系数<span class="math inline">\(\{a_0,a_1,...,a_m\}\)</span>, 可以通过迭代方式，给定初始值<span class="math inline">\(\{a_{0,0},a_{1,0},...,a_{m,0}\}\)</span>.</p>
<p>首先，计算<span class="math inline">\(\Delta A\)</span></p>
<p><span class="math inline">\(Z_j^TZ_j\Delta A=Z_j^TD \Rightarrow \Delta A=[Z_j^TZ_j]^{-1}D \Rightarrow a_{i,j+1}=a_{i,j}+\Delta a_i\)</span>, 直到 <span class="math inline">\(|e_a|_k=|\frac{a_{k,j+1}-a_{k,j}}{a_{k,j+1}}|\times 100\%\)</span>满足停止条件。</p>
<p><span style="color:red"><strong><em>回归和插值的不同本质</em></strong></span></p>
<p>当数据有较大误差时，多项式插值不合适，因为多项式插值预测中间结果时，可能得不到满意的结果。</p>
<p>最合适的策略是推导一个近似函数，来对数据的形状和总体趋势进行拟合，而不必刚好经过各个数据点。</p>
<p><span style="color:red"><strong><em>理解线性最小二乘回归的推导</em></strong></span></p>
<p>最小二乘回归，通过平方和对系数的偏导。</p>
<p><span style="color:red"><strong><em>理解多项式、多元和非线性回归的适用场合</em></strong></span></p>
<p>适用场合： 每个x都有固定的值，不是随机的，都是已知，没有误差; y值是独立的随机变量，并且它们具有相同的方差; 对于给定的x,y必须服从正态分布。</p>
<p>多项式回归有关算法： 第一步：输入拟合数据的阶数m; 第二步：输入数据点n; 第三步：如果n<m+1。显示出错误信息，表示回归无法完成并终止拟合过程，如果n>m+1.继续; 第四步：以增广矩阵形式计算正规方程的各个元素 第五步：使用一种消去法求解增广矩阵; 第六步：显示</p>
<p><span style="color:red"><strong><em>知道如何推导一阶牛顿插值多项式</em></strong></span></p>
<p><span class="math inline">\(\frac{f(x)-f(x_0)}{x-x_0}=\frac{f(x_1)-f(x_0)}{x_1-x_0}\Rightarrow f_1(x)=f(x_0)+\frac{f(x_1)-f(x_0)}{x_1-x_0}(x-x_0)\)</span></p>
<p><span style="color:red"><strong><em>理解牛顿多项式和泰勒级数展开的相似性</em></strong></span></p>
<p>牛顿多项式：<span class="math inline">\(f_n(x)=b_0+b_1(x-x_0)+...+b_n(x-x_0)(x-x_1)...(x-x_{n-1})\)</span> 泰勒多项式：$ f(x) = f(a) + (x - a) + (x - a)^2 + + (x - a)^n + o[(x - a)^{n}] $</p>
<p>牛顿多项式和泰勒多项式一样，都是不断加入一些高阶项来捕获逼近函数的高阶特征。结构上是类似的。</p>
<p><span style="color:red"><strong><em>认识牛顿多项式和拉格朗日方程只是同一个插值多项式的不同形式，理解各自的优缺点</em></strong></span></p>
<p>拉格朗日是牛顿插值的变形，避免了差商计算。</p>
<p>用牛顿插值推导拉格朗日插值：</p>
<p><span class="math inline">\(f(x_1,x_0)=\frac{f(x_1)-f(x_0)}{x_1-x_0}\Rightarrow \frac{f(x_1)}{x_1-x_0}+\frac{f(x_0)}{x_0-x_1}\)</span> <span class="math inline">\(f_1(x)=f(x_0)+f(x_1,x_0)(x-x_0)\rightarrow f_1(x)=\frac{x-x_1}{x_0-x_1}f(x_1)+\frac{x-x_0}{x_0-x_1}f(x_1)\)</span></p>
<p>拉格朗日版本的插值某种程度上更容易编程实现，因为它不需要计算和保存差商值。</p>
<p><span style="color:red"><strong><em>理解为什么养条函数对局部突变的数据有效</em></strong></span></p>
<p>用n阶多项式对n+1个数据点进行插值，可以获得n阶插值多项式，但是由于舍入误差(round-off error)和过度超越(overshoot)，有可能会有问题。</p>
<p>解决方法是用多个低阶多项式对数据点的各个子集进行拟合，这种相互连接的多项式成为样条函数(spline function)</p>
<p>特别是陡峭变化的边缘，高阶多项式会震荡。相比之下，样条函数也是连接同样的点，由于它仅限于低阶变化，振荡可以保持在最小的幅度内。因此对局部变化很大的函数，样条函数对通常可以更好地逼近这类函数。</p>
<!-- <span style="color:red">***8) 认识到傅立叶级数是如何用周期函数进行拟合的***</span>


傅立叶技术利用三角函数进行拟合。

<span style="color:red">***总结***</span>

|方法|与各个数据点的匹配方式|准确匹配的数据点个数|
|---|---|---|
|回归|||
|线性回归|近似|0|
|多项式回归|近似|0|
|多元线性回归|近似|0|
|插值|||
|牛顿差商多项式|准确|n+1|
|拉格朗日多项式|准确|n+1|
|三次样条|准确|数据点分段拟合|


|方法|公式|误差|
|---|---|---|
|线性回归|$\begin{cases}y=\alpha_0+\alpha_1 x\\a_1=\frac{n\sum x_iy_i-\sum x_i y_i}{n\sum x_i^2-(\sum x_i)^2}\\a_0=\bar{y}-a_1\bar{x}\end{cases}$|$s_{y/x}=\sqrt{S_r/(n-2)}$|
|多项式回归|$y=a_0+a_1x+a_2x^+...+a_mx^m$，计算系数等价于求解m+1个线性方程|$s_{y/x}=\sqrt{\frac{S_r}{n-(m+1)}}$|
|多元线性回归|$y=a_0+a_1x+...+a_mx_m$||
|牛顿差商插值多项式|$\begin{cases}f_2(x)=b_0+b_1(x-x_0)+b_2(x-x_0)(x-x_1)\\b_0=f(x_0)\\b_1=f(x_1,x_0)\\b_2=f(x_2,x_1,x_0)\end{cases}$||
|拉格朗日插值多项式|$f_2(x)=f(x_0)\frac{x-x_1}{x_0-x_1}\frac{x-x_2}{x_0-x_2}+f(x_1)\frac{x-x_0}{x_1-x_0}\frac{x-x_2}{x_1-x_2}+f(x_2)\frac{x-x_0}{x_2-x_0}\frac{x-x_1}{x_2-x_1}$||
|三次样条|在相邻2个节点之间的区间内拟合一个三次多项式$a_ix^3+b_ix^2+c_ix+d_i$|| -->
</body>
</html>
