<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h2 id="learning-to-compose-neural-networks-for-question-answering"><a href="https://github.com/jacobandreas/nmn2">Learning to Compose Neural Networks for Question Answering</a></h2>
<p>Code : <a href="https://github.com/jacobandreas/nmn2.git">Dynamically predicted neural network structures for multi-domain question answering</a></p>
<h1 id="abstract">abstract</h1>
<p>We describe a question answering model that applies to both images and structured knowledge bases. The model uses natural language strings to automatically assemble neural networks from a collection of composable modules. Parameters for these modules are learned jointly with network-assembly parameters via reinforcement learning, with only (world, question, answer) triples as supervision. Our approach, which we term a <span style="color:green"><span class="math inline">\(\mathbf{dynamic neural module network}\)</span></span>, achieves state-of-the-art results on benchmark datasets in both visual and structured domains.</p>
<h1 id="introduction">Introduction}</h1>
<p>This paper presents a compositional, attentional model for answering questions about a variety of world representations, including images and structured knowledge bases.</p>
<p>The model translates from questions to dynamically assembled neural networks, then applies these networks to world representations (images or knowledge bases) to produce answers. We take advantage of two largely independent lines of work: on one hand, an extensive literature on answering questions by mapping from strings to logical representations of meaning; on the other, a series of recent successes in deep neural models for image recognition and captioning. By constructing neural networks instead of logical forms, our model leverages the best aspects of both linguistic compositionality and continuous representations.</p>
<p>Our model has two components, trained jointly: 1. first, a collection of neural &quot;modules&quot; that can be freely composed 2. second, a network layout predictor that assembles modules into complete deep networks tailored to each question.</p>
<p>Previous work has used manually-specified modular structures for visual learning (<a href="">&quot;Deep compositional question answering with neural module networks&quot;</a>). Here we:</p>
<ol style="list-style-type: decimal">
<li><span style="color:green"><span class="math inline">\(\mathbf{learn}\)</span></span> a network structure predictor jointly with module parameters themselves</li>
<li><span style="color:green"><span class="math inline">\(\mathbf{extend}\)</span></span> visual primitives from previous work to reason over structured world representations</li>
</ol>
<p>Training data consists of (world, question, answer) triples: our approach requires no supervision of network layouts. We achieve state-of-the-art performance on two markedly different question answering tasks: one with questions about natural images, and another with more compositional questions about United States geography.</p>
<h1 id="deep-networks-as-functional-programs">Deep networks as functional programs}</h1>
<p>We begin with a high-level discussion of the kinds of composed networks we would like to learn.</p>
<p>(<a href="">&quot;Andreas15NMN&quot;</a>) describe a heuristic approach for decomposing visual question answering tasks into sequence of modular sub-problems. For example, the question <span style="color:green"><span class="math inline">\(\mathbf{What color is the bird?}\)</span></span> might be answered in two steps: first, &quot;where is the bird?&quot; (a), second, &quot;what color is that part of the image?&quot;</p>
<p>This first step, a generic  called <span style="color:green"><span class="math inline">\(\mathbf{find}, can be expressed as a fragment of a neural network that maps from image features and a lexical item (here &lt;span style=&quot;color:green&quot;&gt;\)</span><span class="math inline">\(&lt;/span&gt;) to a distribution over pixels. This operation is commonly referred to as the &lt;span style=&quot;color:green&quot;&gt;\)</span>, and is a standard tool for manipulating images (<a href="">&quot;Show, attend and tell: neural image caption generation with visual attention&quot;</a>) and text representations (<a href="">&quot;Teaching machines to read and comprehend&quot;</a>)</p>
<p>The first contribution of this paper is an extension and generalization of this mechanism to enable fully-differentiable reasoning about more structured semantic representations.</p>
<p>Figure1. b shows how the same module can be used to focus on the entity <span style="color:green">$ in a non-visual grounding domain; more generally, by representing every entity in the universe of discourse as a feature vector, we can obtain a distribution over entities that corresponds roughly to a logical set-valued denotation.</p>
<p>Having obtained such a distribution, existing neural approaches use it to immediately compute a weighted average of image features and project back into a labeling decision---a</p>
<h2 id="module">module</h2>
<p>But the logical perspective suggests a number of novel modules that might operate on attentions: e.g. combining them (by analogy to conjunction or disjunction) or inspecting them directly without a return to feature space (by analogy to quantification,Figure1. b).</p>
<p>Unlike their formal counterparts, they are differentiable end-to-end, facilitating their integration into learned models. Building on previous work, we learn behavior for a collection of heterogeneous modules from (world, question, answer) triples.</p>
<p>The second contribution of this paper is a model for learning to assemble such modules compositionally. Isolated modules are of limited use---to obtain expressive power comparable to either formal approaches or monolithic deep networks, they must be composed into larger structures.  shows simple examples of composed structures, but for realistic question-answering tasks, even larger networks are required. Thus our goal is to automatically induce variable-free, tree-structured computation descriptors. We can use a familiar functional notation from formal semantics (e.g. Liang et al., 2011) to represent these computations. We write the two examples in Figure.2 as</p>
<pre><code>(describe[color] find[bird])</code></pre>
<p>and</p>
<pre><code>(exists find[state])</code></pre>
<p>respectively. These are <span class="math inline">\(\textbf{network layouts}\)</span>: they specify a structure for arranging modules (and their lexical parameters) into a complete network. (<a href="">&quot;Deep compositional question answering with neural module networks&quot;</a>) use hand-written rules to deterministically transform dependency trees into layouts, and restricted to producing simple structures like the above for non-synthetic data. For full generality, we will need to solve harder problems, like transforming <span style="color:green"><span class="math inline">\(\mathbf{What \ cities \ are \ in \ Georgia?}\)</span></span></p>
<pre><code>    (and
        find[city]
        (relate[in] lookup[Georgia]))</code></pre>
<p>In this paper, we present a model for learning to select such structures from a set of automatically generated candidates. We call this model a <span class="math inline">\(\textbf{dynamic neural module network}\)</span>.</p>
<h1 id="related-work">Related work</h1>
<p>There is an extensive literature on database question answering, in which strings are mapped to logical forms, then evaluated by a black-box execution model to produce answers. Supervision may be provided either by annotated logical forms (<a href="">&quot;Learning synchronous grammars for semantic parsing with lambda calculus,Inducing probabilistic {CCG} grammars from logical form with higher-order unification,Semantic parsing as machine translation&quot;</a>) or from (world, question, answer) triples alone (<a href="">&quot;Learning dependency-based compositional semantics,Compositional semantic parsing on semi-structured tables&quot;</a>). In general the set of primitive functions from which these logical forms can be assembled is fixed, but one recent line of work focuses on inducing new predicates functions automatically, either from perceptual features (<a href="">&quot;Jointly learning to parse and perceive: connecting natural language to the physical world&quot;</a>) or the underlying schema (<a href="">&quot;Scaling semantic parsers with on-the-fly ontology matching&quot;</a>). The model we describe in this paper has a unified framework for handling both the perceptual and schema cases, and differs from existing work primarily in learning a differentiable execution model with continuous evaluation results.</p>
<p>Neural models for question answering are also a subject of current interest. These include approaches that model the task directly as a multiclass classification problem (<a href="">&quot;A neural network for factoid question answering over paragraphs&quot;</a>), models that attempt to embed questions and answers in a shared vector space (<a href="">&quot;Question answering with subgraph embeddings&quot;</a>) and attentional models that select words from documents sources (<a href="">&quot;Teaching machines to read and comprehend&quot;</a>). Such approaches generally require that answers can be retrieved directly based on surface linguistic features, without requiring intermediate computation. A more structured approach described by (<a href="">&quot;Neural enquirer: Learning to query tables&quot;</a>) learns a query execution model for database tables without any natural language component. Previous efforts toward unifying formal logic and representation learning include those of (<a href="">&quot;Towards a formal distributional semantics: Simulating logical calculi with tensors&quot;</a>) and (<a href="">&quot;Vector space semantic parsing: A framework for compositional vector space models&quot;</a>).</p>
<p>The visually-grounded component of this work relies on recent advances in convolutional networks for computer vision (<a href="">&quot;Very deep convolutional networks for large-scale image recognition&quot;</a>), and in particular the fact that late convolutional layers in networks trained for image recognition contain rich features useful for other downstream vision tasks, while preserving spatial information. These features have been used for both image captioning (<a href="">&quot;Show, attend and tell: neural image caption generation with visual attention&quot;</a>) and visual question answering (<a href="">&quot;Stacked attention networks for image question answering&quot;</a>).</p>
<p>Most previous approaches to visual question answering either apply a recurrent model to deep representations of both the image and the question (<a href="">&quot;Image question answering: A visual semantic embedding model and a new dataset,Ask your neurons: A neural-based approach to answering questions about images&quot;</a>), or use the question to compute an attention over the input image, and then answer based on both the question and the image features attended to (<a href="">&quot;Stacked attention networks for image question answering,Ask, attend and answer: Exploring question-guided spatial attention for visual question answering&quot;</a>). Other approaches include the simple classification model described by (<a href="">&quot;Simple baseline for visual question answering&quot;</a>) and the dynamic parameter prediction network described by (<a href="">&quot;Image question answering using convolutional neural network with dynamic parameter prediction&quot;</a>). All of these models assume that a fixed computation can be performed on the image and question to compute the answer, rather than adapting the structure of the computation to the question.</p>
<p>As noted, (<a href="">&quot;Deep compositional question answering with neural module networks&quot;</a>) previously considered a simple generalization of these attentional approaches in which small variations in the network structure per-question were permitted, with the structure chosen by (deterministic) syntactic processing of questions. Other approaches in this general family include the &quot;universal parser&quot; sketched by (<a href="">&quot;From machine learning to machine reasoning&quot;</a>), and the recursive neural networks of (<a href="">&quot;Parsing with compositional vector grammars&quot;</a>), which use a fixed tree structure to perform further linguistic analysis without any external world representation. We are unaware of previous work that succeeds in simultaneously learning both the parameters for and structures of instance-specific neural networks.</p>
<h1 id="model">Model</h1>
<p>Recall that our goal is to map from questions and world representations to answers. This process involves the following variables:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(w\)</span> a world representation</li>
<li><span class="math inline">\(x\)</span> a question</li>
<li><span class="math inline">\(y\)</span> an answer</li>
<li><span class="math inline">\(z\)</span> a network layout 5.<span class="math inline">\(\theta\)</span> a collection of model parameters</li>
</ol>
<p>Our model is built around two distributions: - a  <span class="math inline">\(p(z|x;\lparam)\)</span> which chooses a layout for a sentence, - and a  <span class="math inline">\(p_z(y|w;\theta_e)\)</span> which applies the network specified by <span class="math inline">\(z\)</span> to <span class="math inline">\(w\)</span>.</p>
<h2 id="evaluating-modules">Evaluating modules</h2>
<p>Given a layout <span class="math inline">\(z\)</span>, we assemble the corresponding modules into a full neural network , and apply it to the knowledge representation. Intermediate results flow between modules until an answer is produced at the root. We denote the output of the network with layout <span class="math inline">\(z\)</span> on input world <span class="math inline">\(w\)</span> as <span class="math inline">\(\denote{z}_w\)</span>; when explicitly referencing the substructure of <span class="math inline">\(z\)</span>, we can alternatively write <span style="color:green"><span class="math inline">\(\denote{m(h^1, h^2)}\)</span></span> for a top-level module <span class="math inline">\(m\)</span> with submodule outputs <span class="math inline">\(h^1\)</span> and <span class="math inline">\(h^2\)</span>. We then define the execution model: <span class="math display">\[
  p_z(y|w) = (\denote{z}_w)_y
  \label{eq:simple-execution}
\]</span> (This assumes that the root module of <span class="math inline">\(z\)</span> produces a distribution over labels <span class="math inline">\(y\)</span>.)</p>
<p>The set of possible layouts <span class="math inline">\(z\)</span> is restricted by module <span style="color:green"><span class="math inline">\(\mathbf{type constraints}\)</span></span>: some modules (like <span style="color:green"><span class="math inline">\(\text{find}\)</span></span> above) operate directly on the input representation, while others (like <span style="color:green"><span class="math inline">\(\mathbf{describe}\)</span></span> above) also depend on input from specific earlier modules. Two base types are considered in this paper are <span style="color:green"><span class="math inline">\(\text{(a distribution over pixels or entities)}\)</span></span> and <span style="color:green"><span class="math inline">\(\text{(a distribution over answers)}\)</span></span>.</p>
<p>Parameters are tied across multiple instances of the same module, so different instantiated networks may share some parameters but not others. Modules have both <span style="color:green"><span class="math inline">\(\text{parameter arguments}\)</span></span> (shown in square brackets) and ordinary inputs (shown in parentheses). Parameter arguments, like the running <span style="color:green">$ example above, are provided by the layout, and are used to specialize module behavior for particular lexical items. Ordinary inputs are the result of computation lower in the network. In addition to parameter-specific weights, modules have global weights shared across all instances of the module (but not shared with other modules). We write <span class="math inline">\(A, a, B, b, \dots\)</span> for global weights and <span class="math inline">\(u^i, v^i\)</span> for weights associated with the parameter argument <span class="math inline">\(i\)</span>. <span class="math inline">\(\oplus\)</span> and <span class="math inline">\(\odot\)</span> denote (possibly broadcasted) elementwise addition and multiplication respectively. The complete set of global weights and parameter-specific weights constitutes <span class="math inline">\(\theta_e\)</span>.</p>
<p><span style="color:green"><span class="math inline">\(\mathbf{Every}\)</span></span> module has access to the world representation, represented as a collection of vectors <span class="math inline">\(w^1, w^2, \dots\)</span> (or <span class="math inline">\(W\)</span> expressed as a matrix). The nonlinearity <span class="math inline">\(\sigma\)</span> denotes a rectified linear unit.</p>
<p>The modules used in this paper are shown below, with names and type constraints in the first row and a description of the module's computation following.</p>
<table style="width:17%;">
<colgroup>
<col width="5%" />
<col width="5%" />
<col width="5%" />
</colgroup>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(\textbf{Lookup}\)</span></td>
<td align="left">(<span class="math inline">\(\to\)</span> Attention)</td>
<td align="left"><span class="math inline">\(\textbf{lookup[i]} = e_{f(i)}\)</span></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"><span style="color:green"><span class="math inline">\(\mathbf{lookup[i]}\)</span></span> produces an attention focused entirely at the index <span class="math inline">\(f(i)\)</span>, where the relationship <span class="math inline">\(f\)</span> between words and positions in the input map is known ahead of time (e.g. string matches on database fields).</td>
<td align="left"><span class="math inline">\(e_i\)</span> is the basis vector that is <span class="math inline">\(1\)</span> in the <span class="math inline">\(i\)</span>th position and 0 elsewhere.</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\textbf{Find}\)</span></td>
<td align="left">(<span class="math inline">\(\to\)</span> Attention)</td>
<td align="left"><span class="math inline">\(\mathbf{find[i]} = \text{softmax}(a \odot \sigma(B v^i \oplus C W \oplus d))\)</span></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"><span style="color:green"><span class="math inline">\(\mathbf{find[i]}\)</span></span> computes a distribution over indices by concatenating the parameter argument with each position of the input feature map, and passing the concatenated vector through a MLP</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\textbf{Relate}\)</span></td>
<td align="left">(Attention <span class="math inline">\(\to\)</span> Attention)</td>
<td align="left"><span class="math inline">\(\mathbf{relate[i]}(h) = \text{softmax}(a \odot \sigma(B v^i \oplus C W \oplus D\bar{w}(h) \oplus e))\)</span></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"><span style="color:green"><span class="math inline">\(\mathbf{relate}\)</span></span> directs focus from one region of the input to another. It behaves much like the <span style="color:green"><span class="math inline">\(\mathbf{find}\)</span></span> module, but also conditions its behavior on the current region of attention <span class="math inline">\(h\)</span>.</td>
<td align="left"><span class="math inline">\(\bar{w}(h) = \sum_k h_k w^k\)</span>, where <span class="math inline">\(h_k\)</span> is the <span class="math inline">\(k^{th}\)</span></span> element of <span class="math inline">\(h\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\textbf{And}\)</span></td>
<td align="left">(Attention* <span class="math inline">\(\to\)</span> Attention)</td>
<td align="left"><span class="math inline">\([\mathbf{and}(h^1, h^2, ...)] = h^1 \odot h^2 ...\)</span></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"><span style="color:green"><span class="math inline">\(\mathbf{and}\)</span></span> performs an operation analogous to set intersection for attentions. The analogy to probabilistic logic suggests multiplying probabilities</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\textbf{Describe}\)</span></td>
<td align="left">(Attention <span class="math inline">\(\to\)</span> Labels)</td>
<td align="left"><span class="math inline">\(\mathbf{describe[i]}(h) = \text{softmax}(A \sigma(B \bar{w}(h) + v^i))\)</span></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"><span style="color:green"><span class="math inline">\(\mathbf{describe[i]}\)</span></span> computes a weighted average of <span class="math inline">\(w\)</span> under the input attention. This average is then used to predict an answer representation. With <span class="math inline">\(\bar{w}\)</span> as above,</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\textbf{Exists}\)</span></td>
<td align="left">(Attention <span class="math inline">\(\to\)</span> Labels)</td>
<td align="left"><span class="math inline">\(\mathbf{exists]}(h) = \textrm{softmax}((\max_k h_k)a + b)\)</span></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"><span style="color:green"><span class="math inline">\(\mathbf{exists}\)</span></span> is the existential quantifiers, and inspects the incoming attention directly to produce a label, rather than producing an intermediate feature vector like <span style="color:green"><span class="math inline">\(\mathbf{describe}\)</span></span></td>
<td align="left"></td>
</tr>
</tbody>
</table>
<p>With <span class="math inline">\(z\)</span> observed, the model we have described so far corresponds largely to that of ([&quot;Andreas15NMN}, though the module inventory is different---in particular, our new <span style="color:green"><span class="math inline">\(\mathbf{exists}\)</span></span> and <span style="color:green"><span class="math inline">\(\mathbf{relate}\)</span></span> modules do not depend on the two-dimensional spatial structure of the input. This enables generalization to non-visual world representations.</p>
<p>Learning in this simplified setting is straightforward. Assuming the top-level module in each layout is a <span style="color:green"><span class="math inline">\(\mathbf{describe}\)</span></span> or <span style="color:green"><span class="math inline">\(\mathbf{exists}\)</span></span> module, the fully-instantiated network corresponds to a distribution over labels conditioned on layouts. To train, we maximize <span class="math inline">\(\sum_{(w,y,z)} \log p_z(y|w;\theta_e)\)</span>directly.This can be understood as a parameter-tying scheme, where the decisions about which parameters to tie are governed by the observed layouts <span class="math inline">\(z\)</span>.</p>
<h2 id="assembling-networks">Assembling networks</h2>
<p>Next we describe the layout model <span class="math inline">\(p(z|x;\theta_l)\)</span>. We first use a fixed syntactic parse to generate a small set of candidate layouts, analogously to the way a semantic grammar generates candidate semantic parses in previous work (<a href="">&quot;Semantic parsing via paraphrasing&quot;</a>).</p>
<p>A semantic parse differs from a syntactic parse in two primary ways. First, lexical items must be mapped onto a (possibly smaller) set of semantic primitives. Second, these semantic primitives must be combined into a structure that closely, but not exactly, parallels the structure provided by syntax. For example, <span style="color:green"><span class="math inline">\(\mathbf{state}\)</span></span> and <span style="color:green"><span class="math inline">\(\mathbf{province}\)</span></span> might need to be identified with the same field in a database schema, while <span style="color:green"><span class="math inline">\(\mathbf{all \ states \ have \ a \ capital}\)</span></span> might need to be identified with the correct (<span style="color:green"><span class="math inline">\(\mathbf{in \ situ})\)</span></span> quantifier scope.</p>
<p>While we cannot avoid the structure selection problem, continuous representations simplify the lexical selection problem. For modules that accept a vector parameter, we associate these parameters with <span style="color:green"><span class="math inline">\(\mathbf{words}\)</span></span> rather than semantic tokens, and thus turn the combinatorial optimization problem associated with lexicon induction into a continuous one. Now, in order to learn that <span style="color:green"><span class="math inline">\(\mathbf{province}\)</span></span> and <span style="color:green"><span class="math inline">\(\mathbf{state}\)</span></span> have the same denotation, it is sufficient to learn that their associated parameters are close in some embedding space---a task amenable to gradient descent.</p>
<p>(Note that this is easy only in an optimizability sense, and not an information-theoretic one---we must still learn to associate each independent lexical item with the correct vector.) The remaining combinatorial problem is to arrange the provided lexical items into the right computational structure. In this respect, layout prediction is more like syntactic parsing than ordinary semantic parsing, and we can rely on an off-the-shelf syntactic parser to get most of the way there. In this work, syntactic structure is provided by the Stanford dependency parser.</p>
<p>The construction of layout candidates is depicted . We assume queries are conjunctive at the top level, and collect the set of attributes and prepositional relations that depend on the wh-word or copula in the question. The parser is free to consider subsets of this conjunction, and optionally to insert an existential quantifier. These are strong simplifying assumptions, but appear sufficient to cover most of the examples that appear in both of our tasks. As our approach includes both categories, relations and simple quantification, the range of phenomena considered is generally broader than previous perceptually-grounded QA work (<a href="">&quot;Jointly learning to parse and perceive: connecting natural language to the physical world,A joint model of language and perception for grounded attribute learning&quot;</a>).</p>
<p>Having generated a set of candidate parses, we need to score them. This is a reranking problem; as in the rest of our approach, we solve it using standard neural machinery. In particular, we produce an LSTM representation of the question, a feature-based representation of the query (with indicators on the type and number of modules used), and pass both representations through a multilayer perceptron (MLP). While one can easily imagine a more sophisticated parse-scoring model, this simple approach works well for our tasks.</p>
<p>Formally, for a question <span class="math inline">\(x\)</span>, let <span class="math inline">\(h_q(x)\)</span> be an LSTM encoding of the question (i.e. the last hidden layer of an LSTM applied word-by-word to the input question). Let ${z_1, z_2, } be the proposed layouts for <span class="math inline">\(x\)</span>, and let <span class="math inline">\(f(z_i)\)</span> be a feature vector representing the <span class="math inline">\(i\)</span>th layout. Then the score <span class="math inline">\(s(z_i|x)\)</span> for the layout <span class="math inline">\(z_i\)</span> is <span class="math display">\[
    s(z_i|x) = a^\top \sigma(B h_q(x) + C f(z_i) + d)
\]</span> i.e. a the output of an MLP with inputs <span class="math inline">\(h_q(x)\)</span> and <span class="math inline">\(f(z_i)\)</span>, and parameters <span class="math inline">\(\theta_l = \{a, B, C, d\}\)</span>. Finally, we normalize these scores to obtain a distribution: <span class="math display">\[
    p(z|x;\theta_l) = e^{s(z_i|x)} / \sum_{j=1}^n e^{s(z_j|x)}
\]</span></p>
<p>Having defined a layout selection module <span class="math inline">\(p(z|x;\theta_l)\)</span> and a network execution model <span class="math inline">\(p_z(y|w;\theta_e)\)</span>, we are ready to define a model for predicting answers given only (world, question) pairs. The key constraint is that we want to minimize evaluations of <span class="math inline">\(p_z(y|w;\theta_e)\)</span> (which involves expensive application of a deep network to a large input representation), but can tractably evaluate <span class="math inline">\(p(z|x;\theta_l)\)</span> for all <span class="math inline">\(z\)</span> (which involves application of a shallow network to a relatively small set of candidates). This is the opposite of the situation usually encountered semantic parsing, where calls to the query execution model are fast but the set of candidate parses is too large to score exhaustively.</p>
<p>In fact, the problem more closely resembles the scenario faced by agents in the reinforcement learning setting (where it is cheap to score actions, but potentially expensive to execute them and obtain rewards). We adopt a common approach from that literature, and express our model as a stochastic policy. Under this policy, we first <span style="color:green"><span class="math inline">\(\mathbf{sample}\)</span></span> a layout <span class="math inline">\(z\)</span> from a distribution <span class="math inline">\(p(z|x;\theta_l)\)</span>, and then apply <span class="math inline">\(z\)</span> to the knowledge source and obtain a distribution over answers <span class="math inline">\(p(y|z,w;\theta_e)\)</span>.</p>
<p>After <span class="math inline">\(z\)</span> is chosen, we can train the execution model directly by maximizing <span class="math inline">\(\log p(y|z,w;\theta_e)\)</span> with respect to <span class="math inline">\(\theta_e\)</span> as before (this is ordinary backpropagation). Because the hard selection of <span class="math inline">\(z\)</span> is non-differentiable, we optimize <span class="math inline">\(p(z|x;\theta_l)\)</span> using a policy gradient method. ([&quot;Williams92Reinforce} showed that the gradient of the reward surface <span class="math inline">\(J\)</span> with respect to the parameters of the policy is <span class="math display">\[
  \nabla J(\theta_l) = \mathbb{E}[ \nabla \log p(z|x;\theta_l) \cdot r ]
\]</span> (this is the {reinforce} rule). Here the expectation is taken with respect to rollouts of the policy, and <span class="math inline">\(r\)</span> is the reward. Because our goal is to select the network that makes the most accurate predictions, we take the reward to be identically the negative log-probability from the execution phase, i.e. <span class="math display">\[
  \mathbb{E} [(\nabla \log p(z|x;\theta_l)) \cdot \log p(y|z,w;\theta_e)]
\]</span> Thus the update to the layout-scoring model at each timestep is simply the gradient of the log-probability of the chosen layout, scaled by the accuracy of that layout's predictions.</p>
<p>At training time, we approximate the expectation with a single rollout, so at each step we update <span class="math inline">\(\theta_l\)</span> in the direction</p>
<p>$ (p(z|x;_l)) p(y|z,w;_e) $ % for a single <span class="math inline">\(z \sim p(z|x;\theta_l)\)</span>. <span class="math inline">\(\theta_e\)</span> and <span class="math inline">\(\theta_l\)</span> are optimized using {adadelta} (<a href="">&quot;{ADADELTA}: {A}n adaptive learning rate method&quot;</a>) with <span class="math inline">\(\rho=0.95,\)</span> <span class="math inline">\(\varepsilon=1e-6\)</span> and gradient clipping at a norm of <span class="math inline">\(10\)</span>.</p>
<h1 id="experiments">Experiments}</h1>
<p>The framework described in this paper is general, and we are interested in how well it1 performs on datasets of varying domain, size and linguistic complexity. To that end, weevaluate our model on tasks at opposite extremes of both these criteria: a large visual question answering dataset, and a small collection of more structured geography questions.</p>
<h2 id="questions-about-images">Questions about images}</h2>
<p>Our first task is the recently-introduced Visual Question Answering challenge (VQA) (<a href="">&quot;{VQA}: Visual question answering&quot;</a>). The VQA dataset consists of more than 200,000 images paired with human-annotated questions and answers.</p>
<p>We use the VQA 1.0 release, employing the development set formodel selection and hyperparameter tuning, and reporting final results from the evaluation server on the test-standard set. For the experiments described in this section, the input feature representations <span class="math inline">\(w_i\)</span> are computed by the the fifth convolutional layer of a 16-layer VGG-Net after pooling (<a href="">&quot;Very deep convolutional networks for large-scale image recognition&quot;</a>). Input images are scaled to 448$<span class="math inline">\(448 before computing their representations. We found that performance on this task was best if the candidate layouts were relatively simple: only &lt;span style=&quot;color:green&quot;&gt;\)</span><span class="math inline">\(&lt;/span&gt;, &lt;span style=&quot;color:green&quot;&gt;\)</span><span class="math inline">\(&lt;/span&gt; and &lt;span style=&quot;color:green&quot;&gt;\)</span>$</span> modules are used, and layouts contain at most two conjuncts.</p>
<p>One weakness of this basic framework is a difficulty modeling prior knowledge about answers (of the form <span style="color:green"><span class="math inline">\(\mathbf{bears are brown}\)</span></span>). This kinds of linguistic &quot;prior&quot; is essential for the VQA task, and easily incorporated. We simply introduce an extra hidden layer for recombining the final module network output with the input sentence representation <span class="math inline">\(h_q(x)\)</span> (see ), replacing  with: <span class="math display">\[
  \log p_z(y|w,x) = (A h_q(x) + B \denote{z}_w)_y
\]</span> (Now modules with output type should be understood as producing an answer embedding rather than a distribution over answers.) This allows the question to influence the answer directly.</p>
<p>Results are shown in . The use of dynamic networks provides a small gain, most noticeably on yes/no questions. We achieve state-of-the-art results on this task, outperforming a highly effective visual bag-of-words model (<a href="">&quot;Simple baseline for visual question answering&quot;</a>),a model with dynamic network parameter prediction (but fixed network structure) (<a href="">&quot;Image question answering using convolutional neural network with dynamic parameter prediction&quot;</a>), and a previous approach using neural module networks with no structure prediction (<a href="">&quot;Deep compositional question answering with neural module networks&quot;</a>). For this last model, we report both the numbers from the original paper, and a reimplementation of the model that uses the same image preprocessing as the dynamic module network experiments in this paper. A more conventional attentional model has also been applied to this task (<a href="">&quot;Stacked attention networks for image question answering&quot;</a>); while we also outperform their reported performance, the evaluation uses different train/test split, so results are not directly comparable.</p>
<p>Some examples are shown in . In general, the model learns to focus on the correct region of the image, and tends to consider a broad window around the region. This facilitates answering questions like <span style="color:green"><span class="math inline">\(\mathbf{Where is the cat?}\)</span></span>, which requires knowledge of the surroundings as well as the object in question.</p>
<h2 id="questions-about-geography">Questions about geography}</h2>
<p>The next set of experiments we consider focuses on GeoQA, a geographical question-answering task first introduced by (<a href="">&quot;Jointly learning to parse and perceive: connecting natural language to the physical world&quot;</a>). This task was originally paired with a visual question answering task much simpler than the one just discussed, and is appealing for a number of reasons. In contrast to the VQA dataset, GeoQA is quite small, containing only 263 examples. Two baselines are available: one using a classical semantic parser backed by a database, and another which induces logical predicates using linear classifiers over both spatial and distributional features. This allows us to evaluate the quality of our model relative to other perceptually grounded logical semantics, as well as strictly logical approaches.</p>
<p>The GeoQA domain consists of a set of entities (e.g. states, cities, parks) which participate in various relations (e.g. north-of, capital-of). Here we take the world representation to consist of two pieces: a set of category features (used by the <span style="color:green"><span class="math inline">\(\mathbf{find}\)</span></span> module) and a different set of relational features (used by the <span style="color:green"><span class="math inline">\(\mathbf{relate}\)</span></span> module). For our experiments, we use a subset of the features originally used by Krishnamurthy et al.</p>
<p>The original dataset includes no quantifiers, and treats the questions <span style="color:green"><span class="math inline">\(\mathbf{What cities are in Texas?}\)</span></span> and <span style="color:green"><span class="math inline">\(\mathbf{Are there any cities in Texas?}\)</span></span><br />
identically. Because we're interested in testing the parser's ability to predict a variety of different structures, we introduce a new version of the dataset, GeoQA+Q, which distinguishes these two cases, and expects a Boolean answer to questions of the second kind.</p>
<p>Results are shown in . As in the original work, we report the results of leave-one-environment-out cross-validation on the set of 10 environments. Our dynamic model (D-NMN) outperforms both the logical (LSP-F) and perceptual models (LSP-W) described by ([&quot;Krish2013Grounded}, as well as a fixed-structure neural module net (NMN). This improvement is particularly notable on the dataset with quantifiers, where dynamic structure prediction produces a 20% relative improvement over the fixed baseline. A variety of predicted layouts are shown in .</p>
<h1 id="conclusion">Conclusion}</h1>
<p>We have introduced a new model, the <span style="color:green"><span class="math inline">\(\mathbf{dynamic neural module network}\)</span></span>, for answering queries about both structured and unstructured sources of information. Given only (question, world, answer) triples as training data, the model learns to assemble neural networks on the fly from an inventory of neural models, and simultaneously learns weights for these modules so that they can be composed into novel structures. Our approach achieves state-of-the-art results on two tasks.</p>
<p>We believe that the success of this work derives from two factors:</p>
<p><span style="color:green"><span class="math inline">\(\mathbf{Continuous representations improve the expressiveness and learnability of semantic parsers}\)</span></span>: by replacing discrete predicates with differentiable neural network fragments, we bypass the challenging combinatorial optimization problem associated with induction of a semantic lexicon. In structured world representations, neural predicate representations allow the model to invent reusable attributes and relations not expressed in the schema. Perhaps more importantly, we can extend compositional question-answering machinery to complex, continuous world representations like images.</p>
<p><span style="color:green"><span class="math inline">\(\mathbf{Semantic structure prediction improves generalization in deep networks}\)</span></span>: by replacing a fixed network topology with a dynamic one, we can tailor the computation performed to each problem instance, using deeper networks for more complex questions and representing combinatorially many queries with comparatively few parameters. In practice, this results in considerable gains in speed and sample efficiency, even with very little training data.</p>
<p>These observations are not limited to the question answering domain, and we expect that they can be applied similarly to tasks like instruction following, game playing, and language generation.</p>
</body>
</html>
