<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<h1 id="generating-images-with-recurrent-adversarial-networks"><a href="http://arxiv.org/pdf/1602.05110v4.pdf">Generating images with recurrent adversarial networks</a></h1>
<iframe width="560" height="315" src="https://www.youtube.com/embed/QPkb5VcgXAM" frameborder="0" allowfullscreen>
</iframe>
<p>http://arxiv.org/abs/1602.05110</p>
<div class="figure">
<img src="https://www.dropbox.com/s/yoyz8pnjk3ecupn/Generating%20images%20with%20recurrent%20adversarial%20networks.png?dl=1" />

</div>
<ul>
<li>Motivation: generate good image samples</li>
<li>Abbreviation: GRAN</li>
<li>Similar models: LAGAN and DRAW</li>
<li>Idea:
<ul>
<li>image generation is an iterative process.</li>
<li>Start with a sample from the distribution of latent variables, feed it to the decoder to get an image. Feed the generated sample to the encoder, which will generate a vector representation of the image. Concatenate that representation with the sample from the distribution and feed that to the decoder, to generate another image. This process is repeated t times, with t fixed aprori. To generate the final image, the t image generated samples are added together and the tanh function is applied to ensure the final image has entries in between 0 and 1.</li>
<li>encoders and decoders can be represented by any function, they use deep convolutional adversarial nets</li>
</ul></li>
<li>Evaluation idea:
<ul>
<li>to evaluate two GAN models, with discriminator D1 and generator G1 and discriminator D2 and generator G2, one cam compare them by seeing how well D1 can discriminate samples from G2 and how well D2 can discriminate samples from G1</li>
</ul></li>
<li>Differences between proposed model and DRAW:
<ul>
<li>in DRAW at each time step, a new sample from the latent space is generated. In GRAN, only one sample is generated and then reused at each time step.</li>
<li>GRAN starts with the decoding phase, not with the encoding phase</li>
<li>GRAN has no attention mechanism</li>
<li>A discriminator from GRAN can differentiate between DRAW generated samples and images from MNIST with a 10% error</li>
</ul></li>
</ul>
</body>
</html>
