<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<h2 id="neural-responding-machine-for-short-text-conversation"><a href="http://arxiv.org/abs/1503.02364">Neural Responding Machine for Short-Text Conversation</a></h2>
<p>TLDR; The author train a three variants of a seq2seq model to generate a response to social media posts taken from Weibo. The first variant, NRM-glo is the standard model without attention mechanism using the last state as the decoder input. The second variant, NRM-loc, uses an attention mechanism. The third variant, NRM-hyb combines both by concatenating local and global state vectors. The authors use human users to evaluate their responses and compare them to retrievel-based and SMT-based systems. The authors find that SRM models generate reasonable responses ~75% of the time.</p>
<h4 id="key-points">Key Points</h4>
<ul>
<li>STC: Short-text conversation. Generate only a response to a post. Don't need to keep track of a whole conversation.</li>
<li>Training data: 200k posts, 4M responses.</li>
<li>Authors use GRU with 1000 hidden units.</li>
<li>Vocabulary: Most frequent 40k words for both input and response.</li>
<li>Retrieval is done using beam search with beam size 10.</li>
<li>Hybrid model is difficult to train jointly. The authors train the model individually and then fine-tune the hybrid model.</li>
<li>Tradeoff with retrieval based methods: Responses are written by a human and don't have grammatical errors, but cannot easily generalize to unseen inputs.</li>
</ul>
</body>
</html>
