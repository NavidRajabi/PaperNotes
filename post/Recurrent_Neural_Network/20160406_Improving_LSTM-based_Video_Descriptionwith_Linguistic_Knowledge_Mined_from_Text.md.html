<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h2 id="improving-lstm-based-video-descriptionwith-linguistic-knowledge-mined-from-text"><a href="http://arxiv.org/abs/1604.01729">Improving LSTM-based Video Descriptionwith Linguistic Knowledge Mined from Text</a></h2>
<p>http://www.cs.utexas.edu/~vsub/</p>
<p>This paper investigates how <a href="http://dict.youdao.com/w/linguistic/#keyfrom=dict.top">linguistic</a> knowledge mined from large text corpora can aid the generation of natural language descriptions of videos.</p>
<p>Given a sequence of inputs <span class="math display">\[(x_1,...,x_T)\]</span> the LSTM computes the cell memory sequences <span class="math display">\[(c_1,...,c_T)\]</span> and hidden control sequences <span class="math display">\[(h_1,...,h_T)\]</span> as follows:</p>
<p><span class="math display">\[
\begin{cases}
i_t=\text{sigm}(W_{xi}x_t+W_{hi}h_{t-1}+b_i)\\
i_t=\text{sigm}(W_{xf}x_t+W_{hf}h_{t-1}+b_f)\\
i_t=\text{sigm}(W_{xo}x_t+W_{ho}h_{t-1}+b_o)\\
i_t=\text{tanh}(W_{xg}x_t+W_{hg}h_{t-1}+b_g)\\
c_t=f_t\odot c_{t-1}+i_t \odot g_t\\
h_t = o_t \odot \tanh (c_t)
\end{cases} \qquad(1)
\]</span></p>
<p>During decoding,the model essentially defines a probability over the output sequence <span class="math display">\[\vec{y}\]</span> by decomposing the joint probability into ordered conditionals:</p>
<p><span class="math display">\[
p(\vec{y}|x_1,...,x_T)=\prod_{t=1}^N p(y_t|h_T,y_1,...,y_{t-1})
\]</span></p>
<p>This is done by applying a softmax function on the decoder LSTMâ€™sh-sequence. Hence, for a word in the vocabulary (w2V), <span class="math display">\[
p(y_t=w|h_T,\vec{y}&lt;t)=\text{softmax}(W_vh_T+b_v)  \qquad(2)
\]</span></p>
<p>Thus the overall objective of the network is to maximize log-likelihood of the output word sequence.</p>
<p><span class="math display">\[
\log p(\vec{y}|\vec{x})=\sum_{t=1}^N \log p(y_t|h_T, \vec{y}&lt;t) \qquad(3)
\]</span></p>
</body>
</html>
