<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<h2 id="a-unified-tagging-solution-bidirectional-lstm-recurrent-neural-network-with-word-embedding"><a href="http://arxiv.org/abs/1511.00215">A Unified Tagging Solution: Bidirectional LSTM Recurrent Neural Network with Word Embedding</a></h2>
<p>TLDR; The authors evaluate the use of a Bidirectional LSTM RNN on POS tagging, chunking and NER tasks. The inputs are task-independent input features: The word and its capitalization. The authors incorporate prior knowledge about the taging tasks by restricting the decoder to output valid sequences of tags, and also propose a novel way of learning word embeddings: Randomly replacing words in a sequence and using an RNN to predict which words are correct vs. incorrect. The authors show that their model combined with pre-trained word embeddings performs on par state of the art models.</p>
<h4 id="key-points">Key Points</h4>
<ul>
<li>Bidirectional LSTM with 100-dimensional embeddings, and 100-dimensional cells. Both 1 and 2 layers are evaluated. Predict tags at each step. Higher dimensionality of cells resultes in little improvement.</li>
<li>Word vector pretraining: Randomly replace words and use LSTM to predict correct/incorrect words.</li>
</ul>
<h4 id="notesquestions">Notes/Questions</h4>
<ul>
<li>The fact that we need a task-specific decoder kind of defeats the purpose of this paper. The goal was to create a &quot;task-independent&quot; system. To be fair, the need for this decoder is probably only due to the small size of the training data. Not all tag combination appear in the training data.</li>
<li>The comparisons with other state of the art systems are somewhat unfair since the proposed model heavily relies on pre-trained word embeddings from external data (trained on more than 600M words) to achieve good performance. It also relies on external embeddings trained in yet another way.</li>
<li>I'm surprised that the authors didn't try combining all of the tagging tasks into one model, which seem like an obvious extension.</li>
</ul>
</body>
</html>
