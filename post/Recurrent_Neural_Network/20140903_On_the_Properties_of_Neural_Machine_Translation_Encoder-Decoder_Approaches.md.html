<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<h2 id="on-the-properties-of-neural-machine-translation-encoder-decoder-approaches"><a href="http://arxiv.org/abs/1409.1259">On the Properties of Neural Machine Translation: Encoder-Decoder Approaches</a></h2>
<p>TLDR; The authors empirically evaluate seq2seq Neural Machine Translation systems. They find that performance degrades significantly as sentences get longer, and as the number of unknown words in the source sentence increases. Thus, they propose that more investigation into how to deal with large vocabularies and long-range dependencies is needed. The authors also present a new gated recursive convolutional network (grConv) architecture, which consists of a binary tree using GRU units. While this network architecture does not perform as well as the RNN encoder, it seems to be learning grammatical properties represented in the gate activations in an unsupervised fashion.</p>
<h4 id="key-points">Key Points</h4>
<ul>
<li>GrConv: Neuron computed as combination between left and right neuron in previous layer, gated with the activations of those neurons. 3 gates: Left, right, reset.</li>
<li>In experiments, encoder varies between RNN and grConv. Decoder is always RNN.</li>
<li>Model size is only 500MB. 30k vocabulary. Only trained on sentences &lt;= 30 tokens. Networks not trained to convergence.</li>
<li>Beam search with scores normalized by sequence length to choose translations.</li>
<li>Hypothesis is that fixed vector representation is a bottleneck, or that decoder is not powerful enough.</li>
</ul>
<h4 id="notesquestions">Notes/Questions</h4>
<ul>
<li>THe network is only trained on sequences &lt;= 30 tokens. Can we really expect it to perform well on long sequences? Long sequences may inherently have grammatical structures that cannot be observed in short sequences.</li>
<li>There's a mistake in the new activation formula, wrong time superscript, should be (t-1).</li>
</ul>
</body>
</html>
