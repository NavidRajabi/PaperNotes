<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="generative-adversarial-networks"><a href="https://arxiv.org/pdf/1406.2661v1.pdf">Generative Adversarial Networks</a>,</h1>
<!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/KeJINHjyzOU" frameborder="0" allowfullscreen></iframe> -->
<p>The offical code was released, it can be found <a href="http://www.github.com/goodfeli/adversarial">here</a>,<a href="http://cs.stanford.edu/people/karpathy/gan/">Homepage</a>. <u>The motivation of GAN is to generate good samples</u>, it can be used for: <u>start of the art image generation with Laplacian pyramids of GANs</u>. The background of GAN is the &quot;min-max&quot; game, see the demo <a href="http://cs.stanford.edu/people/karpathy/gan/">here</a>.</p>
<center>
<image src="http://cs.stanford.edu/people/karpathy/gan/gan.png" width="800"></image>
</center>
<p>In GAN, the <u>Generator (<span class="math inline">\(G\)</span>)</u> tries to generate good samples, and <u>Discriminator (<span class="math inline">\(D\)</span>)</u> tries to learn which samples come from the <u>true distribution of the input</u> and </u>which samples come from <span class="math inline">\(G\)</span></u>. At the end of training, <strong>we hope that <span class="math inline">\(D\)</span> cannot distinguish between real samples and samples from <span class="math inline">\(G\)</span></strong>.</p>
<p>The loss is <span class="math inline">\(\mathcal{L}=\min_{G}\max_{D}V(D,G)=\mathbb{E}_{x\sim p_{data}(x)}[\log D(x)]+\mathbb{E}_{z\sim p_z(z)} [ \log(1-D(G(z)))]\)</span>. During the training process, we update <span class="math inline">\(D\)</span> <span class="math inline">\(k\)</span> times for each update of <span class="math inline">\(G\)</span>. Be careful, <span class="math inline">\(\min \log(1-D(G(z)))\)</span> does poorly at the begging of training because <span class="math inline">\(D\)</span> has an advantage (easy to distinguish between samples from <span class="math inline">\(G\)</span> and samples from the real distribution), so instead <span class="math inline">\(\max \log(D(G(z)))\)</span>, which has a stronger gradient.</p>
<div class="figure">
<img src="https://dl.dropbox.com/s/qzl4x1ce1c3jvbv/GAN.png" />

</div>
<h2 id="theoretical">Theoretical</h2>
<p>The Generator <span class="math inline">\(G\)</span> implicitly defines a probability distribution <span class="math inline">\(p_g\)</span>. Then if given enough capacity and training time, <span class="math inline">\(G\)</span> can be a good estimator of <span class="math inline">\(p_{data}\)</span>. This min-max game has a global optimum for <span class="math inline">\(p_g=p_{data}\)</span>. Consider we have a optimal Discriminator <span class="math inline">\(D\)</span> for any given Generator <span class="math inline">\(G\)</span>.</p>
<p><u><font color="red"><em>Proposition/命题 1</em></font>: For <span class="math inline">\(G\)</span> fixed, the optimal Discriminator <span class="math inline">\(D\)</span> is <span class="math inline">\(D_G^{\star}(\mathbf{x})=\frac{p_{data}(\mathbf{x})}{p_{data}(\mathbf{x})+p_g(\mathbf{x})}\)</span>.</u> <em>Proof:</em> the training criterion for the Discriminator <span class="math inline">\(D\)</span> (given any Generator <span class="math inline">\(G\)</span>) is to maximize the quantity <span class="math inline">\(V(G,D)\)</span>. Then the problem become maximizing the log-likelihood for estimating the conditional probability <span class="math inline">\(P(Y=y|\mathbf{x})\)</span>. For the minmax game, we can reformulated as : <span class="math display">\[
\begin{cases}
C(G)=\max_D V(G,D)\\
C(G)=\mathbb{E}_{x\sim p_{data}}[\log D_G^{\star}(\mathbf{x})]+\mathbb{E}_{z\sim p_{z}}[\log (1-D_G^{\star}(G(\mathbf{z})))]\\
C(G)=\mathbb{E}_{x\sim p_{data}}[\log D_G^{\star}(\mathbf{x})]+\mathbb{E}_{\mathbf{x}\sim p_{g}}[\log (1-D_G^{\star}(\mathbf{x}))]\\
C(G)=\mathbb{E}_{x\sim p_{data}}[\log (\frac{p_{data}(\mathbf{x})}{p_{data}(\mathbf{x})+p_g(\mathbf{x})})]+\mathbb{E}_{x\sim p_{g}}[\log (\frac{p_{g}(\mathbf{x})}{p_{data}(\mathbf{x})+p_g(\mathbf{x})})]
\end{cases}
\]</span></p>
<p><u><font color="red"><em>Theorem 1</em></font>. The global minimum of the virtual training criterion <span class="math inline">\(C(G)\)</span> is achieved if and only if <span class="math inline">\(p_g =p_{data}\)</span>. At that point, <span class="math inline">\(C(G)\)</span> achieves the value <span class="math inline">\(-\log 4\)</span>.<u> By subtracting this expression from <span class="math inline">\(V(G)=V(D_G^{\star},G)\)</span>, we obtain: <span class="math inline">\(C(G)=-\log(4)+KL(p_{data}||\frac{p_{data}+p_g}{2}+KL(p_g||\frac{p_{data}+p_g}{2}))\)</span>.</p>
<p><u><font color="red"><em>Proposition 2. If <span class="math inline">\(G\)</span> and <span class="math inline">\(D\)</span> have enough capacity, and at each step of Algorithm 1, the discriminator is allowed to reach its optimum given <span class="math inline">\(G\)</span>, and <span class="math inline">\(p_g\)</span> is updated so as to improve the criterion C(G), then <span class="math inline">\(p_g\)</span> converges to <span class="math inline">\(p_{data}\)</span></em></font></u>.</p>
</body>
</html>
