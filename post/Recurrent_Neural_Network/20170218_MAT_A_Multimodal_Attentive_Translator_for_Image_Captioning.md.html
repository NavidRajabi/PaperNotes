<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="mat-a-multimodal-attentive-translator-for-image-captioning"><a href="">MAT: A Multimodal Attentive Translator for Image Captioning</a></h1>
<div class="figure">
<img src="http://img.blog.csdn.net/20170320191325365?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvQU1EUzEyMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" />

</div>
<h2 id="object-function">Object function</h2>
<p>The Traditional target function of image caption is :</p>
<p><span class="math display">\[
\log p(S|I)=\sum_{t=1}^N \log p(S_t|I,S_{1:t-1})
\]</span></p>
<p>In this work, they formulate image captioning as a Multimodal translation problem, that is, feeding one object at one time to RNN during encoding, and generating next word based on <strong>attention mechanism</strong> and <strong>encoded results</strong>.</p>
<p>Then their new object function is : <span class="math display">\[
\log p(S|\text{seq}(I))=\sum_{t_B=1}^{T_B}\log p(S_{t_B}|C_{t_B}, S_1, \cdots, S_{t_B-1})
\]</span> where <span class="math inline">\(C_{t_B}=\text{ATT}(H,d_{t_B-1})\)</span> , <span class="math inline">\(d_{t_B-1}\)</span> denotes the decoding hidden state of last time step. <span class="math inline">\(H=[h_1,\cdots, h_{T_A}]\)</span> denotes the encoding hidden states, w.r.t, <span class="math inline">\(h_{t_A}=RNN_{en}(seq(I)_{t_A},h_{t_{A-1}})\)</span></p>
<h2 id="main-component">Main component</h2>
<h3 id="object-feature-detection-r-fcn">Object feature detection [<a href="https://github.com/daijifeng001/R-FCN">R-FCN</a>]</h3>
<p>They leverage R-FCN trained on MS COCO dataset, using the Resnet101 CNN monarchical.</p>
<div>
<!-- htmlmin:ignore -->
<div class="mermaid">
graph LR;
style RESNET fill:#f9f,stroke:#333,stroke-width:4px;
style OUT fill:#f9f,stroke:#333,stroke-width:4px;
RESNET[Resnet 101];
RPN[Region Proposal Network]; CONV5[Last Conv feature mps of VGG16]; ROIP[ROI Pooling]; DR[Dim reduction]; CLS[Classifier]; RANK[Ranking]; SELECT[Select]

RESNET--&gt;CONVIN;CONVIN[feature maps input]--&gt;RPN; CONVIN--&gt;CONV5;
RPN--&gt;ROIP;CONV5--&gt;ROIP;ROIP--&gt;DR;DR--&gt;SELECT;CONV5--&gt;CLS;CLS--&gt;RANK;RANK--&gt;SELECT;SELECT--&gt;OUT[Output object features]
</div>
<!-- htmlmin:ignore -->
</div>
</body>
</html>
