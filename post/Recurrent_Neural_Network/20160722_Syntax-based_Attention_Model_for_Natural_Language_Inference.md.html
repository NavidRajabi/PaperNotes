<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<h1 id="syntax-based-attention-model-for-natural-language-inference"><a href="https://arxiv.org/pdf/1607.06556v1.pdf">Syntax-based Attention Model for Natural Language Inference</a></h1>
<p>TLDR: Their idea is based on that any well-formed sentence has its accompanying syntactic tree structure, which is a much rich topology. Applying attention to such topology not only exploits the underlying syntax, but also makes attention more interpretable. They integrate syntax structure into attention model which called SAT-LSTM. Compared with sequence-based attention model, their model can easily capture phrase-level alignment.</p>
<div class="figure">
<img src="https://www.dropbox.com/s/kegu30ka2x98y4h/Syntax-based%20Attention%20Model%20for%20Natural%20Language%20Inference.png?dl=1" />

</div>
</body>
</html>
