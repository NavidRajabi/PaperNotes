<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<h2 id="leveraging-visual-question-answering-for-image-caption-ranking"><a href="https://arxiv.org/abs/1605.01379">Leveraging Visual Question Answering for Image-Caption Ranking</a></h2>
<p>https://filebox.ece.vt.edu/~linxiao/ https://github.com/frkl</p>
<p>In this work they view VQA as a “feature extraction” module to extract image and caption representations. They employ these representations for the task of image-caption ranking. Each feature dimension captures (imagines) whether a fact (question-answer pair) could <a href="http://dict.youdao.com/w/plausibly/#keyfrom=dict.top">plausibly</a> be true for the image and caption. they propose score-level and representation-level fusion models to incorporate VQA knowledge in an existing state-of-the-art VQA-agnostic image caption ranking model.</p>
<div class="figure">
<img src="https://www.dropbox.com/s/our60c0vrleyo1z/Leveraging%20Visual%20Question%20Answering%20for%20Image-Caption%20Ranking.png?dl=1" />

</div>
</body>
</html>
