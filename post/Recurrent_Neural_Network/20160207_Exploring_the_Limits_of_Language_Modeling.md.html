<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<h2 id="exploring-the-limits-of-language-modeling"><a href="http://arxiv.org/abs/1602.02410">Exploring the Limits of Language Modeling</a></h2>
<p>TLDR; The authors train large-scale language modeling LSTMs on the 1B word dataset to achieve new state of the art results for single models (51.3 -&gt; 30 Perplexity) and ensemble models (41 -&gt; 24.2 Perplexity). The authors evaluate how various architecture choices impact the model performance: Importance Sampling Loss, NCE Loss, Character-Level CNN inputs, Dropout, character-level CNN output, character-level LSTM Output.</p>
<h4 id="key-points">Key Points</h4>
<ul>
<li>800k vocab, 1B words training data</li>
<li>Using a CNN on characters instead of a traditional softmax significantly reduces number of parameters, but lacks the ability to differentiate between similar-looking words with very different meanings. Solution: Add correction factor</li>
<li>Dropout on non-recurrent connections significantly improves results</li>
<li>Character-level LSTM for prediction performs significantly worse than softmax or CNN softmax</li>
<li>Sentences are not pre-processed, fed in 128-sized batches without resetting any LSTM state in between examples. Max word length for character-level input: 50</li>
<li>Training: Adagrad and learning rate of 0.2. Gradient norm clipping 1.0. RNN unrolled for 20 steps. Small LSTM beats state of the art after just 2 hours training, largest and best model trained for 3 weeks on 32 K40 GPUs.</li>
<li>NC vs. Importance Sampling: IC is sufficient</li>
<li>Using character-level CNN word embeddings instead of a traditional matrix is sufficient and performs better</li>
</ul>
<h4 id="notesquestions">Notes/Questions</h4>
<ul>
<li>Exact hyperparameters in table 1 are not clear to me.</li>
</ul>
</body>
</html>
