<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h2 id="compositional-memory-for-visual-question-answering"><a href="http://arxiv.org/abs/1511.05676">Compositional Memory for Visual Question Answering</a></h2>
<p>Visual Question Answering (VQA) emerges as one of the most fascinating topics in computer vision recently. Many state of the art methods naively use holistic visual features with language features into a Long Short-Term Memory (LSTM) module, neglecting the sophisticated interaction between them. This coarse modeling also blocks the possibilities of exploring finer-grained local features that contribute to the question answering dynamically over time.</p>
<p>This paper addresses this fundamental problem by directly modeling the temporal dynamics between language and all possible local image patches. When traversing the question words sequentially, our end-to-end approach explicitly fuses the features associated to the words and the ones available at multiple local patches in an attention mechanism, and further combines the fused information to generate dynamic messages, which we call episode. We then feed the episodes to a standard question answering module together with the contextual visual information and linguistic information. Motivated by recent practices in deep learning, we use auxiliary loss functions during training to improve the performance. Our experiments on two latest public datasets suggest that our method has a superior performance. Notably, on the DAQUAR dataset we advanced the state of the art by 6<span class="math inline">\(\%\)</span>, and we also evaluated our approach on the most recent MSCOCO-VQA dataset.</p>
<h1 id="introduction">Introduction</h1>
<p>Given an image and a question, the goal of Visual Question Answering (VQA) is to directly infer the answer(s) automatically from the image. This is undoubtedly one of the most interesting, and arguably one of the most challenging, topics in computer vision in recent times.</p>
<p>Almost all state-of-the-art methods use predominantly holistic visual features in their systems. For example, Malinowski et.al. (<a href="">&quot;Ask your neurons: A neural-based approach to answering questions about images&quot;</a>) used the concatenation of linguistic feature and visual feature extracted by a Convolutional Neural Network (CNN), and Ren et.al. (<a href="">&quot;Exploring models and data for image question answering&quot;</a>) considered the visual feature as the first word to initialize the sequential learning.</p>
<p>While the use of holistic approach is straightforward and convenient, it is, however, debatably problematic. For example, in the VQA problems many answers are directly related to the contents of some image regions. Therefore, it is dubious if the holistic features are rich enough to provide the information only available at regions. Also, it may hinder the exploration of finer-grained local features for VQA.</p>
<p>In this paper we propose a Compositional Memory for an end-to-end training framework. Our approach takes the advantage of the recent progresses in image captioning (<a href="">&quot;Deep visual-semantic alignments for generating image descriptions&quot;</a>), natural language processing (<a href="">&quot;Ask me anything: Dynamic memory networks for natural language processing&quot;</a>), and computer vision to advance the study of the VQA. Our goal is to fuse local visual features and the linguistic information over time, in a Long Short-Term Memory (LSTM) based framework. The fused information, which we call &quot;episodes&quot;, characterizes the interaction and dynamics between vision and language.</p>
<p>Explicitly addressing the interaction between question words and local visual features has a number of advantages. To begin with, regions provide rich info towards capturing the dynamics in question. Intuitively, parts of an image serve as &quot;candidates&quot; that may have varying importance at different time when parsing a question sentence. Recent study of image captioning (<a href="">&quot;Show, attend and tell: Neural image caption generation with visual attention&quot;</a>), a closely related research topic, suggests that visual attention mechanism is very important in generating good descriptions. Obviously, this idea will also improve the accuracy in question answering.</p>
<p>Going deeper, this candidacy is closely related to the concept of semantic &quot;facts&quot; in reasoning. For example, one often begins to dynamically search useful local visual evidences at the same time when (s)he reads words. The use of facts has been explored in the natural language processing recently (<a href="">&quot;Ask me anything: Dynamic memory networks for natural language processing&quot;</a>), but this useful concept cannot be explored without local visual information in computer vision.</p>
<p>While the definition of &quot;visual facts&quot; is still elusive, we can approach the problem through modeling interactions between vision and language. This &quot;sensory interaction&quot; plays a phenomenal role in the information processing and reasoning. It has a significant meaning in memory study as well. Eichenbaum and Cohen argued that part of the human memory needs to be modeled as a form of relationship between spatial, sensory and temporal information (<a href="">&quot;Memory, Amnesia, and the Hippocampal System&quot;</a>).</p>
<p>%The above reasons makes it compelling to use regional information as the building blocks for VQA, however, they also bring us two major problems: 1) where to extract, and 2) what to extract?</p>
<p>%The first problem (where) is a chicken and egg problem: we know the answer immediately if we know where to look at. %To bypass this problem, we propose to use a fixed number of regions proposals. As a result, instead of dynamically finding important regions at each time, we opt to modify the information based on the dynamics in the previous processing. %This remedy naturally allows us to solve the second question (what). %Our solution is then rooted on the study of the dynamics information extracted from local image patches, which we call &quot;Visual Facts&quot; in this paper.</p>
<p>Specifically, our method traverses the words in a question sequentially, and explicitly fuses the linguistic and the ones available at local patches to episodes. An attention mechanism is used to re-weight the importance of the regions (<a href="">&quot;Show, attend and tell: Neural image caption generation with visual attentio&quot;</a>). The fused information is fed to a dynamic network to generate episodes. We then feed the episodes to a standard question answering module together with the contextual visual information and linguistic information.</p>
<p>The use of local features inevitably leads to the quest about region selection. In principle, the regions can be 1) patches generated by object proposals, such as those obtained by edgebox (<a href="">&quot;Edge boxes: Locating object proposals from edges&quot;</a>) and faster-RCNN (<a href="">&quot;Faster {R-CNN}: Towards real-time object detection with region proposal network&quot;</a>), and 2) overlapping patches that cover most important contents in image. In this paper, we choose the latter and use the features of the last convolutional layer in the CNNs.</p>
<p>Our experiments on two latest public datasets suggest that our method outperforms the other state of the art methods. We tested on the DAQUAR (<a href="">&quot;A multi-world approach to question answering about real-world scenes based on uncertain input&quot;</a>) and MSCOCO-VQA (<a href="">&quot;{VQA:} visual question answering&quot;</a>). Notably, on the DAQUAR dataset we advanced the state of the art by 6<span class="math inline">\(\%\)</span>. We also compared a few variants of our method and demonstrated the usefulness of the Compositional Memory for VQA. We further verified our idea on the latest MSCOCO-VQA dataset.</p>
<p>The main contributions of the paper are: 1. We present an end-to-end approach that explores the local fine grained visual information for VQA tasks, 2. We develop a Compositional Memory that explicitly models the interactions of vision and language, 3. Our method has a superior performance and it outperforms the state of the art methods.</p>
<h1 id="related-work">Related Work</h1>
<h4 id="cnn-rnn-and-lstm">CNN, RNN, and LSTM</h4>
<p>Recently, deep learning has achieved great success on many computer vision tasks. For example, CNN has set records on standard object recognition benchmarks (<a href="">&quot;Imagenet classification with deep convolutional neural networks&quot;</a>). With a deep structure, CNN can effectively learn complicated mappings from raw images to the target, which requires less domain knowledge compared to handcrafted features and shallow learning frameworks.</p>
<p>Recurrent Neural Networks (RNN) have been used for modeling temporal sequences and gained attention in speech recognition (<a href="">&quot;Speech recognition with deep recurrent neural networks&quot;</a>), machine translation (<a href="">&quot;Neural machine translation by jointly learning to align and translate&quot;</a>), image captioning (<a href="">&quot;Deep visual-semantic alignments for generating image descriptions&quot;</a>). The recurrent connections are feedback loops in the unfolded network, and because of these connections, RNNs are suitable for modeling time series with strong nonlinear dynamics and long time correlations. The traditional RNN is hard to train due to the vanishing gradient problem, $ the weight updates computed via error backpropagation through time may become very small.</p>
<p>Long Short Term Memory model (<a href="">&quot;Long short-term memory&quot;</a>) has been proposed as a solution to overcome these problems. The LSTM architecture uses memory cells with gated access to store and output information, which alleviates the vanishing gradient problem in backpropagation over multiple time steps. Specifically, in addition to the hidden state, the LSTM also includes an input gate, a forget gate, an output gate, and the memory cell. In this architecture, input gate and forget gate are sigmoidal gating functions, and these two terms learn to control the portions of the current input and the previous memory that the LSTM takes into consideration for overwriting the previous state. Meanwhile, the output gate controls how much of the memory should be transferred to the hidden state. These mechanisms allow LSTM networks to learn temporal dynamics.</p>
<p>%In this architecture, <span class="math inline">\(i_t\)</span> and <span class="math inline">\(f_t\)</span> are sigmoidal gating functions, and these two terms learn to control the portions of the current input and the previous memory that the LSTM takes into consideration for overwriting the previous state. Meanwhile, the output gate <span class="math inline">\(o_t\)</span> controls how much of the memory should be transferred to the hidden state. These mechanisms allow LSTM networks to learn temporal dynamics with long time constants.</p>
<h4 id="language-and-vision">Language and vision</h4>
<p>The effort of combining language and vision attracts a lot of attention recently. Image captioning and VQA are two most intriguing problems.</p>
<p>Question answering (QA) is a classical problem in natural language processing (<a href="">&quot;Ask me anything: Dynamic memory networks for natural language processing&quot;</a>). When images are involved, the goal of VQA is to infer the answer of a question directly from the image (<a href="">&quot;{VQA:} visual question answering&quot;</a>). Multiple questions and answers can be associated to the same image during training.</p>
<p>It has been shown that VQA can borrow the idea from image captioning. Being a related area, image captioning also uses RNN for sentence generation (<a href="">&quot;Long-term recurrent convolutional networks for visual recognition and description&quot;</a>). Attention mechanism is recently adopted in image captioning and proves to be a useful component (<a href="">&quot;Show, attend and tell: Neural image caption generation with visual attention&quot;</a>).</p>
<h4 id="lstm-for-vqa">LSTM for VQA</h4>
<p>Because a VQA system needs to process language and visual information simultaneously, most recent work adopted the LSTM in their approaches. A typical LSTM-VQA uses holistic image features extracted by CNNs as &quot;visual words&quot;.</p>
<p>The visual word features are used either as the first or at the end of question sequence (<a href="">&quot;Exploring models and data for image question answering&quot;</a>) or they are concatenated together with question word vectors into a LSTM (<a href="">&quot;Are you talking to a machine? dataset and methods for multilingual image question answering&quot;</a>) .</p>
<p align="center">
<img src="https://dl.dropboxusercontent.com/s/raiijwp1pnd9ezk/basiclstm-vqa.jpg?dl=0" width="500" >
</p>
<p>This LSTM-VQA framework is straightforward and useful. However, treating the feature interaction as feature vector concatenation lacks the capability that explicitly extracts finer-grained information. As we discussed before, details of facts may be neglected if global visual features are used. This leads to the quest for more effective information fusion model for language and image in VQA problems.</p>
<h1 id="our-approach">Our Approach</h1>
<p>We present our approach in this section. First, we present our model overview. Then, we discuss the technical details and explain the training process.</p>
<h2 id="our-end-to-end-vqa-model">Our End-to-End VQA model</h2>
<h3 id="overview-and-notation">Overview and notation</h3>
<p>Compared to the basic LSTM approach for VQA , We made two major improvements of the model. The diagram of the network is shown in Figure below.</p>
<p align="center">
<img src="https://dl.dropboxusercontent.com/s/lkfgjmiyn2nuep7/dynamicmemorynetwork.jpg?dl=0" width="500" >
</p>
<p>The first, and the most important addition, is a Compositional Memory. It reasons over those input features to produce an image-level representation for answer module. It reflects an experience over image contents.</p>
<p>The second addition is the LSTM module for parsing (factorizing) the question. It provides input for both the Compositional Memory and the question answering LSTM. In the experiment we will show the importance of this module for the VQA tasks.</p>
<p>In part, this implementation is aligned with the findings in cognitive neuroscience. It is well known that semantic (e.g. , visual features and classifiers) and episodic memory (e.g. , temporal questioning sentence) together make up the declarative memory of human beings, and the interactions among them become the key in representation and reasoning (<a href="">&quot;Episodic and semantic memory&quot;</a>). Our model captures this interaction naturally.</p>
<h3 id="compositional-memory">Compositional Memory</h3>
<p align="center">
<img src="https://dl.dropboxusercontent.com/s/df4qi0jsdbi2653/compositelstmunit.jpg?dl=0" width="500" >
</p>
<p>We present the details of our Compositional Memory in this section. This unit consists of a number of Region-LSTM and <span class="math inline">\(\alpha\)</span> gates. All these Region-LSTM share the same set of parameters, and their results are combined to generate an episode at each time step.</p>
<p>The region-LSTMs are mainly in charge of processing input image region contents in parallel. It dynamically generates language-embedded visual information for each region, conditioned on previous episodes, local visual feature, and current question word. The state, gates and cells of of each Region-LSTM are updated as follows: <span class="math display">\[
\begin{cases}
 &amp; i_t ^k = \sigma(W_{qi}q_t + W_{hi}h_{t-1} + W_{xi} X_k + b_i)  \\
 &amp; f_t ^k= \sigma(W_{qf}q_t + W_{hf}h_{t-1} + W_{xf} X_k + b_f) \\
 &amp; o_t ^k= \sigma(W_{qo}q_t + W_{ho}h_{t-1} + W_{xo} X_k + b_o) \\
 &amp; g_t ^k= \tanh(W_{qg}q_t + W_{hg}h_{t-1} + W_{xg} X_k + b_g) \\
 &amp; c_t ^k= f_t ^k \odot c_{t-1} ^k + i_t ^k \odot g_t ^k\\
 &amp; m_t ^k= o_t ^k \odot \tanh(c_t ^k)
\end{cases}
\]</span> where the hidden state <span class="math inline">\(h_t\)</span> denotes an episode at every time step, <span class="math inline">\(q_t\)</span> is the language information (e.g. features generated by <span class="math inline">\(\textbf{word2vec}\)</span>), and <span class="math inline">\(X_k\)</span> is the <span class="math inline">\(k^{th}\)</span> regional CNN features. <span class="math inline">\(m_t\)</span> is the output of region-LSTM <span class="math inline">\(t\)</span>. Please note that the superscripts are omitted in the above notations for simplicity.</p>
<p>In Region-LSTM , <span class="math inline">\(c_t\)</span> denotes the memory cell, <span class="math inline">\(g_t\)</span> denotes an input modulation gate. <span class="math inline">\(i_t\)</span> and <span class="math inline">\(f_t\)</span> are input and forget gates, which control the portions of the current input and the previous memory that LSTM takes into consideration. <span class="math inline">\(o_t\)</span> is output gate that determines how much of the memory to transfer to the hidden state. <span class="math inline">\(\odot\)</span> is the element-wise multiplication operation, and <span class="math inline">\(\sigma()\)</span> and <span class="math inline">\(tanh()\)</span> denote the sigmoidal and tanh operations, respectively. These mechanisms allow LSTM to learn long-term temporal dynamics.</p>
<p>The implementation of Region-LSTM is similar to that of traditional LSTM. However, the important differences lie in the parallel strategy that all parameters (<span class="math inline">\(W_{q*}, W_{h*}, W_{x*}, b_{*}\)</span>) are shared across different regions. Please note that each Region-LSTM has its own gate and cell state, respectively.</p>
<p>The <span class="math inline">\(\alpha\)</span> gate is also conditioned on previous episode <span class="math inline">\(h_{t-1}\)</span>, region feature <span class="math inline">\(X_k\)</span>, and current input language feature <span class="math inline">\(q_t\)</span>. It returns a single scalar for each region. This gate is mainly used for weighted combination of region messages for generating episodes, which are dynamically pooled into image-level information. Similar to <span class="math inline">\(W_{zq}, W_{zh}, W_{zx}, b_z, W_\alpha, b_\alpha\)</span>, the parameters of <span class="math inline">\(\alpha\)</span> gate, are also shared across regions. At every time step <span class="math inline">\(t\)</span>, <span class="math inline">\(\alpha\)</span> gate dynamically generates values <span class="math inline">\(\alpha _k ^t\)</span> for <span class="math inline">\(k^{th}\)</span> region. <span class="math display">\[
\begin{array}{l}
{z_k ^t} = \tanh \left( {{W_{zq}}{q_t} + {W_{zh}}{h_{t - 1}} + {W_{zx}}{x_k} + {b_z}} \right)\\
{\alpha _k ^t} = \sigma \left( {{W_\alpha }{z_k ^t} + {b_\alpha }} \right)
\end{array}
\]</span></p>
<p>In order to summarize region-level information to an image-level feature, we employ a modified pooling mechanism of gated recurrent style (<a href="">&quot;Empirical evaluation of gated recurrent neural networks on sequence modeling&quot;</a>). The episode <span class="math inline">\(h_t\)</span> acts as dynamic feature as well as hidden state of Compositional Memory unit. It is updated to renew the input information of Region-LSTMs together with language input at every time step.</p>
<p><span class="math display">\[
\begin{array}{l}
 \beta  = 1 - \frac{1}{K}\sum\limits_k {{\alpha _k}}\\
 {h_t} =  \beta {h_{t - 1}} + \frac{1}{K} \sum\limits_k {{\alpha _k ^t}m_t^k}
\end{array}
\]</span> where <span class="math inline">\(K\)</span> is the total number of regions.</p>
<p>One of the advantages of Compositional Memory is that it goes beyond traditional static image feature. The unit incorporates both the merits of LSTM and attention mechanism, and thus it is suitable to generate dynamic episodic messages for visual question answering applications.</p>
<h3 id="language-lstm">Language LSTM</h3>
<p>We use a Language LSTM to process linguistic inputs. We consider this representation as &quot;factorization of questions&quot;, which captures the recurrent relations of question word sequence, and stores semantic memory information about the questions. This strategy for processing language has also been proven to be important in image captioning (<a href="">&quot;Long-term recurrent convolutional networks for visual recognition and description&quot;</a>).</p>
<h3 id="other-components">Other components</h3>
<h4 id="answer-generation">Answer generation</h4>
<p>The answer generation module takes dynamic episodes, together with language and static visual context generated by CNNs to generate the answers in a LSTM framework.</p>
<h4 id="cnn">CNN</h4>
<p>Our approach is compatible with all the major CNN network, such as AlexNet (<a href="">&quot;Imagenet classification with deep convolutional neural networks&quot;</a>) and GoogLeNet (<a href="">&quot;Going deeper with convolutions&quot;</a>). In each CNN, we use the last convolution layer as region selections. For example, GoogleNet, we use the <span class="math inline">\(1024 \times 7 \times 7\)</span> feature map from the inception_5b/output. This means that Compositional Memory operates on 49 regions, each of which is represented by 1024-dim feature vector. Following the current practice, we used the output of the last CNN layer as visual context.</p>
<h2 id="training">Training</h2>
<p>Generally, there are three types of the VQA tasks: 1) Single Word, 2) Multiple Words (or called &quot;Open-ended&quot;), and 3) Multiple Choice. The single word category restricts the answer to have only one single word. Multiple Words VQA is an open-ended task, where the length of answer is not limited and the VQA system needs to sequentially generate possible answer words until generating an answer-ending mark. The multiple choice task refers to select most probable answer from a set of answer candidates.</p>
<p>Among these three, the &quot;Open-ended&quot; VQA task is the most difficult one, thus we chose this category to demonstrate the performance of our approach. In this section, we briefly present the training procedure, the loss function, and other implementation details.</p>
<h3 id="protocol">Protocol</h3>
<p>During the training of our VQA system, CNN and LSTMs are jointly learned in an end-to-end way. The unfolded network of our proposed model is shown in Figure below.</p>
<p align="center">
<img src="https://dl.dropboxusercontent.com/s/s98uq56g0swui0g/unfoldednetwork.jpg?dl=0" width="700" >
</p>
<p>In this Open-ended category of VQA, questions may have multiple word answers. We consequently decompose the problem to predict a set of answer words <span class="math inline">\(A = \left\{ {{a_1},{a_2},...,{a_M}} \right\}\)</span>, where <span class="math inline">\(a_i\)</span> are words from a finite vocabulary <span class="math inline">\(\Omega &#39;\)</span> and M is the number of answer words for a given question and image. To deal with open-ended VQA task, we add an extra token <span class="math inline">\(\langle EOA \rangle\)</span> into the vocabulary <span class="math inline">\(\Omega = \Omega &#39;\cup \{\langle EOA \rangle\}\)</span>. The <span class="math inline">\(\langle EOA \rangle\)</span> indicates the end of the answer sequence. Therefore, we formulate the prediction procedure recursively as: <span class="math display">\[
{\widehat a_t} = \arg \max p(a|X,q,{\widehat A_{t - 1}};\vartheta )
\]</span> where <span class="math inline">\({\widehat A_{t - 1}}\)</span> is the set of previously predicted answer words, with <span class="math inline">\({\widehat A_{0}} = \{\}\)</span> at start. The prediction procedure is terminated when <span class="math inline">\({\widehat a_t} = \langle EOA \rangle\)</span>.</p>
<p>Feed the VQA system with a question as a sequence of words, $ <span class="math inline">\(q = [{q_1},{q_2},...,{q_{n - 1}},\left[\kern-0.15em\left[ ? \right]\kern-0.15em\right]]\)</span>, where <span class="math inline">\(\left[\kern-0.15em\left[ ? \right]\kern-0.15em\right]\)</span> encodes the end of question. In the training phase, we augment the question word sequence with the corresponding ground truth answer sequence <span class="math inline">\(a\)</span>, <span class="math inline">\(\textbf{i.e.}\)</span> <span class="math inline">\(\widehat q: = [q,a]\)</span>. During the test phase, at <span class="math inline">\(t^{th}\)</span> time step we augment question <span class="math inline">\(q\)</span>, with previously predicted answer words <span class="math inline">\({\widehat q_t}: = [q,{\widehat a_{1,...,t - 1}}]\)</span>.</p>
<h3 id="loss-function">Loss function</h3>
<p>All the parameters are jointly learned with cross-entropy loss. The output predictions that occur before the ending question mark <span class="math inline">\(\left[\kern-0.15em\left[ ? \right]\kern-0.15em\right]\)</span> are excluded from the loss computation, so that the model is solely penalized based on the predicted answer words.</p>
<p>Motivated by the recent success of GoogLeNet, We adopt a multi-task training strategy for learning the parameters of our network. Specifically, in additional to the question answering LSTM, we add a &quot;Language Only&quot; loss layer on the Language LSTM, and an &quot;Episode Only&quot; loss layer on the Compositional Memory. These two auxiliary loss functions are added during training to improve the performance, and they are removed during testing.</p>
<h3 id="implementation">Implementation</h3>
<p>We implemented our end-to-end VQA network using Caffe. The CNN models are pre-trained, and then fine-tuned in our recurrent network training. The source code of our implementation will be available in public.</p>
<h1 id="experiments">Experiments</h1>
<p>We test our approach on two large data sets, namely, DAQUAR (<a href="">&quot;A multi-world approach to question answering about real-world scenes based on uncertain input&quot;</a>) and MSCOCO-VQA. In the experiments on these two data sets, our method outperforms the state of the arts in different well recognized metrics.</p>
<h2 id="datasets">Datasets</h2>
<p><span class="math inline">\(\textbf{DAQUAR}\)</span> contains 12,468 human question answer pairs on 1,449 images of indoor scene. The training set contains 795 images and 6,793 question answer pairs, and the testing set contains 654 images and 5,675 question answer pairs.</p>
<p>We run experiments for the full dataset with all classes, instead of their &quot;reduced set&quot; where the output space is restricted to only 37 object categories and 25 test images in total. This is because the full dataset is much more challenging and the results are more meaningful in statistics. The performance is reported using the &quot;Multiple Answers&quot; category but the answers are generated using open-ended approach.</p>
<p><span class="math inline">\(\textbf{MSCOCO-VQA}\)</span> is the latest VQA dataset that contains open-ended questions about images. This dataset contains 369,861 questions and 3,698,610 ground truth answers based on 123,287 MSCOCO images. These questions and answers are sentence-based and open-ended. The training and testing split follows MSCOCO-VQA official split. Specifically, we use 82,783 images for training and 40,504 validation images for testing.</p>
<h2 id="evaluation-criteria">Evaluation criteria</h2>
<h4 id="daquar">DAQUAR</h4>
<p>On the DARQUAR dataset, we use the Wu-Palmer Similarity (WUPS) (<a href="">&quot;Verbs semantics and lexical selection&quot;</a>) score at different thresholds for comparison.</p>
<p><span class="math display">\[
\frac{1}{N}\sum\limits_{i = 1}^N {\min \{ {\prod\limits_{a \in {A_i}} {\mathop {\max }\limits_{t \in {T_i}} \mu ( {a,t} )} ,\prod\limits_{t \in {T_i}} {\mathop {\max }\limits_{a \in {A_i}} \mu ( {t,a} )} } \}}
\]</span></p>
<p>where, <span class="math inline">\(A_i\)</span> is the predicted answer set of question . <span class="math inline">\(T_i\)</span> is its ground truth answer set. <span class="math inline">\(\mu\)</span> is membership measure, for instance, Wu-Palmer similarity(WUP). <span class="math inline">\(N\)</span> is the total num of questions. There are three metrics: Standard Metric, Average Consensus Metric and Min Consensus Metric. The Standard Metric is the basic score. The last two metrics are used to study the effects of consensus in question answering tasks. Please refer to (<a href="">&quot;A multi-world approach to question answering about real-world scenes based on uncertain input&quot;</a>) for the details.</p>
<p><span class="math display">\[
\begin{cases}
&amp; \frac{1}{N}\sum\limits_{i = 1}^N {\sum\limits_{k = 1}^K {\min \{ {\prod\limits_{a \in {A_i}} {\mathop {\max }\limits_{t \in {T_i}^k} \mu ( {a,t} )} ,\prod\limits_{t \in {T_i}^k} {\mathop {\max }\limits_{a \in {A_i}} \mu ( {t,a} )} } \}} } \label{Eval:AverageConsensus}\\
&amp; \frac{1}{N}\sum\limits_{i = 1}^N {\mathop {\max }\limits_{k = 1}^K ( {\min \{ {\prod\limits_{a \in {A_i}} {\mathop {\max }\limits_{t \in {T_i}^k} \mu ( {a,t} )} ,\prod\limits_{t \in {T_i}^k} {\mathop {\max }\limits_{a \in {A_i}} \mu ( {t,a} )} } \}} )} \label{Eval:min-consensus}
\end{cases}
\]</span> where, <span class="math inline">\(T_i ^k\)</span> is the k-th possible human answer corresponding to the k-th interpretation of question.</p>
<h4 id="mscoco-vqa">MSCOCO-VQA</h4>
<p>On the MSCOCO-VQA dataset, we use the evaluation criteria provided by the organizers. For the open-ended tasks, the generated answers are evaluated using accuracy metric. It is computed as the percentage of answers that exactly agree with the ground truth provided by human.</p>
<h2 id="experimental-settings">Experimental settings</h2>
<p>We choose AlexNet and GoogLeNet in our experiments, respectively. For AlexNet, the region features are from Pool5 layer. For GoogLeNet we use the inception_poo5b/output layer. That means our Compositional Memory processes flattened <span class="math inline">\(36\)</span> regions for AlexNet, and <span class="math inline">\(49\)</span> for GoogLeNet.</p>
<p>In our current implementation, the parameters of region-LSTMs and <span class="math inline">\(\alpha\)</span> gates in Compositional Memory are shared across regions, therefore the computational burden is minimal. However, as regions have to store their respective memory states, the storage space is more than traditional LSTM. In our experiments, the dimension of region inputs <span class="math inline">\(d\)</span> is 1024 for GoogLeNet and 256 for AlexNet. The dimension of the episodes is set to 200 for all LSTMs (including our Compositional Memory) on the DAQUAR dataset and the MSCOCO-VQA.</p>
<p>We used separate dictionaries for training and testing. To generate the dictionary, we first remove the punctuation marks, except those used in timing and measurements, separate them and convert them to lower cases.</p>
<p>We stop our training procedure after 100,000 iterations. The base learning rate is set to be 0.01. During training, it takes about 0.4 sec for one iteration on GTX Nvidia 780.</p>
<h2 id="results-on-daquar">Results on DAQUAR</h2>
<h3 id="comparisons-with-state-of-the-art-methods">Comparisons with state-of-the-art methods</h3>
<p>We compare our proposed model with (<a href="">&quot;A multi-world approach to question answering about real-world scenes based on uncertain input&quot;</a>). Because they used different CNN methods, we tested both AlexNet and GoogLeNet. The performance on &quot;Multiple Answers&quot; (&quot;Open-ended&quot;) category are shown in Table~.</p>
<p>The statistical results shows that the performance of our model substantially is better than the state of the art. On the WUPS$@$0.9, our method is <span class="math inline">\(6\%\)</span> higher (from <span class="math inline">\(23.31\%\)</span> to <span class="math inline">\(29.77\%\)</span>). When we lower the threshold in the WUPS, we are <span class="math inline">\(5.25\%\)</span> superior than the state of the art.</p>
<p>In other two measurements, where &quot;consensus&quot; is calculated on the answers from multiple subjects, our method also outperforms the state of the art <span class="math inline">\(2\%\)</span> to <span class="math inline">\(7\%\)</span>. This further confirms our system is more accurate and robust.</p>
<p>We also find that our method has better performance when GoogLeNet is used in our framework, although the difference is marginally noticeable.</p>
<h3 id="effectiveness-of-compositional-memory-module">Effectiveness of Compositional Memory module</h3>
<p>We further present our study on the effectiveness of Compositional Memory and the Language LSTM in our VQA system. Specifically, we show the comparison in performance when we toggle on and off these components.</p>
<p>We consider the configuration where all modules are used as the &quot;full model&quot;, and also name the configuration of traditional approach as &quot;baseline&quot;. We then introduce three variants, where only language is used (&quot;Factorized Language Only&quot;), only Compositional Memory is used (&quot;Episodes Only&quot;), and both are used together (&quot;Language+Episodes&quot;).</p>
<h2 id="results-on-mscoco-vqa">Results on MSCOCO-VQA</h2>
<p>Compared to DAQUAR, MSCOCO-VQA is the latest VQA dataset. It is much larger and contains more scenes and question types that are not covered by DARQUAR.</p>
<p>Possibly because this is the latest outcome, there are different ways of evaluating the performances and reporting the results. For example, while the measurement of accuracy is well defined, the evaluation protocols are not standardized. Some practitioners use the organizer's previous release for training and validating, and further split the validation sets. Only until recently the organizers release their  set online, however, there are still many ways of handling the input. For example, the official version (<a href="">&quot;{VQA:} visual question answering&quot;</a>) selects the most frequent <span class="math inline">\(1000\)</span> answers, which covers only <span class="math inline">\(82.67\%\)</span> of the answer set. Different selections of dictionary can lead to fluctuations in the accuracy. Finally, the tokenizers used in different practitioners may lead to other uncertainties in accuracy.</p>
<p>Due to the above concerns, we conclude that it is in the early stage of the evaluation, and would like to clearly outline our practices when readers examine the numbers.</p>
<ul>
<li>We used a naive tokenizer as specified in Sec. .</li>
<li>We used 13,880 words appeared in the training + validation answer set as our answer dictionary.</li>
<li>We report results on both the  and the full validation set.</li>
</ul>
<p>We first show results of our method in Fig. ~. Compared to Fig. , one can see that MSCOCO-VQA is more diversified. More results are shown in Supplementary Materials.</p>
<h3 id="statistical-results">Statistical results</h3>
<p>MSCOCO-VQA is grouped to a number of categories based on the types of the questions, and the types of answers. We show the statistics on both categories in this section.</p>
<h4 id="answer-type">Answer type</h4>
<p>We report the overall accuracy and those of different answer types using both  and the full validation set . Please note that we used a larger answer dictionary, which means potentially it is more difficult to deliver correct answers, but still our method achieved similar performance of the state of the art.</p>
<p>One can notice that the accuracy of simple answer type (e.g. &quot;yes/no&quot;) is very high, but the accuracies drop significantly when the answers become more sophisticated. This indicates the potential issues and directions of our method.</p>
<h4 id="question-type">Question type</h4>
<p>We use validation set to report the accuracy of our method when question type varies (Figure~). The colored bar chart and sorted according to the accuracy, with the numbers displayed next to the bars.</p>
<p>It is interesting to see a significant drop when the questions ascend from simple forms (e.g. , &quot;is there?&quot;) to complicated ones (e.g. , &quot;what&quot;,&quot;how&quot;). This suggest that a practical VQA system needs to take this prior into consideration.</p>
<h1 id="conclusion">Conclusion}</h1>
<p>In this paper we propose to use the Compositional Memory as the core element in the VQA. Our end-to-end approach is capable of dynamically extracting local the features. The Long Short-Term Memory (LSTM) based approach fuses image regions and language, and generates the episodes that is effective for high level reasoning. Our experiments on the latest public datasets suggest that our method has a superior performance.</p>
</body>
</html>
