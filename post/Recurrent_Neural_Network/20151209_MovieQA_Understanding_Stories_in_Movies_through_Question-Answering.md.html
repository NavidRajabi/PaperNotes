<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="movieqa-understanding-stories-in-movies-through-question-answering"><a href="http://arxiv.org/pdf/1512.02902v1.pdf">MovieQA: Understanding Stories in Movies through Question-Answering</a></h1>
<h1 id="abstract">abstract</h1>
<p>We introduce the MovieQA dataset which aims to evaluate automatic story comprehension from both video and text. The dataset consists of 7702 questions about 294 movies with high semantic diversity. The questions range from simpler &quot;Who&quot; did &quot;What&quot; to &quot;Whom&quot;, to &quot;Why&quot; and &quot;How&quot; certain events occurred. Each question comes with a set of five possible answers; a correct one and four deceiving answers provided by human annotators. Our dataset is unique in that it contains multiple sources of information -- full-length movies, plots, subtitles, scripts and for a subset DVS from (<a href="">&quot;A Dataset for Movie Description&quot;</a>). We analyze our data through various statistics and intelligent baselines. We further extend existing QA techniques to show that question-answering with such open-ended semantics is hard. We plan to create a benchmark with an active leader board, to encourage inspiring work in this challenging domain.</p>
<h1 id="introduction">Introduction</h1>
<p>Fast progress in Deep Learning as well as a large amount of available labeled data has significantly pushed forward the performance in many visual tasks such as image tagging, object detection and segmentation, action recognition, and image/video captioning. We are steps closer to applications such as assistive solutions for the visually impaired, or cognitive robotics, which require a holistic understanding of the visual world by reasoning about all these tasks in a common framework. However, a truly intelligent machine would ideally also infer high-level semantics underlying human actions such as motivation, intent and emotion, in order to react and, possibly, communicate appropriately. These topics have only begun to be explored in the literature (<a href="">&quot;Inferring the Why in Images,Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books&quot;</a>), as well as have the ability to communicate with the humans in natural language. %, and to react appropriately in a variety of situations.</p>
<p>A great way of showing one's understanding about the scene is to be able to answer any question about it (<a href="">&quot;A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input1&quot;</a>). This idea gave rise to several question-answering datasets which provide a set of questions for each image along with multi-choice answers. These datasets are either based on RGB-D images (<a href="">&quot;A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input&quot;</a>) or a large collection of static photos such as Microsoft COCO (<a href="">&quot;VQA,Visual Madlibs: Fill in the blank Image Generation and Question Answering&quot;</a>). The types of questions typically asked are <span class="math inline">\(\mathbf{what}\)</span> is there and <span class="math inline">\(\mathbf{where}\)</span> is it, what attributes an object has, what is its relation to other objects in the scene, and <span class="math inline">\(\mathbf{how many}\)</span> objects of certain type are present.</p>
<p>While these questions verify the holistic nature of our vision algorithms, there is an inherent limitation in what can be asked about a static image. High-level semantics about actions and their intent is mostly lost and can typically only be inferred from temporal, possibly life-long visual observations.</p>
<p>In this paper, we argue that question-answering about movies ... Movies provide us with snapshots from people's lives that link into stories, allowing an experienced human viewer to get a high-level understanding of the characters, their actions, and the motivations behind them. % as well as the emotions they are feeling by taking them. %We believe that a machine able to answer a diverse set of questions about stories of such complexity, demonstrates both deep understanding</p>
<p>Our goal here is to create a question-answering dataset that will push such automatic semantic understanding to the next level, required to truly understand stories of such complexity. Our goal is to create a question-answering dataset to evaluate machine comprehension of both, complex videos such as movies and their accompanying text. We believe that this data will help push automatic semantic understanding to the next level, required to truly understand stories of such complexity.</p>
<p>This paper introduces MovieQA, a large-scale question-answering dataset about movies. Our dataset consists of 7702 questions about 294 movies with high semantic diversity. For 34 of these movies, we have timestamp annotations indicating the location of the question in the video. The questions range from simpler Who did What to Whom that can be solved by vision alone, to Why and How something happened, questions that can only be solved by exploiting both the visual information and dialogs. Each question has a set of five possible answers; one correct and four deceiving answers provided by the human annotators. Our dataset is unique in that it contains multiple sources of information: full-length movies, subtitles, scripts, and plots. For a subset of our movies, DVS (described video for the blind) is also available from (<a href="">&quot;A Dataset for Movie Description&quot;</a>). We analyze our data through various statistics and intelligent baselines that mimic how different &quot;students&quot; would approach the quiz. We further extend existing QA techniques to work with our data and show that question-answering with such open-ended semantics is hard.</p>
<p>We plan to create an <span class="math inline">\(\mathbf{online benchmark}\)</span>, encouraging inspiring work in this challenging domain. We expect this benchmark to be online in early 2016 . It will have 15,000 questions and 75,000 answers, with the test set ground-truth for 5,000 questions held-out. Various sub challenges will evaluate performance with different sources of information (visual and various forms of text).</p>
<h1 id="related-work">Related work</h1>
<p>Integration of language and vision is a natural step towards improved understanding and is receiving increasing attention from the research community. This is in large part due to efforts in large-scale data collection such as Microsoft's COCO (<a href="">&quot;Microsoft COCO: Common Objects in Contex&quot;</a>), Flickr30K (<a href="">&quot;Flickr30k&quot;</a>) and Abstract Scenes (<a href="">&quot;Adopting abstract images for semantic scene understanding&quot;</a>) providing tens to hundreds of thousand images with natural language captions. Another way of conveying semantic understanding of both vision and text is by retrieving semantically meaningful images given a natural language query (<a href="">&quot;Deep Visual-Semantic Alignments for Generating Image Descriptions&quot;</a>). An interesting direction, particularly for the goals of our paper, is also the task of learning common sense knowledge from captioned images (<a href="">&quot;Learning Common Sense Through Visual Abstraction&quot;</a>). This has so far been demonstrated only on synthetic clip-art scenes which enable perfect visual parsing.</p>
<p><span class="math inline">\(\mathbf{Video understanding via language}\)</span></p>
<p>In the video domain, there are fewer works on integrating vision and language, likely due to less available labeled data. In (<a href="">&quot;Long-term Recurrent Convolutional Networks for Visual Recognition and Description,Translating Videos to Natural Language Using Deep Recurrent Neural Networks&quot;</a>), the authors caption video clips using LSTMs, (<a href="">&quot;Translating Video Content to Natural Language Descriptions&quot;</a>) formulates description as a machine translation model, while older work uses templates (<a href="">&quot;Video-In-sentences Out,A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching,Generating Natural-Language Video Descriptions Using Text-Mined Knowledge&quot;</a>). In (<a href="">&quot;Visual Semantic Search: Retrieving Videos via Complex Textual Queries&quot;</a>), the authors retrieve relevant video clips for natural language queries, while (<a href="">&quot;Video Event Understanding using Natural Language Descriptions&quot;</a>) exploits captioned clips to learn action and role models. For TV series in particular, the majority of work aims at recognizing and tracking characters in the videos (<a href="">&quot;Semi-supervised Learning with Constraints for Person Identification in Multimedia Dat,Finding Actors and Actions in Movies,Linking People in Videos with &quot;Their&quot; Names Using Coreference Resolution,Who are you?- Learning person specific classifiers from vide&quot;</a>). In (<a href="">&quot;Movie/Script: Alignment and Parsing of Video and Text Transcription,Subtitle-free Movie to Script Alignment&quot;</a>), the authors aligned videos with movie scripts in order to improve scene prediction. (<a href="">&quot;Aligning Plot Synopses to Videos for Story-based Retrieval&quot;</a>) aligns movies with their plot synopses with the aim to allow semantic browsing of large video content via textual queries. Just recently, (<a href="">&quot;Book2Movie: Aligning Video scenes with Book chapter,Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books&quot;</a>) aligned movies to books with the aim to ground temporal visual data with verbose and detailed descriptions available in books.</p>
<p><span class="math inline">\(\mathbf{Question-answering}\)</span> QA is a popular task in NLP with significant advances made recently with neural models such as memory networks (<a href="">&quot;End-To-End Memory Networks&quot;</a>), deep LSTMs (<a href="">&quot;Teaching Machines to Read and Comprehend&quot;</a>), and structured prediction (<a href="">&quot;Machine Comprehension with Syntax, Frames, and Semantics&quot;</a>). In computer vision, (<a href="">&quot;A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input&quot;</a>) proposed a Bayesian approach on top of a logic-based QA system (<a href="">&quot;Learning dependency-based compositional semantics&quot;</a>), while (<a href="">&quot;Ask Your Neurons: A Neural-based Approach to Answering Questions about Images,Exploring Models and Data for Image Question Answering&quot;</a>) encoded both an image and the question using an LSTM and decoded an answer. We are not aware of QA methods addressing the temporal domain.</p>
<p><span class="math inline">\(\mathbf{QA Datasets}\)</span> Most available datasets focus on image (<a href="">&quot;What are you talking about? Text-to-Image Coreference,Microsoft COCO: Common Objects in Context,Flickr30k,Adopting abstract images for semantic scene understanding&quot;</a>) or video description (<a href="">&quot;Collecting highly parallel data for paraphrase evaluation,A Dataset for Movie Description,A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching&quot;</a>). Particularly relevant to our work is the MovieDescription dataset (<a href="">&quot;A Dataset for Movie Description&quot;</a>) which transcribed text from the Described Video Service (DVS), a narration service for the visually impaired, for a collection of 100 movies.</p>
<p>For QA, (<a href="">&quot;A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Inpu&quot;</a>) provides questions and answers (mainly lists of objects, colors, ) for the NYUv2 RGB-D dataset, while (<a href="">&quot;VQA,Visual Madlibs: Fill in the blank Image Generation and Question Answering&quot;</a>) do so for MS-COCO with a dataset of a million QAs. While these datasets are unique in testing the vision algorithms in performing various tasks such as recognition, attribute induction and counting, they are inherently limited to static images. In our work, we collect a large QA dataset of about 300 movies with challenging questions that require semantic reasoning over a long temporal domain.</p>
<p>Our dataset is also related to purely text-datasets such as MCTest (<a href="">&quot;Mctest: A challenge dataset for the open-domain machine comprehension of text&quot;</a>) which contains 660 short stories with multi-choice QAs, and (<a href="">&quot;Teaching Machines to Read and Comprehen&quot;</a>) which converted 300K news summaries into Cloze-style questions. We go beyond these datasets by having significantly longer text, as well as multiple sources of available information (video clips, plots, subtitles, scripts and DVS). This makes our data one of a kind.</p>
<h1 id="movieqa-dataset">MovieQA dataset</h1>
<p>The goal of our paper is to create a challenging benchmark that evaluates semantic understanding over long temporal data. We collect a dataset with very diverse sources of information that can be exploited in this challenging domain. Our data consists of quizzes about movies that the automatic systems will have to answer. For each movie, a quiz comprises of a set of questions, each with 5 multiple-choice answers, only one of which is correct. The system has access to various sources of textual and visual information, which we describe in detail below.</p>
<p>We collected 294 movies with subtitles, and obtained their extended summaries in the form of plot synopses from <span class="math inline">\(\mathbf{Wikipedia}\)</span>. Additionally, we crawled  for scripts, which were available for 40% (117) of our movies. A fraction of our movies (45) come from the MovieDescription dataset (<a href="">&quot;A Dataset for Movie Description&quot;</a>) which contains movies with DVS transcripts.</p>
<p><span class="math inline">\(\mathbf{Plot synopses}\)</span> Plot synopses are extended movie summaries that fans write after watching the movie. This makes them faithful to the story that takes place in the movie. Synopses widely vary in detail, and range from one to 20 paragraphs, but focus on describing content that is directly relevant to the story. They rarely contain detailed visual information such as how a character looks or dresses, and mainly focus on describing the movie events, character interactions and at times emotions. Plots are thus in many ways what a perfect automatic algorithm should get from &quot;watching&quot; the movie. We exploit such plots to gather our quizzes.</p>
<p><span class="math inline">\(\mathbf{Videos and subtitles}\)</span> An average movie is about 2 hours in length and has over 198K frames and almost 2000 shots. Note that on its own, video contains information about e.g., who did what to whom, but does not contain sufficient information to explain why something happened. Dialogs play an important role, and only both modalities together allow us to fully understand the story. Note that subtitles do not contain speaker information.</p>
<p><span class="math inline">\(\mathbf{DVS}\)</span> is a service that narrates movie scenes to the visually impaired by inserting relevant descriptions in between dialogs. These descriptions contain sufficient &quot;visual&quot; information about the scene that allows the visually impaired audience to follow the movie. DVS is thus a proxy for a perfect vision system, and potentially allows quizzes to be answered without needing to process the videos.</p>
<p><span class="math inline">\(\mathbf{Scripts}\)</span> The scripts that we collected are written by screenwriters and serve as a guideline for movie making. They typically contain detailed descriptions of scenes, and, unlike subtitles, contain both dialogs and speaker information. Scripts are thus similar, if not richer in content to DVS+subtitles, however are not always entirely faithful to the movie as the director may aspire to artistic freedom.</p>
<h2 id="qa-collection-method">QA Collection method}</h2>
<p>Since videos are difficult and expensive to provide to annotators, we used plot synopses as a proxy for the movie. Thus, while creating quizzes, our annotators were only looking at text and were &quot;forced&quot; to ask questions that are at a higher semantic level and more story-like. In particular, we split our annotation efforts into two parts to ensure high quality of the collected data.</p>
<p><span class="math inline">\(\mathbf{Q and correct A}\)</span> Our annotators were first asked to select a movie from a provided list, and were then shown its plot synopsis one paragraph at a time. For each paragraph, the annotator had the freedom of forming any number and type of questions. On average they formed 5.4 questions per paragraph. Each annotator was also asked to provide the correct answer, and was additionally required to mark a minimal set of sentences within the plot synopsis paragraph which are needed to both frame the question and answer it. This acts as ground-truth for localizing the QA in the plot.</p>
<p>In our instructions, we asked the annotators to provide context to each question, such that the person taking their quiz would be able to answer it by watching the movie alone (without having access to the synopsis). The purpose of this was to ensure questions that are localizable in the video and story as opposed to generic questions such as &quot;What are they talking about?&quot;. We trained our annotators for about one to two hours and gave them the option to re-visit and correct their data. We paid them by the hour, a strategy that allowed us to collect more thoughtful and complex QAs, rather than short questions and single-word answers.</p>
<p><span class="math inline">\(\mathbf{Multi-choice}\)</span> In the second step of data collection, we collected multiple-choice answers for each question. Our annotators were shown a paragraph and a question at a time, but not the correct answer. They were then asked to answer the question correctly as well as to provide 4 wrong answers. These answers were either deceiving facts from the same paragraph or common-sense answers. The annotator was also allowed to re-formulate or correct the question. We used this to sanity check all the questions received in the first step, and was one of the main reasons as to why we split our data collection into two phases.</p>
<p><span class="math inline">\(\mathbf{Time-stamp to video}\)</span> Parallel to our movie QA collection, we asked in-house annotators to align each sentence in the plot synopsis to video, by marking the beginning and end (in seconds) in the video that the sentence describes. Long and complicated sentences were often aligned to multiple, non-consecutive video clips. Annotation took roughly 2 hours per movie. Since we have each QA aligned to a sentence (or multiple ones) in the plot synopsis, this alignment provides QA time-stamped with corresponding video clips. We will provide these clips as part of our benchmark.</p>
<h2 id="dataset-statistics">Dataset Statistics</h2>
<p>In the following, we present some statistics about the questions and answers in our MovieQA dataset. Table~ presents an overview of popular and recent Question-Answering datasets in the field. Most datasets (except MCTest) use very short answers and are thus limited to covering simpler visual / textual forms of understanding. To the best of our knowledge, our dataset is also the first to use videos in the form of movies.</p>
<p><span class="math inline">\(\mathbf{Multi-choice QA}\)</span> So far, we have collected a total of 7702 QAs about 294 movies% . Each question comes with one correct and four deceiving answers. Table~ presents an overview of the dataset along with the information about the train/test splits, which will be used to train and evaluate the automatic QA models. Unlike most previous datasets, our questions and answers are fairly long and have on average about 9 and 5 words, respectively. We create a video-based answering split for our dataset, currently based on the number of movies we have collected with plot synopses alignment. Note that the QA methods needs to look at a long video clip ($$150 seconds) to answer the question.</p>
<p>Fig.~ presents the number of questions (as area) depending on the first word of the question. We see the diversity among questions and the number of words used to answer them. &quot;Why&quot; questions require verbose answers which is justified by having the largest average number of words in the correct answer. On the other hand, Does answers are very short (&quot;Yes&quot;, or &quot;No, he killed John&quot;), while the question itself needs to describe a lot of things to pinpoint to a particular part of the story.</p>
<p>A different way to look at QAs is to decide their type based on the answer. For example, especially, &quot;What&quot; questions can cover a large variety in types of answers (&quot;What happens ...&quot;, &quot;What did X do?&quot;, &quot;What is the name ...&quot;, &quot;What is X's purpose?&quot;, ). In Fig.~ we show the questions from our dataset in a variety of answers. In particular, reasoning based questions (~top half of the pie) are a large part of our data. In the bottom left quadrant we see typical question types which can likely be answered using vision alone. Note however, that even the reasoning questions typically require vision, as the question provides context which is typically a visual description of a scene (, &quot;When John runs after Marry...&quot;).</p>
<p><span class="math inline">\(\mathbf{Text sources for answering}\)</span> Finally, Table~ presents different statistics of the various text sources. For plot synopses, we see that the average number of words per sentence stands above all other forms of text which speaks for the richness of the descriptions.</p>
<h1 id="multi-choice-question-answering">Multi-choice Question-Answering</h1>
<p>We investigate a number of intelligent baselines for question-answering ranging from very simple ones to more complex architectures, building on the recent work on automatic QA. We also study inherent biases in the data and try to answer the quiz based simply on characteristics such as word length or within answer diversity.</p>
<p>% general formulation Formally, let <span class="math inline">\(S\)</span> denote the story, which can take the form of any of the available sources of information -- ~plots, subtitles, or video shots. Each story <span class="math inline">\(S\)</span> has a set of questions, and we assume that the (automatic) student reads one question <span class="math inline">\(q^S\)</span> at a time. %Let <span class="math inline">\(\mathcal{Q}^S = \{q_i^S\}\)</span> be a set of questions obtained from a story <span class="math inline">\(S\)</span>. %Since we will assume that the questions are independent, we drop the subscript and only consider a question <span class="math inline">\(q^S\)</span> to be answered. %Let <span class="math inline">\(\mathcal{A}_i^S = \{a_{ij}^S\}_{j=1}^{M}\)</span> be the set of multiple choice answers (only one of which is correct) corresponding to <span class="math inline">\(q_i^S\)</span>. Let <span class="math inline">\(\{a_{j}^S\}_{j=1}^{M}\)</span> be the set of multiple choice answers (only one of which is correct) corresponding to <span class="math inline">\(q^S\)</span>, with <span class="math inline">\(M=5\)</span> in our dataset.</p>
The general problem of multi-choice question answering can be formulated by a three-way scoring function <span class="math inline">\(f(S,q^S,a^S)\)</span>. This function evaluates the &quot;quality&quot; of the answer given the story and the question. Our goal is thus to pick the correct answer <span class="math inline">\(a^S\)</span> for <span class="math inline">\(q^S\)</span> that maximizes <span class="math inline">\(f\)</span>:
\begin{equation}
j^* = \arg\max_{j=1\ldots M} f(S, q^S, a_{j}^S) \,
\end{equation}
<p>%Here <span class="math inline">\(f(\cdot, \cdot, \cdot)\)</span> uses information from the story <span class="math inline">\(S\)</span> to select an answer among <span class="math inline">\(a_{j}^S\)</span> for the question <span class="math inline">\(q^S\)</span>. We next discuss different possibilities for <span class="math inline">\(f\)</span>. We drop the superscript <span class="math inline">\((\cdot)^S\)</span> for simplicity of notation.</p>
<h2 id="the-hasty-student">The Hasty Student</h2>
<p>%We now discuss various formulations of <span class="math inline">\(f\)</span> representing different ways to answer the questions. We first consider <span class="math inline">\(f\)</span> which ignores the story and attempts to answer the question directly based on latent biases and similarities. We call such a baseline as the &quot;hasty student&quot; since he/she does not care to read/watch the actual story.</p>
<p>The extreme case of a hasty student is to try and answer the question by only looking at the answers. Here, <span class="math inline">\(f(S, q, a_{j}) = g_{H1}(a_{j}|\)</span>)$, where <span class="math inline">\(g_{H1}(\cdot)\)</span> captures some properties of the answers.</p>
<p><span class="math inline">\(\mathbf{Answer length}\)</span> We use the number of words in the multiple choices to select the correct answer. This idea explores the bias in the data where the number of words in the correct answer is slightly larger than the number of words in wrong answers. We choose the correct answer by: (i) selecting the longest answer; (ii) selecting the shortest answer; or (iii) selecting the answer with the most different length.</p>
<p><span class="math inline">\(\mathbf{Within answer similarity/difference}\)</span> While still looking only at the answers, we compute a distance between all answers based on their representations (discussed in Sec.~). We then select the correct answer as either the most similar or most distinct among all answers.</p>
<p><span class="math inline">\(\mathbf{Q and A similarity}\)</span> We now consider a hasty student that looks at both the question and answer, <span class="math inline">\(f(S, q, a_j) = g_{H2}(q, a_{j})\)</span>. We compute similarity between the question and each answer and pick the most similar answer.</p>
<h2 id="the-searching-student">The Searching Student</h2>
<p>While the hasty student ignores the story, we consider a student that tries to answer the question by trying to locate a subset of the story <span class="math inline">\(S\)</span> which is most similar to both the question and the answer. The similarity of the question and the answer is ignored in this case.</p>
The scoring function <span class="math inline">\(f\)</span> is thus factorized into two parts:
\begin{equation}
f(S, q, a_{j}) = g_I(S, q) + g_I(S, a_{j}) \, .
\end{equation}
<p>%In particular, when <span class="math inline">\(S\)</span> is a story composed of <span class="math inline">\(L\)</span> different sentences (or shots, or dialogs), we can attempt to find the best set of sentences, observed <span class="math inline">\(H\)</span> at a time, as: We use two possible similarity functions: a simple cosine similarity defined over a window, and one using a neural architecture. We describe these next.</p>
$ We aim to find the best window of <span class="math inline">\(H\)</span> sentences (or shots) in <span class="math inline">\(S\)</span> that maximize similarity between the story and the question, and the story and the answer. We define our similarity function:% follows:
\begin{equation}
f(S, q, a_{j}) = \max_l \sum_{k = l}^{l+H} g_{ss}(s_k, q) + g_{ss}(s_k, a_{j}) \, ,
\end{equation}
<p>where <span class="math inline">\(s_k\)</span> denotes a sentence (or shot) from the story <span class="math inline">\(S\)</span>. We use <span class="math inline">\(g_{ss}(s, q) = x(s)^T x(q)\)</span> as a dot product between the (normalized) representations of the two sentences (shots). We discuss these representations in detail in Sec.~.</p>
<p>$ Instead of factoring <span class="math inline">\(f(S, q, a_{j})\)</span> as a fixed (unweighted) sum of two similarity functions <span class="math inline">\(g_{I}(S, q)\)</span> and <span class="math inline">\(g_{I}(S, a_{j})\)</span>, we build a neural network that learns such a function. Assuming the story <span class="math inline">\(S\)</span> is of length <span class="math inline">\(n\)</span>, ~<span class="math inline">\(n\)</span> sentences in the plot, or <span class="math inline">\(n\)</span> shots in the video clip, <span class="math inline">\(g_{I}(S, q)\)</span> and <span class="math inline">\(g_{I}(S, a_{j})\)</span> can be seen as two vectors of length <span class="math inline">\(n\)</span>. The <span class="math inline">\(k\)</span>-th entry in e.g., the former vector is <span class="math inline">\(g_{ss}(s_k, q)\)</span>. We further combine all <span class="math inline">\([g_I(S, a_{j})]_j\)</span> for the <span class="math inline">\(5\)</span> answers into a <span class="math inline">\(n\times 5\)</span> matrix. We then replicate the vector <span class="math inline">\(g_{I}(S, q)\)</span> <span class="math inline">\(5\)</span>-times, and stack the question and answer matrix together to obtain a tensor of size <span class="math inline">\(n \times 5 \times 2\)</span>.</p>
<p>Our neural similarity model is a convolutional neural net (CNN), shown in Fig.~, that takes this tensor, and several layers of <span class="math inline">\(1 \times 1\)</span> convolutions to approximate a family of functions <span class="math inline">\(\phi(g_I(S, q), g_I(S, a_{j}))\)</span>. We also add a max pooling layer with kernel size <span class="math inline">\(3\)</span> to allow for scoring the similarity within a window in the story. The last convolutional output is a matrix of size $  5$, and we apply both mean and max pooling across the storyline, add them, and make predictions using the softmax. We train our network on a randomized train/val split of our training set using cross-entropy loss and Adam optimizer (<a href="">&quot;Adam: A method for stochastic optimization&quot;</a>).</p>
<h2 id="memory-network-for-complex-qa">Memory Network for Complex QA</h2>
<p>Memory Networks were originally proposed specifically for QA tasks and model complex three-way relationships between the story, question and an answer. We briefly describe MemN2N proposed by (<a href="">&quot;End-To-End Memory Networks&quot;</a>) and suggest simple extensions to make it suitable for our data and task.</p>
<p>The original MemN2N takes a story and a question related to it. The answering is restricted to single words and is done by picking the most likely word from the vocabulary <span class="math inline">\(\mathcal{V}\)</span> of 20-40 words. This is not directly applicable to our domain, as our data set does not have a fixed set of answers.</p>
A question <span class="math inline">\(q\)</span> is encoded as a vector <span class="math inline">\(u \in \mathbb{R}^d\)</span> using a word embedding <span class="math inline">\(B \in \mathbb{R}^{d \times |\mathcal{V}|}\)</span>. Here, <span class="math inline">\(d\)</span> is the embedding dimension, and <span class="math inline">\(u\)</span> is obtained by mean-pooling the representations of words in the question. Simultaneously, the sentences of the story <span class="math inline">\(s_l\)</span> are encoded using word embeddings <span class="math inline">\(A\)</span> and <span class="math inline">\(C\)</span> to provide two different sentence representations <span class="math inline">\(m_l\)</span> and <span class="math inline">\(c_l\)</span>, respectively. Here, <span class="math inline">\(m_l\)</span>, the representation of sentence <span class="math inline">\(l\)</span> in the story, is used in conjunction with <span class="math inline">\(u\)</span> to produce an attention-like mechanism which selects sentences in the story most similar to the question via a softmax function:
\begin{equation}
p_l = \mathrm{softmax}(u^T m_l) \, .
\end{equation}
The probability <span class="math inline">\(p_l\)</span> is used to weight the second sentence embedding <span class="math inline">\(c_l\)</span>, and the story representation <span class="math inline">\(o = \sum_l p_l c_l\)</span> is obtained by pooling the weighted sentence representations across the story. Finally, a linear projection <span class="math inline">\(W \in \mathbb{R}^{|\mathcal{V}| \times d}\)</span> decodes the question <span class="math inline">\(u\)</span> and the story representation <span class="math inline">\(o\)</span> to provide a soft score for each vocabulary word
\begin{equation}
a = \mathrm{softmax}(W (o + u)) \,,
\end{equation}
<p>and finds the answer <span class="math inline">\(\hat a\)</span> as the top scoring word. The free parameters to train %in the MemN2N are the <span class="math inline">\(B\)</span>, <span class="math inline">\(A\)</span>, <span class="math inline">\(C\)</span>, and <span class="math inline">\(W\)</span> embeddings for different words which can be shared across different layers.</p>
<p>Due to its fixed set of output answers, the MemN2N in the current form is not designed for multi-choice answering with open, natural language answers. We propose two key modifications to make the network suitable for our task.</p>
<span class="math inline">\(\mathbf{Memory Network for natural language answers}\)</span> To allow the Memory Network to rank multiple answers written in natural language, we can add an additional embedding layer <span class="math inline">\(F\)</span> which maps each multi-choice answer <span class="math inline">\(a_j\)</span> to a vector <span class="math inline">\(g_j\)</span>. Note that <span class="math inline">\(F\)</span> is similar to previous word embeddings <span class="math inline">\(B\)</span>, <span class="math inline">\(A\)</span> and <span class="math inline">\(C\)</span>, but operates on answers instead of question and story respectively. To predict the correct answer, we compute the similarity between the answers <span class="math inline">\(g\)</span>, the embedding <span class="math inline">\(u\)</span> of the question and the story representation <span class="math inline">\(o\)</span>:
\begin{equation}
\label{eq:memnet_multichoice_ans}
a = \mathrm{softmax}((o + u)^T g)
\end{equation}
and simply pick the most probably answer as correct. In our general QA formulation, this is equivalent to
\begin{equation}
f(S, q, a_{j}) = g_{M1}(S, q, a_{j}) + g_{M2}(q, a_{j}),
\end{equation}
<p>that is, a function <span class="math inline">\(g_{M1}\)</span> that considers the story, question and answer, and a second function <span class="math inline">\(g_{M2}\)</span> that directly considers similarities between the question and the answer.</p>
<p><span class="math inline">\(\mathbf{Weight sharing and fixed word embeddings}\)</span> The original MemN2N learns embeddings for each word based directly on the task of question-answering. However, to scale this to large vocabulary data sets like ours, this requires unreasonable amounts of training data. For example, training a model with vocabulary size 12000 (obtained from plot synopses) and <span class="math inline">\(d = 100\)</span> would entail learning 1.2M parameters for each embedding. To prevent overfitting, we can share all word embeddings <span class="math inline">\(B, A, C, F\)</span> of the memory network. Nevertheless, this is still a large amount of parameters.</p>
We make the following crucial modification that allows us to use the Memory Network for our dataset. We drop <span class="math inline">\(B\)</span>, <span class="math inline">\(A\)</span>, <span class="math inline">\(C\)</span>, <span class="math inline">\(F\)</span> and replace them by a fixed (pre-trained) word embedding <span class="math inline">\(Z \in \mathbb{R}^{d_1 \times |\mathcal{V}|}\)</span> obtained from the Word2Vec model and learn a shared linear projection layer <span class="math inline">\(T \in \mathbb{R}^{d_2 \times d_1}\)</span> to map all sentences (stories, questions and answers) into a common space. Here, <span class="math inline">\(d_1\)</span> is the dimension of the Word2Vec embedding, and <span class="math inline">\(d_2\)</span> is the projection dimension. Thus, the new encodings are
\begin{equation}
u = T \cdot Z q, \, m_l = T \cdot Z s_l, \, \mathrm{and} \, g_j = T \cdot Z a_j .
\end{equation}
<p>Answer prediction is performed as before in Eq.~.</p>
<p>We initialize the projections either using an identity matrix <span class="math inline">\(d_1 \times d_1\)</span> or using PCA to lower the dimension from <span class="math inline">\(d_1 = 300\)</span> to <span class="math inline">\(d_2 = 100\)</span>. Training is performed using stochastic gradient descent with a batch size of 32 for plots and DVS. For subtitles and scripts we needed to use a batch size of 16 to ensure that the story data fits in our 6GB Titan Black GPU memory.</p>
<h2 id="representations-for-text-and-video">Representations for Text and Video}</h2>
<p><span class="math inline">\(\mathbf{TF-IDF}\)</span> is a popular and successful feature in information retrieval. In our case, we treat plots (or the other forms of text) of different movies as documents and compute a weighting for each word. We set all words to lower case, use stemming and compute the vocabulary <span class="math inline">\(\mathcal{V}\)</span> which consists of all words <span class="math inline">\(w\)</span> in the documents. % that appear at least <span class="math inline">\(c\)</span> times. We represent each sentence (or question or answer) as a bag-of-words weighted with an TF-IDF score for each word. %For a sentence <span class="math inline">\(k\)</span> coming from document <span class="math inline">\(d\)</span> we compute its representation <span class="math inline">\(x(s_k) = \sum_{w \in s_k} d_w\)</span> by summing over its words, where %<span class="math inline">\(d_w \in \mathbb{R}^{|\mathcal{V}|}\)</span> is a vector of all zeros except at the location of the word <span class="math inline">\(w\)</span> and is weighted by the TF-IDF score for the word. %We treat questions and answers in a similar way and represent them using the document to which they belong.</p>
<p><span class="math inline">\(\mathbf{Word2Vec}\)</span> A disadvantage of TF-IDF is that it is unable to capture the similarities between words. We use the skip-gram model proposed by (<a href="">&quot;Efficient estimation of word representations in vector space&quot;</a>) and train it on roughly 1200 movie plots to obtain domain-specific, <span class="math inline">\(300\)</span> dimensional word embeddings. A sentence is then represented by mean-pooling its word embeddings. %A story sentence, question, or answer is now represented as the sum over the embeddings for each word within it. We normalize the resulting vector to have unit norm.</p>
<p><span class="math inline">\(\mathbf{SkipThoughts}\)</span> While the sentence representation using mean pooled Word2Vec discards word order, SkipThoughts (<a href="">&quot;Skip-Thought Vectors&quot;</a>) use a Recurrent Neural Network to capture the underlying sentence semantics. We use the pre-trained model by (<a href="">&quot;Skip-Thought Vectors&quot;</a>) to compute a <span class="math inline">\(4800\)</span> dimensional sentence representation. %, a question and answer.</p>
<p>To answer questions from the video, we learn an embedding between a shot and a sentence, which maps the two modalities in a common space. In this joint space, one can score the similarity between the two modalities via a simple dot product. This allows us to apply all of our proposed question-answering techniques in their original form.</p>
<p>To learn the joint embedding we follow (<a href="">&quot;Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books&quot;</a>) which extends (<a href="">&quot;Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models&quot;</a>) to video. Specifically, we use the GoogLeNet architecture (<a href="">&quot;Going deeper with convolutions&quot;</a>) as well as hybrid-CNN (<a href="">&quot;Learning Deep Features for Scene Recognition using Places Database&quot;</a>) for extracting frame features, and mean-pool the representations over all frames of a shot. The embedding is then a linear mapping of the shot representation and an LSTM on word embeddings on the sentence side. The model evaluates the dot product of mapped vectors on both sides using the ranking loss. We train the embedding on the MovieDescription Dataset (<a href="">&quot;A Dataset for Movie Description&quot;</a>) as in (<a href="">&quot;Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books&quot;</a>).</p>
<h1 id="qa-evaluation">QA Evaluation</h1>
<p>We present results for question-answering with the proposed intelligent baselines on our MovieQA dataset. We study how various sources of information influence the performance, and how different level of complexity encoded in <span class="math inline">\(f\)</span> affects the quality of automatic QA.</p>
<p><span class="math inline">\(\mathbf{Protocol}\)</span> Note that we have two primary tasks for evaluation. (i) $: where the story is represented with plots, subtitles, scripts and/or DVS; and (ii) $: which uses video and dialogs (subtitles). For each task, the train and test split statistics are presented in Table~. We will provide more details on the project page with the release of our dataset.</p>
<p><span class="math inline">\(\mathbf{Metrics}\)</span> Multiple choice QA leads to simple and objective evaluation. We measure accuracy as the number of questions where an automatic model chooses the correct answer over the total number of questions.</p>
<p>In addition to accuracy, we propose to use another metric &quot;Quiz Score&quot; (QS) inspired by real-world multiple-choice examinations. This metric penalizes students for choosing wrong answers and also (albeit by a smaller amount) for unanswered questions. Similar to the concept of &quot;refuse to predict&quot; schemes, we want to stress that it might be better to leave answers blank (say &quot;I don't know&quot;) than pick the wrong answer. We plan to use this scoring scheme in the leader board rankings for the benchmark.</p>
The score is computed as
\begin{equation}
\text{Quiz Score} = 100\cdot \frac{\text{\#CA} - 0.25 \cdot \text{\#WA} - 0.05 \cdot \text{\#UA}}{\text{Total no. of questions}} \, . \nonumber
\end{equation}
<p>CA, WA and UA stand for orrect, rong and nanswered questions respectively.</p>
<p>% We discuss some extreme cases to analyze this metric:\ % (i) No question is answered leads to <span class="math inline">\(\text{QS} = -5\)</span>.\ % (ii) All questions answered incorrectly, <span class="math inline">\(\text{QS} = -25\)</span>.\ % (iii) All questions answered at random, <span class="math inline">\(\text{QS} = 0\)</span>, since each QA has 5 multiple choice options.\ % (iv) All QAs answered correctly gives <span class="math inline">\(\text{QS} = 100\)</span>.\ % (v) Finally, the penalty for not attempting QAs requires that more than 20% QAs are answered correctly to obtain an overall QS above 0. % For example, in a test of 26 questions, the student needs 2 correct answers out of the attempted 6 (increasing the ratio to 1/3 instead of 1/5) to score 0.</p>
<p><span class="math inline">\(\mathbf{Answering to maximize Quiz Score}\)</span> An easy way to decide which questions are not worth attempting (leave unanswered) is to learn a threshold on a subset of the training set. We learn a threshold on the difference between the top 2 highest scoring options via grid search, by optimizing for the Quiz Score as the metric. The difference in score between the top 2 options can be considered as our model confidence in answering questions correctly. We then use the learned threshold on the test set to decide whether a question should be answered.</p>
<h2 id="hasty-student">Hasty Student</h2>
<p>The first part of Table~ shows performance of the three models when trying to answer questions based on the length of the answers. Selecting the longest answer performs better (28.2%) than random (20%) while the answer with the most different length is only slightly better at 22.6%. The second part of Table~ presents results when using feature-based similarity within answers. We see that the most similar answer is likely to be correct when the representations are generic and try to capture the semantics of the sentence (Word2Vec, SkipThoughts). On the other hand, when using TF-IDF, discriminating between different names is very easy and thus the most distinct answer is likely to be more correct. Finally, in the last part of Table~ we see that questions and answers are very different from each other. Especially, TF-IDF performs worse than random since words in the question rarely appear in the answer.</p>
<p>Performance of the methods using our second metric &quot;Quiz Score&quot; is indicated by numbers in paranthesis in the Table~. We see the bias towards longer answers results in the highest QS. More interestingly, while the difference between accuracy for within-answer similarity and answer length is not high (27.0% vs. 28.2%), the large difference in QS (8.5 vs. 18.7) reveals that answer length is a more confident way to predict answers. Most other methods result in a quiz score close to 0.</p>
<h2 id="hasty-turker">Hasty Turker</h2>
<p>To analyze the quality of our collected multi-choice answers and their deceiving nature, we tested humans (via AMT) on a subset of 200 QAs. The turkers were not shown the story in any form and were asked to pick the best possible answer given the question and a set of options. The purpose of this experiment is to analyze whether our multi-choice answers are difficult enough, so as to even deceive humans when provided with no context. We asked each question to 10 turkers, and rewarded each with a bonus if their answer agreed with the majority. %Note that some questions reveal the source of the movie due to names (e.g., &quot;Darth Vader&quot;) or places in the questions and answers.</p>
<p>% Fig.~ shows the results of this experiment. % We see that without access to the story, humans are able to pick the correct answer with an accuracy of 27.6%. % This bias may likely be due to the fact that some of the QAs reveal the movie due to names (e.g., &quot;Darth Vader&quot;) and the turker may have seen this movie. % Interestingly, we see that for 12.0% of all questions, the correct answer was not picked by any of the 10 turkers. % This shows the genuine difficulty of our task.</p>
<p>The results are presented in Fig.~. The  is computed as the number of all correct answers over all annotators. We also compute , which is the number of times a correct answer was chosen by the majority of the turkers divided by the total number of questions. Finally,  is the percentage of questions for which none of the turkers selected the correct answer.</p>
<p>In Fig.<sub></sub>(a) we see that 27.6% of all answers were correct, and 37% questions got a correct answer via the majority vote. Since some of the questions and answers reveal the identity of the movie (~a reference to &quot;Darth Vader&quot;, &quot;Indiana Jones&quot;, &quot;Borat&quot;), we decided to also select a subset of these questions for which the names did not necessarily indicate a movie. This removed the possibility of an annotator actually remembering the movie while answering the question. We present the results of this experiment (evaluated on 135 QAs) in Fig.<sub></sub>(b). While the overall accuracy is closer to random, it is still slightly higher (24.7% overall accuracy and 30.4% by majority vote). This may indicate that some of the wrong answers are somewhat correlated, making the test slightly easier for a human. It also indicates that a machine which takes into account all answers should likely do better than looking at each answer in isolation.</p>
<p>The small bias of answer length in our dataset was not noticed by the turkers. 31.3% of the annotators chose the longest answer as the correct one, and in fact 37.3% of them picked the shortest answer.</p>
<h2 id="searching-student">Searching Student</h2>
<p><span class="math inline">\(\mathbf{Cosine similarity in window}\)</span> The first three rows of Table~ present results of the proposed method using different representations and input story types. % For TF-IDF we consider two settings depending on the vocabulary size which is controlled by the number of times a word occurs in the documents seen together. Using the plot to answer questions outperforms other information sources such as subtitles, scripts or DVS. This is most likely due to the fact that the data was collected using plot synopses and while framing the QAs annotators often reproduce parts of the plot verbatim.</p>
<p>We show the results of using Word2Vec or SkipThought representations in the following rows of Table~. Both perform significantly worse than the TF-IDF representation and Word2Vec is consistently better than SkipThoughts. We suspect that while Word2Vec and SkipThoughts are good at capturing the overall semantic structure of the words and sentences respectively, but proper nouns -- names, places -- are often hard to distinguish. This is more evident as we move from individual word representations (Word2Vec) towards the sentence representation (SkipThoughts) which is then likely to ignore the subtleties between different names.</p>
<p>Fig.~ presents a breakup of the overall accuracy based on the first word of the questions. The story here is the plot synopsis and answering method employed is the searching student with cosine similarity. While TF-IDF works better on all question types, the difference between TF-IDF with respect to the semantic representations is extremely high when answering questions of type &quot;Who&quot; and &quot;Where&quot;. On &quot;Why&quot; and &quot;How&quot;, we see a more gradual decay in performance.</p>
<p><span class="math inline">\(\mathbf{Influence of window}\)</span> We notice that the window size <span class="math inline">\(H\)</span> significantly influences the results of using TF-IDF based representations on stories of subtitles and scripts. We believe that this results from two factors: (i) the questions are about the story and answering them by just looking at one dialog is a very hard task; and (ii) the TF-IDF representation in particular sees more words which directly makes matching less sparse and easier.</p>
<p>We analyze the case of using subtitles as stories and show the variation in accuracy in Fig.~. Each subtitle, on average, corresponds to 4.74 seconds of video. The figure shows that the performance improves strongly up to a window of size 100 -- which corresponds to about 8 minutes of video -- and then shows small improvement thereafter.</p>
<h2 id="search-student-with-convolutional-brain">Search student with convolutional brain</h2>
<p><span class="math inline">\(\mathbf{SSCB}\)</span>. The middle rows of Table~ show the result of our neural similarity model. Here we also tried to combine all text features () via our CNN. We randomly split the training set into <span class="math inline">\(80\%\)</span> train / <span class="math inline">\(20\%\)</span> val, keeping all questions / answers of the same movie in the same split, and train our model on train and monitor performance on val. During training, we also create several model replicas and pick the ones with the best validation performance.</p>
<p>Table~ shows that the neural model outperforms the simple cosine similarity on most tasks, while the fusion method achieves the highest performance on two out of four story types. %This is likely because the fusion CNN consists of 3 times as many parameters and does not generalize well when trained with <span class="math inline">\(45\)</span> movies with DVS. Overall, the accuracy is capped at <span class="math inline">\(35\%\)</span> for most modalities showing the difficulty of our dataset.</p>
<h2 id="memory-network">Memory network}</h2>
<p>The original MemN2N which allows to train the word embeddings overfits strongly on our dataset leading to a test error near random performance ($$20%). However, our modifications help in restraining the learning. Table~ presents results for MemN2N with Word2Vec initialization and a linear projection layer. Using plot synopses, we see a performance similar to SSCB on Word2Vec features. However, with longer stories, the attention mechanism in the network is able to sift through thousands of story sentences and perform well on DVS, subtitles and scripts. This shows that complex three-way scoring functions are needed to tackle such complex QA sources. In terms of modalities, the network performs best for scripts which contain the most information (descriptions, dialogs and speaker information). %On both, the DVS which is a proxy for vision, and subtitles containing the dialogs, the method performs similarly</p>
<h2 id="video-baselines">Video baselines</h2>
<p>We now evaluate two of our best performing QA models, SSCB and MemN2N, on the split of our data that has video. We evaluate two settings: answering questions by &quot;watching&quot; the full movie, or via the ground-truth video clips (time-stamped sentences from the plot to which the question/answer refers to). The results are shown in Table~.</p>
<p>Since visual information alone is insufficient to answer high level semantic questions we also combine video and dialog (subtitles). We encode each subtitle as before using Word2Vec. %We compare the performance of considering the story made up of video shots only, subtitles dialogs only, or a fusion of the two. For SSCB we perform late fusion of the CNNs for the two modalities. For the memory network we create two branches, one for each modality, and sum up the scores before the final softmax. We train the full model jointly. %The results are shown in Table~.</p>
<h1 id="conclusion">Conclusion</h1>
<p>We introduced the MovieQA data set which aims to evaluate automatic story comprehension from both video and text. The dataset currently stands at 7702 multiple choice questions from 294 movies with high semantic diversity. Our dataset is unique in that it contains several sources of information -- full-length movies, subtitles, scripts, plots and DVS. We provided several intelligent baselines and extend existing QA techniques to analyze the difficulty of our task. %to show that question-answering with such open-ended semantics is extremely hard. %We plan to create a benchmark with an active leader board, encouraging inspiring work in this challenging domain.</p>
<p>Owing to the variety in information sources, our data set is applicable to Vision, Language and Machine Learning communities. We evaluate a variety of answering methods which discover the biases within our data and demonstrate the limitations on this high level semantic task. Current state-of-the-art methods do not perform well and are often only a little better than random. Using this data set we will create an evaluation campaign that can help breach the next frontier in improved vision and language understanding.</p>
</body>
</html>
