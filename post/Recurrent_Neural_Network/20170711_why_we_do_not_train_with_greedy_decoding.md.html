<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="whey-we-do-not-train-with-greedy-decoding">Whey we do not train with greedy decoding?</h1>
<p>In greedy decoding, we follow the conditional dependency pathand pick the symbol with the highest conditional probability so far at each node. This is equivalent to picking the best symbol one at a time from left to right in conditional language modelling. A decoded translation of greedy decoding is <span class="math inline">\(\hat{Y}=(\hat{y_1}, \hat{y_2}, \cdots, \hat{y_T})\)</span>. Bespite its preferable computational complexity, greedy decoding has been over time found to be undesirably sub-optimal.</p>
<p>由于在training时，policy gradient 方法是基于sample的，所以在training的时候需要基于scheduled sampling。</p>
<p>最新的<a href="https://arxiv.org/pdf/1702.02429.pdf">Trainable Greedy Decoding for Neural Machine Translation</a>探讨了如何用greedy decoding去学习。简单的来说就是把整个模型当做environment，而另外引入一个agent。</p>
</body>
</html>
