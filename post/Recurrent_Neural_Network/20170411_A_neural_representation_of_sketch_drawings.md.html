<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="a-neural-representation-of-sketch-drawings"><a href="https://arxiv.org/abs/1704.03477">A Neural Representation of Sketch Drawings</a></h1>
<p>Recently, there have been major advancements in generative modelling of images using neural networks as a generative tool. GANs, Variational Inference(VI), and Autoregressive (AR) models have become popular tools in this fast growing area.Most of the work, however, has been targeted towards modelling rasterized images represented as a <font style="color:red">two dimensional grid of pixel values</font>. While these models are currently able to generate realistic,low resolution pixel images, a key challenge for many of these models is to generate images with coherent structure.</p>
<h2 id="dataset">Dataset</h2>
<p>They constructed a dataset from sketch data obtained from The Quickdraw A.I. Experiment, an online demo where the users are asked to draw objects belonging to a particular object class in less than 20 seconds. We have selected 75 classes from the raw dataset to construct the quick draw-75dataset. Each class consists of a training set of 70K samples, in addition to 2.5K samples each for validation and test sets.</p>
<p>A sketch is a list of points, and each point is a vector consisting of 5 elements: <span class="math inline">\((\Delta x, \Delta y, p_1, p_2, p_3)\)</span>. The first two elements are the offset distance in the x and y directions of the pen from the previous point.he last 3 elements represents a binary one-hot vector of 3possible states. The first pen state, <span class="math inline">\(p_1\)</span>, indicates that the pen is currently touching the paper, and that a line will be drawn connecting the next point with the current point. The second pen state, <span class="math inline">\(p_2\)</span> ,indicates that the pen will be lifted from the paper after the current point, and that no line will bed rawn next. The final pen state, <span class="math inline">\(p_3\)</span>, indicates that the drawing has ended, and subsequent points,including the current point, will not be rendered.</p>
<h2 id="model">Model</h2>
<p>Sequence-to-Sequence Variational Autoencoder (VAE), their encoder is a bidirectional <span class="math inline">\(RNN\)</span> [26] that takes in a sketch as an input, and outputs a latent vector of size <span class="math inline">\(N_z\)</span>. Specifically, they feed the sketch sequence, <span class="math inline">\(S\)</span>, and also the same sketch sequence in reverse order, <span class="math inline">\(S_{\text{reverse}}\)</span>, into two encoding RNNs that make up the bidirectional RNN, to obtain two final hidden states: <span class="math display">\[
\begin{cases}
h_{\rightarrow}=\text{encode}_{\rightarrow}(S)\\
h_{\leftarrow}=\text{decode}_{\leftarrow}(S_{\text{reverse}})\\
h=[h_{\rightarrow}, h_{\leftarrow}]
\end{cases}
\]</span></p>
<p>Then take this final concatenated hidden state, <span class="math inline">\(h\)</span>, and project it into two vectors <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\hat{\sigma}\)</span>, each of size <span class="math inline">\(N_z\)</span>, using a fully connected layer. They convert <span class="math inline">\(\hat{\sigma}\)</span> into a non-negative standard deviation parameterÏƒusing an exponential operation. They use <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\hat{\sigma}\)</span>, along with <span class="math inline">\(\mathcal{N}(0,1)\)</span>, a vector of IID Gaussian variables of size <span class="math inline">\(N_z\)</span>, to construct a random vector, <span class="math inline">\(z\in \mathbb{R}^{N_z}\)</span>, as in the approach for a Variational Autoencoder:</p>
<p><span class="math display">\[\begin{cases}\mu=W_{\mu}h+b_{\mu}\\
\hat{\sigma}=W_{\sigma}h+b_{\sigma}\\
\sigma=\exp (\frac{\hat{\sigma}}{2})\\
z=\mu+\sigma\odot \mathcal{N}(0,1)
\end{cases}\]</span> Under this encoding scheme, the latent vector <span class="math inline">\(z\)</span> is not a deterministic output for a given input sketch,but a random vector conditioned on the input sketch.</p>
<p>Their decoder is an autoregressive RNN that samples output sketches conditional on a given latent vector <span class="math inline">\(z\)</span>. The initial hidden statesh0, and optional cell statesc0(if applicable) of the decoder RNN is the output of a single layer network: <span class="math display">\[[h_0;c_0]=\tanh (W_z z+b_z)\]</span></p>
<p>Their generated sequence is conditioned from a latent code <span class="math inline">\(z\)</span> sampled from our encoder, which is trained end-to-end alongside the decoder. <span class="math display">\[
p(\Delta x, \Delta y)=\sum_{j=1}^M \prod_j \mathcal{N}(\Delta_x, \Delta_y|\mu_{x,j}, \mu_{y,j},\sigma_{x,j},\sigma_{y,j},\rho_{xy,j})
\]</span></p>
<p>The next hidden state of the RNN, generated with its forward operation, projects into the output vector <span class="math inline">\(y_i\)</span> using a fully-connected layer: <span class="math display">\[x_i=[S_{i-1}:z]\]</span> <span class="math display">\[[h_i;c_i]=\text{forward}(x_i, [h_{i-1};c_{i-1}])\]</span> <span class="math display">\[y_i=W_yh_i+b_y\]</span></p>
<p>The vector <span class="math inline">\(y_i\)</span> is broken down into the parameters of the probability distribution of the next data point. <span class="math display">\[[(\hat{\prod_{1}}\mu_x,\mu_y,\hat{\sigma}_x,\hat{\sigma_y},\hat{\rho_{xy}})_1,\cdots,\hat{\prod_{1}}\mu_x,\mu_y,\hat{\sigma}_x,\hat{\sigma_y},\hat{\rho_{xy}})_M (\hat{q}_1,\hat{q}_2,\hat{q}_3) ]=y_i\]</span></p>
<p>They also apply <span class="math inline">\(\exp\)</span> and <span class="math inline">\(\tanh\)</span> to ensure the standard deviation values are non-negative. <span class="math display">\[\begin{cases}\sigma_x=\exp (\hat{\sigma_x})\\\sigma_y=\exp (\hat{\sigma_y})\\rho_{xy}=\tanh (\hat{\rho_{xy}})\end{cases}\]</span></p>
<p>The probabilities for the categorical distributions are calculated using the outputs as logit values. <span class="math display">\[\begin{cases}
q_k=\frac{\exp (\hat{q}_k)}{\sum_{j=1}^3\exp (\hat{q}_j)}\\
\prod_k=\frac{\exp (\hat{\prod}_k)}{\sum_{=1}^M \exp (\hat{\prod}_j)}
\end{cases}\]</span></p>
</body>
</html>
