<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="ask-attend-and-answer-exploring-question-guided-spatial-attention-for-visual-question-answering"><a href="http://arxiv.org/abs/1511.05234">Ask, Attend and Answer: Exploring question-guided spatial attention for visual question answering</a></h1>
<h1 id="brief-info">Brief info</h1>
<ol style="list-style-type: decimal">
<li>They propose a model they call the Spatial Memory Network and apply itto the VQA task.</li>
<li>Memory networks are RNN with an explicit attention mechanism that selects certain parts of the information stored in memory.</li>
<li>Their Spatial Memory Network stores neuron activations from different spatial regions of the image in its memory, and uses the question to choose relevant regions for computing the answer, aprocess of which constitutes a single &quot;hop&quot; in the network.</li>
<li>Propose anovel spatial attention architecture that aligns words with image patchesin the first hop, and obtain improved results by adding a second attention hop which considers the whole question to choose visual evidencebased on the results of the first hop.</li>
<li>To better understand the inferenceprocess learned by the network, they design synthetic questions that specifically require spatial inference and visualize the attention weights.</li>
<li>They evaluate their model on two published visual question answering datasets, DAQUAR <a href="">&quot;A multi-world approach to question answering aboutreal-world scenes based on uncertain input&quot;</a> and VQA <a href="">&quot;VQA: visual question answering&quot;</a>, and obtain improved results compared to astrong deep baseline model (iBOWIMG) which concatenates image andquestion features to predict the answer <a href="">&quot;Simple baseline forvisual question answering&quot;</a>.</li>
</ol>
<h1 id="related-works">Related works</h1>
<p>Before the popularity of VQA, QA had already been established as a mature research problem in thearea of natural language processing.</p>
<p>Previous QA methods include : 1. searching for the key words of the question in a search engine <a href="">&quot;Natural language questions for the web of data&quot;</a>; 2. parsing the questionas a knowledge base (KB) query [&quot;Semantic parsing via paraphrasing&quot;]; 3. embedding the question and using asimilarity measurement to nd evidence for the answer [&quot;Question answering with subgraph embeddings&quot;].</p>
<p>Recently, memory networks were proposed for solving the QA problem. 1. [&quot;Memory networks&quot;] 1-st introduces the memory network as a general model that consists of a memory and four compo-nents: input feature map, generalization, output feature map and response. The model is investigated in the context of question answering, where the long-term memory acts as a dynamic knowledge base and the output is a textual response. 2. <a href="">&quot;End-to-end memory networks&quot;</a> proposes a competitive memory network model that uses less supervision,called end-to-end memory network, which has a recurrent attention model over alarge external memory. 3. The Neural Turing Machine (NTM) <a href="">&quot;Neural turing machine&quot;</a> couples a neuralnetwork to external memory and interacts with it by attentional processes to in-fer simple algorithms such as copying, sorting, and associative recall from inputand output examples.</p>
<p>The neural attention mechanism has been widely used in di erent areas ofcomputer vision and natural language processing, see for example the attention models in image captioning [&quot;Show, attend and tell: Neural image caption generation with visual attention&quot;]*, video description generation <a href="">&quot;Describing videos by exploiting temporal structure&quot;</a>, machine translation <a href="">Neural machine translation by jointly learningto align and translate.</a><a href="">&quot;Effective approaches to attention-basedneural machine translation&quot;</a> and machine reading systems <a href="">:eaching machines to read and comprehend</a>.</p>
<p>Most methods use the soft attention mechanism first proposed in <a href="">&quot;Neural machine translation by jointly learningto align and translate&quot;</a>, which adds a layer to the network that predicts soft weights and uses them to compute a weighted combination ofthe items in memory.</p>
<p>The two main types of soft attention mechanisms differ in the function that aligns the input feature vector and the candidate feature vectors in order to compute the soft attention weights. 1. The first type uses an alignment function based on &quot;concatenation&quot; of the input and each candidate(we use the term &quot;concatenation&quot; as described <a href="">&quot; ective approaches to attention-basedneural machine translation&quot;</a>), 2. The second type usesan alignment function based on the dot product of the input and each candidate. The &quot;concatenation&quot; alignment function adds one input vector (e.g. hiddenstate vector of the LSTM) to each candidate feature vector, embeds the result-ing vectors into scalar values, and then applies the softmax function to generatethe attention weight for each candidate. 3. [19][20][21][23] use the &quot;concatenation&quot; alignment function in their soft attention models and [24] gives a literaturereview of such models applied to di erent tasks. On the other hand, the dotproduct alignment function rst projects both inputs to a common vector em-bedding space, then takes the dot product of the two input vectors, and appliesa softmax function to the resulting scalar value to produce the attention weightfor each candidate. 4. The end-to-end memory network [13] uses the dot product alignment function. 5. In [22], the authors compare these two alignment functionsin an attention model for the neural machine translation task, and nd thattheir implementation of the &quot;concatenation&quot; alignment function does not yieldgood performance on their task.</p>
<p>Motivated by this, in this paper they use the dot product alignment function in our Spatial Memory Network.VQA is related to image captioning. Several early papers about VQA directly adapt the image captioning models to solve the VQA problem <a href="">&quot;Exploring models and data for image questionanswering&quot;</a><a href="">&quot;Ask your neurons: A neural-based ap-proach to answering questions about image&quot;</a> by generating the answer using a recurrent LSTM network conditioned on the CNN output. But these models' performance is still limited (<a href="">&quot;Exploring models and data for image questionanswering&quot;</a><a href="">&quot;Ask your neurons: A neural-based ap-proach to answering questions about image&quot;</a>). 1. <a href="">&quot;isual7w: Grounded question an-swering in images&quot;</a> proposes anew dataset and uses a similar attention model to that in image captioning <a href="">&quot;Show, attend and tell: Neural image caption generation with visual attention&quot;</a>,but does not give results on the more common VQA benchmark. 2. <a href="">&quot;Simple baseline forvisual question answering&quot;</a> summarizes several recent papers reporting results on the VQA dataset and gives a simple but strong baseline model (iBOWIMG) on this dataset. This simple baseline concatenates the image features with the bag of word embedding question representation and feeds them into a softmax classifier to predict the answer. 3. The iBOWIMG model beats most VQA models considered in the paper. 4. Here, we compare our proposed model to the VQA models(namely, the ACK model <a href="">&quot;Ask me anything: Free-form visual question answering based on knowledge from external sources&quot;</a> and the DPPnet model <a href="">&quot;mage question answering using convolutional neu-ral network with dynamic parameter prediction&quot;</a>) which have comparable or better results than the iBOWIMG model. 5. The ACK model in <a href="">&quot;Ask me anything: Free-form visual question answering based on knowledge from external sources&quot;</a> is essentially the same as the LSTM model in <a href="">&quot;Exploring models and data for image questionanswering&quot;</a>, except that it uses image at-tribute features, the generated image caption and relevant external knowledgefrom a knowledge base as the input to the LSTM's rst time step. 6. The DPPnetmodel in <a href="">&quot;mage question answering using convolutional neu-ral network with dynamic parameter prediction&quot;</a> tackles VQA by learning a convolutional neural network (CNN)with some parameters predicted from a separate parameter prediction network.Their parameter prediction network uses a Gate Recurrent Unit (GRU) to gen-erate a question representation, and based on this question input, maps thepredicted weights to CNN via hashing. 7. Neither of these models (<a href="">&quot;Image question answering using convolutional neu-ral network with dynamic parameter prediction&quot;</a>,<a href="">Ask me anything: Free-form visual question answering based on knowledge from external sources</a>) containa spatial attention mechanism, and they both use external data in addition tothe VQA dataset, e.g. the knowledge base in <a href="">&quot;Image question answering using convolutional neu-ral network with dynamic parameter prediction&quot;</a> and the large-scale textcorpus used to pre-train the GRU question representation <a href="">&quot;mage question answering using convolutional neu-ral network with dynamic parameter prediction&quot;</a>. 8. In this paper, weexplore a complementary approach of spatial attention to both improve perfor-mance and visualize the network's inference process, and obtain improved resultswithout using external data compared to the iBOWIMG model <a href="">&quot;Simple baseline forvisual question answering&quot;</a> as well as the ACK model <a href="">&quot;Ask me anything: Free-form visual question answering based on knowledge from external sources&quot;</a> and the DPPnet model <a href="">&quot;mage question answering using convolutional neu-ral network with dynamic parameter prediction&quot;</a> which use external data.</p>
<ol style="list-style-type: decimal">
<li>In one of the early works <a href="">&quot;A multi-world approach to question answering aboutreal-world scenes based on uncertain input&quot;</a>, VQA is seen as a Turing test proxy. The authors propose an approach based on handcrafted features using a semantic parse of the question and scene analysis of the image combined in a latent-world Bayesian framework.</li>
<li>More recently, several end-to-end deep neural networks that learnfeatures directly from data have been applied to this problem (<a href="">&quot;sk your neurons: A neural-based ap-proach to answering questions about image&quot;</a>,<a href="">&quot;Exploring models and data for image questionanswering&quot;</a>).</li>
<li>Most ofthese are directly adapted from captioning models(<a href="">&quot;Long-term recurrent convolutional networks for visualrecognition and description&quot;</a>,<a href="">&quot;Show and tell: A neural imagecaption generator&quot;</a>,<a href="">&quot;eep fragment embeddings for bidirectionalimage sentence mapping&quot;</a>), and utilize a recurrent LSTM network, which takes the question and Convolutional Neural Net (CNN) image features as input, and outputs the answer.</li>
<li>Though the deep learning methods in (<a href="">&quot;sk your neurons: A neural-based ap-proach to answering questions about image&quot;</a>,<a href="">&quot;xploring models and data for image questionanswering&quot;</a>) have shown great improvement compared to the hand crafted feature method <a href="">&quot;A multi-world approach to question answering aboutreal-world scenes based on uncertain input&quot;</a>, they have their own drawbacks. These models based on theLSTM reading in both the question and the image features do not show a clear improvement compared to an LSTM reading in the question only (<a href="">&quot;sk your neurons: A neural-based ap-proach to answering questions about image&quot;</a>,<a href="">&quot;Exploring models and data for image questionanswering&quot;</a>).</li>
<li>Furthermore, the rather complicated LSTM models obtain similar or worse accuracy to a baseline model which concatenates CNN features and a bag-of-words ques-tion embedding to predict the answer, see the IMG+BOW model in <a href="">&quot;Exploring models and data for image questionanswering&quot;</a> and the iBOWIMG model in <a href="">&quot;Simple baseline forvisual question answering&quot;</a>.</li>
</ol>
<h1 id="contributions">contributions</h1>
<h2 id="drawback-of-existing-models">drawback of existing models</h2>
<ol style="list-style-type: decimal">
<li>A major drawback of existing models is that they do not have any explicit notion of object position, and do not support the computation of intermediate results based on spatial attention.</li>
<li>Their intuition is that answering visualquestions often involves looking at different spatial regions and comparing theircontents and/or locations.</li>
</ol>
<p align="center">
<img src="https://dl.dropboxusercontent.com/s/x1olgzae6ty0cvx/Screenshot%20from%202016-05-26%2014%3A41%3A01.png?dl=0" width="500" >
</p>
<p><font size="2">For example, to answer the questions in Fig. 1, we need to look at a portion of the image, such as the child or the phone booth. Similarly, to answer the question &quot;Is there a cat in the basket?&quot; in Fig. 2, wecan rst nd the basket and the cat objects, and then compare their locations.</font></p>
<p>They propose a new deep learning approach to VQA that incorporates explicit spatial attention, which we call the Spatial Memory Network VQA (SMem-VQA).</p>
<p>Their approach is based on memory networks, which have recently been proposed for text Question Answering (QA). Memory networks combine learned text embeddings with an attention mechanism and multi-step inference.The text QA memory network stores textual knowledge in its &quot;memory&quot; in the form of sentences, and selects relevant sentences to infer the answer. However,in VQA, the knowledge is in the form of an image, thus the memory and thequestion come from di erent modalities.</p>
<p>We adapt the end-to-end memory network to solve visual question answering by storing the convolutional networkoutputs obtained from di erent receptive elds into the memory, which explicitlyallows spatial attention over the image. We also propose to repeat the processof gathering evidence from attended regions, enabling the model to update theanswer based on several attention steps, or &quot;hops&quot;. The entire model is trained end-to-end and the evidence for the computed answer can be visualized usingthe attention weights.</p>
<p>To summarize their contributions: 1. In this paper they propose a novel multi-hop memory network with spatial attention for the VQA task which allows one to visualize the spatial inference process used by the deep network. 2. they design an attention architecture in the first hop which uses each word em-bedding to capture fine-grained alignment between the image and question, 3. they create a series of synthetic questions that explicitly require spatial inferenceto analyze the working principles of the network, and show that it learns logical inference rules by visualizing the attention weights, 4. they provide an extensive evaluation of several existing models and their own model on the same publicly available datasets.</p>
<h1 id="spatial-memory-network-for-vqa">Spatial Memory Network for VQA</h1>
<p align="center">
<img src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-03-25/1cf6bc0866226c1f8e282463adc8b75d92fba9bb/2-Figure2-1.png" width="700" >
</p>
<p><font size="2">Figure 2. their proposed spatial-attention memory network for visual question answering.</span></p>
<p><span class="math inline">\(a\)</span>. Overview of <span style="color:red">one-hop network</span>: The <span style="color:red">question word vectors <span class="math inline">\(v_j\)</span></span> and the <span style="color:red">CNN activation vectors <span class="math inline">\(\mathbf{S}=s_i\)</span></span> at each <span style="color:red">image location <span class="math inline">\(i\)</span></span> are embedded into a common semantic space of the question word vectors <span class="math inline">\(v_j\)</span> using the &quot;attention&quot; visual embedding <span class="math inline">\(\mathbf{W_A}\)</span>, and then used to infer spatial attention weights <span class="math inline">\(\mathbf{W}_{att}\)</span> via <span class="math inline">\((b)\)</span> or <span class="math inline">\((c)\)</span>. These weights are then used to compute a weighted sum over the visual features embedded using a separate transformation <span class="math inline">\(\mathbf{W_E}\)</span> . The resulting ”evidence” vector <span class="math inline">\(\mathbf{S}_{att}\)</span> is combined with the question embedding <span class="math inline">\(Q\)</span> and the answer is predicted. An additional hop can repeat the process to gather additional evidence.</p>
<p><span class="math inline">\(b\)</span>. The sentence-guided attention model, which computes a weighted sum over the word vectors and uses its correlation with the spatial features to produce attention weights.</p>
<p><span class="math inline">\(c\)</span>. The finer-grained word-guided attention model, which selects the word with the maximum correlation at each spatial location. See text for more details.</p>
<p>The input to network is a question comprised of a variable-length se-quence of words, and an image of fixed size. Each word in the question is first represented as a one-hot vector in the size of the vocabulary, with a valueof one only in the corresponding word position and zeros in the other posi-tions. Each one-hot vector is then embedded into a real-valued word vector, <span class="math inline">\(V=\{v_j|v_j\in \mathbb{R}^N ;j= 1,..., T\}\)</span>, where <span class="math inline">\(T\)</span> is the maximum number of wordsin the question and <span class="math inline">\(N\)</span> is the dimensionality of the embedding space. Sentences with length less than <span class="math inline">\(T\)</span> are padded with special1 value, which are embedded to all-zero word vector.The words in questions are used to compute attention over the visual memory, which contains extracted image features. The input image is processed by a CNN to extract high-levelM-dimensional visual features on a grid of spatial locations. Specifically, we use <span class="math inline">\(S=\{s_i|s_i \in \mathbb{R}^M;i= 1,..., L\}\)</span> to represent the spatial CNN features at each of theLgridlocations. In this paper, the spatial feature outputs of the last convolutional layer of GoogLeNet (inception_5b=output) <a href="">&quot;Going deeper with convolutions&quot;</a> are used as the visual features for theimage.</p>
<p>The convolutional image feature vectors at each location are embedded into a common semantic space with the word vectors. Two diffient embeddings areused: the &quot;attention&quot; embedding <span class="math inline">\(W_A\)</span> and the &quot;evidence&quot; embedding <span class="math inline">\(W_E\)</span>. The attention embedding projects each visual feature vector such that its combination with the embedded question words generates the attention weight at that location. The evidence embedding detects the presence of semantic concepts orobjects, and the embedding results are multiplied with attention weights andsummed over all locations to generate the visual evidence vector <span class="math inline">\(S_{att}\)</span>.</p>
<p>Finally, the visual evidence vector is combined with the question represen-tation and used to predict the answer for the given image and question. In thenext section, we describe the one-hop Spatial Memory network model and thespeci c attention mechanism it uses in more detail.</p>
<h2 id="word-guided-spatial-attention-in-one-hop-model">Word Guided Spatial Attention in One-Hop Model</h2>
<p>Rather than using the bag-of-words question representation to guide attention, the attention architecture in the first hop (Fig.~(b)) uses each word vector separately to extract correlated visual features in memory. The intuition is that the BOW representation may be too coarse, and letting each word select a related region may provide more fine-grained attention. The correlation matrix <span class="math inline">\(C \in \mathbb{R}^{T\times L}\)</span> between word vectors <span class="math inline">\(V\)</span> and visual features <span class="math inline">\(S\)</span> is computed as</p>
<p><span class="math display">\[
C = V \cdot (S \cdot W_A + b_A)^T
\]</span></p>
<p>where <span class="math inline">\(W_A \in \mathbb{R}^{M\times N}\)</span> contains the attention embedding weights of visual features <span class="math inline">\(S\)</span>, and <span class="math inline">\(b_A \in \mathbb{R}^{L\times N}\)</span> is the bias term. This correlation matrix is the dot product result of each word embedding and each spatial location's visual feature, thus each value in correlation matrix <span class="math inline">\(C\)</span> measures the similarity between each word and each location's visual feature.</p>
<p>The spatial attention weights <span class="math inline">\(W_{att}\)</span> are calculated by taking maximum over the word dimension <span class="math inline">\(T\)</span> for the correlation matrix <span class="math inline">\(C\)</span>, selecting the highest correlation value for each spatial location, and then applying the softmax function</p>
<p><span class="math display">\[
W_{att} = \text{softmax}(\max_{i=1,\cdots,T}(C_i)), ~C_i \in \mathbb{R}^L
\]</span></p>
<p>The resulting attention weights <span class="math inline">\(W_{att} \in \mathbb{R}^{L}\)</span> are high for selected locations and low for other locations, with the sum of weights equal to <span class="math inline">\(1\)</span>. For instance, in the example shown in Fig.~, the question ``Is there a cat in the basket?'' produces high attention weights for the location of the basket because of the high correlation of the word vector for  with the visual features at that location.</p>
<p>The evidence embedding <span class="math inline">\(W_E\)</span> projects visual features <span class="math inline">\(S\)</span> to produce high activations for certain semantic concepts. E.g., in Fig.~, it has high activations in the region containing the cat. The results of this evidence embedding are then multiplied by the generated attention weights <span class="math inline">\(W_{att}\)</span>, and summed to produce the selected visual ``evidence'' vector <span class="math inline">\(S_{att} \in \mathbb{R}^N\)</span>,</p>
<p><span class="math display">\[
S_{att} = W_{att} \cdot (S \cdot W_E + b_E)
\]</span></p>
<p>where <span class="math inline">\(W_E \in \mathbb{R}^{M\times N}\)</span> are the evidence embedding weights of the visual features <span class="math inline">\(S\)</span>, and <span class="math inline">\(b_E \in \mathbb{R}^{L\times N}\)</span> is the bias term. In our running example, this step accumulates  presence features at the  location.</p>
<p>Finally, the sum of this evidence vector <span class="math inline">\(S_{att}\)</span> and the question embedding <span class="math inline">\(Q\)</span> is used to predict the answer for the given image and question. For the question representation <span class="math inline">\(Q\)</span>, we choose the bag-of-words (BOW). Other question representations, such as an LSTM, can also be used, however, BOW has fewer parameters yet has shown good performance. As noted in~, the simple BOW model performs roughly as well if not better than the sequence-based LSTM for the VQA task. Specifically, we compute</p>
<p><span class="math display">\[
Q = W_Q \cdot V + b_Q
\]</span></p>
<p>where <span class="math inline">\(W_Q \in \mathbb{R}^T\)</span> represents the BOW weights for word vectors <span class="math inline">\(V\)</span>, and <span class="math inline">\(b_Q \in \mathbb{R}^{N}\)</span> is the bias term. The final prediction <span class="math inline">\(P\)</span> is</p>
<p><span class="math display">\[
P = \text{softmax}(W_P \cdot f(S_{att} + Q) + b_P)
\]</span></p>
<p>where <span class="math inline">\(W_P \in \mathbb{R}^{K\times N}\)</span>, bias term <span class="math inline">\(b_P \in \mathbb{R}^{K}\)</span>, and <span class="math inline">\(K\)</span> represents the number of possible prediction answers. <span class="math inline">\(f\)</span> is the activation function, and we use ReLU here. In our running example, this step adds the evidence gathered for  near the basket location to the question, and, since the cat was not found, predicts the answer ``no''. The attention and evidence computation steps can be optionally repeated in another hop, before predicting the final answer, as detailed in the next section.</p>
<h2 id="spatial-attention-in-two-hop-model">Spatial Attention in Two-Hop Model</h2>
<p>We can repeat hops to promote deeper inference, gathering additional evidence at each hop. Recall that the visual evidence vector <span class="math inline">\(S_{att}\)</span> is added to the question representation <span class="math inline">\(Q\)</span> in the first hop to produce an updated question vector,</p>
<p><span class="math display">\[{O_{hop1} = S_{att} + Q}\]</span></p>
<p>On the next hop, this vector <span class="math inline">\(O_{hop1} \in \mathbb{R}^{N}\)</span> is used in place of the individual word vectors <span class="math inline">\(V\)</span> to extract additional correlated visual features to the whole question from memory and update the visual evidence.</p>
<p>The correlation matrix <span class="math inline">\(C\)</span> in the first hop provides fine-grained local evidence from each word vectors <span class="math inline">\(V\)</span> in the question, while the correlation vector <span class="math inline">\(C_{hop2}\)</span> in next hop considers the global evidence from the whole question representation <span class="math inline">\(Q\)</span>. The correlation vector <span class="math inline">\(C_{hop2} \in \mathbb{R}^L\)</span> in the second hop is calculated by</p>
<p><span class="math display">\[
C_{hop2} = (S \cdot W_E + b_E) \cdot O_{hop1}
\]</span></p>
<p>where <span class="math inline">\(W_E \in \mathbb{R}^{M\times N}\)</span> should be the attention embedding weights of visual features <span class="math inline">\(S\)</span> in the second hop and <span class="math inline">\(b_E \in \mathbb{R}^{L\times N}\)</span> should be the bias term. Since the attention embedding weights in the second hop are shared with the evidence embedding in the first hop, so we directly use <span class="math inline">\(W_E\)</span> and <span class="math inline">\(b_E\)</span> from first hop here.</p>
<p>The attention weights in the second hop <span class="math inline">\(W_{att2}\)</span> are obtained by applying the softmax function to the correlation vector <span class="math inline">\(C_{hop2}\)</span>.</p>
<p><span class="math display">\[
W_{att2} = \softmax(C_{hop2})
\]</span></p>
<p>Then, the correlated visual information in the second hop <span class="math inline">\(S_{att2} \in \mathbb{R}^N\)</span> is extracted using attention weights <span class="math inline">\(W_{att2}\)</span>.</p>
<p><span class="math display">\[
S_{att2} = W_{att2} \cdot (S \cdot W_{E_2} + b_{E_2})
\]</span></p>
<p>where <span class="math inline">\(W_{E_2} \in \mathbb{R}^{M\times N}\)</span> are the evidence embedding weights of visual features <span class="math inline">\(S\)</span> in the second hop, and <span class="math inline">\(b_{E_2} \in \mathbb{R}^{L\times N}\)</span> is the bias term.</p>
<p>The final answer <span class="math inline">\(P\)</span> is predicted by combining the whole question representation <span class="math inline">\(Q\)</span>, the local visual evidence <span class="math inline">\(S_{att}\)</span> from each word vector in the first hop and the global visual evidence <span class="math inline">\(S_{att2}\)</span> from the whole question in the second hop,</p>
<p><span class="math display">\[
P = \softmax(W_P \cdot f(O_{hop1} + S_{att2}) + b_P)
\]</span></p>
<p>where <span class="math inline">\(W_P \in \mathbb{R}^{K\times N}\)</span>, bias term <span class="math inline">\(b_P \in \mathbb{R}^{K}\)</span>, and <span class="math inline">\(K\)</span> represents the number of possible prediction answers. <span class="math inline">\(f\)</span> is activation function. More hops can be added in this manner.</p>
<p>The entire network is differentiable and is trained using stochastic gradient descent via standard backpropagation, allowing image feature extraction, image embedding, word embedding and answer prediction to be jointly optimized on the training image/question/answer triples.</p>
<p align="center">
<img src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-03-25/1cf6bc0866226c1f8e282463adc8b75d92fba9bb/7-Table1-1.png" width="500" >
</p>
<p>Table 1. Results on the VQA and the DAQUAR datasets (in percentage). The column marked ∗ are results reported in other papers.</p>
<p align="center">
<img src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-03-25/1cf6bc0866226c1f8e282463adc8b75d92fba9bb/4-Figure3-1.png" width="500" >
</p>
<p>Figure 3. Object presence experiment: for each image and question pair, we show the original image (left), the evidence embeddingWE of the convolutional layer (middle), and the attention weights Watt of the one-hop model (right). The evidence embedding WE almost always has a high response on the whole cat or dog object, and low response elsewhere. We see that the model learns the following two inference rules to answer the question: if the question word does not match the animal (i.e. the answer is ”no”), it uses attention to gather visual features from the region including the face of the cat or dog (bottom row); if the question word matches the animal (i.e. the answer is ”yes”), it gathers features from regions other than the face (top row). It then uses the gathered features to predict the answer: if the features are animal face features, predict ”no”, otherwise, predict ”yes”.</p>
<p align="center">
<img src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-03-25/1cf6bc0866226c1f8e282463adc8b75d92fba9bb/5-Figure4-1.png" width="500" >
</p>
<p>Figure 4. Absolute position experiment: for each image and question pair, we show the original image (left) and the attention weights Watt of the one-hop model (right). The attention weights follow the following two rules. The first rule (top row) looks at the position specified in the question (top|bottom|right|left), if it contains a square, answer ”yes”; otherwise answer ”no”. The second rule (bottom row) looks at the region where there is a square, and answers ”yes” if the question contains that position and ”no” if it contains one of the other three positions.</p>
<p align="center">
<img src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-03-25/1cf6bc0866226c1f8e282463adc8b75d92fba9bb/5-Figure5-1.png" width="500" >
</p>
<p>Figure 5. Relative position experiment: for each image and question pair, we show the original image (left), the evidence embedding WE of the convolutional layer (middle) and the attention weightsWatt (right). The evidence embeddingWE has high activations on both the cat and red square. The attention weights follow the inference rules similar to those in Sec. 4.1.2, with the difference that the attention position is around the cat.</p>
<p align="center">
<img src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-03-25/1cf6bc0866226c1f8e282463adc8b75d92fba9bb/7-Figure6-1.png" width="500" >
</p>
<p>Figure 6. Visualization of the spatial attention weights in the one-hop and two-hop model on VQA (top two rows) and DAQUAR (bottom row) datasets. For each image and question pair, we show the original image, the attention weights Watt of the one-hop model, and the two attention weightsWatt andWatt2 of the two-hop model in order.</p>
<p align="center">
<img src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-03-25/1cf6bc0866226c1f8e282463adc8b75d92fba9bb/10-Figure7-1.png
Figure 7. Visualization of the attention wei" width="500" >
</p>
<p>ghts in the one-hop and two-hop models on the DAQUAR dataset. For each image and question pair, we show the original image, the attention weightsWatt of the one-hop model, and the two sets of attention weights (Watt andWatt2) of the two-hop model, in that order. For some examples only the two-hop model predicts the correct answer (Rows a, b and c), while for other examples both models predict the correct answer (Row d).</p>
<p align="center">
<img src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-03-25/1cf6bc0866226c1f8e282463adc8b75d92fba9bb/11-Figure8-1.png" width="500" >
</p>
<p>Figure 8. Visualization of the attention weights in the one-hop and two-hop models on the VQA dataset. For each image and question pair, we show the original image, the attention weightsWatt of the one-hop model, and the two sets of attention weights (Watt andWatt2) of the two-hop model, in that order. We show several examples of different types of questions.</p>
</body>
</html>
