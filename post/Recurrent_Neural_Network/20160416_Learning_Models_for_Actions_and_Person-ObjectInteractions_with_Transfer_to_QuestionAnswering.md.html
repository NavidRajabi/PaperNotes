<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<h1 id="learning-models-for-actions-and-person-object-interactions-with-transfer-to-question-answering"><a href="https://arxiv.org/pdf/1604.04808.pdf">Learning Models for Actions and Person-Object Interactions with Transfer to Question Answering</a></h1>
<p>They propose a convolutional deep network model which utilizes local and global context through feature fusion to make human activity label predictions and achieve state-of-the-art performance on two different activity recognition datasets, the HICO and MPII Human Pose Dataset. They use Multiple Instance Learning to handle the lack of full person instance-label supervision and weighted loss to handle the unbalanced training data. Further, They show how expert knowledge from these specialized datasets can be transferred to improve accuracy on the Visual Question Answering (VQA) task, in the form of multiple choice fill-in-the-blank questions (Visual Madlibs). Specifically, They tackle two types of questions on person's activity and person-object relationship and show improvements over generic features trained on the ImageNet classification task.</p>
<div class="figure">
<img src="https://www.dropbox.com/s/afjkjt8j37ura7g/Learning%20Models%20for%20Actions%20and%20Person-Object%20Interactions%20with%20Transfer%20to%20Question%20Answering.png?dl=1" />

</div>
</body>
</html>
