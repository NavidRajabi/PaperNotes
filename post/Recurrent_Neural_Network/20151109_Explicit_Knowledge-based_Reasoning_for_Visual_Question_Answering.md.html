<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="explicit-knowledge-based-reasoning-for-visual-question-answering"><a href="https://arxiv.org/abs/1511.02570">Explicit Knowledge-based Reasoning for Visual Question Answering</a></h1>
<h2 id="recent-advances">Recent advances</h2>
<p>Despite the implied need to perform general reasoning about the content of images, most VQA methods perform <span style="color:green">no explicit reasoning at all</span>. There are a number of problems with the LSTM approach:</p>
<ol style="list-style-type: decimal">
<li>The first is that the method does <span style="color:green">not explain how it arrived at its answer</span>.</li>
<li>This means that it is <span style="color:green">impossible to tell whether it is answering the question based on image information, or just the prevalence of a particular answer in the training set</span>.</li>
<li>The second problem is that <span style="color:green">the amount of prior information</span> that can be encoded within a LSTM system is very limited.</li>
<li>DBpedia <a href="">&quot;DBpedia: A nucleus for a webof open data&quot;</a>, with millions of concepts and hundred millions of relationships, contains a small subset of the information required to truly reason about the world in general.</li>
<li>The third, and major, problem with the LSTM approach is that it is <span style="color:green">incapable of explicit reasoning except in very limited situations</span>.</li>
</ol>
<h1 id="contribution">Contribution</h1>
<ol style="list-style-type: decimal">
<li>They describe a method for VQA which is capable of <span style="color:red">reasoning about contents of an image</span> on the basis of information extracted from a <span style="color:red">large-scale knowledge base</span>. It is capable of correctly answering a far broader range of image-based questions than competing methods, and provides an explanation of the reasoning by which it arrived at the answer. Ahab exploits DBpedia as its source of external information, and requires no VQA training data (it does use ImageNet and MS COCO to train the visual concept detector).</li>
<li>The method not only answers natural language questions using concepts not contained in the image, but can <span style="color:red">provide an explanation of the reasoning by which it developed its answer</span>.</li>
<li>The method is capable of answering far <span style="color:red">more complex questions than the predominant LSTM-based approach, and outperforms it signicantly in the testing</span>.</li>
<li>They provide a <span style="color:red">dataset</span> and a <span style="color:red">protocol</span> by which to evaluate such methods, thus addressing one of the key issues in general visual question answering. They propose a dataset, and protocol for measuring performance, for general visual question answering. The questions in the dataset are generated by human subjects based on a number of predefined templates.</li>
</ol>
<h1 id="introduction">Introduction</h1>
<p>They propose Ahab, a new approach to VQA which is based on explicit reasoning about the content of images.</p>
<ol style="list-style-type: decimal">
<li>Ahab first detects <span style="color:red">relevant content in the image</span>, and relates it to information available in a knowledge base.</li>
<li>A natural language question is processed into a suitable query which is run over the <span style="color:red">combined image and knowledge base information</span>.</li>
<li>This process allows complex questions to be asked which rely on information not available in the image. Examples include questioning the relationships between two images, or asking whether two depicted animals are close taxonomic relatives.</li>
</ol>
<h1 id="related-works">Related works</h1>
<h2 id="lstm-approach">LSTM approach</h2>
<ol style="list-style-type: decimal">
<li><a href="">“A multi-world approach to question answering about real-world scenes based on uncertain input”</a> proposed to process questions using semantic parsing <a href="">&quot;Learning dependency-based compositional semantics&quot;</a> and obtain answers through Bayesian reasoning.</li>
<li><a href="">&quot;Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering&quot;</a> used independent LSTM networks for question encoding and answer decoding</li>
<li><a href="">&quot;Ask Your Neurons: A Neural-based Approach to Answering Questions about Images&quot;</a> used one LSTM for both tasks.</li>
</ol>
<h2 id="knowledge-base-mthods">Knowledge base mthods</h2>
<p>Significant advances have been made, however, in the construction of large-scale structured Knowledge Bases (KBs):</p>
<ol style="list-style-type: decimal">
<li><a href="">VQA: Visual Question Answering</a>,</li>
<li><a href="">Open information extraction for the web</a>,</li>
<li><a href="">Freebase: a collaboratively created graph database for structuring human knowledge</a>,</li>
<li><a href="">Toward an Architecture for Never-Ending Language Learning</a>,</li>
<li><a href="">Neil: Ex- tracting visual knowledge from web data</a>,</li>
<li><a href="">A multi-world approach to question answering about real-world scenes based on uncertain input</a>,</li>
<li><a href="">Wikidata: a free collaborative knowledgebase</a>.</li>
</ol>
<p>In structured KBs, <span style="color:red">knowledge is typically represented by a large number of triples of the form <span class="math inline">\((arg1,rel,arg2)\)</span>, where <span class="math inline">\(arg1\)</span> and <span class="math inline">\(arg2\)</span> denote two entities in the KB and rel denotes a predicate representing the relationship between these two entities</span>.</p>
<p>A collection of such triples can be seen as a large interlinked graph. Such triples are often described in terms of a Resource Description Framework <a href="">Resource description framework</a> (RDF) specification, and housed in a relational database management system (RDBMS), or triple-store, which allows queries over the data. The knowledge that “a cat is a domesticated animal”, for instance, is stored in an RDF KB by the triple (cat,is-a,domesticated animal). The information in KBs can be accessed efficiently using a query language. In this work we use SPARQL Protocol <a href="">SPARQL query language for RDF</a> to query the Open-Link Virtuoso <a href="">a Hybrid RDBMS/Graph Column Store</a> RDBMS. For example, the query ?x:(?x,is-a,domesticated animal) returns all domesticated animals in the graph.</p>
<p>Popular large-scale structured KBs are constructed either by 1. Mmanual-annotation/crowd-sourcing: - DBpedia <a href="">DBpedia: A nucleus for a web of open data</a>, - Freebase <a href="">Freebase: a collaboratively created graph database for structuring human knowledge</a> - Wikidata <a href="">Wikidata: a free collaborative knowledgebase</a>), 2. Automatically extracting from unstructured/semi-structured data - YAGO<a href="">YAGO2: A spatially and temporally enhanced knowledge base from Wikipedia</a>, - <a href="">YAGO3:A knowledge base from multilingual Wikipedias</a>, - OpenIE<a href="">Open information extraction for the web</a>, - <a href="">Open Information Extraction: The Second Generation</a>, - <a href="">Identifying relations for open information extraction</a>, - NELL <a href="">Toward an Architecture for Never-Ending Language Learning</a>, - NEIL <a href="">Neil: Ex- tracting visual knowledge from web data</a>).</p>
<p><span style="color:red">The KB they use here is DBpedia</span>, which contains structured information extracted from Wikipedia. Compared to KBs extracted automatically from unstructured data (such as <span style="color:red">OpenIE</span>), the data in DBpedia is more accurate and has a well-defined ontology.</p>
<p>The method they propose is applicable to any KB that admits SPARQL queries, however, including those listed above and the huge variety of subject-specific RDF databases available.</p>
<p>The advances in structured KBs, have driven an increasing interest in the NLP and AI communities in the problem of natural language question answering using structured KBs (refer to as KB-QA):</p>
<ol style="list-style-type: decimal">
<li><a href="">Semantic Parsing on Freebase from Question-Answer Pairs</a>,</li>
<li><a href="">Question answering with subgraph embeddings</a>,</li>
<li><a href="">Large-scale Semantic Parsing via Schema Matching and Lexicon Extension</a>,</li>
<li><a href="">Open question answering over curated and extracted knowl- edge bases</a>,</li>
<li><a href="">A survey on question answering technology from an information retrieval perspective</a>,</li>
<li><a href="">Scaling semantic parsers with on-the-fly ontology matching</a>,</li>
<li><a href="">Learning dependency-based compositional semantics</a>,</li>
<li><a href="">nformation extraction over structured data: Question answering with Free- base</a>,</li>
<li><a href="">Template-based question answering over RDF data</a>).</li>
</ol>
<p>The VQA approach which is closest to KB-QA is that of Zhu et al. <a href="">Building a large-scale multimodal Knowledge Base for Answer- ing Visual Queries</a> as they use a KB and RDBMS to answer image-based questions. They build the KB for the purpose, however, using an <a href="https://en.wikipedia.org/wiki/Markov_random_field">MRF model</a>, with image features and scene/attribute/affordance labels as nodes. The undirected links between nodes represent mutual compatibility/incompatibility relationships. The KB thus relates specific images to specified image-based quantities to the point where the database schema prohibits recording general information about the world. The queries that this approach can field are crafted in terms of this particular KB, and thus relate to the small number of attributes specified by the schema. The questions are framed in an RDMBS query language, rather than natural language.</p>
<h1 id="the-ahab-vqa-approach">The Ahab VQA approach</h1>
<h2 id="rdf-graph-construction">RDF Graph Construction</h2>
<p>In order to reason about the content of an image we need to amass the relevant information. This is achieved by detecting concepts in the <span style="color:red">query image</span> and <span style="color:red">linking them to the relevant parts of the KB</span>.</p>
<p>Visual Concepts Three types of visual concepts are detected in the query image, including:</p>
<p>− <span style="color:red">Objects</span>: They trained two Fast-RCNN detectors on MS COCO 80-class objects and ImageNet 200-class objects. Some classes with low precision were removed from the models, such as “ping-pong ball” and “nail”. The finally merged detector contains 224 object classes, which can be found in the supplementary material. − <span style="color:red">Image Scenes</span>: The scene classifier is obtained from [42], which is a VGG-16 [36] CNN model trained on the MIT Places205 dataset. In our system, the scene classes corresponding to the top-3 scores are selected. − <span style="color:red">Image Attributes</span>: The vocabulary of attributes defined in <a href="">Image Captioning with an Intermediate Attributes Layer</a> covers a variety of high-level concepts related to an image, such as actions, objects, sports and scenes.</p>
<p>They select the <span style="color:red">top-10 attributes for each image</span>. Linking to the KB Having extracted a set of concepts of interest from the image, we now need to relate them to the appropriate information in the KB.</p>
<p>The <span style="color:red">visual concepts (object, scene and attribute cat- egories) are stored as RDF triples</span>.</p>
<p>For example, the information that “The image contains a giraffe object” is expressed as:</p>
<p><span class="math display">\[\text{(Img,contain,Obj-1) and (Obj-1,name,ObjCat-giraffe)}\]</span></p>
<p>Each visual concept is linked to DBpedia entities with the same semantic meaning (identified through a uniform resource identifier2 (URI)), for ex- ample (ObjCat-giraffe, same-concept, KB:Giraffe). The resulting RDF graph includes all of the relevant information in DBpedia, linked as appropriate to the visual concepts extracted from the query image.</p>
<h1 id="answering-questions">Answering Questions</h1>
<p>Having gathered all of the relevant information from the image and DBpedia, now use them to answer questions.</p>
<p>Parsing NLQs Given a question posed in natural language,</p>
<ol style="list-style-type: decimal">
<li>first need to <span style="color:red">translate it to a format which can be used to query the RDBMS</span>(Quepy3 is a Python framework designed within the NLP community to achieve exactly this task).</li>
<li>To achieve this Quepy requires a set of templates, framed in terms of regular expressions. Quepy begins by tagging each word in the question using NLTK, which is composed of a tokenizer, a part-of-speech tagger and a lemmatizer. The tagged question is then parsed by a set of regular expressions (regex), each defined for a specific question template. These regular expressions are built using REfO4 to increase the flexibility of question expression as much as possible.</li>
<li>Once a regex matches the question, it will extract the slot- phrases and forward them for further processing. Mapping Slot-Phrases to KB-entities Note that the slot-phrases are still expressed in natural language.</li>
<li>The next step is to <span style="color:red">find the correct correspondences between the slot-phrases and entities in the constructed graph</span>.</li>
</ol>
<h1 id="experiments">Experiments</h1>
<p>Metrics Performance evaluation in VQA is complicated by the fact that two answers can have no words in common and yet both be perfectly correct.</p>
<p>Malinowski and Fritz <a href="">&quot;A multi-world approachto question answering about real-world scenes basedon uncertain input&quot;</a> used the <span style="color:red">Wu-Palmer similarity (WUPS)</span> to measure the similarity between two words based on their common subsequence in the taxonomy tree. However, this evaluation metric restricts the answer to be a single word.</p>
<p>Antol et al. <a href="">&quot;QA: Visual Questio nAnswering&quot;</a> provided an evaluation metric for the open-answer task which records the percentage of answers in agreement with ground truth from several human subjects. This evaluation metric requires around 10 ground truth answers for each question, and only partly solves the problem (as indicated by the fact that even human performance is very low in some cases in <a href="">&quot;QA: Visual Questio nAnswering&quot;</a>, such as ‘Why ...?’ questions.).</p>
<p>In the paper's case, the existing evaluation metrics are particularly unsuitable because most of the questions in their dataset are open-ended, especially for the “KB- knowledge” questions. In addition, there is no automated method for assessing the reasons provided by our system.</p>
<p>Extending VQA forms Typically, a VQA problem involves one image and one natural language question (IMG+NLQ). Here we extend VQA to problems involving more images. With this extension, we can ask more interesting questions and more clearly demonstrate the value of using a structured knowl- edge base. The first type of question (Q7-Q9) asks for the common properties between two whole images; the second type (Q10-Q12) gives a concept and asks which image is the most related to this concept.</p>
<p>For the first question type, Ahab obtains the answers by searching all common transitive categories shared by the visual concepts extracted from the two query images. For example, although the two images in Q9 are significantly different visually (even at the object level), and share no attributes in common,their scene categories (railway station and airport) are linked to the same concept “transport infrastruc- ture” in DBpedia. For the second type, the corre- lation between each visual concept and the query concept is measured by a scoring function and the correlation be- tween an image and this concept is calculated by aver- aging the top three scores. As we can see in Q11 and Q12, attributes “kitchen” and “computer” are most related to the concepts “chef” and “programmer” re- spectively, so it is easy to judge that the answer for Q11 is the left image and the one for Q12 is the right. The flexibility of Quepy, and the power of Python, make adding additional question types quite sim- ple. It would be straightforward to add question types requiring an image as an answer, for instance (IMG1+NLQ → IMGs).</p>
<h1 id="conclusion">Conclusion</h1>
<p>Described a method capable of reasoning about the content of general images, and interactively answering a wide variety of questions about them. The method develops a structured represen- tation of the content of the image, and relevant information about the rest of the world, on the basis of a large external knowledge base.</p>
<p>It is capable of explaining its reasoning in terms of the entities in the knowledge base, and the connections between them.</p>
<p>Ahab is applicable to any knowledge base for which a SPARQL interface is available. This includes any of the over a thousand RDF datasets online <a href="">&quot;State of the LOD Cloud2014&quot;</a> which relate information on taxonomy, music, UK government statistics, Brazilian politicians, and the articles of the New York Times, amongst a host of other topics. Each could be used to provide a specific visual question answering capability, but many can also be linked by common identifiers to form larger repositories. If a knowledge base containing common sense were available, the method we have described could use it to draw sensible general conclusions about the content of images.</p>
</body>
</html>
