<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<h2 id="neural-machine-translation-with-recurrent-attention-modeling"><a href="https://arxiv.org/abs/1607.05108">Neural Machine Translation with Recurrent Attention Modeling</a></h2>
<p>TLDR; The standard attention model does not take into account the &quot;history&quot; of attention activations, even though this should be a good predictor of what to attend to next. The authors augment a seq2seq network with a dynamic memory that, for each input, keep track of an attention matrix over time. The model is evaluated on English-German and Englih-Chinese NMT tasks and beats competing models.</p>
<h4 id="notes">Notes</h4>
<ul>
<li>How expensive is this, and how much more difficult are these networks to train?</li>
<li>Sequentiallly attending to neighboring words makes sense for some language pairs, but for others it doesn't. This method seems rather restricted because it only takes into account a window of k time steps.</li>
</ul>
</body>
</html>
