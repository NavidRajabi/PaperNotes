<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<h2 id="a-neural-network-approach-to-context-sensitive-generation-of-conversational-responses"><a href="http://arxiv.org/abs/1506.06714">A Neural Network Approach to Context-Sensitive Generation of Conversational Responses</a></h2>
<p>TLDR; The authors propose three neural models to generate a response (r) based on a context and message pair (c,m). The context is defined as a single message. The first model, RLMT, is a basic Recurrent Language Model that is fed the whole (c,m,r) triple. The second model, DCGM-1, encodes context and message into a BoW representation, put it through a feedforward neural network encoder, and then generates the response using an RNN decoder. The last model, DCGM-2, is similar but keeps the representations of context and message separate instead of encoding them into a single BoW vector. The authors train their models on 29M triple data set from Twitter and evaluate using BLEU, METEOR and human evaluator scores.</p>
<h4 id="key-points">Key Points:</h4>
<ul>
<li>3 Models: RLMT, DCGM-1, DCGM-2</li>
<li>Data: 29M triples from Twitter</li>
<li>Because (c,m) is very long on average the authors expect RLMT to perform poorly.</li>
<li>Vocabulary: 50k words, trained with NCE loss</li>
<li>Generates responses degrade with length after ~8 tokens</li>
</ul>
<h4 id="notesquestions">Notes/Questions:</h4>
<ul>
<li>Limiting the context to a single message kind of defeats the purpose of this. No real conversations have only a single message as context, and who knows how well the approach works with a larger context?</li>
<li>Authors complain that dealing with long sequences is hard, but they don't even use an LSTM/GRU. Why?</li>
</ul>
</body>
</html>
