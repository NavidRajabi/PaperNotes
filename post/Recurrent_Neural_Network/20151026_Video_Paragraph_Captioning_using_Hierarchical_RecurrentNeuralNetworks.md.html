<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="video-paragraph-captioning-using-hierarchical-recurrent-neural-networksvideo"><a href="http://arxiv.org/abs/1510.07712">Video Paragraph Captioning using Hierarchical Recurrent Neural Networks</a>/<a href="https://www.youtube.com/watch?v=gX9rkJsfp2w">video</a></h1>
<p>They present a approach that exploits <strong>hierarchical RNNs to tackle the video captioning problem</strong>. Their hierarchical framework contains a <strong>sentence generator</strong> and a <strong>paragraph generator</strong>.</p>
<p>The sentence generator produces one simple short sentence that describes a specific short video interval.</p>
<p>It exploits both temporal and spatial-attention mechanisms to selectively focus on visual elements during generation.</p>
<p>The paragraph generator captures the inter-sentence dependency by taking as input the senential embedding produced by the sentence generator, combining it with the paragraph history, and outputting the new initial state for the sentence generator.</p>
<p><strong>Sentence Generator</strong></p>
<p>First compute an attention score <span class="math display">\[q_m^t\]</span> or each frame <span class="math display">\[m\]</span>, conditioning on the previous hidden state <span class="math display">\[h^{t-1}\]</span>: <span class="math display">\[
q_m^t = w^T \phi (W_q v_m + U_q h^{t-1} + b_q)
\]</span> After this, they set up a sequential soft-max layer to get the attention weights: <span class="math display">\[
\beta_m^t = e^{q_m^t} / \sum_{m=1}^{KM} e^{q_{m^{\prime}}^t}\]</span></p>
<p>Finally, a single feature vector is obtained by weighted averaging: <span class="math display">\[
u^t = \sum_{m=1}^{KM}\beta_m^t v_m\]</span></p>
<p>The multi-modal layer maps the two features, together with the hidden state <span class="math display">\[h^t\]</span> of the recurrent layer <span class="math display">\[I\]</span>, into a 1024 dimensional feature space and add them up: <span class="math display">\[
m^t = \phi(W_{m,o}u_o^t + W_{m,a}u_a^t+U_mh^t+b_m)\]</span> where <span class="math display">\[\phi\]</span> is set to the element-wise tanh function.</p>
<p><strong>Training and Generation</strong>: They treat the activation value indexed by a training word <span class="math display">\[w_t^n\]</span> int he soft-max layer of their sentence generator as the likelihood of generating that word:</p>
<p><span class="math display">\[
P(w_t^n  | s_{1:n-1}, w_{1:t-1}^n, V)\]</span> They further define the cost of generating the whole paragraph <span class="math display">\[s_{1:N}\]</span> (<span class="math display">\[N\]</span> is the number of sentences in the paragraph) as : <span class="math display">\[
PPL(s_{1:N}|V)=-\sum_{n=1}{N}\sum_{t=1}{T_n}log P(w_t^n | s_{1:n-1},w_{1:t-1}^n, V)/\sum_{n=1}^N T_n
\]</span></p>
<p>Finally, the cost function over the entire training set is defined as : <span class="math display">\[
PPL = \sum_{y=1}^Y (PPL(s_{1:N_y}^y | V_y). \sum_{n=1}^{N_y} T_n^y) / \sum_{y=1}^Y \sum_{n=1}^{N_y} T_n^y
\]</span></p>
</body>
</html>
