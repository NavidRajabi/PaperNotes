<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="a-focused-dynamic-attention-model-for-visual-question-answering"><a href="https://arxiv.org/abs/1604.01485">A Focused Dynamic Attention Model for Visual Question Answering</a></h1>
<h1 id="related-works">Related works</h1>
<ul>
<li><p>VQA is a quite challenging task and undoubtedly important for developing modern AI systems. The VQA problem can be regarded as a Visual Turing Test (<a href="http://www.pnas.org/content/112/12/3618.abstract">&quot;Visual turing test for computer vision systems&quot;</a>,<a href="http://arxiv.org/abs/1410.0210">&quot;A multi-world approach to question answering about real-world scenes based on uncertain input&quot;</a>), and besides contributing to the advancement of the involved research areas, it has other important applications, such as blind person assistance and image retrieval.</p></li>
<li><p>The <span style="color:red">first feasible solution</span> to VQA problems was provided by Malinowski and Fritz in <a href="http://arxiv.org/abs/1410.0210">&quot;A multi-world approach to question answering about real-world scenes based on uncertain input&quot;</a>, where they used a <span style="color:red">semantic language parser</span> and a Bayesian reasoning model, to understand the meaning of questions and to generate the proper answers.</p></li>
<li><p>Malinowski and Fritz also constructed the first VQA benchmark dataset, named as <a href="http://www.cs.toronto.edu/~mren/imageqa/results/">DAQUAR</a>, which contains 1,449 images and 12,468 questions generated by humans or automatically by following a template and extracting facts from a database(<a href="http://arxiv.org/abs/1410.0210">&quot;A multi-world approach to question answering about real-world scenes based on uncertain input&quot;</a>).</p></li>
<li><p>Ren et al. <a href="https://arxiv.org/abs/1505.02074">&quot;Exploring models and data for image question answering&quot;</a> released the TORONTO–QA dataset, which contains a large number of images (123,287) and questions (117,684), but the questions are automatically generated and thus can be answered without complex reasoning.</p></li>
<li><p>Antol et al. <a href="http://visualqa.org/">&quot;Vqa: Visual question answering&quot;</a> published the currently largest VQA dataset. It consists of three human posed questions and ten answers given by different human subjects, for each one of the 204,721 images found in the Microsoft COCO dataset. Answering the 614,163 questions requires complex reasoning, common sense, and real-world knowledge, making the VQA dataset suitable for a true Visual Turing Test. The VQA authors split the evaluation on their dataset on two tasks: an open-ended task, where the method should generate a natural language an- swer, and a multiple-choice task, where for each question the method should chose one of the 18 different answers.</p></li>
<li><p>The current top performing methods (<a href="http://arxiv.org/abs/1511.05756">&quot;Image question answering using convolutional neu- ral network with dynamic parameter prediction&quot;</a>, <a href="https://arxiv.org/abs/1511.06973">&quot;Ask me anything: Free- form visual question answering based on knowledge from external sources&quot;</a>, <a href="https://arxiv.org/abs/1506.00333">&quot;Learning to answer questions from image using convolutional neural network&quot;</a>) employ deep neural network model that predominantly uses the CNN to extract image features and a LSTM network to extract the representations for questions.</p></li>
<li><p>The CNN and LSTM representation vectors are then usually fused by concatenation (<a href="http://arxiv.org/abs/1512.02167">&quot;Simple baseline for visual question answering&quot;</a>.<a href="https://arxiv.org/abs/1505.02074">&quot;Exploring models and data for image question answering&quot;</a>,<a href="http://arxiv.org/abs/1505.01121">&quot;Ask your neurons: A neural-based ap- proach to answering questions about images&quot;</a>).</p></li>
<li><p>Other approaches additionally incorporate some kind of attention mechanism over the image features(<a href="https://arxiv.org/abs/1511.05960">&quot;Abc-cnn: An attention based convolutional neural network for visual question answering&quot;</a>,<a href="http://arxiv.org/abs/1511.02274">&quot;Stacked attention networks for image question answering&quot;</a>,<a href="http://arxiv.org/abs/1511.05234">&quot;Ask, attend and answer: Exploring question-guided spatial attention for visual question answering&quot;</a>).</p></li>
<li><p>Using global feature is arguably insufficient to capture all the necessary visual information and provide full understanding of image contents such as multiple objects, spatial configuration of the objects and informative background.</p></li>
<li><p>This issue can be relieved to some extent by extracting features from object proposals – the image regions that possibly contain objects of interest.However, using features from all image regions (<a href="https://arxiv.org/abs/1511.05960">&quot;Abc-cnn: An attention based convolutional neural network for visual question answering&quot;</a>,<a href="http://arxiv.org/abs/1511.02274">&quot;Stacked attention networks for image question answering&quot;</a>) may provide too much noise or over- whelming information irrelevant to the question and thus hurt the overall VQA performance.</p></li>
</ul>
<h1 id="related-work">Related work</h1>
<ol style="list-style-type: decimal">
<li><p>The most similar model is the Stacked Attention Networks (SAN) proposed by Yang et al. <a href="http://arxiv.org/abs/1511.02274">&quot;Stacked attention networks for image question answering&quot;</a>. Both models use attention mechanism that combines the words and image regions.</p></li>
<li><p>Another model that uses attention mechanism in solving VQA problems is the ABC-CNN model described in <a href="https://arxiv.org/abs/1511.05960">&quot;Abc-cnn: An attention based convolutional neural network for visual question answering&quot;</a>. ABC-CNN uses the question embedding to configure convolutional kernels that will define an attention weighted map over the image features.</p></li>
<li><p>Another attention model for visual question answering is proposed in <a href="http://arxiv.org/abs/1511.07394">&quot;Where to look: Focus regions for visual question answering&quot;</a>. The work, is closely related to the work by <a href="https://arxiv.org/abs/1511.05960">&quot;Abc-cnn: An attention based convolutional neural network for visual question answering&quot;</a>, in that it also applies a weighted map over the image and the question word features.</p></li>
<li><p>Similar to their work, they use object proposals from <a href="http://research.microsoft.com/pubs/220569/ZitnickDollarECCV14edgeBoxes.pdf">&quot;Locating object proposals from edges&quot;</a> to select image regions instead of the whole image.</p></li>
<li><p>In contrast, the model proposed in <a href="http://research.microsoft.com/pubs/220569/ZitnickDollarECCV14edgeBoxes.pdf">&quot;Locating object proposals from edges&quot;</a> straightforwardly concatenate all the image region features with the question word features and feed them all at once to a two layer network.</p></li>
<li><p>Another interesting approach worth mentioning is the work by Andreas et al. <a href="https://arxiv.org/abs/1601.01705">&quot;Learning to compose neural networks for question answering&quot;</a>. They use a <span style="color:red">semantic grammar parser to parse the question</span> and propose neural network layouts accordingly. They train a model to learn to compose a network from one of the proposed network layouts using several types of neural modules, each specifically designed to address the different sub-tasks of the VQA problem (e.g. counting, locating an object, etc.).</p></li>
</ol>
<h1 id="contribution">Contribution</h1>
<p>Different from above work, they propose a <span style="color:red">question driven attention model</span>(<span style="color:red">Focused Dynamic Attention (FDA)</span>) that is able to <span style="color:red">automatically identify and focus on image regions relevant for the current question</span>.</p>
<p>With the FDA model, <span style="color:red">computers can select and recognize the image regions in a well-aligned sequence with the key words containing in a given question</span>. <span style="color:red">FDA mechanism</span> can learns to use the <span style="color:red">question word</span> order to shift the focus from one image object(the corresponding object bounding boxes), to another. It fuses <span style="color:red">local and global context</span> visual features with textual features.</p>
<p>The main contributions of this work is introduce a <span style="color:red">Focused Dynamic Attention (FDA) mechanism</span> that learns to use the question word order to shift the focus from one image object, to another(A <span style="color:red">question driven attention model</span> that is able to <span style="color:red">automatically identify and focus</span> on image regions relevant for the current question).</p>
<blockquote>
<p>Answer the question of “How many apples are in the basket?”</p>
</blockquote>
<ul>
<li>FDA would first localize the regions corresponding to the key words <span style="color:red">“apples”</span> and <span style="color:red">“basket”</span> (with the help of a <span style="color:red">generic object detector</span>)</li>
<li>Extract description features from these regions of interest.</li>
<li>VQA compliments(赞美) the features <span style="color:red">from selected image regions</span> with a <span style="color:red">global image feature</span> providing contextual information for the overall image</li>
<li>and reconstruct a visual representation by encoding them with a LSTM unit.</li>
</ul>
<h1 id="model">Model</h1>
<pre><code>Motivation:
The proposed attention mechanism is loosely inspired on the human visual attention mechanism. Humans shift the focus from one image region to another, before understanding how the regions relate to each other and grasping the meaning of the whole image.</code></pre>
<p>The visual question answering problem can be represented as predicting the best answer <span class="math inline">\(\hat{a}\)</span> given an image <span class="math inline">\(I\)</span> and a question <span class="math inline">\(q\)</span>. Common practice is to use the 1,000 most common answers in the training set and thus simplify the VQA task to a classification problem. The following equation represents the problem mathematically:</p>
<p><span class="math display">\[
\hat{a}=\text{argmax}_{a\in \Omega}p(a|I,q;\theta) \tag{1}
\]</span></p>
<p>where <span class="math inline">\(\Omega\)</span> is the set of all possible answers and <span class="math inline">\(\theta\)</span> are the model weights.</p>
<p align="center">
<img src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-03-25/e959a426d02dd014c1346131ac38ed50114c17b7/5-Figure1-1.png" width="500" >
</p>
<p><font size="2">Fig. 1. Focused dynamic attention model diagram.</font></p>
<h2 id="focused-dynamic-attention-for-vqa">Focused Dynamic Attention for VQA</h2>
<ol style="list-style-type: decimal">
<li><p>Question Understanding Following a common practice, their FDA model uses an LSTM network to encode the question in a vector representation.</p></li>
<li><p>Image Understanding Use a pre-trained CNN to extract image feature vectors. Contrary to the existing approaches, they employ an LSTM network to combine the local and global visual features into a joint representation</p></li>
<li><p>Focused Dynamic Attention Mechanism The attention mechanism works as follows. For each image object1 it uses word2vec word embeddings <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">&quot;Distributed repre- sentations of words and phrases and their compositionality&quot;</a> to <span style="color:red">measure the similarity between the question words and the object label</span>.</p></li>
<li><p>Next, it selects objects with similarity score greater than 0.5 and extracts the feature vectors of the <span style="color:red">objects bounding boxes</span> with a pre-trained ResNet model <a href="https://arxiv.org/abs/1512.03385">&quot;Deep residual learning for image recognition&quot;</a>.</p></li>
<li><p>Following the question word order, it feeds the LSTM network with the corresponding object feature vectors.</p></li>
<li><p>Finally, it feeds the LSTM network with the <span style="color:red">feature vector of the whole image</span> and it uses the <span style="color:red">resulting LSTM state as a visual representation</span>.</p></li>
<li><p>Thus, <span style="color:red">the attention mechanism enables the model to combine the local and global visual features into a single representation</span>, necessary for answering complex visual questions.</p></li>
</ol>
<h2 id="multimodal-representation-fusion">Multimodal Representation Fusion</h2>
<p>They regard the final state of the <span style="color:red">two LSTM networks as a question and image representation</span>. Start fusing them into <span style="color:red">single representation by applying Tanh on the question representation and ReLU2 on the image representation 3</span>. They proceed by doing an element-wise multiplication of the two vector representations and the resulting vector is fed to a fully-connected neural network. Finally a SoftMax layer classify the multimodal representation into one of the possible 4 answers.</p>
<h1 id="evaluation">Evaluation</h1>
<h2 id="dataset-baseline">Dataset &amp; Baseline</h2>
<p align="center">
<img src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-03-25/e959a426d02dd014c1346131ac38ed50114c17b7/6-Figure2-1.png" width="500" >
</p>
<p><font size="2">Fig. 2. Representative examples of questions (black), (a subset of the) answers given when looking at the image (green), and answers given when not looking at the image (blue) for two images from the VQA dataset.</font></p>
<ul>
<li>Dataset : <a href="http://visualqa.org/">VQA dataset</a></li>
<li>Baseline Model</li>
<li><a href="https://github.com/abhshkdz/neural-vqa">&quot; Exploring Models and Data for Image Question Answering&quot;</a>: LSTM + CNN</li>
<li><a href="https://github.com/VT-vision-lab/VQA_LSTM_CNN">&quot;Deeper lstm and normalized cnn visual question answering model&quot;</a>: Two layer LSTM + CNN</li>
</ul>
<h2 id="implementation-training-details">Implementation &amp; Training Details</h2>
<ol style="list-style-type: decimal">
<li>Transform the <span style="color:red">question words</span> into <span style="color:red">a vector</span> form by multiplying one-hot vector representation with <span style="color:red">a word embedding matrix</span>. The vocabulary size is <span style="color:red">12,602</span> and the word embeddings are <span style="color:red">300 dimensional</span>.</li>
<li>Feed a pre-trained ResNet network <a href="https://arxiv.org/abs/1512.03385">&quot;Deep residual learning for image recognition&quot;</a> and use the <span style="color:red">2,048 dimensional</span> weight vector of the layer <span style="color:red">before the last fully-connected layer</span>.</li>
<li>The <span style="color:red">word</span> and <span style="color:red">image</span> vectors are feed into two separate LSTM networks. The LSTM networks are standard implementation of one layer LSTM network, with a <span style="color:red">512</span> dimensional state vector. The final state of the question LSTM is passed through <span style="color:red">Tanh</span>, while the final state of the image LSTM is passed through <span style="color:red">ReLU</span>.</li>
<li>They do <span style="color:red">element-wise multiplication</span> on the resulting vectors, to obtain a <span style="color:red">multimodal representation vector</span>, which is then fed to a fully-connected neural network.</li>
</ol>
<h2 id="evaluation-comparison">Evaluation &amp; Comparison</h2>
<ol style="list-style-type: decimal">
<li>FDA VS <a href="http://visualqa.org/">&quot;Vqa: Visual question answering&quot;</a>.</li>
</ol>
<p>The advantage of employing focused dynamic attention in FDA is more significant when solving the multiple-choice VQA problems.</p>
<p>Table 1. Comparison between the baselines from <a href="http://visualqa.org/">VQA</a>, the state-of-the-art models and their FDA model on VQA test-dev and test-standard data for the open-ended task. Results from most recent methods including <a href="http://arxiv.org/abs/1511.05676">CM</a>, <a href="https://arxiv.org/abs/1511.06973">ACK</a>, <a href="http://arxiv.org/abs/1512.02167">iBOWIMG</a>, <a href="http://arxiv.org/abs/1511.05756">DPPnet</a>, <a href="https://arxiv.org/abs/1601.01705">D-NMN</a>, <a href="https://github.com/VT-vision-lab/VQA_LSTM_CNN">D-LSTM</a>, and <a href="http://arxiv.org/abs/1511.02274">SAN</a> are provided and compared with.</p>
<p align="center">
<img src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-03-25/e959a426d02dd014c1346131ac38ed50114c17b7/8-Table1-1.png" width="400" >
</p>
<p>Table 2. Comparison between the baselines from <a href="http://visualqa.org/">VQA</a>, the state-of-the-art models and our FDA model on VQA test-dev and test-standard data for the multiple-choice task. Results from most recent methods including <a href="http://arxiv.org/abs/1511.07394">WR</a>, <a href="http://arxiv.org/abs/1512.02167">iBOWIMG</a>, <a href="http://arxiv.org/abs/1511.05756">DPPnet</a>, and <a href="https://github.com/VT-vision-lab/VQA_LSTM_CNN">D-LSTM</a> are also shown for comparison.</p>
<p align="center">
<img src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-03-25/e959a426d02dd014c1346131ac38ed50114c17b7/8-Table2-1.png" width="400" >
</p>
<h2 id="results">Results</h2>
<p align="center">
<img src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-03-25/e959a426d02dd014c1346131ac38ed50114c17b7/10-Figure3-1.png" width="500" >
</p>
<font size="2">Fig. 3. Representative examples where focusing on the <span style="color:red">question related objects</span> helps FDA answer “What color” type of questions. The question words in bold have been matched with an image region. The yellow region caption box contains the question word, followed by the region label, and in parenthesis their cosine similarity
</p>
<p align="center">
<img src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-03-25/e959a426d02dd014c1346131ac38ed50114c17b7/11-Figure4-1.png" width="500" >
</p>
<font size="2">Fig. 4. Representative examples where the model focuses on <span style="color:red">different regions from the same image</span>, depending on the question. The question words in bold have been matched with an image region. The yellow region caption box contains the question word, followed by the region label, and in parenthesis their cosine similarity
</p>
<p align="center">
<img src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-03-25/e959a426d02dd014c1346131ac38ed50114c17b7/12-Figure5-1.png" width="500" >
</p>
<font size="2">Fig. 5. Representative examples of questions that require image object identification. The question words in bold have been matched with an image region. The yellow region caption box contains the question word, followed by the region label, and in parenthesis their cosine similarity
</p>
<h1 id="conclusion">Conclusion</h1>
<ol style="list-style-type: decimal">
<li>Propose FDA model to solve the challenging VQA problems. FDA is built upon a generic object-centric attention model for extracting question related visual features from an image as well as a stack of multiple LSTM layers for feature fusion.</li>
<li>By only focusing on the identified regions specific for proposed questions, FDA was shown to be able to filter out overwhelming irrelevant informations from cluttered back-ground or other regions, and thus substantially improved the quality of visual representations in the sense of answering proposed questions.</li>
<li>By fusing cleaned regional representation, global context and question representation via LSTM layers, FDA provided significant performance improvement over baselines on the VQA benchmark datasets, for both the open-ended and multiple-choices VQA tasks.</li>
</ol>
</body>
</html>
