<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h2 id="where-to-look-focus-regions-for-visual-question-answering"><a href="http://arxiv.org/abs/1511.07394">Where to look: Focus regions for visual question answering</a></h2>
<h1 id="abstract">abstract</h1>
<p>We present a method that learns to answer visual questions by selecting image regions relevant to the text-based query. Our method maps textual queries and visual features from various regions into a shared space where they are compared for relevance with an inner product. Our method exhibits significant improvements in answering questions such as &quot;what color,&quot; where it is necessary to evaluate a specific location, and &quot;what room,&quot; where it selectively identifies informative image regions. Our model is tested on the recently released VQA (<a href="">&quot;VQA&quot;</a>) dataset, which features free-form human-annotated questions and answers.</p>
<h1 id="introduction">Introduction</h1>
<h2 id="intuition-why-do-you-think-this-approach-will-work">Intuition: why do you think this approach will work?</h2>
<p>Visual question answering (VQA) is the task of answering a natural language question about an image. VQA includes many challenges in language representation and grounding, recognition, common sense reasoning, and specialized tasks like counting and reading. In this paper, we focus on a key problem for VQA and other visual reasoning tasks: knowing where to look. Consider Figure.</p>
<p align="center">
<img src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-03-25/175e9bb50cc062c6c1742a5d90c8dfe31d2e4e22/0-Figure1-1.png" width="500" >
</p>
<p><font size="2">Figure 1. Our goal is to identify the correct answer for a natural language question, such as “What color is the walk light?” or “Is it raining?” We particularly focus on the problem of learning where to look. This is a challenging problem as it requires grounding language with vision and learning to recognize objects, use relations, and determine relevance. For example, whether it is raining may be determined by detecting the presence of puddles gray skies, or umbrellas in the scene, whereas the color of the walk light requires focused attention on the light alone. The above figure shows example attention regions produced by our proposed model.</font></p>
<p>It's easy to answer &quot;What color is the walk light?&quot; if the light bulb is localized, while answering whether it's raining may be dealt with by identifying umbrellas, puddles, or cloudy skies. We want to learn where to look to answer questions supervised by only images and question/answer pairs. For example, if we have several training examples for &quot;What time of day is it?&quot; or similar questions, the system should learn what kind of answer is expected and where in the image it should base its response.</p>
<p>Learning where to look from question-image pairs has many challenges. Questions such as &quot;What sport is this?&quot; might be best answered using the full image. Other questions such as &quot;What is on the sofa?&quot; or &quot;What color is the woman's shirt?&quot; require focusing on particular regions. Still others such as &quot;What does the sign say?&quot; or &quot;Are the man and woman dating?&quot; require specialized knowledge or reasoning that we do not expect to achieve. The system needs to learn to recognize objects, infer spatial relations, determine relevance, and find correspondence between natural language and visual features. Our key idea is to learn a non-linear mapping of language and visual region features into a common latent space to determine relevance. The relevant regions are then used to score a specific question-answer pairing. The latent embedding and the scoring function are learned jointly using a margin-based loss supervised solely by question-answer pairings. We perform experiments on the VQA dataset (<a href="">&quot;VQA&quot;</a>) because it features open-ended language, with a wide variety of questions. Specifically, we focus on its multiple-choice format because its evaluation is much less ambiguous than open-ended answer verification.</p>
<p>We focus on learning where to look but also provide useful baselines and analysis for the task as a whole. Our contributions are as follows:</p>
<ul>
<li>We present an image-region selection mechanism that learns to identify image regions relevant to questions.</li>
<li>We present a learning framework for solving multiple-choice visual QA with a margin-based loss that significantly outperforms provided baselines from (<a href="">&quot;VQA&quot;</a>).</li>
<li>We compare with baselines that answer questions without the image, use the whole image, and use all image regions with uniform weighting, providing a detailed analysis for when selective regions improve VQA performance.</li>
</ul>
<h1 id="related-works">Related Works</h1>
<p>Many recent works in tying text to images have explored the task of automated image captioning ([&quot;Improving image-sentence embeddings using large weakly annotated photo collections&quot;],<a href="">&quot;Show, attend and tell: Neural image caption generation with visual attention&quot;</a>,<a href="">&quot;Deep visual-semantic alignments for generating image descriptions&quot;</a>,<a href="">&quot;Deep captioning with multimodal recurrent neural networks&quot;</a>,<a href="">&quot;Explain images with multimodal recurrent neural networks&quot;</a>,<a href="">&quot;Long-term recurrent convolutional networks for visual recognition and description&quot;</a>,<a href="">&quot;Mind's eye: A recurrent visual representation for image caption generation&quot;</a>,<a href="">&quot;Show and tell: A neural image caption generator&quot;</a>). While VQA can be considered as a type of directed captioning task, our work relates to some (<a href="">&quot;Show, attend and tell: Neural image caption generation with visual attention&quot;</a>, <a href="">&quot;From captions to visual concepts and back&quot;</a>) in that we learn to employ an attention mechanism for region focus, though our formulation makes determining region relevance a more explicit part of the learning process.</p>
<p>In Fang et al. (<a href="">&quot;From captions to visual concepts and bac&quot;</a>), words are detected in various portions of the image and combined together with a language model to generate captions. Similarly, Xu et al. (<a href="">&quot;Show, attend and tell: Neural image caption generation with visual attention&quot;</a>) uses a recurrent network model to detect salient objects and generate caption words one by one. Our model works in the opposite direction of these caption models at test time by determining the relevant image region given a textual query as input. This allows our model to determine whether a question-answer pair is a good match given evidence from the image.</p>
<p>Partly due to the difficulty of evaluating image captioning, several visual question answering datasets have been proposed along with applied approaches. We choose to experiment on VQA (<a href="">&quot;VQA&quot;</a>) due to the open ended nature of its question and answer annotations. Questions are collected by asking annotators to pose a difficult question for a smart robot, and multiple answers are collected for each question. We experiment on the multiple-choice setting as its evaluation is less ambiguous than that of open-ended response evaluation. Most other visual question answering datasets (<a href="">&quot;Exploring models and data for image question answering,Visual madlibs: Fill in the blank image generation and question answering&quot;</a>) are based on reformulating existing object annotations into questions, which provides an interesting visual task but limits the scope of visual and abstract knowledge required.</p>
<p>Our model is inspired by End-to-End Memory Networks (<a href="">&quot;Weakly supervised memory networks&quot;</a>) proposed for answering questions based on a series of sentences. The regions in our model are analogous to the sentences in theirs, and, similarly to them, we learn an embedding to project question and potential features into a shared subspace to determine relevance with an inner product. Our method differs in many details such as the language model and more broadly in that we are answering questions based on an image, rather than a text document. Ba et al. (<a href="">&quot;Predicting deep zero-shot convolutional neural networks using textual descriptions&quot;</a>) also uses a similar architecture, but in a zero-shot learning framework to predict classifiers for novel categories. They project language and vision features into a shared subspace to perform similarity computations with inner products like us, though the score is used to guide the generation of object classifiers rather than to rank image regions.</p>
<p>Existing approaches in VQA tend to use recurrent networks to model language and predict answers (<a href="">&quot;Exploring models and data for image question answering,VQA,Visual madlibs: Fill in the blank image generation and question answering,Ask your neurons: A neural-based approach to answering questions about images&quot;</a>), though simpler Bag-Of-Words (BOW) and averaging models have been shown to perform roughly as well if not better than sequence-based LSTM (<a href="">&quot;Exploring models and data for image question answering, VQA&quot;</a>). Yu et al. (<a href="">&quot;Visual madlibs: Fill in the blank image generation and question answering&quot;</a>), which proposes a Visual Madlibs dataset for fill-in-the-blank and question answering, focuses their approach on learning latent embeddings and finds normalized CCA on averaged word2vec representations (<a href="">&quot;Improving image-sentence embeddings using large weakly annotated photo collections,Efficient estimation of word representations in vector space&quot;</a>) to outperform recurrent networks for embedding. Similarly in our work, we find a fixed-length averaged representation of word2vec vectors for language to be highly effective and much simpler to train, and our approach differs at a high level in our focus on learning where to look.</p>
<h1 id="approach">Approach</h1>
<p align="center">
<img src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-03-25/175e9bb50cc062c6c1742a5d90c8dfe31d2e4e22/2-Figure3-1.png" width="800" >
</p>
<p>Table 2. Accuracy comparison on Test-dev (top) and Test-standard (bottom). Our model outperforms the best performing image and text models from [1].</p>
<p>Our method learns to embed the textual question and the set of visual image regions into a latent space where the inner product yields a relevance weighting for each region.</p>
<p>The input is a question, potential answer, and image features from a set of automatically selected candidate regions. We encode the parsed question and answer using word2vec (<a href="">&quot;Efficient estimation of word representations in vector space&quot;</a>) and a two-layer network. Visual features for each region are encoded using the top two layers (including the output layer) of a CNN trained on ImageNet (<a href="">&quot;ImageNet Large Scale Visual Recognition Challenge&quot;</a>). The language and vision features are then embedded and compared with a dot product, which is soft-maxed to produce a per-region relevance weighting. Using these weights, a weighted average of concatenated vision and language features is the input to a 2-layer network that outputs a score for whether the answer is correct.</p>
<h2 id="qa-objective">QA Objective}</h2>
<p>Our model is trained for the multiple choice task of the VQA dateset. For a given question and its corresponding choices, the objective of our network aims to maximize a margin between correct and incorrect choices in a structured-learning fashion. We achieve this by using a hinge loss over predicted confidences <span class="math inline">\(y\)</span>.</p>
<p>In our setting, multiple answers could be acceptable to varying degrees, as correctness is determined by the consensus of 10 annotators. For example, most may say that the color of a scarf is &quot;blue&quot; while a few others say &quot;purple&quot;. To take this into account, we scale the margin by the gap in number of annotators returning the specific answer:</p>
<p><span class="math display">\[
  {\mathcal L}(y) = \max_{\forall n \ne p} (0,y_n + (a_p - a_n) - y_p).
\]</span></p>
<p>The above objective requires that the score of the correct answer (<span class="math inline">\(y_p\)</span>) is at least some margin above the score of the highest-scoring incorrect answer (<span class="math inline">\(y_n\)</span>) selected from among the set of incorrect choices (<span class="math inline">\(n \ne p\)</span>). For example, if 6/10 of the annotators answer <span class="math inline">\(p\)</span> (<span class="math inline">\(a_p = 0.6\)</span>) and 2 annotators answer <span class="math inline">\(n\)</span> (<span class="math inline">\(a_n = 0.2\)</span>), then <span class="math inline">\(y_p\)</span> should outscore <span class="math inline">\(y_n\)</span> by a margin of at least 0.4.</p>
<h2 id="region-selection-layer">Region Selection Layer</h2>
<p>Our region selection layer selectively combines incoming text features with image features from relevant regions of the image. To determine relevance, the layer first projects the image features and the text features into a shared N-dimensional space, after which an inner product is computed for each question-answer pair and all available regions.</p>
<p>Let <span class="math inline">\(G_r\)</span> be the projection of all region features in column vectors of <span class="math inline">\(X_r\)</span>, and <span class="math inline">\(\vec{g}_l\)</span> be the projection of a single embedded question-answer pair. The feedforward pass to compute the relevance weightings is computed as follows: <span class="math display">\[
\begin{align}
  G_r =&amp; AX_r+\vec{b}_r\\
  \vec{g}_l =&amp; B\vec{x}_l + \vec{b}_l\\
  \vec{s}_{l,r} =&amp; \sigma(G_r^T\vec{g}_l)\\
  \sigma(\vec{z}) = &amp;\frac{e^{z_j}}{\sum_{k=1}^{K}e^{z_k}}\mbox{ for }
  j = 1,...K
\end{align}
\]</span> Here, the output <span class="math inline">\(\vec{s}_{l,r}\)</span> is the softmax normalized weighting (<span class="math inline">\(\sigma\)</span>) of the inner products of <span class="math inline">\(\vec{g}_l\)</span> with each projected region feature in <span class="math inline">\(G_r\)</span>. Vectors <span class="math inline">\(\vec{b}\)</span> represent biases. The purpose of the inner product is to force the model to determine region relevance in a vector similarity fashion.</p>
<p>Using 100 regions per image, this gives us 100 region weights for a question-answer pair. Next, the text features are concatenated directly with image features for each region to produce 100 different feature vectors. This is shown in the horizontal stacking of <span class="math inline">\(X_r\)</span> and repetitions of <span class="math inline">\(\vec{x}_l\)</span> below. Each feature vector is linearly projected with <span class="math inline">\(W\)</span>, and the weighted average is computed using <span class="math inline">\(\vec{s}_r\)</span> to attain feature vector <span class="math inline">\(\vec{a}_l\)</span> for each question and answer pair, which is then fed through relu and batch-normalization layers. <span class="math display">\[
\begin{align}
  P_{l,r} =&amp; W\left[ {\begin{array}{c} X_r\\\begin{array}{ccc}- &amp;\vec{x}_l &amp;-\end{array} \end{array}} \right]+\vec{b}_o\\
  \vec{a}_l =&amp; P\vec{s}_{l,r}
\end{align}
\]</span> We also tried learning to predict a relevance score directly from concatenated vision and language features, rather than computing the dot product of the features in a latent embedded space. However, the resulting model appeared to learn a salient region weighting scheme that varied little with the language component. The inner-product based relevance was the only formulation we tried that successfully takes account of both the query and the region information.</p>
<h2 id="language-representation">Language Representation</h2>
<p>We represent our words with 300-dimensional word2vec vectors (<a href="">&quot;Efficient estimation of word representations in vector space&quot;</a>) for their simplicity and compact representation. We are also motivated by the ability of vector-based language representations to encode similar words with similar vectors, which may aid answering open-ended questions. Using averages across word2vec vectors, we construct fixed-length vectors for each question-answer pair, which our model then learns to score. In our results section, we show that our vector-averaging language model noticeably outperforms a more complex LSTM-based model from (<a href="">&quot;VQA&quot;</a>), demonstrating that BOW-like models provide very effective and simple language representations for VQA tasks.</p>
<p>We first tried separately averaging vectors for each word with the question and answer, concatenating them to yield a 600-dimensional vector, but since the word2vec representation is not sparse, averaging several words may muddle the representation. We improve the representation using the Stanford Parser (<a href="">&quot;Generating typed dependency parses from phrase structure parse&quot;</a>) to bin the question into additional separate semantic bins. The bins are defined as follows:</p>
<p><span class="math inline">\(\mathbf{Bin 1}\)</span> captures the type of question by averaging the word2vec representation of the first two words. For example, &quot;How many&quot; tends to require a numerical answer, while &quot;Is there&quot; requires a yes or no answer.</p>
<p><span class="math inline">\(\mathbf{Bin 2}\)</span> contains the nominal subject to encode subject of question.</p>
<p><span class="math inline">\(\mathbf{Bin 3}\)</span> contains the average of all other noun words.</p>
<p><span class="math inline">\(\mathbf{Bin 4}\)</span> contains the average of all remaining words, excluding determiners such as &quot;a,&quot; &quot;the,&quot; and &quot;few.&quot;</p>
<p>Each bin then contains a 300-dimensional representation, which are concatenated with a bin for the words in the candidate answer to yield a 1500-dimensional question/answer representation. This representation separates out important components of a variable-length question while maintaining a fixed-length representation that simplifies the network architecture.</p>
<p align="center">
<img src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-03-25/175e9bb50cc062c6c1742a5d90c8dfe31d2e4e22/3-Figure4-1.png" width="400" >
</p>
<p>Figure 4. Example parse-based binning of questions. Each bin is represented with the average of the word2vec vectors of its members. Empty bins are represented with a zero-vector.</p>
<h2 id="image-features">Image Features}</h2>
<p>The image features are fed directly into the region-selection layer from a pre-trained network. We first select candidate regions by extracting the top-ranked 99 Edge Boxes (<a href="">&quot;Edge boxes: Locating object proposals from edge&quot;</a>) from the image after performing non-max suppression with a 0.2 intersection over union overlap criterion. We found this aggressive non-max suppression to be important for selecting smaller regions that may be important for some questions, as the top-ranked regions tend to be highly overlapping large regions. Finally, a whole-image region is also added to ensure that the model at least has the spatial support of the full frame if necessary, bringing the total number of candidate regions to 100 per image. While we have not experimented with the number of regions, it is possible that the improved recall from additional regions may improve performance.</p>
<p>We extract features using the VGG-s network (<a href="">&quot;Return of the devil in the details: Delving deep into convolutional nets&quot;</a>), concatenating the output from the last fully connected layer (4096 dimensions) and the pre-softmax layer (1000 dimensions) to get a 5096 dimensional feature per region. The pre-softmax classification layer was included to provide a more direct signal for objects from the Imagenet (<a href="">&quot;ILSVRC15&quot;</a>) classification task.</p>
<h2 id="training">Training}</h2>
<p>Our overall network architecture is implemented in MatConvNet(<a href="">&quot;Matconvnet -- convolutional neural networks for matlab&quot;</a>). Our fully connected layers are initialized with Xavier initialization (<span class="math inline">\(\frac{1}{\sqrt{n_{in}}}\)</span>) (<a href="">&quot;Understanding the difficulty of training deep feedforward neural networks&quot;</a>) and separated with a batch-normalization (<a href="">&quot;Batch normalization: Accelerating deep network training by reducing internal covariate shift&quot;</a>) and relu layer (<a href="">&quot;Deep sparse rectifier neural networks&quot;</a>) between each. The word2vec text features are fed into the network's input layer, whereas the image region features feed in through the region selection layer.</p>
<p>Our network sizes are set as follows. The 1500 dimensional language features first pass through 3 fully connected layers with output dimensions 2048, 1500, and 1024 respectively. The embedded language features are then passed through the region selection layer to be combined with the vision features. Inside the region selection layer, projections <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> project both vision and language representations down to 900 dimensions before computing their inner product. The exiting feature representation passes through <span class="math inline">\(W\)</span> with an output dimension of 2048. then finally through two more fully connected layers with output dimensions of 900 and 1 where the output scalar is the question-answer pair score.</p>
<p>It is necessary to pay extra attention to the initialization of the region-selection layer. The magnitude of the projection matrices <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span> and <span class="math inline">\(W\)</span> are initialized to <span class="math inline">\(0.001\)</span> times the standard normal distribution. We found that low initial values were important to prevent the softmax in selection from spiking too early and to prevent the higher-dimensional vision component from dominating early in the training.</p>
<h1 id="experiments">Experiments}</h1>
<p>We evaluate the effects of our region-selection layer on the multiple-choice format of the MS COCO Visual Question Answering (VQA) dataset (<a href="">&quot;VQA&quot;</a>). This dataset contains 82,783 images for training, 40,504 for validation, and 81,434 for testing. Each image has 3 corresponding questions with recorded free-response answers from 10 annotators. Any response that comes from at least 3 annotators is considered correct. We use the 18-way multiple choice task because its evaluation is much less ambiguous than the open-ended response task, though our method could be applied to the latter by treating the most common or likely K responses as a large K-way multiple choice task. We trained using only the training set, with 10% set aside for model selection and parameter tuning. We perform detailed evaluation on the validation set and further comparison on the test set using the provided submission tools.</p>
<p>We evaluate and analyze how much our region-weighting improves accuracy compared to using the whole image or only language (Tables<sub>,</sub>,~) and show examples in Figure~. We also perform a simple evaluation on a subset of images showing that relevant regions tend to have higher than average weights (Fig.~). We also show the advantage of our language model over other schemes (Table~).</p>
<h2 id="comparisons-between-region-image-and-language-only-models">Comparisons between region, image, and language-only models}</h2>
<p>We compare our region selection model with several baseline methods, described below.</p>
<p><span class="math inline">\(\mathbf{Language-only}\)</span> We train a network to score each answer purely from the language representation. This provides a baseline to demonstrate improvement due to image features, rather than just good guesses.</p>
<p><span class="math inline">\(\mathbf{Word+Whole image}\)</span> We concatenate CNN features computed over the entire image with the language features and score them using a 3-layer neural network, essentially replacing the region-selection layer with features computed over the whole image.</p>
<p><span class="math inline">\(\mathbf{Word+Uniform averaged region features}\)</span> To test that region weighting is important, we also try uniformly averaging features across all regions as the image representation and train as above.</p>
<p>Our proposed region-selection model outperforms all other models. Also, we can see that uniform weighting of regions is not helpful. We also include the best-performing LSTM question+image model from the authors of the VQA dataset (<a href="">&quot;VQA&quot;</a>). This model significantly underperforms even our much simpler baselines, which could be partly because the model was designed for open-ended answering and adapted for multiple choice.</p>
</body>
</html>
