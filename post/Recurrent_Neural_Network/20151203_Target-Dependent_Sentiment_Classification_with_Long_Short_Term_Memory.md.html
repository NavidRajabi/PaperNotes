<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<h2 id="target-dependent-sentiment-classification-with-long-short-term-memory"><a href="http://arxiv.org/abs/1512.01100">Target-Dependent Sentiment Classification with Long Short Term Memory</a></h2>
<p>TLDR; The authors propose two LSTM-based models for target-dependent sentiment classification. TD-LSTM uses two LSTM networks running towards to target word from left and right respectively, making a prediction at the target time step. TC-LSTM is the same, but additionally incorporates the an averaged target word vector as an input at each time step. The authors evaluate their models with pre-trained word embeddings on a Twitter sentiment classification dataset, achieving state of the art.</p>
<h4 id="key-points">Key Points</h4>
<ul>
<li>TD-LSTM: Two LSTM networks, running from left to right towards the target. The final states of both networks are concatenated and the prediction is made at the target word.</li>
<li>TC-LSTM: Same architecture as TD-LSTM, but also incorporates the word vector as an input at each time step. The word vector is the average of the word vectors for the target phrase.</li>
<li>Embeddings seem to make a huge difference, state of the art is only obtained with 200-dimensional GloVe embeddings.</li>
</ul>
<h4 id="notesquestions">Notes/Questions</h4>
<ul>
<li>A <em>huge</em> fraction of the performance improvement comes from pre-trained word embeddings. Without these, the proposed models clearly underperforms simpler models. This raises the question of whether incorporating the same embeddings into the simpler models would do.</li>
<li>Would've liked to see performance without <em>any</em> pre-trained embeddings.</li>
<li>The authors also experimented with attention mechanisms, but weren't able to achieve good results. Small size of training corpus may be the reason for this.</li>
</ul>
</body>
</html>
