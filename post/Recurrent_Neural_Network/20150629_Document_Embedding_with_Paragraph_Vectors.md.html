<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<h2 id="document-embedding-with-paragraph-vectors"><a href="http://arxiv.org/abs/1507.07998">Document Embedding with Paragraph Vectors</a></h2>
<p>TLDR; The authors evaluate Paragraph Vectors on large Wikipedia and arXiv document retrieval tasks and compare the results to LDA, BoW and word vector averaging models. Paragraph Vectors either outperform or match the performance of other models. The authors show how the embedding dimensionality affects the results. Furthermore, the authors find that one can perform arithemetic operations on paragraph vectors and obtain meaningful results and present qualitative analyses in the form of visualizations and document examples.</p>
<h4 id="data-sets">Data Sets</h4>
<p>Accuracy is evaluated by constructing triples, where a pair of items are close to each other and the third one is unrelated (or less related). Cosine similarity is used to evaluate semantic closeness.</p>
<p>Wikipedia (hand-built) PV: 93% Wikipedia (hand-built) LDA: 82% Wikipedia (distantly supervised) PV: 78.8% Wikipedia (distantly supervised) LDA: 67.7% arXiv PV: 85% arXiv LDA: 85%</p>
<h4 id="key-points">Key Points</h4>
<ul>
<li>Jointly training PV and word vectors seems to improve performance.</li>
<li>Used Hierarchical Softmax as Huffman tree for large vocabulary</li>
<li>The use only the PV-BoW model, because it's more efficient.</li>
</ul>
<h4 id="questionsnotes">Questions/Notes</h4>
<ul>
<li>Why the performance discrepancy between the arXiv and Wikipedia tasks? BoW performs surprisingly well on Wikipedia, but not arXiv. LDA is the opposite.</li>
</ul>
</body>
</html>
