<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="learning-online-alignments-with-continuous-rewards-policy-gradient"><a href="https://arxiv.org/pdf/1608.01281v1.pdf">Learning Online Alignments with Continuous Rewards Policy Gradient</a></h1>
<iframe width="560" height="315" src="https://www.youtube.com/embed/KHZVXao4qXs" frameborder="0" allowfullscreen>
</iframe>
<iframe width="560" height="315" src="https://www.youtube.com/embed/_tn-FN-jEPk" frameborder="0" allowfullscreen>
</iframe>
<p>TLDR: They present a new method forsolving sequence-to-sequence problems using <u>hard online alignments</u> instead of soft offline alignments. They use <u>hard binary stochastic decisions</u> to select the timesteps at which outputs will be produced. At each time step <span class="math inline">\(i\)</span>, a RNN decides whether to emit an output token (with a stochastic binary logistic unit <span class="math inline">\(b_i\)</span>), the previous time step <span class="math inline">\(\tilde{b}_{i-1}\)</span> and the previous target <span class="math inline">\(t_{i-1}\)</span> are fed into the model as input. This feedback ensures that the modelâ€™s outputs are maximallydependent and thus the model is from the sequence to sequence family.</p>
<h2 id="their-method">Their method</h2>
<div class="figure">
<img src="https://www.dropbox.com/s/4y7ja0t49xqro8h/Online_Alignments_Continuous_Rewards_Policy_Gradient.png?dl=1" />

</div>
<h2 id="conclusions">Conclusions</h2>
<p>In this work, they presented a simple model that can solve sequence-to-sequence problems without the need to process the entire input sequence first. Their model directly maximizes the log probability ofthe correct answer by combining standard supervised backpropagation and a policy gradient method. Their results also suggest that policy gradient methods are reasonably powerful, and that they can trainhighly complex neural networks that learn to make nontrivial stochastic decisions</p>
</body>
</html>
