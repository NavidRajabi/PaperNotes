<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h2 id="image-question-answering-using-convolutional-neural-networkwith-dynamic-parameter-prediction"><a href="http://arxiv.org/abs/1511.05756">Image question answering using convolutional neural networkwith dynamic parameter prediction</a></h2>
<p>Home : http://cvlab.postech.ac.kr/research/dppnet/</p>
<p align="center">
<img src="https://camo.githubusercontent.com/630c2499f26c6685f41caa61508a8a7f1179b9dd/687474703a2f2f7333322e706f7374696d672e6f72672f633677646e623561642f53637265656e5f53686f745f323031365f30355f30385f61745f365f30395f33305f504d2e706e67" width="500" >
</p>
<ul>
<li>This is a very creative paper that is different from others</li>
<li>Solves only for single answer question, to reduce the main network to solve for a classification problem</li>
<li>Other architectures have LSTM as the main framework, this paper uses the convnet / regular DNN as the main body</li>
<li>Question is embedded via GRUs to dynamically control the weights of the second last layer of the CNN / DNN, which is very original</li>
<li>To predict a large number of weights in the dynamic parameter layer effectively and efficiently, applies hashing trick, which reduces the number of parameters significantly with little impact on network capacity</li>
<li>Shows that the hashing trick does not deteriorate the accuracy of the classification network</li>
</ul>
<h1 id="abstract">ABSTRACT</h1>
<p>They tackle <span style="color:red">image question answering (ImageQA)</span> problem by learning a convolutional neural network (CNN) with a dynamic parameter layer whose weights are determined adaptively based on questions.</p>
<p>For the adaptive parameter prediction, they employ a <span style="color:red">separate parameter prediction network</span>, which consists of <span style="color:red">gated recurrent unit (GRU)</span> taking a question as its input and a fully-connected layer generating a set of candidate weights as its output.</p>
<p>Since the dynamic parameter layer is a fully connected layer, it is challenging to predict a large number of parameters in the layer to construct the CNN for ImageQA.</p>
<p>However, it is challenging to construct a parameter prediction network for a large number of parameters in the fully-connected dynamic parameter layer of the CNN.</p>
<p>They reduce the complexity of this problem by incorporating a hashing technique, where the candidate weights given by the parameter prediction network are selected using a predefined hash function to determine individual weights in the dynamic parameter layer.</p>
<p>The proposed network---joint network with the CNN for ImageQA and the parameter prediction network---is trained end-to-end through back-propagation, where its weights are initialized using a pre-trained CNN and GRU.</p>
<p>The proposed algorithm illustrates the state-of-the-art performance on all available public ImageQA benchmarks. such as DAQUAR, COCO-QA and VQA.</p>
<h1 id="introduction">introduction}</h1>
<h2 id="holistic-scene-understanding"><a href="">Holistic scene understanding</a></h2>
<p>One of the ultimate goals in computer vision is holistic scene understanding (<a href="">&quot;Describing the scene as a whole: Joint object detection, scene classification and semantic segmentation&quot;</a>), which requires a system to capture various kinds of information such as objects, actions, events, scene, atmosphere, and their relations in many different levels of semantics.</p>
<p>Although significant progress on various recognition tasks has been made in recent years, these works focus only on solving relatively simple recognition problems in controlled settings, where each dataset consists of concepts with similar level of understanding (e.g. object, scene, bird species, face identity, action, texture etc). There has been less efforts made on solving various recognition problems simultaneously, which is more complex and realistic, even though this is a crucial step toward holistic scene understanding.</p>
<h2 id="imageqa---tackles-previously-mentioned-challenge"><a href="">ImageQA - tackles previously mentioned challenge</a></h2>
<p>ImageQA aims to solve the holistic scene understanding problem by proposing a task unifying various recognition problems.</p>
<p>The critical challenge of this problem is that different questions require different types and levels of understanding of an image to find correct answers.</p>
<p>For example, to answer the question like &quot;how is the weather?&quot; we need to perform classification on multiple choices related to weather, while we should decide between yes and no for the question like &quot;is this picture taken during the day?&quot;</p>
<p>For this reason, not only the performance on a single recognition task but also the capability to select a proper task is important to solve ImageQA problem.</p>
<p>Since different question require different type or level of understanding to answer, not only the performance on single recognition task but also the ability to perform proper recognition task selectively is improtant to solve this problem.</p>
<h2 id="existing-methods-based-on-a-single-classifier"><a href="">Existing methods based on a single classifier</a></h2>
<p>ImageQA problem has a short history in computer vision and machine learning community, but there already exist several approaches(<a href="">&quot;Are you talking to a machine? dataset and methods for multilingual image question answering&quot;</a>,<a href="">&quot;Learning to answer questions from image using convolutional neural network&quot;</a>,<a href="">&quot;A multi-world approach to question answering about real-world scenes based on uncertain input&quot;</a>,<a href="">&quot;Ask your neurons: A neural-based approach to answering questions about images&quot;</a>,<a href="">&quot;Exploring models and data for image question answering&quot;</a>).</p>
<p>These approaches extract image features using a CNN, and use CNN or bag-of-words to obtain feature descriptors from question.</p>
<p>They can be interpreted as a method that the answer is given by the co-occurrence of a particular combination of features extracted from an image and a question.</p>
<h1 id="their-approach"><a href="">Their approach</a></h1>
<p>Contrary to the existing approaches, they define a different recognition task depending on a question.</p>
<p>To realize this idea, we propose a deep CNN with a dynamic parameter layer whose weights are determined adaptively based on questions.</p>
<p>they claim that a single deep CNN architecture can take care of various tasks by allowing adaptive weight assignment in the dynamic parameter layer.</p>
<p>For the adaptive parameter prediction, they employ a parameter prediction network, which consists of gated recurrent units (GRU) taking a question as its input and a fully-connected layer generating a set of candidate weights for the dynamic parameter layer.</p>
<p>Since the dynamic parameter layer is a fully connected layer, it is challenging to predict a large number of parameters in the layer to construct the CNN for ImageQA.</p>
<p>they reduce the complexity of this problem by incorporating a hashing technique, where the candidate weights given by the parameter prediction network are selected using a predefined hash function to determine individual weights in the dynamic parameter layer.</p>
<p>The entire network including the CNN for ImageQA and the parameter prediction network is trained end-to-end through back-propagation, where its weights are initialized using pre-trained CNN and GRU.</p>
<p>Their main contributions in this work are summarized below:</p>
<ul>
<li>We successfully adopt a deep CNN with a dynamic parameter layer for ImageQA, which is a fully-connected layer whose parameters are determined dynamically based on a given question.</li>
<li>To predict a large number of weights in the dynamic parameter layer effectively and efficiently, we apply hashing trick<a href="">&quot;Compressing neural networks with the hashing trick&quot;</a>, which reduces the number of parameters significantly with little impact on network capacity.</li>
<li>We fine-tune GRU pre-trained on a large-scale text corpus<a href="">&quot;Skip-thought vectors&quot;</a> to improve generalization performance of our network. Pre-training GRU on a large corpus is natural way to deal with a small number of training data, but no one has attempted it yet to our knowledge.</li>
<li>This is the first work to report the results on all currently available benchmark datasets such as DAQUAR, COCO-QA and VQA.</li>
</ul>
<h1 id="related-work">Related Work</h1>
<h2 id="bayesian-approach"><a href="">Bayesian approach</a></h2>
<p>There are several recent papers to address ImageQA; the most of them are based on deep learning except.</p>
<p>Malinowski and Fritz<a href="">&quot;A multi-world approach to question answering about real-world scenes based on uncertain input&quot;</a> propose a Bayesian framework, which exploits recent advances in computer vision and natural language processing.</p>
<p>Specifically, it employs semantic image segmentation and symbolic question reasoning to solve ImageQA problem.</p>
<p>However, this method depends on a pre-defined set of predicates, which makes it difficult to represent complex models required to understand input images.</p>
<h2 id="deep-learning"><a href="">Deep learning</a></h2>
<p>Deep learning based approaches demonstrate competitive performances in ImageQA</p>
<p>LSTM is used to generate answer phrases.</p>
<p>Most approaches based on deep learning commonly use CNNs to extract features from image while they use different strategies to handle question sentences.</p>
<p>Some algorithms employ embedding of joint features based on image and question.</p>
<p>However, learning a softmax classifier on the simple joint features---concatenation of CNN-based image features and continuous bag-of-words representation of a question---performs better than LSTM-based embedding on COCO-QA dataset.</p>
<p>Another line of research is to utilize CNNs for feature extraction from both image and question and combine the two features<a href="">&quot;Learning to answer questions from image using convolutional neural network&quot;</a>; this approach demonstrates impressive performance enhancement on DAQUAR <a href="">&quot;A multi-world approach to question answering about real-world scenes based on uncertain input&quot;</a> dataset by allowing fine-tuning the whole parameters. %However, predicting the parameters of the network for ImageQA has not been tried.</p>
<h2 id="predicting-parameter-of-neural-network"><a href="">Predicting parameter of neural network</a></h2>
<p>Discuss hashing trick here together, but the current paragraph should be reduced a lot and hashing need to be discussed with more importance.</p>
<p>The prediction of the weight parameters in deep neural networks has been explored in <a href="">&quot;Predicting deep zero-shot convolutional neural networks using textual descriptions&quot;</a> in the context of zero-shot learning.</p>
<p>To perform classification of unseen classes, it trains a multi-layer perceptron to predict a binary classifier for class-specific description in text. However, this method is not directly applicable to ImageQA since finding solutions based on the combination of question and answer is a more complex problem than the one discussed in <a href="">&quot;Predicting deep zero-shot convolutional neural networks using textual descriptions&quot;</a>, and ImageQA involves a significantly larger set of candidate answers, which requires much more parameters than the binary classification case.</p>
<p>Recently, a parameter reduction technique based on a hashing trick is proposed by Chen <a href="">&quot;Compressing neural networks with the hashing trick&quot;</a> to fit a large neural network in a limited memory budget.</p>
<p>However, applying this technique to the dynamic prediction of parameters in deep neural networks is not attempted yet to our knowledge.</p>
<ol style="list-style-type: decimal">
<li>First, contrary to zero-shot learning where each texture-description blongs to one object class, ImageQA have to figure out more complex relation between question and answer.}</li>
<li>Second, ImageQA cope with a lot more number of possible classes compare to the mentioned algorithm <a href="">&quot;Predicting deep zero-shot convolutional neural networks using textual descriptions&quot;</a>; the mentioned algorithm is tested on dataset with 200 classes but the largest ImageQA dataset contains more than 20K possible answers.</li>
</ol>
<p>However, applying their approach to the image question answering is not trivial as a lot more parameters should be involved to perform various operation according to given question compare to binary classification problem.</p>
<h2 id="learning-with-multimodal-input">learning with multimodal input?</h2>
<h1 id="algorithm-overview">Algorithm Overview</h1>
<p>Briefly describe the motivation and formulation of our approach in this section.</p>
<p align="center">
<img src="https://dl.dropboxusercontent.com/s/klivjxulp43bcka/figure2.png?dl=0" width="700" >
</p>
<h1 id="motivation">Motivation</h1>
<p>Existing approaches (<a href="">&quot;Learning to answer questions from image using convolutional neural network&quot;</a>,<a href="">&quot;Are you talking to a machine? dataset and methods for multilingual image question answering&quot;</a>,<a href="">&quot;Ask your neurons: A neural-based approach to answering questions about images&quot;</a>) typically interpret ImageQA as a set of heterogeneous visual recognition problems.</p>
<p>Although ImageQA requires different types and levels of image understanding, existing approaches(<a href="">&quot;Learning to answer questions from image using convolutional neural network&quot;</a>,<a href="">&quot;Are you talking to a machine? dataset and methods for multilingual image question answering&quot;</a>,<a href="">&quot;Ask your neurons: A neural-based approach to answering questions about images&quot;</a>) pose the problem as a flat classification task.</p>
<p>However, we believe that it is difficult to solve ImageQA using a single deep neural network with fixed parameters.</p>
<p>In many CNN-based recognition problems, it is well-known to fine-tune a few layers for the adaptation to new tasks.</p>
<p>In addition, some networks are designed to solve two or more tasks jointly by constructing multiple branches connected to a common CNN architecture.</p>
<p>In this work, we hope to solve the heterogeneous recognition tasks using a single CNN by adapting the weights in the dynamic parameter layer.</p>
<p>Since the task is defined by the question in ImageQA, the weights in the layer are determined depending on the question sentence.</p>
<p>In addition, a hashing trick is employed to predict a large number of weights in the dynamic parameter layer and avoid parameter explosion.</p>
<h1 id="problem-formulation">Problem Formulation</h1>
<p>ImageQA systems predict the best answer <span class="math inline">\(\hat{a}\)</span> given an image <span class="math inline">\(I\)</span> and a question <span class="math inline">\(q\)</span>. Conventional approaches~ typically construct a joint feature vector based on two inputs <span class="math inline">\(I\)</span> and <span class="math inline">\(q\)</span> and solve a classification problem for ImageQA using the following equation:</p>
<p><span class="math display">\[
\hat{a} = \underset{a\in{\Omega}}{\operatorname{argmax}} \hspace{0.1cm} p(a \vert {I}, q;{\mathbf{\theta}})
\]</span> where <span class="math inline">\(\Omega\)</span> is a set of all possible answers and <span class="math inline">\({\mathbf{\theta}}\)</span> is a vector for the parameters in the network.</p>
<p>On the contrary, we use the question to predict weights in the classifier and solve the problem. We find the solution by <span class="math display">\[
\hat{a} = \underset{a\in{\Omega}}{\operatorname{argmax}} \hspace{0.1cm} p(a \vert I ;{\mathbf{\theta}}_{s},{\mathbf{\theta}}_{d} (q))
\]</span> where <span class="math inline">\({\mathbf{\theta}}_{s}\)</span> and <span class="math inline">\({\mathbf{\theta}}_{d}(q)\)</span> denote static and dynamic parameters, respectively. Note that the values of <span class="math inline">\({\mathbf{\theta}}_{d}(q)\)</span> are determined by the question <span class="math inline">\(q\)</span>.</p>
<h1 id="network-architecture">Network Architecture</h1>
<h2 id="architecture">architecture</h2>
<p>The network is composed of two sub-networks: classification network and parameter prediction network.</p>
<p>The classification network is a CNN.</p>
<p>One of the fully-connected layers in the CNN is the dynamic parameter layer, and the weights in the layer are determined adaptively by the parameter prediction network.</p>
<p>The parameter prediction network has GRU cells and a fully-connected layer. It takes a question as its input, and generates a real-valued vector, which corresponds to candidate weights for the dynamic parameter layer in the classification network.</p>
<p>Given an image and a question, our algorithm estimates the weights in the dynamic parameter layer through hashing with the candidate weights obtained from the parameter prediction network.</p>
<p>Then, it feeds the input image to the classification network to obtain the final answer.</p>
<h2 id="classification-network">Classification Network</h2>
<h3 id="detailed-description-of-classification-network"><a href="">Detailed description of classification network</a></h3>
<p>The classification network is constructed based on VGG 16-layer net</p>
<p>We remove the last layer in the network and attach three fully-connected layers.</p>
<p>The second last fully-connected layer of the network is the dynamic parameter layer whose weights are determined by the parameter prediction network, and the last fully-connected layer is the classification layer whose output dimensionality is equal to the number of possible answers. The probability for each answer is computed by applying a softmax function to the output vector of the final layer.</p>
<h3 id="why-not-putting-dynamic-parameters-at-the-end"><a href="">Why not putting dynamic parameters at the end</a></h3>
<p>We put the dynamic parameter layer in the second last fully-connected layer instead of the classification layer because it involves the smallest number of parameters.</p>
<p>As the number of parameters in the classification layer increases in proportion to the number of possible answers, predicting the weights for the classification layer may not be a good option to general ImageQA problems in terms of scalability.</p>
<p>Our choice for the dynamic parameter layer can be interpreted as follows.</p>
<p>By fixing the classification layer while adapting the immediately preceding layer, we obtain the task-independent semantic embedding of all possible answers and use the representation of an input embedded in the answer space to solve an ImageQA problem.</p>
<p>Therefore, the relationships of the answers globally learned from all recognition tasks can help solve new ones involving unseen classes, especially in multiple choice questions. For example, when not the exact ground-truth word (e.g.,, kitten) but similar words (e.g.,, cat and kitty) are shown at training time, the network can still predict the close answers (e.g.,, kitten) based on the globally learned answer embedding. Even though we could also exploit the benefit of answer embedding based on the relations among answers to define a loss function, we leave it as our future work. % Since the embedding of answers is globally learnt from all recognition tasks, it helps the network answer unseen words in a specific recognition task. %Especially in a multiple choice problem, although not the exact ground truth word (e.g. kitten) but similar words (e.g. cat, kitty) were shown at training time for the given recognition task, the network can still predict the closest answer in the choices based on the globally learnt answer embedding. %We can also utilize the characteristics of answer embedding and relations among answers in a loss function even though we leave it as our future work.}</p>
<h2 id="parameter-prediction-network">Parameter Prediction Network</h2>
<h3 id="paramter">paramter</h3>
<h4 id="what-is-dynamic-parameter-layer"><a href="">What is dynamic parameter layer</a></h4>
<p>As mentioned earlier, our classification network has a dynamic parameter layer. That is, for an input vector of the dynamic parameter layer <span class="math inline">\({\bf{f}}^{i}=\left[f^{i}_1,\dots,f^{i}_{\scriptscriptstyle N}\right]^{\scriptscriptstyle T}\)</span>, its output vector denoted by <span class="math inline">\({\bf{f}}^{o}=\left[f^{o}_1,\dots,f^{o}_{\scriptscriptstyle M}\right]^{\scriptscriptstyle T}\)</span> is given by</p>
<p><span class="math display">\[
{\bf{f}}^{o} = {\bf{W}}_{d}(q){\bf{f}}^{i}+{\bf{b}}
\]</span></p>
<p>where <span class="math inline">\({\bf{b}}\)</span> denotes a bias and <span class="math inline">\({\bf{W}}_{d}(q)\in\mathbb{R}^{\scriptscriptstyle M \times N}\)</span> denotes the matrix constructed dynamically using the parameter prediction network given the input question. %by applying a hash function to the output of the parameter prediction network the given question as its input. In other words, the weight matrix corresponding to the layer is parametrized by a function of the input question <span class="math inline">\(q\)</span>.</p>
<h3 id="what-is-the-parameter-prediction-network">[What is the parameter prediction network]}</h3>
<p>The parameter prediction network is composed of GRU cells~ followed by a fully-connected layer, which produces the candidate weights to be used for the construction of weight matrix in the dynamic parameter layer within the classification network.</p>
<p>The number of GRU units is equal to the number of words in the question sentence, and the fully-connected layer produces the candidate weights to be used for the prediction of weight parameters in the dynamic parameter layer within the classification network. GRU, which is similar to LSTM, is designed to model dependency in multiple time scales.</p>
<p>However, contrary to LSTM, which maintains a separate memory cell explicitly, GRU directly updates its hidden states with a reset gate and an update gate. The detailed procedure of the update is described below.</p>
<h3 id="description-of-gru"><a href="">Description of GRU</a></h3>
<p>Let <span class="math inline">\(w_{1} ,...,w_{\scriptscriptstyle T}\)</span> be the words in a question <span class="math inline">\(q\)</span>, where <span class="math inline">\(T\)</span> is the number of words in the question. In each time step <span class="math inline">\(t\)</span>, given the embedded vector <span class="math inline">\({\bf{x}}_t\)</span> for a word <span class="math inline">\(w_t\)</span>, the GRU encoder updates its hidden state at time <span class="math inline">\(t\)</span>, denoted by <span class="math inline">\({\bf h}_t\)</span>, using the following equations: <span class="math display">\[
\begin{cases}
{\bf{r}}_{t} &amp;= \sigma({\bf{W}}_r{\bf{x}}_{t}+{\bf U}_r{\bf{h}}_{t-1}) \\
{\bf{z}}_{t} &amp;= \sigma({\bf{W}}_z{\bf{x}}_{t}+{\bf U}_z{\bf{h}}_{t-1}) \\
\bar{{\bf{h}}}_{t} &amp;= \tanh({\bf{W}}_h{\bf{x}}_{t} + {\bf U}_h({\bf{r}}_{t}\odot{\bf{h}}_{t-1})) \\
{\bf{h}}_{t} &amp;= (1-{\bf{z}}_{t})\odot{\bf{h}}_{t-1}+{\bf{z}}_t\odot\bar{{\bf{h}}}_{t}
\end{cases}
\]</span> where <span class="math inline">\({\bf{r}}_{t}\)</span> and <span class="math inline">\({\bf{z}}_{t}\)</span> respectively denote the reset and update gates at time <span class="math inline">\({t}\)</span>, and <span class="math inline">\(\bar{{\bf{h}}}_{t}\)</span> is candidate activation at time <span class="math inline">\(t\)</span>. In addition, <span class="math inline">\(\odot\)</span> indicates element-wise multiplication operator and <span class="math inline">\(\sigma(\cdot)\)</span> is a sigmoid function. Note that the coefficient matrices related to GRU such as <span class="math inline">\({\bf{W}}_r\)</span>, <span class="math inline">\({\bf{W}}_z\)</span>, <span class="math inline">\({\bf{W}}_h\)</span>, <span class="math inline">\({\bf{U}}_r\)</span>, <span class="math inline">\({\bf{U}}_z\)</span>, and <span class="math inline">\({\bf{U}}_h\)</span> are learned by our training algorithm. By applying this encoder to a question sentence through a series of GRU cells, we obtain the final embedding vector <span class="math inline">\({\bf{h}}_{\scriptscriptstyle T}\in\mathbb{R}^{\scriptscriptstyle L}\)</span> of the question sentence.</p>
<h3 id="generating-output-of-parameter-prediction-network"><a href="">Generating output of parameter prediction network</a></h3>
<p>Once the question embedding is obtained by GRU, the candidate weight vector, <span class="math inline">\({\bf{p}}=\left[p_{1},\dots,p_{\scriptscriptstyle K}\right]^{\rm T}\)</span>, is given by applying a fully-connected layer to the embedded question <span class="math inline">\({\bf{h}}_{\scriptscriptstyle T}\)</span> as <span class="math display">\[
{\bf{p}} = {\bf{W}}_{p}{\bf{h}}_{\scriptscriptstyle T}
\]</span> where <span class="math inline">\({\bf{p}} \in \mathbb{R}^{\scriptscriptstyle K}\)</span> is the output of the parameter prediction network, and <span class="math inline">\({\bf{W}}_{p}\)</span> is the weight matrix of the fully-connected layer in the parameter prediction network. Note that even though we employ GRU for a parameter prediction network since the pre-trained network for sentence embedding---skip-thought vector model~---is based on GRU, any form of neural networks, e.g.,, fully-connected and convolutional neural network, can be used to construct the parameter prediction network.</p>
<h2 id="parameter-hashing">Parameter Hashing</h2>
<h3 id="why-do-we-need-to-reduce-parameters"><a href="">Why do we need to reduce parameters?</a></h3>
<p>The weights in the dynamic parameter layers are determined based on the learned model in the parameter prediction network given a question.</p>
<p>The most straightforward approach to obtain the weights is to generate the whole matrix <span class="math inline">\({\bf{W}}_{d}(q)\)</span> using the parameter prediction network.</p>
<p>However, the size of the matrix is very large, and the network may be overfitted easily given the limited number of training examples.</p>
<p>In addition, the number of parameters to construct <span class="math inline">\({\bf{W}}_{d}(q)\)</span> is closely related to the number of parameters in the parameter prediction network.</p>
<p>In addition, since we need quadratically more parameters between GRU and the fully-connected layer in the parameter prediction network to increase the dimensionality of its output, it is not desirable to predict full weight matrix using the network.</p>
<p>Therefore, it is preferable to construct <span class="math inline">\({\bf{W}}_{d}(q)\)</span> based on a small number of candidate weights using a hashing trick.</p>
<p>In this case, the output dimensionality of the parameter prediction layer increases in proportion to the product of the input and output dimensionalities of the dynamic parameter layer.</p>
<p>In practice, since the second fully-connected layer in our classification network needs the least parameters among the three fully-connected layers, it is more desirable to determine the weight parameters adaptively for the layer.</p>
<p>This issue is also related to the limited number of candidate weights denoted by <span class="math inline">\(K\)</span>; it is better to have a small ratio of the number of parameters predicted to the number of candidate parameters.</p>
<p>Since we need quadratically more parameters between GRU and the fully-connected layer in the parameter prediction network to increase the number of weight candidates, it is also better to maintain a small number of candidate weights.</p>
<h3 id="hash-based-k-reduction"><a href="">Hash based K reduction</a></h3>
<p>We employ the recently proposed random weight sharing technique based on hashing~ to construct the weights in the dynamic parameter layer.</p>
<p>Specifically, a single parameter in the candidate weight vector <span class="math inline">\({\bf{p}}\)</span> is shared by multiple elements of <span class="math inline">\({\bf{W}}_{d}(q)\)</span>, which is done by applying a predefined hash function that converts the 2D location in <span class="math inline">\({\bf{W}}_{d}(q)\)</span> to the 1D index in <span class="math inline">\({\bf{p}}\)</span>. By this simple hashing trick, we can reduce the number of parameters in <span class="math inline">\({\bf{W}}_{d}(q)\)</span> while maintaining the accuracy of the network~.</p>
<h3 id="formal-description-of-hashing"><a href="">Formal description of hashing</a></h3>
<p>Let <span class="math inline">\(w^{d}_{mn}\)</span> be the element at <span class="math inline">\((m,n)\)</span> in <span class="math inline">\({\bf{W}}_{d}(q)\)</span>, which corresponds to the weight between <span class="math inline">\(m^{\rm th}\)</span> output and <span class="math inline">\(n^{\rm th}\)</span> input neuron. Denote by <span class="math inline">\(\psi(m,n)\)</span> a hash function mapping a key <span class="math inline">\((m,n)\)</span> to a natural number in <span class="math inline">\(\left\{1,\dots,K \right\}\)</span>, where <span class="math inline">\(K\)</span> is the dimensionality of <span class="math inline">\({\bf p}\)</span>. The final hash function is given by <span class="math display">\[
w^{d}_{mn}={p}_{\psi(m,n)} \cdot \xi(m,n)
\]</span> where <span class="math inline">\(\xi(m,n):\mathbb{N}\times\mathbb{N}\rightarrow \{+1, -1 \}\)</span> is another hash function independent of <span class="math inline">\(\psi(m,n)\)</span>. This function is useful to remove the bias of hashed inner product~. In our implementation of the hash function, we adopt an open-source implementation of {}.</p>
<h3 id="why-hashing-works"><a href="">Why hashing works</a></h3>
<p>We believe that it is reasonable to reduce the number of free parameters based on the hashing technique as there are many redundant parameters in deep neural networks <a href="">&quot;Predicting parameters in deep learning&quot;</a> and the network can be parametrized using a smaller set of candidate weights. Instead of training a huge number of parameters without any constraint, it would be advantageous practically to allow multiple elements in the weight matrix to share the same value. It is also demonstrated that the number of free parameter can be reduced substantially with little loss of network performance .</p>
<h1 id="training-algorithm">Training Algorithm</h1>
<h2 id="training">training</h2>
<p>This section discusses the error back-propagation algorithm in the proposed network and introduces the techniques adopted to enhance performance of the network.</p>
<h2 id="training-by-error-back-propagation">Training by Error Back-Propagation</h2>
<p>The proposed network is trained end-to-end to minimize the error between the ground-truths and the estimated answers. The error is back-propagated by chain rule through both the classification network and the parameter prediction network and they are jointly trained by a first-order optimization method.</p>
<h2 id="notations"><a href="">Notations</a></h2>
<p>Let <span class="math inline">\({\mathcal{L}}\)</span> denote the loss function. The partial derivatives of <span class="math inline">\({\mathcal{L}}\)</span> with respect to the <span class="math inline">\(k^{\rm th}\)</span> element in the input and output of the dynamic parameter layer are given respectively by <span class="math display">\[
{\delta}^{i}_k \equiv \frac{\partial\mathcal{L}}{\partial {f}^{i}_k}  ~~~~\text{and}~~~~
{\delta}^{o}_k \equiv \frac{\partial\mathcal{L}}{\partial {f}^{o}_k}.
\]</span> The two derivatives have the following relation: <span class="math display">\[
{\delta}^{i}_n = \sum _{ m=1 }^{ M }{ w^d_{mn}\delta^o_{m} }
\]</span> Likewise, the derivative with respect to the assigned weights in the dynamic parameter layer is given by <span class="math display">\[
{\frac{\partial\mathcal{L}}{\partial w^{d}_{mn}}}=f^{i}_{n}{\delta}^{o}_{m}.
\]</span></p>
<h2 id="gradient-over-parameter-hashing">Gradient over Parameter Hashing</h2>
<p>As a single output value of the parameter prediction network is shared by multiple connections in the dynamic parameter layer, the derivatives with respect to all shared weights need to be accumulated to compute the derivative with respect to an element in the output of the parameter prediction network as follows: <span class="math display">\[
\begin{cases}
{\frac{\partial\mathcal{L}}{\partial p_{k}}}
&amp;= \sum _{m=1}^{\scriptscriptstyle M}{\sum _{n=1}^{\scriptscriptstyle N}{   {\frac{\partial\mathcal{L}}{\partial w^d_{mn}}} {\frac{\partial w^d_{mn}}{\partial p_{k}}}}}  \nonumber \\
&amp;= \sum _{m=1}^{\scriptscriptstyle M}{\sum _{n=1}^{\scriptscriptstyle N} {  {\frac{\partial\mathcal{L}}{\partial w^d_{mn}}}   {\xi(m,n)}  {{\mathbb{I}} [ \psi(m,n)=k ]}      }},
\end{cases}
\]</span> where <span class="math inline">\({\mathbb{I}} [ \cdot ]\)</span> denotes the indicator function. The gradients of all the preceding layers in the classification and parameter prediction networks are computed by the standard back-propagation algorithm.</p>
<h2 id="using-pre-trained-gru">Using Pre-trained GRU</h2>
<h3 id="necessity-of-employing-pre-trained-gru-model">necessity of employing pre-trained GRU model</h3>
<p>Although encoders based on recurrent neural networks (RNNs) such as LSTM and GRU demonstrate impressive performance on sentence embedding(<a href="">Recurrent neural network based language model</a>,<a href="">Sequence to sequence learning with neural networks</a>), their benefits in the ImageQA task are marginal in comparison to bag-of-words model <a href="">&quot;Exploring models and data for image question answering&quot;</a>.</p>
<p>One of the reasons for this fact is the lack of language data in ImageQA dataset. Contrary to the tasks that have large-scale training corpora, even the largest ImageQA dataset contains relatively small amount of language data; for example, VQA contains 750K questions in total.</p>
<p>Note that the model in <a href="">&quot;Sequence to sequence learning with neural networks&quot;</a> is trained using a corpus with more than 12M sentences.</p>
<h2 id="finetuning-skip-thought-vector">finetuning skip-thought vector</h2>
<p>To deal with the deficiency of linguistic information in ImageQA problem, we transfer the information acquired from a large language corpus by fine-tuning the pre-trained embedding network. We initialize the GRU with the skip-thought vector model trained on a book-collection corpus containing more than 74M sentences <a href="">&quot;Skip-thought vectors&quot;</a>. Note that the GRU of the skip-thought vector model is trained in an unsupervised manner by predicting the surrounding sentences from the embedded sentences. As this task requires to understand context, the pre-trained model produces a generic sentence embedding, which is difficult to be trained with a limited number of training examples. By fine-tuning our GRU initialized with a generic sentence embedding model for ImageQA, we obtain the representations for questions that are generalized better.</p>
<h2 id="fine-tuning-cnn">Fine-tuning CNN</h2>
<h3 id="fine-tuning-cnn-with-gru-is-not-easy">Fine-tuning CNN with GRU is not easy</h3>
<p>It is very common to transfer CNNs for new tasks in classification problems, but it is not trivial to fine-tune the CNN in our problem.</p>
<p>jointly with RNNs and achieve noticeable performance improvement~.</p>
<p>since gradient vectors from RNNs tend to be noisy.</p>
<p>We observe that the gradients below the dynamic parameter layer in the CNN are noisy since the weights are predicted by the parameter prediction network. Hence, a straightforward approach to fine-tune the CNN typically fails to improve performance, and we employ a slightly different technique for CNN fine-tuning to sidestep the observed problem. We update the parameters of the network using new datasets except the part transferred from VGG 16-layer net at the beginning, and start to update the weights in the subnetwork if the validation accuracy is saturated.</p>
<p>after the gradients through the dynamic parameter layer become less noisy.</p>
<h2 id="how-to-determine-less-noisy">[How to determine &quot;less noisy&quot;?]</h2>
<h2 id="proposed-network-training-overview">proposed network training overview</h2>
<p>The proposed network can be trained end-to-end by back-propagation with an objective of minimizing the error between the ground-truths and the estimated answers. By chain-rule, the gradient over the training objective is propagated through both the classification network and the parameter prediction network and they are jointly} trained with first-order optimization methods.</p>
<h2 id="gradient-over-network-components">Gradient over Network Components</h2>
<h3 id="notations-1">Notations</h3>
<p>Let <span class="math inline">\({\mathcal{L}}\)</span> denotes the objective function for training, then the gradient with respect to} the output of dynamic parameter layer can be formally represented by <span class="math inline">\(\frac{\partial\mathcal{L}}{\partial {\bf{f}}_{o}}\)</span>. We use <span class="math inline">\(\delta^{o}_{m}\)</span> for the <span class="math inline">\(m\)</span>-th element of the <span class="math inline">\(\frac{\partial\mathcal{L}}{\partial {\bf{f}}_{o}}\)</span>. Similarly, we use <span class="math inline">\(\delta^{i}_{n}\)</span> to denote the <span class="math inline">\(n\)</span>-th element of <span class="math inline">\(\frac{\partial\mathcal{L}}{\partial {\bf{f}}_{i}}\)</span>.</p>
<h3 id="gradient-over-dynamic-parameter-layer-input">Gradient over Dynamic Parameter Layer input</h3>
<p>The gradient with respect to the output of dynamic parameter layer <span class="math inline">\(\delta^{o}_{m}\)</span> can be computed by {applying the} standard back-propagation method to the upper layers of the classification network}.</p>
<p>The gradient with respect to} the input of the dynamic} parameter layer also can be computed by standard back-propagation as follows: <span class="math display">\[
{\delta}^{i}_n = \sum _{ m=1 }^{ M }{ w^d_{nm}\delta^o_{m} }
\]</span></p>
<p>Gradient over Dynamic Parameter Layer weight} Likewise, the gradient over the assigned weights of the dynamic parameter layer can be computed as follows: <span class="math display">\[
{\frac{\partial\mathcal{L}}{\partial w^{d}_{nm}}}=f^{i}_{n}{\delta}^{o}_{m}
\]</span></p>
<h2 id="gradient-over-parameter-hashing-1">Gradient over Parameter Hashing</h2>
<p>As single output value of the parameter prediction layer is shared by several connections of the dynamic parameter layer, the gradient from each connection need to be accumulated to construct the gradient over the output value of the parameter prediction layer .</p>
<p><span class="math display">\[
{\frac{\partial\mathcal{L}}{\partial p_{k}}}
= \sum _{n=1}^{\scriptscriptstyle N}{\sum _{m=1}^{\scriptscriptstyle M}{   {\frac{\partial\mathcal{L}}{\partial w^d_{nm}}} {\frac{\partial w^d_{nm}}{\partial p_{k}}}}} \\
= \sum _{n=1}^{\scriptscriptstyle N}{\sum _{m=1}^{\scriptscriptstyle M} {  {\frac{\partial\mathcal{L}}{\partial w^d_{nm}}}   {\xi(n,m)}  {{\mathbb{I}}\{ \psi(n,m)=k \}}      }}
\]</span> where <span class="math inline">\({\mathbb{I}}\{ \cdot \}\)</span> denotes the indicator function.</p>
<h2 id="gradient-over-parameter-prediction-layer">Gradient over Parameter Prediction Layer</h2>
<p>The gradients over the parameters of the parameter prediction network can then be computed by the standard back-propagation from the propagated gradient over the output of this layer.</p>
<h2 id="pre-training-gru">Pre-training GRU</h2>
<p>necessity of employing pre-trained GRU model</p>
<p>Although RNN based encoders (e.g., LSTM , GRU ) shows impressive performance on embedding sentences , its benefit in the ImageQA task is marginal in comparison to the continuous bag-of-words model <a href="">&quot;Exploring models and data for image question answering&quot;</a>.</p>
<p>One of the possible reason for the marginal performance gain is the lack of language data in ImageQA dataset.</p>
<p>Contrary to the other tasks for which there are large-scale training corpora} (e.g.,<a href="">&quot;Sequence to sequence learning with neural networks&quot;</a> train the model on a corpus with more than 12M sentences), even the largest ImageQA dataset contains relatively small number of language data (e.g., VQA contains 750K questions in total).</p>
<h2 id="finetuning-skip-thought-vector-1">finetuning skip-thought vector</h2>
<p>To deal with the deficiency of liguistic information in ImageQA datasets, we transfer the information acquired from a large language corpus by finetuning the pre-trained embedding network}. We initialize the GRU with the skip-thought vector model<a href="">&quot;Skip-thought vectors&quot;</a> trained on a book-collection corpus containing more than 74M sentences.</p>
<p>The GRU of the skip-thought vector model is trained in an unsupervised manner by predicting the surrounding sentences from the embedded sentence}.</p>
<p>As this task requires to understand the context, the pre-trained model could produce generic sentence embedding}, which is hard to be trained with a limited number of examples.</p>
<p>By finetuning our GRU initialized by the generic sentence embedding model for ImageQA, we can obtain a more} generalized question feature extractor.</p>
<h2 id="training-details">Training Details</h2>
<h3 id="data-pre-processing">data pre-processing</h3>
<p>Before training, question sentences are normalized to lower cases and preprocessed by a simple tokenization technique as in <a href="">&quot;Show, attend and tell: Neural image caption generation with visual attention&quot;</a>. We normalize the answers to lower cases and regard a whole answer in a single or multiple words as a separate class.</p>
<h3 id="optimization">optimization</h3>
<p>The network is trained end-to-end by back-propagation. <a href="">&quot;Adam&quot;</a> is used for optimization with initial learning rate 0.01. We clip the gradient to 0.1 to handle the gradient explosion from the recurrent structure of GRU.</p>
<p>Training is terminated when there is no progress on validation accuracy for 5 epochs.</p>
<h2 id="optmization-of-dynamic-parameter-layer">optmization of dynamic parameter layer</h2>
<p>Optimizing the dynamic parameter layer is not straightforward since the distribution of the outputs in the dynamic parameter layer is likely to change significantly in each batch. Therefore, we apply batch-normalization to the output activations of the layer to alleviate this problem.</p>
<h2 id="early-stopping-in-gru-finetuning-and-word-embedding-finetuning">early stopping in GRU finetuning and word-embedding finetuning</h2>
<p>In addition, we observe that GRU tends to converge fast and overfit data easily if training continues without any restriction. We stop fine-tuning GRU when the network start to overfit and continue to train the other parts of the network; this strategy improves performance in practice. %We stop fine-tuning GRU earlier than the other parts of the network, which helps performance improvement in practice.</p>
<h1 id="experiments">Experiments</h1>
<p>We now describe the details of our implementation and evaluate the proposed method in various aspects.</p>
<h2 id="datasets">Datasets</h2>
<p>To demonstrate the performance of proposed method on a wide variety of image and question pairs, we evaluate the proposed network on all the available ImageQA benchmarks such as DAQUAR~, COCO-QA~ and VQA~.</p>
<p>We evaluate the proposed network on all public ImageQA benchmark datasets such as DAQUAR~, COCO-QA~ and VQA~.</p>
<p>They collected question-answer pairs from existing image datasets and most of the answers are single words or short phrases.</p>
<h3 id="daquar">DAQUAR</h3>
<p>DAQUAR is based on NYUDv2 <a href="">&quot;Indoor segmentation and support inference from rgbd images&quot;</a> dataset, which is originally designed for indoor segmentation using RGBD images.</p>
<p>Therefore, most of images are indoor scene and questions are usually about recognizing objects their attributes appearing in indoor scene and their spatial relations.</p>
<p>DAQUAR provides two benchmarks, which are distinguished by the number of classes and the amount of data; DAQUAR-all consists of 6,795 and 5,673 questions for training and testing respectively, and includes 894 categories in answer.</p>
<p>DAQUAR-reduced includes only 37 answer categories for 3,876 training and 297 testing questions. Some questions in this dataset are associated with a set of multiple answers instead of a single one.</p>
<h3 id="coco-qa">COCO-QA</h3>
<p>The questions in COCO-QA are automatically generated from the image descriptions in MS COCO dataset <a href="">&quot;{Microsoft COCO:} common objects in context&quot;</a> using the constituency parser with simple question-answer generation rules.</p>
<p>As automatically generated from the image descriptions,</p>
<p>The questions in this dataset are typically long and explicitly classified into 4 types depending on the generation rules: object questions, number questions, color questions and location questions.</p>
<p>All answers are with one-words and there are 78,736 questions for training and 38,948 questions for testing.</p>
<h3 id="vqa">VQA</h3>
<p>Similar to COCO-QA, VQA is also constructed on MS COCO but each question is associated with multiple answers annotated by different people.</p>
<p>This dataset contains the largest number of questions: 248,349 for training, 121,512 for validation, and 244,302 for testing, where the testing data is splited into test-dev, test-standard, test-challenge and test-reserve as in Mscoco.</p>
<p>Each question is provided with 10 answers to take the consensus of annotators into account. About 90% of answers have single words and 98% of answers do not exceed three words.</p>
<h2 id="evaluation-metrics">Evaluation Metrics</h2>
<p>As most answers are single word, plain accuracy is used as standard evaluation metric with slight variations for each dataset.</p>
<h3 id="wups">WUPS</h3>
<p>DAQUAR and COCO-QA employ both classification accuracy and its relaxed version based on word similarity, WUPS <a href="">&quot;A multi-world approach to question answering about real-world scenes based on uncertain input&quot;</a>.</p>
<p>It uses thresholded Wu-Palmer similarity (<a href="">&quot;Verbs semantics and lexical selection&quot;</a>) based on WordNet (<a href="">&quot;Wordnet: An electronic database&quot;</a>) taxonomy to compute the similarity between words. For predicted answer set <span class="math inline">\(\mathcal{A}^{i}\)</span> and ground-truth answer set <span class="math inline">\(\mathcal{T}^{i}\)</span> of the <span class="math inline">\(i^{\rm th}\)</span> example, WUPS is given by <span class="math display">\[
\begin{cases}
&amp;{\text{WUPS}} = \nonumber \\
&amp;\frac { 1 }{ N } \sum _{ i=1 }^{ N }{ \min { \left\{ \prod _{ a\in \mathcal{A}^{ i } }^{  }{ \max _{ t\in \mathcal{T}^{ i } }{ \mu \left( a,t \right)  }  } , \prod _{ t\in \mathcal{T}^{ i } }^{  }{ \max _{ a\in \mathcal{A}^i }{ \mu \left( a,t \right)  }  }  \right\}  }  } ,
\end{cases}
\]</span> where <span class="math inline">\(\mu \left(\cdot, \cdot \right)\)</span> denotes the thresholded Wu-Palmer similarity between prediction and ground-truth. where WUP is down-weighted if WUP is less than the threshold. We use two threshold values (<span class="math inline">\(0.9\)</span> and <span class="math inline">\(0.0\)</span>) in our evaluation.</p>
<h2 id="vqa-evaluation-openended-multiplechoice">VQA evaluation (OpenEnded, MultipleChoice)</h2>
<p>VQA dataset provides open-ended task and multiple-choice task for evaluation. For open-ended task, the answer can be any word or phrase while an answer should be chosen out of 18 candidate answers in the multiple-choice task. In both cases, answers are evaluated by accuracy reflecting human consensus. For predicted answer <span class="math inline">\(a_i\)</span> and target answer set <span class="math inline">\(\mathcal{T}^{i}\)</span> of the <span class="math inline">\(i^{\rm th}\)</span> example, the accuracy is given by <span class="math display">\[
\text{Acc}_\textrm{VQA} = \frac{1}{N} \sum _{i=1}^{N}{\min { \left\{ \frac{\sum _{t\in \mathcal{T}^{i}}{  {\mathbb{I}} \left[a_i=t\right]}}{3} , 1\right\} }  }
%\min {\left( \frac{\# \text{humans that provided that answer}}{3}, 1 \right)}
\]</span> where <span class="math inline">\({\mathbb{I}}\left[ \cdot \right]\)</span> denotes an indicator function. In other words, a predicted answer is regarded as a correct one if at least three annotators agree, and the score depends on the number of agreements if the predicted answer is not correct.</p>
<h1 id="results">Results</h1>
<p>We test three independent datasets, VQA, COCO-QA, and DAQUAR.</p>
<p>The proposed Dynamic Parameter Prediction network (DPPnet) outperforms all existing methods nontrivially.</p>
<p>We performed controlled experiments to analyze the contribution of individual components in the proposed algorithm---dynamic parameter prediction, use of pre-trained GRU and CNN fine-tuning, and trained 3 additional models, CONCAT, RAND-GRU, and CNN-FIXED. CNN-FIXED is useful to see the impact of CNN fine-tuning since it is identical to DPPnet except that the weights in CNN are fixed. RAND-GRU is the model without GRU pre-training, where the weights of GRU and word embedding model are initialized randomly. It does not fine-tune CNN either. CONCAT is the most basic model, which predicts answers using the two fully-connected layers for a combination of CNN and GRU features. Obviously, it does not employ any of new components such as parameter prediction, pre-trained GRU and CNN fine-tuning.</p>
<h2 id="general-descriptions-of-controlled-models">General descriptions of controlled models</h2>
<p>We train 4 different models: CONCAT, RAND-GRU, CNN-FIXED and DPPnet. RAND-GRU, CNN-FIXED and DPPnet all use the proposed network with equal parameterization but trained differently. DPPnet is the model trained with all the components described in this paper: dynamic parameter prediction, usage of pre-trained GRU and CNN finetuning. To see the effect of fine-tuning CNN, we use CNN-FIXED which is equal to DPPnet before the CNN finetuning. RAND-GRU trained equally with CNN-FIXED, but weights of GRU and word embeddings are initialized randomly. CONCAT uses different network to see the effect of dynamic parameter prediction and pre-trained GRU and CNN finetuning is not applied to this model for comparison with RAND-GRU.</p>
<h2 id="detailed-configuration-of-concat">Detailed configuration of CONCAT</h2>
<p>CONCAT combines CNN and GRU features by concatenation and use two-layer neural network as classifier. We control the number of parameters of CONCAT as similar as possible to RAND-GRU; the dimension of CNN feature is same as the input of parameter prediction layer and the dimension of GRU feature is same as the output of parameter prediction network. Concatenated features are embedded by a fully-connected layer to have equal dimensionality as the output of the dynamic parameter layer and the embedded feature is fed into the final classifier. To reduce the effect of batch-normalization, we apply batch-normalization before the final classification layer as well.</p>
<h2 id="conclusion">Conclusion</h2>
<p>We proposed a novel architecture for image question answering based on two subnetworks---classification network and parameter prediction network. The classification network has a dynamic parameter layer, which enables the classification network to adaptively determine its weights through the parameter prediction network. While predicting all entries of the weight matrix is infeasible due to its large dimensionality, we relieved this limitation using parameter hashing and weight sharing. % With the qualitative results and analysis on the question embedding, we showed that the parameter prediction network successfully determine the weights of dynamic parameter layer based on the specific task the network need to solve.} The effectiveness of the proposed architecture is supported by experimental results showing the state-of-the-art performances on three different datasets. Note that the proposed method achieved outstanding performance even without more complex recognition processes such as referencing objects. We believe that the proposed algorithm can be extended further by integrating attention model <a href="">&quot;Show, attend and tell: Neural image caption generation with visual attention&quot;</a> to solve such difficult problems.</p>
</body>
</html>
