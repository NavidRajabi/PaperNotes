<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h2 id="an-attention-based-convolutional-neural-network-for-visual-question-answeringhttpsarxiv.orgabs1511.05960"><a href="">&quot;An attention based convolutional neural network for visual question answering&quot;</a>)(https://arxiv.org/abs/1511.05960)</h2>
<p><a href="">&quot;Role of Attention for Visual Question Answering&quot;</a>)(https://computing.ece.vt.edu/~harsh/visualAttention/ProjectWebpage/)</p>
<h1 id="abstract">abstract</h1>
<p>We propose a novel attention based deep learning architecture for visual question answering task (VQA). Given an image and an image-related question, VQA returns a natural language answer. Since different questions inquire about the attributes of different image regions, generating correct answers requires the model to have question-guided attention, i.e., the attention on the regions corresponding to the input question's intent. We introduce an attention-based configurable convolutional neural network (ABC-CNN) to locate the question-guided attention based on input queries. ABC-CNN determines the attention regions by finding the corresponding visual features in the visual feature maps with a ``configurable convolution'' operation. With the help of the question-guided attention, ABC-CNN can achieve both higher VQA accuracy and better understanding of the visual question answering process. We evaluate the ABC-CNN architecture on three benchmark VQA datasets: Toronto COCO-QA, DAQUAR, and VQA dataset. ABC-CNN model achieves significant improvements over state-of-the-art methods. The question-guided attention generated by ABC-CNN is also shown to be the regions that are highly relevant to the questions' intents.</p>
<h1 id="introduction">Introduction</h1>
<p>Visual Question Answering (VQA) is the task of answering questions, posed in natural language, about the semantic content in an image (or video). Given an image and an image related question, VQA answers the question in one word or a natural language sentence. VQA is of great importance to many applications, including image retrieval, early education, and navigation for blind people as it provides user-specific information through the understanding of both the natural language questions and image content. VQA is a highly challenging problem as it requires the machine to understand natural language queries, extract semantic contents from images, and relate them in a unified framework. In spite of these challenges, an exciting set of methods have been developed by the research community in recent years.</p>
<p>Current state-of-the-art VQA models (<a href="">&quot;Exploring models and data for image question answering&quot;</a>))(<a href="">&quot;Ask your neurons: A neural-based approach to answering questions about images&quot;</a>))(<a href="">&quot;Are you talking to a machine? dataset and methods for multilingual image question answering&quot;</a>)) generally contain a vision part, a question understanding part and an answer generation part. The vision part extracts visual features through a deep convolutional neural network (CNN) (<a href="">&quot;Backpropagation applied to handwritten zip code recognition&quot;</a>)) or using a traditional visual feature extractor. The question understanding part learns a dense question embedding feature vector to encode question semantics, either with a Bag-of-Words model(<a href="">&quot;Exploring models and data for image question answering&quot;</a>)) or a recurrent neural network (RNN) (<a href="">&quot;Long short-term memory&quot;</a>)) model. The answer generation part produces an answer conditioned on the visual features and the question embeddings. The answer can either be a single word generated by a multi-class classifier (<a href="">&quot;Exploring models and data for image question answering&quot;</a>)) or a full sentence generated by an additional RNN decoder (<a href="">&quot;Ask your neurons: A neural-based approach to answering questions about images&quot;</a>))(<a href="">&quot;Are you talking to a machine? dataset and methods for multilingual image question answering&quot;</a>)). The visual features and dense question embeddings are integrated through a linear(<a href="">&quot;Exploring models and data for image question answering&quot;</a>)) / non-linear(<a href="">&quot;Ask your neurons: A neural-based approach to answering questions about images&quot;</a>))(<a href="">&quot;re you talking to a machine? dataset and methods for multilingual image question answerin&quot;</a>)) transform which jointly projects the features from image space and semantic space into answer space. This integration is normally not sufficient to fully exploit the relationship of the vision part and the question understanding part because it loses the opportunity to exploit the intent of queries to focus on different regions in an image.</p>
<p>When trying to answer a question about an image, humans tend to search the informative regions according to the question's intent before giving the answer.</p>
<p>Based on this observation, we propose a novel attention-based configurable convolutional neural network (ABC-CNN) to locate such informative regions and give more correct answers for VQA. We call the mechanism of finding informative regions based on the input question's intent as ``<span class="math inline">\(\mathbf{question-guided attention}\)</span>'', because these regions are determined by both images and image-related questions.</p>
<p align="center">
<img src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-03-25/0ad1fefa54f69d9efa0112f2e60c19841d5e9346/2-Figure2-1.png" width="500" >
</p>
<p>As shown in Fig, ABC-CNN contains a vision part, a question understanding part, an answer generation part, and an attention extraction part. We employ a CNN to extract visual features in the vision part. Instead of extracting a single global visual feature, we extract a spatial feature map to retain crucial spatial information, by either applying a CNN in a sliding window way or with a fully convolutional neural network. A long-short term memory (LSTM)(<a href="">&quot;Long short-term memory&quot;</a>)) model is used to obtain question embeddings in the question understanding part. In this paper, we only consider the VQA task with single word answers which can be generated by utilizing a multi-class classifier in the answer generating part. Our method can be extended to generate full sentences by using an RNN decoder.</p>
<p>We present the question-guided attention information as a question-guided attention map (QAM), which is the core of the ABC-CNN framework. We model the QAM as latent information, and do not require explicit labeling of such maps for all kinds of possible queries. The QAM is generated by searching for visual features that correspond to the input query's semantics in the spatial image feature map. We achieve the search via a configurable convolution neural network, which convolves the visual feature map with a configurable convolutional kernel (CCK). This kernel is generated by transforming the question embeddings from the semantic space into the visual space, which contains the visual information determined by the intent of the question.</p>
<p>Convolving the CCK with image feature map adaptively represents each region's importance for answering the given question as a QAM. The generated QAMs can be utilized to spatially weight the visual feature maps to filter out noise and unrelated information. With the visual features conditioned on the input query, ABC-CNN can return more accurate answers from the multi-class classifier in answer generation part. The whole framework can be trained in an end-to-end way without requiring any manual labeling of attention regions in images.</p>
<p>In the experiments, we evaluate the ABC-CNN framework on three benchmark VQA datasets: Toronto COCO-QA (<a href="">&quot;torontoQa&quot;</a>)), DAQUAR (<a href="">&quot;malinowski2014nips&quot;</a>)) and VQA(<a href="">&quot;antol2015vqa&quot;</a>)). Our method significantly outperforms state-of-the-art methods. Visualization of attention maps demonstrates that the ABC-CNN architecture is capable of generating attention maps that well reflect the regions queried by questions.</p>
<p>In summary, we propose a unified ABC-CNN framework to effectively integrate the visual and semantic information for VQA via question-guided attention. Not only does the question guided attention significantly improve the performance of VQA systems, but it also helps us gain a better understanding of the question answering process.</p>
<h2 id="related-work">Related Work</h2>
<p><span class="math inline">\(\textbf{Image captioning:}\)</span> VQA and image captioning are highly related because both of them need to reason about the visual contents and present the results in a full natural language sentence or in a word. Current state-of-the-art methods in VQA (<a href="">&quot;Are you talking to a machine? dataset and methods for multilingual image question answering&quot;</a>)(<a href="">&quot;Ask your neurons: A neural-based approach to answering questions about images&quot;</a>)(<a href="">&quot;Exploring models and data for image question answering&quot;</a>) and image captioning (<a href="">&quot;Deep captioning with multimodal recurrent neural networks&quot;</a>)(<a href="">&quot;Long-term recurrent convolutional networks for visual recognition and description&quot;</a>)(<a href="">&quot;Deep visual-semantic alignments for generating image descriptions&quot;</a>)(<a href="">&quot;Translating videos to natural language using deep recurrent neural networks&quot;</a>)(<a href="">&quot;Show, attend and tell: Neural image caption generation with visual attention&quot;</a>)(<a href="">&quot;Learning the visual interpretation of sentences&quot;</a>) generally apply a CNN to extract visual features and an LSTM model as a decoder to generate answers or captions. (<a href="">&quot;Are you talking to a machine? dataset and methods for multilingual image question answering&quot;</a>)(<a href="">&quot;Deep captioning with multimodal recurrent neural networks&quot;</a>)(<a href="">&quot;Ask your neurons: A neural-based approach to answering questions about images&quot;</a>) apply a multi-modal layer to combine the visual features and word embedding vectors by a joint projection during the caption generation in the LSTM decoder. (<a href="">&quot;Exploring models and data for image question answering&quot;</a>) employs the projected image features as the starting states of the LSTM decoder, similar to the encoder-decoder framework in sequence to sequence learning~(<a href="">&quot;Sequence to sequence learning with neural networks&quot;</a>). Treating image features as global visual features, these studies in VQA and image captioning fail to exploit the valuable information in questions to focus their attention on the corresponding regions in images.</p>
<p><span class="math inline">\(\textbf{Attention models:}\)</span> Attention models have been successfully adopted in many computer vision tasks, including object detection (<a href="">&quot;Recurrent models of visual attention&quot;</a>)(<a href="">&quot;Multiple object recognition with visual attention&quot;</a>), fine-grained image classification (<a href="">&quot;Attention for fine-grained categorization&quot;</a>)(<a href="">&quot;Bilinear cnn models for fine-grained visual recognition&quot;</a>), and image captioning (<a href="">&quot;Aligning where to see and what to tell: image caption with region-based attention and scene factorization&quot;</a>). Attention can be modeled as a region sequence in an image. An RNN model is utilized to predict the next attention region based on the current attention region's location and visual features. (<a href="">&quot;Recurrent models of visual attention&quot;</a>), (<a href="">&quot;Multiple object recognition with visual attention&quot;</a>) and (<a href="">&quot;Attention for fine-grained categorization&quot;</a>) employ this framework for object recognition, object detection, and fine-grained object recognition, respectively. (<a href="">&quot;Aligning where to see and what to tell: image caption with region-based attention and scene factorizatio&quot;</a>) develops an attention-based model for image captioning that uses an RNN as a decoder. It extracts a set of proposal regions in each image, and learns their attention weights using the decoding LSTM decoder's hidden states and the visual features extracted in each proposal region. In (<a href="">&quot;Bilinear cnn models for fine-grained visual recognitio&quot;</a>), a bilinear CNN structure utilizes two separate branches to combine the location and content information for fine-grained image classification. ABC-CNN is inspired by the successful application of attention on these vision tasks and utilize {} attention to improve VQA performance.</p>
<p><span class="math inline">\(\textbf{Configurable convolutional neural network:}\)</span></p>
<p>In (<a href="">&quot;A dynamic convolutional layer for short range weather prediction&quot;</a>), a dynamic convolutional layer architecture is proposed for short range weather prediction. %It is observed that a radar image can be estimated based on the previous image in the sequence. The convolutional kernels in the dynamic convolutional layer are determined by a neural network encoding the information of weather images in previous time steps. In VQA, the most important clue to determine the attention regions is the question. Thus, the CCKs in ABC-CNN framework are determined by the question embedding.</p>
<h1 id="attention-based-configurable-cnn">Attention Based Configurable CNN</h1>
<p>The framework of ABC-CNN is illustrated in Fig. above.</p>
<p>We restrict to QA pairs with single-word answers in this paper; this allows us to treat the task as a multi-class classification problem, which simplifies the evaluation metrics so that we can concentrate on developing question-guided attention models. %Our model can be extended to generate multi-word sentences as answers by replacing the multi-class classification model with an LSTM decoder.</p>
<p>ABC-CNN is composed of four components: (1) the image feature extraction part, (2) the question understanding part, (3) the attention extraction part and (4) the answer generation part. In the image feature extraction part (green box), a deep CNN is used to extract an image feature map <span class="math inline">\(\mathbf{I}\)</span> for each image as the image representation. %In the image feature extraction part (green box), a CNN is applied to extract an image feature map <span class="math inline">\(\mathbf{I}\)</span> for the input image. We utilize the VGG-19 deep convolutional neural network(<a href="">&quot;Very deep convolutional networks for large-scale image recognition&quot;</a>) pretrained on 1000-class ImageNet classification challenge 2012 dataset(<a href="">&quot;Imagenet: A large-scale hierarchical image database&quot;</a>), and a fully convolutional segmentation neural network(<a href="">&quot;Semantic image segmentation with deep convolutional nets and fully connected crfs&quot;</a>) pretrained on PASCAL 2007 segmentation dataset. The question understanding part (blue box) adopts an LSTM to learn a dense question embedding vector <span class="math inline">\(\mathbf{s}\)</span> to encode semantic information of an image-related question. The attention extraction part (yellow box) configures a set of configurable convolutional kernels (CCK) according to different dense question embeddings. These kernels, emphasizing the visual features of objects asked in the question, are convolved with the image feature maps to generate question-guided attention maps (QAM). The answer generation part, shown in the red box, answers a question using a multi-class classifier based on the image feature map <span class="math inline">\(\mathbf{I}\)</span>, the attention weighted image feature map, and the dense question embedding vector <span class="math inline">\(\mathbf{s}\)</span>. The rest of this section will describe each component of ABC-CNN framework in details.</p>
<h1 id="attention-extraction">Attention Extraction</h1>
<p>A QAM, <span class="math inline">\(\mathbf{m}\)</span>, capturing the image regions queried by the question, is generated for each image-question pair using a configurable convolutional neural network. %This attention map adaptively encodes the spacial relationship and attention distribution according to the input question's semantics <span class="math inline">\(\mathbf{s}\)</span>, which is achieved by a question-specific kernel. The configurable convolution operation can be thought of as searching spatial image feature maps for specific visual features that correspond to the question's intent. The specific visual features are encoded as a CCK <span class="math inline">\(\mathbf{k}\)</span> in this network, which is configured by projecting the dense question embedding <span class="math inline">\(\mathbf{s}\)</span> from semantic space to visual space.</p>
<p><span class="math display">\[
\boldsymbol{k} = \sigma(\mathbf{W}_{sk}\mathbf{s}+\mathbf{b}_k),\ \ \  \sigma(x) = \frac{1}{1+e^{-x}}
\]</span></p>
<p>where <span class="math inline">\(\sigma(.)\)</span> is a sigmoid function.</p>
<p>The dense question embedding <span class="math inline">\(\mathbf{s}\)</span> encodes the semantic object information asked in the question. The projection transforms the semantic information into the corresponding visual information as a CCK, which has the same number of channels as the image feature map <span class="math inline">\(\mathbf{I}\)</span>.</p>
<p>The QAM is generated by convolving the CCK <span class="math inline">\(\mathbf k\)</span> with the image feature map <span class="math inline">\(\mathbf{I}\)</span>, and applying softmax normalization:</p>
<p><span class="math display">\[
\mathbf{m}_{ij} = P(\text{ATT}_{ij}|\mathbf{I}, \mathbf{s}) = \frac{e^{\mathbf{z}_{ij}}}{\sum_i\sum_je^{\mathbf{z}_{ij}}}, \ \ \  \mathbf{z} = \mathbf{k}*\mathbf{I}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{m}_{ij}\)</span> is the element of the QAM at position <span class="math inline">\((i, j)\)</span>, and the symbol * represents the convolution operation. The QAM characterizes the attention distribution across the image feature map. The convolution is padded so that the QAM <span class="math inline">\(\mathbf{m}\)</span> has the same size as the image feature map <span class="math inline">\(\mathbf{I}\)</span>. The QAM corresponds to the regions asked by the question. For example, the question ``What is the color of the umbrella?'' can generate an attention map focusing on umbrella image regions because the CCK is configured to find umbrella visual features.</p>
<p>With the attention map <span class="math inline">\(\mathbf{m}\)</span>, we can improve the question answering accuracy on various classes of questions for the following reasons:</p>
<ul>
<li>For counting questions, such as ``how many cars in the image?'', the attention map filters out the unrelated regions, which makes it easier for the model to infer the number of objects in an image.</li>
<li>For color (and more general attribute) questions, such as ``what is the color of the coat?'', the color of the specific object can be answered more effectively by focusing on the object of interest.</li>
<li>For object questions, such as ``what is sitting on top of the desk?'', the attention map can filter out less relevant regions such as background and infer better locations to look for objects according to their spatial relationship.</li>
<li>For location questions, such as ``where is the car in the image?'', the attention map is crucial for generating the correct answers because it evidently describes where the object is in the image.</li>
</ul>
<h1 id="question-understanding">Question Understanding</h1>
<p>Question understanding is crucial for visual question answering. The semantic meaning of questions not only provides the most important clue for answer generation, but also determines the CCKs to generate attention maps.</p>
<p>Recently, LSTM model has shown good performances in language understanding (<a href="">&quot;Long short-term memory&quot;</a>). We employ an LSTM model to generate a dense question embedding to characterize the semantic meaning of questions. A question <span class="math inline">\(\mathbf{q}\)</span> is first tokenized into word sequence <span class="math inline">\(\{\mathbf{v}_t\}\)</span>. We convert all the upper case characters to lower case characters, and remove all the punctuations. The words that appear in training set but are absent in test set are replaced with a special symbol <span class="math inline">\(\#OOV\#\)</span>. Besides, <span class="math inline">\(\#B\#\)</span> and <span class="math inline">\(\#E\#\)</span> special symbols are added to the head and end of the sequence. According to a {}, each word is represented as a dense word embedding vector, which is learned in an end-to-end way. An LSTM is applied to the word embedding sequence to generate a state <span class="math inline">\(\mathbf{h}_t\)</span> from each vector <span class="math inline">\(\mathbf{v}_t\)</span>, using memory gate <span class="math inline">\(\mathbf{c}_t\)</span> and forget gate <span class="math inline">\(\mathbf{f}_t\)</span>.</p>
<p><span class="math display">\[
\begin{cases}
\mathbf{i}_t &amp;= \sigma(\mathbf{W}_{vi}\mathbf{v}_t+\mathbf{W}_{hi}\mathbf{h}_{t-1}+\mathbf{b}_i)\\
\mathbf{f}_t &amp;= \sigma(\mathbf{W}_{vf}\mathbf{v}_t+\mathbf{W}_{hf}\mathbf{h}_{t-1}+\mathbf{b}_f)\\
\mathbf{o}_t &amp;= \sigma(\mathbf{W}_{vo}\mathbf{v}_t+\mathbf{W}_{ho}\mathbf{h}_{t-1}+\mathbf{b}_o)\\
\mathbf{g}_t &amp;= \phi(\mathbf{W}_{vg}\mathbf{v}_t+\mathbf{W}_{hg}\mathbf{h}_{t-1}+\mathbf{b}_g)\\
\mathbf{c}_t &amp;= \mathbf{f}_t\odot\mathbf{c}_{t-1}+\mathbf{i}_t\odot\mathbf{g}_t\\
\mathbf{h}_t &amp;= \mathbf{o}_t\odot\phi(\mathbf{c}_t)\\
\end{cases}
\]</span></p>
<p>where <span class="math inline">\(\phi\)</span> is the hyperbolic tangent function and <span class="math inline">\(\odot\)</span> represents the element-wise production between two vectors. The LSTM's structure is shown in Fig.~. The semantic information <span class="math inline">\(\mathbf{s}\)</span> of the input question <span class="math inline">\(\mathbf{q}\)</span> is obtained by averaging the LSTM states <span class="math inline">\(\{\mathbf{h}_t\}\)</span> over all time steps .</p>
<h1 id="image-feature-extraction">Image Feature Extraction</h1>
<p>The visual information in each image is represented as an <span class="math inline">\(N \times N \times D\)</span> image feature map. The feature map is generated by dividing an image into an <span class="math inline">\(N\times N\)</span> grid, and extracting a <span class="math inline">\(D\)</span>-dimensional feature vector <span class="math inline">\(f\)</span> in each cell of the grid. We extract 5 windows in the center, upper left, upper right, lower left, and lower right corners in each cell of the original image and the left-right flipped image, resulting in a total of 10 windows for each cell. The VGG-19(<a href="">&quot;Very deep convolutional networks for large-scale image recognitio'</a>) deep convolutional neural network extracts a <span class="math inline">\(D\)</span>-dimensional feature vector for each window. The <span class="math inline">\(D\)</span>-dimensional feature vector for each cell is the average of all the 10 <span class="math inline">\(D\)</span>-dimensional feature vectors. The final <span class="math inline">\(N \times N \times D\)</span> image feature map is the concatenation of <span class="math inline">\(N\times N\times D\)</span>-dimensional feature vectors.</p>
<p>It is also possible to exploit a fully convolutional neural network architecture(<a href="">&quot;Fully convolutional networks for semantic segmentation&quot;</a>) to extract image feature maps more efficiently. We employ the segmentation model([&quot;chen2014semantic} pretrained on PASCAL 2007 segmentation dataset and find it leads to slightly better performance.</p>
<h1 id="answer-generation">Answer Generation</h1>
<p>The answer generation part is a multi-class classifier based on the original image feature map, the dense question embedding, and the attention weighted feature map.</p>
<p>We employ the attention map to spatially weight the image feature map <span class="math inline">\(\mathbf{I}\)</span>. The weighted image feature map focuses on the objects asked in the question. The spatial weighting is achieved by the element-wise production between each channel of the image feature map and the attention map.</p>
<p><span class="math display">\[
\mathbf{I}^\prime_i = \mathbf{I}_i\odot \mathbf{m}
\]</span></p>
<p>where <span class="math inline">\(\odot\)</span> represents element-wise production. <span class="math inline">\(\mathbf{I}^\prime_i\)</span> and <span class="math inline">\(\mathbf{I}_i\)</span> represent the <span class="math inline">\(i\)</span>-th channel of attention weighted feature map <span class="math inline">\(\mathbf{I}^\prime\)</span> and original image feature map <span class="math inline">\(\mathbf{I}\)</span>, respectively. The attention weighted feature map lowers the weights of the regions that are irrelevant to the meaning of question. To avoid overfitting, we apply an 1$$1 convolution on the attention weighted feature map to reduce the number of channels, resulting in a reduced feature map <span class="math inline">\(\mathbf{I}_r\)</span>. The question's semantic information <span class="math inline">\(\mathbf{s}\)</span>, the image feature map <span class="math inline">\(\mathbf{I}\)</span> and the reduced feature map <span class="math inline">\(\mathbf{I}_r\)</span> are then fused by a nonlinear projection.</p>
<p><span class="math display">\[
\mathbf{h} = g(\mathbf{W}_{ih}\mathbf{I}+\mathbf{W}_{rh}\mathbf{I}_r+\mathbf{W}_{sh}\mathbf{s}+\mathbf{b}_h)
\]</span></p>
<p>where <span class="math inline">\(\mathbf{h}\)</span> denotes the final projected feature, and <span class="math inline">\(g(.)\)</span> is the element-wise scaled hyperbolic tangent function: <span class="math inline">\(g(x) = 1.7159\cdot\tanh(\frac{2}{3}x)\)</span> (<a href="">&quot;Efficient backpro&quot;</a>). This function leads the gradients into the most non-linear range of value and enables a higher training speed.</p>
<p>A multi-class classifier with softmax activation, which is trained on the final projected features, predicts the index of an answer word specified in an answer dictionary. The answer generated by ABC-CNN is the word with the maximum probability.</p>
<p><span class="math display">\[
a^* = \arg\max_{a\in\mathcal{V}_a} \mathbf{p}_{a}  \ \ \ s.t.\ \ \
\mathbf{p}_a = g(\mathbf{W}_{ha}\mathbf{h}+\mathbf{b}_{a})
\]</span></p>
<p>Notice that we do not share the word dictionary for questions and answers, i.e., one word can have different indices in the question dictionary and answer dictionary.</p>
<h2 id="training-and-testing">Training and Testing</h2>
<p>Our whole framework is trained in an end-to-end way with stochastic gradient descent and <span class="math inline">\(\mathbf{adadelta}\)</span>(<a href="">&quot;An adaptive learning rate method&quot;</a>) algorithm. Each batch of the stochastic gradient descent randomly samples 64 image question pairs independently, and back propagation is applied to learn all the weights of the ABC-CNN architecture. We randomly adjust the initialization weights of all the layers to ensure that each dimension of the activations in all layers has zero mean and one standard variation. The initial learning rate is set to be 0.1. In our experiments, the weights in image feature extraction part are fixed to allow faster training speed, although it is possible to train all the weights in ABC-CNN in an end-to-end way.</p>
<p>During the testing stage, an image feature map is extracted for each image. Given a question, we can produce its dense question embedding, and utilize the question embedding to configure the CCK to generate the attention map. The multi-class classifier generates the answer using the original feature map, the question embedding, and the attention weighted feature map.</p>
<h1 id="experiments">Experiments</h1>
<p>We evaluate our model on Toronto COCO-QA (<a href="">&quot;Exploring models and data for image question answering&quot;</a>), DAQUAR (<a href="">&quot;A multi-world approach to question answering about real-world scenes based on uncertain input&quot;</a>) and VQA datasets (<a href="">&quot;Vqa: Visual question answering&quot;</a>). We evaluate our method on the QA pairs with single word answers, which accounts for (100%, 85%, 90%) of Toronto-QA, VQA, DAQUAR datasets, respectively. It is also consistent with the evaluation in (<a href="">&quot;Exploring models and data for image question answering&quot;</a>). Besides, our framework can be easily extended to generate answers in full sentences by using an RNN decoder in the answer generation part.</p>
<h2 id="implementation-details">Implementation Details</h2>
<p>In experiments, we first choose the resolution of both the image feature map and the attention map to be 3$$3, which is called ``ATT'' model.</p>
<p>Each image cell generates a 4096-dimensional image feature vector using a pre-trained <span class="math inline">\(\mathbf{VGG}\)</span> network(<a href="">&quot;Return of the devil in the details: Delving deep into convolutional nets&quot;</a>), and we extend each feature vector with the HSV histogram of the cell, resulting in a 4276-dimensional image feature vector for each cell. The image feature vectors from all the image cells constitute an image feature map with dimension 4276 <span class="math inline">\(\times\)</span> 3 <span class="math inline">\(\times\)</span> 3. To avoid overfitting, we reduce the dimension of the feature map to 256 <span class="math inline">\(\times\)</span> 3 <span class="math inline">\(\times\)</span> 3 with an 1 <span class="math inline">\(\times\)</span> 1 convolution. The dimension of the dense question embedding is 256. We also try a second model called &quot;ATT-SEG&quot;, which employs a fully convolutional neural network(<a href="">&quot;chen2014semantic&quot;</a>) pretrained on PASCAL 2007 segmentation dataset to generate 16$<span class="math inline">\(16\)</span>$1024 feature maps, and concatenates them with HSV histograms in each cell as image feature maps. In the end, we combine the VGG features, HSV features and segmentation features together, obtaining a model called ``ATT-VGG-SEG''. It takes around 24 hours to train the network ATT on Toronto COCO-QA dataset with four K40 Nvidia GPUs. The system can generate an answer at 9.89 ms per question on a single K40 GPU.</p>
<h2 id="datasets">Datasets</h2>
<p>We evaluate our models on three datasets: DAQUAR (<a href="">&quot;A multi-world approach to question answering about real-world scenes based on uncertain input&quot;</a>), Toronto COCO-QA (<a href="">&quot;Exploring models and data for image question answering&quot;</a>) and VQA (<a href="">&quot;Vqa: Visual question answering&quot;</a>).</p>
<p>DAQUAR dataset has two versions: the full dataset (DQ-full) and the reduced one (DQ-reduced). DQ-reduced has question answer pairs of 37 object classes, which is a subset of DQ-full dataset that has 894 object classes. Both versions use the indoor scenes images from NYU-Depth V2 dataset ([&quot;silberman2012indoor}. The DQ-full dataset contains 795 training images with 6794 QA pairs, and 654 test images with 5674 QA pairs. The DQ-reduced dataset contains 781 training images with 3825 QA pairs and 25 test images with 286 QA pairs. The questions in both datasets ask the color, number, and object information about the corresponding image. We only train and test DAQUAR dataset on QA pairs with single word answers, which is consistent with the evaluation in ([&quot;torontoQa}. Such QA pairs constitute (90.6%, 89.5%) and (98.7%, 97.6%) in the training and test sets for DQ-full and DQ-reduced datasets, respectively.</p>
<p>Toronto COCO-QA dataset uses images from Microsoft COCO dataset (<a href="">&quot;Microsoft coco: Common objects in context&quot;</a>) (MS-COCO). Its QA pairs only contain single word answers.</p>
<p>VQA dataset(<a href="">&quot;Vqa: Visual question answering&quot;</a>) is a recently collected dataset which is also built with images in MS-COCO dataset. We evaluate the proposed model on VQA Real Image (Open-Ended) task in VQA dataset. It contains 82783 training images, 40504 validation images, and 81434 testing images. Each image in MS-COCO dataset is annotated with 3 questions, and each question has 10 candidate answers. The total number of QA pairs for training, testing, and validation is 248349, 121512, 244302, respectively. We only evaluate our method on the single-word answer QA pairs in VQA dataset, which constitute 86.88% of the total QA pairs in this dataset.</p>
<h1 id="evaluation-metrics">Evaluation Metrics</h1>
<p>As in (<a href="">&quot;Exploring models and data for image question answering&quot;</a>)(<a href="">&quot;A multi-world approach to question answering about real-world scenes based on uncertain input&quot;</a>), we evaluate the performance of the VQA models with <code>answer accuracy'' (ACC.) and</code>Wu-Palmer similarity measure Set'' (WUPS) score([&quot;wu1994verbs}([&quot;malinowski2014nips}. The answer accuracy computes the percentage of the generated answers that exactly match the ground truth answers. The WUPS score is derived from the Wu-Palmer (WUP) similarity (<a href="">&quot;Verbs semantics and lexical selection&quot;</a>), whose value is in the range of <span class="math inline">\([0, 1]\)</span>. WUP similarity measures the similarity of two words based on the depth of their lowest common ancestor in the taxonomy tree(<a href="">&quot;Verbs semantics and lexical selection&quot;</a>). The WUPS score with threshold is the average of the down-weighted WUPS score for all the generated answers and ground truth. If WUPS score of two words <span class="math inline">\(s_{wups}\)</span> is below a threshold, their down-weighted WUPS score is <span class="math inline">\(0.1 s_{wups}\)</span>. Otherwise, its down-weighted WUPS is <span class="math inline">\(s_{wups}\)</span>. %Thus, the WUPS score is always higher than the answer accuracy. We use WUPS scores with thresholds 0.0 and 0.9 in our experiments, which are the same as (<a href="">&quot;A multi-world approach to question answering about real-world scenes based on uncertain input&quot;</a>).</p>
<p>The WUPS score between the generated answers <span class="math inline">\(A\)</span> and ground truth answers <span class="math inline">\(T\)</span> is:</p>
<p><span class="math display">\[
\text{WUPS}(A, T) = \frac{1}{N}\sum_{i=1}^{N} \min\{f_c(A^i, T^i), f_c(T^i, A^i)\}
\]</span></p>
<p>where <span class="math inline">\(A^i, T^i\)</span> represent the similar object sets for <span class="math inline">\(i\)</span>-th generated answer word <span class="math inline">\(a_i\)</span> and corresponding ground truth answer word <span class="math inline">\(t_i\)</span>, respectively. The similarity measure $f_c(A^i, T^i) $ has a threshold of <span class="math inline">\(c\)</span> and is illustrated in Eq.~.</p>
<p><span class="math display">\[
\begin{split}
f_c(A^i, T^i) =&amp; \prod_{a\in A^i}\max_{t\in T^i}\text{WUP}(a,t)\cdot \\
&amp;(0.1+0.9\cdot \textbf{1}(\text{WUP}(a,t)\geq c))\\
\end{split}
\]</span></p>
<p>Following (<a href="">&quot;Exploring models and data for image question answering&quot;</a>), we evaluate our models using WUPS score at threshold of 0.9 and 0.0. The WUPS score measures the similarity between two answers by calculating the maximum length of their common subsequence in the taxonomy tree. { I believe this is wrong. How can a length be 0.9} If the similarity between the generated answer and the ground truth answer is below a threshold, we consider it is as a correct answer. Thus, the WUPS score is always higher than the answer accuracy We set the threshold of the WUPS score to 0.0 and 0.9 in our experiments.</p>
<h2 id="baseline-methods">Baseline Methods</h2>
<p>We compare the proposed method with different benchmark methods used in (<a href="">&quot;A multi-world approach to question answering about real-world scenes based on uncertain input&quot;</a>), (<a href="">&quot;torontoQa&quot;</a>) ,(<a href="">&quot;Vqa: Visual question answering&quot;</a>),(<a href="">&quot;Ask your neurons: A neural-based approach to answering questions about image&quot;</a>). All the baseline models are listed below:</p>
<ul>
<li><span class="math inline">\(\textbf{VIS+LSTM (VL)}\)</span>: It is the framework proposed in (<a href="">&quot;Exploring models and data for image question answering&quot;</a>), with a CNN extracting image features followed by a dimension reduction layer. The image features are then inserted into the head position of the question word embedding sequences as inputs for question LSTM.</li>
<li><span class="math inline">\(\textbf{2-VIS+BLSTM (2VB)}\)</span>: The image features are inserted at the head and the tail of question word embedding sequences.</li>
</ul>
<p>Besides, the question LSTM in (<a href="">&quot;Exploring models and data for image question answering&quot;</a>) is set to go in both forward and backward directions.</p>
<ul>
<li><span class="math inline">\(\textbf{IMG+BOW (IB)}\)</span>: Ren et al.(<a href="">&quot;Exploring models and data for image question answering&quot;</a>) use Bag-of-Words features to generate the dense question embedding.</li>
<li><span class="math inline">\(\textbf{IMG}\)</span>: Only the image features are used for answering the questions. It is called a ``deaf'' model.</li>
<li><span class="math inline">\(\textbf{LSTM}\)</span>: The answers are generated only using the dense question embedding from LSTM. It is called a ``blind'' model.</li>
<li><span class="math inline">\(\textbf{ENSEMBLE}\)</span>: Ren et al. in (<a href="">&quot;Exploring models and data for image question answering&quot;</a>) evaluated the fusion model by using an ensemble of all the above methods.</li>
<li><span class="math inline">\(\textbf{Q+I}\)</span>: In (<a href="">&quot;Vqa: Visual question answering&quot;</a>), the question answering is achieved by training a multi-class classifier using both the dense question embeddings and image features.</li>
<li><span class="math inline">\(\textbf{Q+I+C}\)</span>: Compared to Q+I model in (<a href="">&quot;Vqa: Visual question answering&quot;</a>), the Q+I+C model (<a href="">&quot;Vqa: Visual question answering&quot;</a>) adopts the dense embeddings of labeled image captions as an additional input.</li>
<li><span class="math inline">\(\textbf{ASK}\)</span>: In (<a href="">&quot;Ask your neurons: A neural-based approach to answering questions about images&quot;</a>), the answers are generated by linearly combining CNN features and question embeddings in an LSTM decoder. \end{itemize}</li>
</ul>
<h1 id="results-and-analysis">Results and Analysis</h1>
<p>On DQ-full and VQA datasets, ABC-CNN outperforms state-of-the-art methods on both datasets On DQ-full dataset, the ABC-CNN model is the same as the models on Toronto COCO-QA and DQ-reduced dataset. On VQA dataset, to make a fair evaluation, we employ the same answer dictionary that contains the 1000 most frequent answers (ATT 1000) as (<a href="">&quot;Vqa: Visual question answering&quot;</a>). We also evaluate the ABC-CNN model using the answer dictionary that contains all the answers (ATT Full).</p>
<p>Some of the generated question-guided attention maps and their corresponding images and questions. We can observe that the question-guided attention maps successfully capture different questions' intents with different attention regions. With these attention maps, ABC-CNN is capable of generating more accurate answers by focusing its attention on important regions and filtering out irrelevant information. Since the original feature map is also provided when predicting answers, ABC-CNN can answer the question without using the attention map if the object queried is the only object in the image</p>
</body>
</html>
