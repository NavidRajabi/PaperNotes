<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<h1 id="zero-shot-visual-question-answering"><a href="https://arxiv.org/pdf/1611.05546.pdf">Zero-Shot Visual Question Answering</a></h1>
<p>The contributions of this paper are summarized as follows: - Define the Zero-Shot Visual Question Answering (ZS-VQA) problem, and propose a corresponding evaluation setting where each test instance contains one or several unseen words, i.e. words not present in any training instance. - Propose dataset that focuses exclusively on this setting based on the Visual7W dataset [36], of which we define new training and test splits.</p>
</body>
</html>
