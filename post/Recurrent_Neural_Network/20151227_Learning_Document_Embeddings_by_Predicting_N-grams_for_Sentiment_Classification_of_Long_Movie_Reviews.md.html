<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<h2 id="learning-document-embeddings-by-predicting-n-grams-for-sentiment-classification-of-long-movie-reviews"><a href="http://arxiv.org/abs/1512.08183">Learning Document Embeddings by Predicting N-grams for Sentiment Classification of Long Movie Reviews</a></h2>
<p>TLDR; The authors present DV-ngram, a new method to learn document embeddings. DV-ngrams is a variation on Paragraph Vectors with a training objective of predicting words and n-grams solely based on the document vector, forcing the embedding to capture the semantics of the text. The authors evaluate their model on the IMDB data sets, beating both n-gram based and Deep Learning models.</p>
<h4 id="key-points">Key Points</h4>
<ul>
<li>When the word vectors are already sufficiently predictive of the next words, the standard PV embedding cannot learn anything useful.</li>
<li>Training objective: Predict words and n-grams solely based on document vector. Negative Sampling to deal with large vocabulary. In practice, each n-gram is treated as a special token and appended to the document.</li>
<li>Code will be at https://github.com/libofang/DV-ngram</li>
</ul>
<h4 id="questionnotes">Question/Notes</h4>
<ul>
<li>The argument that PV may not work when the word vectors themselves are predictive enough makes intuitive sense. But what about applying word-level dropout? Wouldn't that also force the PV to learn the document semantics?</li>
<li>It seems to be that predicting n-grams leads to a huge sparse vocabulary space. I wonder how this method scales, even with negative sampling. I am actually surprised this works well at all.</li>
<li>The authors mention that they beat &quot;other Deep Learning models, including PV, but neither their model nor PV are &quot;deep learning&quot;. The networks are not deep ;)</li>
</ul>
</body>
</html>
