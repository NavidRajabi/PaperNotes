<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h2 id="random-walk-initialization-for-training-very-deep-feedforward-networks"><a href="http://arxiv.org/abs/1412.6558">Random Walk Initialization for Training Very Deep Feedforward Networks</a></h2>
<h3 id="background"><strong><em>background</em></strong></h3>
<p>Training very deep networks is an important open problem in machine learning. One of many difficulties is that the norm of the back-propagated error gradient can grow or decay exponentially.</p>
<h3 id="what-they-do"><strong><em>What they do</em></strong></h3>
<p>Here we show that training very deep <strong><em>feed-forward networks (FFNs)</em></strong> is not as difficult as previously thought.</p>
<p>They show that the <strong><em>successive application of correctly scaled random matrices</em></strong> to an <strong><em>initial vector</em></strong> results in a random walk of the log of the norm of the resulting vectors, and they compute the scaling that makes this walk unbiased.</p>
<h2 id="introduction"><strong><em>Introduction</em></strong></h2>
<p>Since the early 90s, it has been appreciated that deep neural networks suffer from a vanishing gra- dient problem:</p>
<ol style="list-style-type: decimal">
<li>(Hochreiter, 1991),</li>
<li>(<a href="http://deeplearning.cs.cmu.edu/pdfs/Bengio_94.pdf">Bengio et al., 1994 : Learning Long-Term Dependencies with Gradient Descent is Difficult</a>).</li>
<li>(<a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=298725&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D298725">Bengio et al., 1994 : he problem of learning long-term dependencies in recurrent networks</a>),</li>
<li>(<a href="ftp://ftp.idsia.ch/pub/juergen/gradientflow.pdf">Hochreiter et al., 2001 : Gradient flow in recurrent nets: the difficulty of learning long-term dependencies</a>).</li>
</ol>
<p>The term <strong><em><a href="">vanishing gradient/Ê¢ØÂ∫¶Ê∂àÂ§±</a></em></strong> refers to the fact that in a feedforward network (FFN) the <strong><em>back- propagated error signal typically decreases</em></strong> (or increases) exponentially as a function of the <strong><em>distance</em></strong> from the final layer.</p>
<p><strong><em><a href="">vanishing gradient/Ê¢ØÂ∫¶Ê∂àÂ§±</a></em></strong> problem is also observed in <a href="">recurrent networks (RNNs)</a>, where the errors are back-propagated in time and the error signal decreases (or increases) exponentially as a function of the distance back in time from the current error.</p>
<p>Because of the <strong><em>vanishing gradient</em></strong>, adding many extra layers in FFNs or time points in RNNs does not usually improve performance.</p>
<p>Although it can be applied to both feedforward and recurrent networks, <a href="">the analysis of the vanishing gradient problem is based on a recurrent architecture</a>.</p>
<p><font size="2">In a recurrent network, <strong><em>back-propagation through time involves applying</em></strong> <a href="https://zh.wikipedia.org/zh-sg/%E7%9B%B8%E4%BC%BC%E7%9F%A9%E9%99%A3">similar matrices</a> repeatedly to compute the error gradient. The outcome of this process depends on whether <a href="">the magnitudes of the leading eigenvalues of these matrices tend to be greater than or less than one</a>.</font></p>
<h3 id="the-magnitude-of-the-error-gradient-ffns"><strong><em>The magnitude of the error gradient FFNs</em></strong></h3>
<p><span class="math display">\[
\begin{cases}\mathbf{a_d}=g\mathbf{W_d}\mathbf{h}_{d-1}\mathbf{b}_d\\ \mathbf{h}_d=f(\mathbf{a_d})\end{cases}
\Rightarrow
\begin{cases}
\mathbf{\delta}_d=g\mathbf{\tilde{W}_{d+1}}\mathbf{\delta}_{d+1}\\
\mathbf{\tilde{W}_d}(i,j)=f&#39;(a_d(i))W_d(j,i)\\
|\mathbf{\delta}_d|^2=g^2z_{d+1}|\mathbf{\delta}_{d+1}|^2\\
z_d=|\mathbf{\tilde{W}_d}\mathbf{\delta}_d/|\mathbf{\delta}_d||^2
\end{cases}
\Rightarrow
\begin{cases}
Z=\frac{|\mathbf{\delta}_0|^2}{|\mathbf{\delta}_D|^2}=g^{2D}\prod_{d=1}^{D}z_d \\
\ln(Z)=D\ln(g^2)+\sum_{d=1}^D\ln(z_d)
\end{cases}
\]</span></p>
<h3 id="calcuation-of-the-optimal-g-values"><strong><em>Calcuation of the optimal <span class="math inline">\(g\)</span> value's</em></strong></h3>
<p><span class="math display">\[
\begin{cases}
\langle \ln(z) \rangle=D(\ln(g^2))+\langle\ln(z) \rangle=0\\
g=\exp(-\frac{1}{2}\langle \ln(z)\rangle)\\
z=|\mathbf{\tilde{W}\mathbf{\delta}}/|\mathbf{\delta}||^2
\end{cases}
\Rightarrow
\begin{cases}
\langle \ln(z) \rangle  \approx  \langle (z-1)\ rangle -\frac{1}{2}\langle (z-1)^2\rangle =-\frac{1}{N}\\
g_{linear}=\exp(\frac{1}{2N})\\
\langle (\ln(z))^2\rangle -\langle \ln(z) \rangle ^2=\frac{1}{2N} \\
\langle \ln(z)\rangle \approx -\ln(2)-\frac{2.4}{\max(N,6)-2.4} \\
\langle (\ln(z))^2\rangle - \langle \ln(z) \rangle^2 \approx \frac{5}{\max(N,6)-4}\\
g_{ReLU}=\sqrt{2}\exp(\frac{1.2}{\max(N,6)-2.4})
\end{cases}
\]</span></p>
<h3 id="random-walk-initialization"><strong><em>Random walk initialization</em></strong></h3>
<p>The general methodology used in the <strong><em>RandomWalk Initialization</em></strong> is to set g according to <strong><em>the values given in equations</em></strong> <span class="math inline">\(\begin{cases}g_{\text{Linear}}=\exp(\frac{1}{2N})\\g_{\text{ReLU}}=\sqrt{2}\exp(\frac{1.2}{\max(N,6)-2.4})\end{cases}\)</span> for the linear and ReLU cases, respectively. For tanh, the values between 1.1 and 1.3 are shown to be good in practice.</p>
<p>The scaling of the input distribution itself should also be <strong><em>adjusted to zero mean</em></strong> and <strong><em>unit variance in each dimension</em></strong>.</p>
<p>Poor input scaling will effect the back-propagation through the derivative terms in equation <span class="math inline">\(\mathbf{\delta}_d=g\mathbf{\tilde{W}_{d+1}}\mathbf{\delta}_{d+1}\)</span> for some number of early layers before the randomness of the initial matrices ‚Äúwashes out‚Äù the poor scaling.</p>
<p><strong><em>A slight adjustment to <span class="math inline">\(g\)</span></em></strong> may be helpful, based on the actual data distribution, as most real-world data is far from a <strong><em>normal distribution</em></strong>. By similar reasoning, the initial scaling of the final output layer may need to be adjusted separately, as the back-propagating errors will be affected by the initialization of the final output layer.</p>
<p>In summary, <strong><em>RandomWalk Initialization requires tuning of three parameters</em></strong>: input scaling (or <span class="math inline">\(g_1\)</span>), <span class="math inline">\(g_D\)</span>, and <span class="math inline">\(g\)</span>, the first two to handle transient effects of the inputs and errors, and the last to generally tune the entire network. By far the most important of the three is <span class="math inline">\(g\)</span>.</p>
</body>
</html>
