<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h2 id="highway-networks">Highway Networks</h2>
<p><em>&quot;Our Highway Networks take inspiration from Long Short Term Memory (LSTM) and allow training of deep, efficient networks (even with hundreds of layers) with conventional gradient-based methods. Even when large depths are not required, highway layers can be used instead of traditional neural layers to allow the network to adaptively copy or transform representations&quot;</em></p>
<h3 id="highway-networks-1">Highway Networks</h3>
<p>一般一个 plain feedforward neural network 有L层网络组成，每层网络对输入进行一个非线性映射变换，可以表达如下:</p>
<p><span class="math display">\[\mathbf{y}=H(\mathbf{x}, \mathbf{W_H})\]</span></p>
<p>一般后续还有其他处理，例如非线性激活函数， convolutional or recurrent</p>
<p>对于高速CNN网络，我们定义一层网络如下</p>
<p><span class="math display">\[
\mathbf{y}=H(\mathbf{x}, \mathbf{W_H}).T(\mathbf{x},\mathbf{W_T})+\mathbf{x}.C(\mathbf{x},\mathbf{W_C})
\]</span></p>
<p>We refer to T as the transform gate and C as the carry gate</p>
<p>在这篇文献中我们设置 C=1-T，则得到下式</p>
<p><span class="math display">\[
\mathbf{y}=H(\mathbf{x}, \mathbf{W_H}).T(\mathbf{x},\mathbf{W_T})+\mathbf{x}.(1-T(\mathbf{x},\mathbf{W_T}))
\]</span></p>
<p>上公式中<span style="color:red"><strong><em>参数的维数须一致</em></strong></span>。</p>
<p>for the Jacobian of the layer transform： <span class="math display">\[
\frac{d \mathbf{y}}{d \mathbf{x}}=
\begin{cases}
\mathbf{I}, if \ T(\mathbf{x},\mathbf{W_T})=0\\
H&#39;(\mathbf{x}, \mathbf{W_H}), if \ T(\mathbf{x},\mathbf{W_T})=1
\end{cases}
\]</span></p>
<p>Thus, depending on the output of the transform gates, a highway layer can smoothly vary its behavior between that of H and that of a layer which simply passes its inputs through</p>
<h3 id="training-deep-highway-networks">Training Deep Highway Networks</h3>
<p>我们定义 transform gate : <span class="math inline">\(T(\mathbf{x})=\sigma(\mathbf{W_T}^T\mathbf{x}+\mathbf{b_T})\)</span></p>
<p>W是权重矩阵， b是 bias 向量</p>
<p>This suggests a simple initialization scheme which is independent of the nature of H: <span class="math inline">\(b_T\)</span> can be initialized with a negative value (e.g. -1, -3 etc.) such that the network is initially biased towards <span style="color:red"><strong><em>carry</em></strong></span> behavior. This scheme is strongly inspired by the proposal to initially bias the gates in an LSTM network, to help bridge long-term temporal dependencies early in learning</p>
<p>初始化时可以给b初始化一个负值，相当于网络在开始的时候侧重于搬运行为（carry behavior），就是什么处理都不做。这个主要是受<a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=818041&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D818041">文献</a>启发。我们的实验也证明了这个推测是正确的。</p>
</body>
</html>
