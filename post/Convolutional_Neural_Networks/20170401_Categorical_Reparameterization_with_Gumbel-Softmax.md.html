<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="categorical-reparameterization-with-gumbel-softmax"><a href="https://arxiv.org/pdf/1611.01144.pdf">Categorical Reparameterization with Gumbel-Softmax</a></h1>
<h2 id="reference">Reference</h2>
<ul>
<li>http://blog.evjang.com/2016/11/tutorial-categorical-variational.html</li>
<li><a href="https://casmls.github.io/general/2017/02/01/GumbelSoftmax.html">The Gumbel-Softmax Trick for Inference of Discrete Variables</a></li>
</ul>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> torch.nn.functional <span class="im">as</span> F
<span class="im">from</span> torch.autograd <span class="im">import</span> Variable

<span class="kw">def</span> sampler(<span class="bu">input</span>, tau, temperature):
    noise <span class="op">=</span> torch.rand(<span class="bu">input</span>.size())
    noise.add_(<span class="fl">1e-9</span>).log_().neg_()
    noise.add_(<span class="fl">1e-9</span>).log_().neg_()
    noise <span class="op">=</span> Variable(noise)
    x <span class="op">=</span> (<span class="bu">input</span> <span class="op">+</span> noise) <span class="op">/</span> tau <span class="op">+</span> temperature
    x <span class="op">=</span> F.softmax(x.view(<span class="bu">input</span>.size(<span class="dv">0</span>), <span class="op">-</span><span class="dv">1</span>))
    <span class="cf">return</span> x.view_as(<span class="bu">input</span>)</code></pre></div>
<h2 id="notes">Notes</h2>
<p>The striking similarities between the main idea of [1] and [2]; namely, the “Gumbel-Softmax trick for re-parameterizing [categorical distributions, 类别分布]” serves as an example of such [simultaneous, 同时发生] discovery in machine learning. Here, the underlying explanation for the [coincidence, 同时发生] is rather obvious: one of the most popular new techniques in [variational inference, 变分推断] and generative modeling; the so-called “re-parameterization trick,” could not be applied to discrete-valued random variables, imposing a significant [hurdle, 障碍] for the development of new results. Now, with the Gumbel-Softmax trick as an add-on, we can do re-parameterization for inference involving discrete latent variables. This creates a new promise for new findings in areas where the primary objects are of discrete nature; e.g. text modeling.</p>
<p>Before stating the results we start by reviewing the <b>re-parameterization trick</b> and its uses.</p>
<blockquote>
<p><font style="color:red">The re-parameterization trick is a hot idea, but it fails on discrete data</font></p>
</blockquote>
<p>Let’s first recall the [Law of the Unconscious Statistician (LOTUS),期望值], a simple rule of [calculus, 微积分学] stating that one can compute the expectation of a measurable function <span class="math inline">\(g\)</span> of a random variable <span class="math inline">\(\epsilon\)</span> by integrating <span class="math inline">\(g(\epsilon)\)</span> with respect to the distribution function of <span class="math inline">\(\epsilon\)</span>, that is: <span class="math inline">\(\mathbb{E}(g(\epsilon))=\int g(\epsilon) d F_\epsilon\)</span>. In other words, <font style="color:red">to compute the expectation of <span class="math inline">\(z =g(\epsilon)\)</span> we only need to know <span class="math inline">\(g\)</span> and the distribution of <span class="math inline">\(\epsilon\)</span>. We do not need to explicitly know the distribution of <span class="math inline">\(z\)</span></font>. We can also express the above with the convenient alternative notation: <span class="math inline">\(\mathbb{E}_{\epsilon \sim p(\epsilon)}(g(\epsilon))= \mathbb{E}_{ z \sim p(z)}(z)\)</span>.</p>
<p>Now, suppose <font style="color:red">a certain variable <span class="math inline">\(z\)</span> has a distribution that depends on a parameter <span class="math inline">\(\phi\)</span>, i.e <span class="math inline">\(z\sim p_\phi(z)\)</span></font>. Moreover, assume one can express <font style="color:red"><span class="math inline">\(z=g(\epsilon,\phi)\)</span> for a certain known function <span class="math inline">\(g\)</span> of the parameters and a certain noise distribution</font> (e.g, <span class="math inline">\(\epsilon \sim \mathcal{N}(0,1)\)</span>) From the LOTUS we know that for any measurable function <span class="math inline">\(f\)</span>:</p>
<p><span class="math display">\[
\mathbb{E}_{z\sim p_\phi(z)}(f(z))= \mathbb{E}_{\epsilon \sim p(\epsilon)}(f(g(\epsilon,\phi))).
\]</span></p>
<p>However, by itself the above formula is not enough. Indeed, in our ML applications we will be faced with the need to compute</p>
<p><span class="math display">\[
\nabla_\phi \mathbb{E}_{Z\sim p_\phi(Z)}(f(Z)) = \nabla_\phi \mathbb{E}_{\epsilon\sim p(\epsilon)}(f(g(\epsilon,\phi))) =  \mathbb{E}_{\epsilon\sim p(\epsilon)}(\nabla f(g(\epsilon,\phi))).
\]</span></p>
<p>The second equality of the above expression constitutes another feature of the re-parameterization trick: <font style="color:red">we have conveniently expressed <span class="math inline">\(z\)</span> so that expectations of functions of <span class="math inline">\(z\)</span> can be expressed as integrals w.r.t a density that does not depend on the parameter</font>; therefore, we exchange the expectation and gradient or “differentiate under the integral sign”.</p>
<p>The final feature of the re-parametrization trick has to do with how to use the above gradient formula to construct good unbiased estimates of the gradient, a task that we will often involved with (see below). The above formula gives an <font style="color:red">immediate way to obtain an unbiased estimate of the above gradient via Monte Carlo</font>:</p>
<p><span class="math display">\[
\nabla_\phi \mathbb{E}_{z\sim p_\phi(z)}(f(z)) \approx \frac{1}{M}\sum_{i=1}^M \nabla f(g(\epsilon^i,\phi)).
\]</span></p>
<p>Although not completely understood, in real applications it is seen this re-parameterization based estimate of the gradient exhibits much less variance than of competing estimators.</p>
<blockquote>
<p>Why do we need this trick: <font style="color:red">variational inference, and adversarial generative modeling</font></p>
</blockquote>
<p>Let’s now briefly state two [scenarios, 方案] where we will need to apply the <font style="color:red">re-parameterization trick</font>. They are not mutually exclusive, but here we state them separately for [pedagogical, 教育学] reasons.</p>
<ol style="list-style-type: decimal">
<li>Variational inference</li>
</ol>
<p>First, consider variational inference in a latent variable model: we want to have access (i.e evaluate, maximize) to the posterior <span class="math inline">\(p_\theta(z \mid x)\)</span>, which will be usually intractable because of the evidence <span class="math inline">\(p_\theta(x)\)</span> in the denominator. In a standard variational setup we find a <font style="color:red">variational approximation <span class="math inline">\(q_\phi(z \mid x)\)</span></font> such that it minimizes its “distance” with the true posterior; or equivalently, it maximizes a lower bound for the log evidence,</p>
<p><span class="math display">\[
\mathcal{L}_{\theta,\phi}(x) = \mathbb{E}_{z\sim q_\phi(z \mid x)}\left(\log(p_\theta(x,z)) - \log(q_\phi(z \mid x))\right).
\]</span></p>
<p>Learning will be performed, then, by a double maximization of this surrogate function (the ELBO): with respect to <span class="math inline">\(\phi\)</span> in order to bridge the gap with the true posterior, and with respect to <span class="math inline">\(\theta\)</span>, to maximize the evidence.</p>
<p>The most direct approach for learning is to do <font style="color:red">gradient descent</font>. However, unfortunately, gradients won’t be available in closed form because they are applied to the often intractable expectation with respect to <font style="color:red"><span class="math inline">\(z\sim q_\phi(z \mid x)\)</span></font>. The standard, then, is to use gradient-based stochastic optimization, a family of methods that solve the above problem by <font style="color:red">assuming that noisy but unbiased estimators of the gradient are available</font>: in that case, we can <font style="color:red">replace the true gradients by the noisy ones as long as we adapt the learning rate of our algorithms accordingly</font>.</p>
<p>Whenever we can <font style="color:red">re-parameterize <span class="math inline">\(q_\phi(z \mid x)\)</span> with respect to a noise distribution, <span class="math inline">\(s(\epsilon)\)</span></font>, we will be able to construct a good unbiased estimator of the ELBO (essential for fast convergence of stochastic optimization methods) based on Monte Carlo samples, as we’ll have:</p>
<p><span class="math display">\[
\mathcal{L}_{\theta,\phi}(x)= \mathbb{E}_{\epsilon \sim s(\epsilon)} (\log (p_{\theta}(x,g(\phi,\epsilon)))-\log (q_\phi(g(\phi, \epsilon)))).
\]</span></p>
<blockquote>
<p>Adversarial learning of generative models</p>
</blockquote>
<p>Another case where we can profit from re-parameterization comes from <font style="color:red">adversarial learning</font>. Suppose we are concerned with learning a (perhaps, very high-dimensional) distribution <span class="math inline">\(p_\theta(x)\)</span> based on observed samples <span class="math inline">\(x_{data}\)</span>. Here, we assume the generative parameters <span class="math inline">\(\theta_g\)</span> allow us to express the complex dependencies between the variables in <span class="math inline">\(x\)</span> (e.g. through a deep generative process) so that <span class="math inline">\(p_{\theta_g}(x)\)</span> will be a rich, parsimonious parametric approximation of the empirical distribution <span class="math inline">\(p_{data}(x)\)</span>.</p>
<p>As nicely stated in [5], if we assume that for some function <span class="math inline">\(G\)</span> and noise distribution <span class="math inline">\(z\)</span> we have <font style="color:red"><span class="math inline">\(x=G(\theta_g,z)=G(z)\)</span></font>, then, we can cast the (intractable) problem of learning <span class="math inline">\(p_{\theta_g}(x)\)</span> as a minimax game where we simultaneously try to minimize with respect to generative model parameters <span class="math inline">\(\theta_g\)</span> and maximize with respect to the parameters <span class="math inline">\(\theta_d\)</span> of an ad-hoc discriminative network <span class="math inline">\(D\)</span> (e.g. a multi-layer perceptron). In other words, we do:</p>
<p><span class="math display">\[\min_G \max_D\;\; \mathbb{E}_{x\sim p_{data}(x)}(\log(D(x))) - \mathbb{E}_{z\sim p(z)}(\log(1-D(G(z))).\]</span></p>
<p>Adversarial learning algorithms iteratively sample batches from the data and noise distributions and use the noisy gradient information to simultaneously ascend in the parameters of <span class="math inline">\(D\)</span> (i.e, <span class="math inline">\(\theta_d\)</span>) while descending in the parameters of <span class="math inline">\(G\)</span> (i.e, <span class="math inline">\(\theta_g\)</span>).</p>
<p><font style="color:red">Notice that if we were not able re-parameterize, we could only express the above as</font></p>
<p><span class="math display">\[\min_{\theta_g} \max_D \;\;\mathbb{E}_{x\sim p_{data}(x)}(\log(D(x))) - \mathbb{E}_{x\sim p_{\theta_g}(x)}(\log(1-D(x)).\]</span></p>
<p>Therefore, alternative unbiased estimates of the gradients of the second term (w.r.t <span class="math inline">\(\theta_g\)</span>) would be required if stochastic optimization was attempted.</p>
<p>One alternative (which also applies to the variational setup) is to use the <font style="color:red">score-function estimator (also called the REINFORCE estimator by historical reasons)</font>, which is based on the log derivative trick. However, the variance of this new estimator can be so high that it could not qualify as a realistic alternative to the re-parameterization based estimator.</p>
<blockquote>
<p>Why things can go wrong in discrete cases</p>
</blockquote>
<p>The reason why we cannot apply the re-parameterization trick to discrete variables is simple: by [elementary, 基本的] real analysis facts, it is mathematically impossible for a non-degenerate function that maps a continuous set onto a discrete set to be differentiable (not even continuous!). That is, for the functional relation <span class="math inline">\(z=g(\phi,\epsilon),\)</span> it does not make any sense to conceive <span class="math inline">\(\frac{\partial z}{\partial \phi}\)</span> in the discrete case, regardless of the value of <span class="math inline">\(\epsilon\)</span>. Alternatively, in deep learning jargon: <font style="color:red">“we cannot backpropagate the gradients through discrete nodes in the computational graph”</font>.</p>
<p>As stated above, in both cases (variational inference and adversarial generative modeling) we still will be able to construct <font style="color:red">alternative estimates of the gradients</font>. However, they may not (and do not!) enjoy the low-variance property of the re-parameterization-based ones.</p>
<blockquote>
<p>The Gumbel distribution and softmax function to the rescue</p>
</blockquote>
<p>The Gumbel-softmax trick is an attempt to <font style="color:red">overcome the inability to apply the re-parameterization trick to discrete data</font>. It is the result of two insights: 1) <font style="color:red">a nice parameterization for a discrete (or [categorical, 分类的]) distribution is given in terms of the Gumbel distribution (the Gumbel trick)</font>; and 2) although the corresponding function is non-continuous, it can be made continous by applying using a <font style="color:red">continuous approximation that depends on a temperature parameter</font>, which in the zero-temperature case degenerates to the discontinuous, original expression. Now we describe both components</p>
<ol style="list-style-type: decimal">
<li>The Gumbel distribution trick</li>
</ol>
<p>Let’s first recall what a <font style="color:red">Gumbel distribution</font> is. The random variable <span class="math inline">\(G\)</span> is said to have a standard Gumbel distribution if <span class="math inline">\(G=-\log(-\log( U))\)</span> with <span class="math inline">\(U\sim \text{Unif}[0,1]\)</span>.</p>
<p>For us, its importance is a consequence that we can <font style="color:red">parameterize any discrete distribution in terms of Gumbel random variables by using the following fact</font>:</p>
<p><font style="color:purple">Let <span class="math inline">\(X\)</span> be a discrete random variable with <span class="math inline">\(P(X=k)\propto \alpha_k\)</span> random variable and let <span class="math inline">\(\{G_k\}_{k\leq K}\)</span> be an i.i.d sequence of standard Gumbel random variables. Then:</font></p>
<p><span class="math display">\[X=\arg\max_k \left(\log \alpha_k +G_k\right).\]</span></p>
<p>In other words, a recipe for sampling from a categorial distribution is: 1) <font style="color:red">draw Gumbel noise by just transforming uniform samples</font>; 2) <font style="color:red">add it to <span class="math inline">\(\log \alpha_k\)</span>, which only has to be known up to a normalizing constant</font>; and 3) <font style="color:red">take the value <span class="math inline">\(k\)</span> that produces the maximum</font>.</p>
<blockquote>
<p>Relaxing the discreteness</p>
</blockquote>
<p>Unfortunately, <font style="color:red">the <span class="math inline">\(\arg\max\)</span> operation that relates the Gumbel samples, the <span class="math inline">\(\alpha_k\)</span>’s and the realizations of the discrete distribution is not continuous</font>. One way of circumvent this, as suggested in [1] and [2] is to relax the discrete set by considering random variables taking values in a larger set. To construct this relaxation we start by recognizing that 1) <font style="color:red">any discrete random variable can always be expressed as a one-hot vector</font> (i.e, a vector filled zeros except for an index where the coordinate is one), by mapping the realization of the variable to the index of the non-zero entry of the vector, and 2) <font style="color:red">that the convex hull of the set of one-hot vector is the probability simplex</font>:</p>
<p><span class="math display">\[\Delta^{K-1}=\{x\in R_{+}^K\quad, \sum_{k=1}^K x_k \leq 1\}.\]</span></p>
<p>Therefore, a natural way to extend (or ‘relax’) a discrete random variables is by allowing it to take values in the probability simplex. Both [1] and [2] propose to consider the <font style="color:red">softmax map (indexed by a temperature parameter)</font>:</p>
<p><span class="math display">\[f_\tau(x)_k =\frac{\exp(x_k/\tau)}{\sum_{k=1}^K \exp(x_k/\tau)}.\]</span></p>
<p>with this definition we can define (instead of the discrete valued random variable <span class="math inline">\(X\)</span>) the sequence of simplex-valued random variables:</p>
<p><span class="math display">\[X^\tau = (X_k^\tau)_k=f_\tau(\log \alpha+G) =\left(\frac{\exp((\log \alpha_k +G_k)/\tau)}{\sum_{i=1}^K \exp((\log \alpha_{i} +G_{i})/\tau)}\right)_k.\]</span></p>
<p>The random variable <span class="math inline">\(X^\tau\)</span> defined as above is said to have the <em>concrete</em> distribution (<em>concrete</em> is a <em>portmanteau</em> between <em>con</em>tinuous and <em>dis</em>crete, in the case you haven’t got the pun already), denoted <span class="math inline">\(x^\tau \sim \text{Concrete}(\alpha,\tau)\)</span>. Its density (follows simply from the change of variable theorem and some integration) is given by:</p>
<p><span class="math display">\[p_{\alpha,\tau}(x)= (n-1)! \tau^{n-1}\prod_{k=1}^K \left(\frac{\alpha_k x_k^{-\tau-1}}{\sum_{i=1}^K\alpha_i x_i^{-\tau}} \right), x \in \Delta^{K-1}\]</span></p>
<p>In practice, what really matters is not the specific expression for the density, but the fact that this expression is in <font style="color:red">closed-form and can be evaluated exactly for different values of <span class="math inline">\(x,\alpha\)</span> and <span class="math inline">\(\tau\)</span></font>. Indeed, in the standard methodology in variational inference, by looking at the definition of the ELBO we see we need to evaluate the entropy term <span class="math inline">\(\mathbb{E}_{z\sim q_\theta(z \mid x)}\left(- \log(q_\theta(z \mid x))\right)\)</span> which explicitly depends on the density.</p>
<blockquote>
<p>Properties</p>
</blockquote>
<p>Now we will briefly comment what the above definitions entail, to better understand the nature of this relaxation. <font style="color:red">The following four properties (first three from [1], fourth from [2]) are specially informative</font></p>
<ol style="list-style-type: decimal">
<li>Rounding: <span class="math inline">\(P(X^\tau_k&gt;X^\tau_i,\forall i\neq k) =\frac{\alpha_k}{\sum_{i=1}^K \alpha_i}\)</span>.</li>
<li>Zero temperature: <span class="math inline">\(P(\lim_{\tau\rightarrow 0} X^\tau_k =1) = \frac{\alpha_k}{\sum_{i=1}^K \alpha_i}\)</span>.</li>
<li>Convex eventually: if <span class="math inline">\(\tau\leq (n-1)^{-1}\)</span> then <span class="math inline">\(p_{\alpha,\tau}(x)\)</span> is a log-convex function of x.</li>
<li>For learning, there is a tradeoff between small and large temperatures.</li>
</ol>
<p>The <font style="color:red">rounding property</font> is important to conceive actual discrete samples in this relaxed framework. <font style="color:red">The rounding property is a simple consequence of the fact that <span class="math inline">\(\exp(\cdot)\)</span> is an increasing function, and implies that even in this relaxed regime with non-zero temperature, we can still easily sample from our original discrete distribution by mapping points <span class="math inline">\(x\)</span> in the simplex to the one-hot vector (an extreme-point of the simplex) with the non-zero coordinate <span class="math inline">\(k\)</span> so that <span class="math inline">\(x_k\)</span> is the closest to one</font>. This property is exploited in [2] to construct the ‘Straight-Through’ Gumbel Estimator, needed in cases where one does not want to destroy the discrete structure in the hidden model.</p>
<p>The zero temperature property says things are well behaved in that in the zero-temperature limit we recover the original discrete distribution. One way to see this is to think in the logistic function modulated by a slope parameter. For high slope (low temperature) the logistic function will become the Heaviside function, taking only two values.</p>
<p>The log-convex function gives us a good guarantee for optimization: recall we will usually faced with optimization that involves the log density. The convexity property tell us we will be better off as long as the temperature we choose is low enough.</p>
<p>Finally, we (almost literally) cite the tradeoff commented in [2], which is stated as an empirical fact but a proof was unavailable:</p>
<blockquote>
<p>For small temperatures the samples are close to one-hot but the variance of the gradients is large. For large temperatures, samples are smooth but the variance of the gradients is small.</p>
</blockquote>
<p>As stated at the beginning, these result have provided a good solution to a common hurdle in machine learning, opening new directions for future research: first (and more immediately), related to the application of this method for the inference/generation of discrete objects. Indeed, [3] is already an example of a direct application of this techniques for the generation of text using GANS. A second sensible direction of research relates to the creation of new discrete reparameterizations based on relaxations, beyond the Gumbel distribution.</p>
<h2 id="references">References</h2>
<ol style="list-style-type: decimal">
<li>Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. “The Concrete Distribution: a Continuous Relaxation of Discrete Random Variables.” ICLR Submission, 2017.</li>
<li>Eric Jang, Shixiang Gu and Ben Poole. “Categorical Reparameterization by Gumbel-Softmax.” ICLR Submission, 2017.</li>
<li>Matt Kusner and José Miguel Hernández-Lobato. “GANS for Sequences of Discrete Elements with the Gumbel-Softmax Distribution.” NIPS workshop on adversarial training, 2016.</li>
<li>Diederik P Kingma, Max Welling. “Auto-Encoding Variational Bayes.” ICLR, 2014.</li>
<li>Ian Goodfellow. “Generative Adversarial Networks.”, NIPS Tutorial, 2016.</li>
<li>Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville and Yoshua Bengio. “Generative Adversarial Nets.” NIPS, 2014</li>
</ol>
</body>
</html>
