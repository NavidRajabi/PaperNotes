<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<h2 id="spatial-transformer-networks"><a href="http://arxiv.org/abs/1506.02025">Spatial Transformer Networks</a></h2>
<p>TLDR; The authors introduce a new spatial transformation module that can be inserted into any Neural Network. The module consists of a spatial transformation network that predicts transformation parameters, a grid generator that chooses a sampling grid from the input, and a sampler that produces the output. Possible learned transformations include things cropping, translation, rotation, scaling or attention. The module can be trained end-to-end using backpropagation. The authors evaluate evaluate the module on both CNNs and MLPs, achieving state on distorted MNIST data, street view numbers, and fine-grained bird classification.</p>
<h4 id="key-points">Key Points:</h4>
<ul>
<li>STMs can be inserted between any layers, typically after the input or extracted features. The transform is dynamic and happens based on the input data.</li>
<li>The module is fast and doesn't adversely impact training speed.</li>
<li>The actual transformation parameters (output of localization network) can be fed into higher layers.</li>
<li>Attention can be seen as a special transformation that increases computational efficiency.</li>
<li>Can also be applied to RNNs, but more investigation is needed.</li>
</ul>
</body>
</html>
