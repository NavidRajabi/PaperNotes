<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<h1 id="spatial-transformer-networks"><a href="https://arxiv.org/pdf/1506.02025.pdf">Spatial Transformer Networks</a></h1>
<p>This paper is released by DeepMind, it aims at boosting the geometric invariance of CNNs in a very elegant way.</p>
<h2 id="spatial-transformer-networks-1">Spatial Transformer networks</h2>
<p>The goal of <a href="http://arxiv.org/abs/1506.02025">spatial transformers</a> is to add to your base network a layer able to perform an <b>explicit geometric transformation</b> on an input. The parameters of the transformation are learnt thanks to the standard backpropagation algorithm, meaning there is no need for extra data or supervision.</p>
<center>
<img src="https://raw.githubusercontent.com/moodstocks/gtsrb.torch/master/resources/spatial-transformer-structure.png" width=500> </image>
</center>
<p>The layer is composed of 3 elements: 1. The <font style="color:red">localization network</font> takes the original image as an input and outputs the parameters of the transformation we want to apply. 2. The <font style="color:red">grid generator</font> generates a grid of coordinates in the input image corresponding to each pixel from the output image. 3. <font style="color:red">The sampler</font> generates the output image using the grid given by the grid generator.</p>
As an example, here is what you get after training a network whose first layer is a ST:
<center>
<img src="https://raw.githubusercontent.com/moodstocks/gtsrb.torch/master/resources/st-mnist.png"></img>
<center>
<p>On the left you see the input image. In the middle you see which part of the input image is sampled. On the right you see the Spatial Transformer output image.</p>
<h2 id="interpretation">Interpretation</h2>
<h3 id="at-training-time">At training time</h3>
<p>Here the goal is to visualize how the Spatial Transformer behaves during training. In the animation below, you can see: - on the left the original image used as input, - on the right the transformed image produced by the Spatial Transformer, - on the bottom a counter that represents training steps (0 = before training, 10/10 = end of epoch 1).</p>
<center>
<img src="https://raw.githubusercontent.com/moodstocks/gtsrb.torch/master/resources/epoch_evolution.gif"></img>
</center>
<p><font size=2>Note: the white dots on the input image show the corners of the part of the image that is sampled. Same applies below.</font></p>
<p>As expected, we see that during the training, the Spatial Transformer learns to focus on the traffic sign, learning gradually to remove background.</p>
<h3 id="post-training">Post-training</h3>
<p>Here the goal is to visualize the ability of the Spatial Transformer (once trained) to produce a stable output even though the input contains geometric noise.</p>
<p>For the record the GTSRB dataset has been initially generated by extracting images from video sequences took while approaching a traffic sign.</p>
<p>The animation below shows for each image of such a sequence (on the left) the corresponding output of the Spatial Transformer (on the right).</p>
<center>
<p><img src="https://raw.githubusercontent.com/moodstocks/gtsrb.torch/master/resources/moving_evolution.gif"></img></cneter></p>
<p>We can see that even though there is an important variability in the input images (scale and position in the image), the output of the Spatial Transformer remains almost static.</p>
<p>This confirms the intuition we had on how the Spatial Transformer simplifies the task for the rest of the network: learning to only forward the interesting part of the input and removing geometric noise.</p>
<p>The Spatial Transformer learned these transformations in an end-to-end fashion, without any modification to the backpropagation algorithm and without any extra annotations.</p>
</body>
</html>
