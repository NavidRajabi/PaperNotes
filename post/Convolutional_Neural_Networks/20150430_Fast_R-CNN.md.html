<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h2 id="fast-r-cnn"><a href="http://arxiv.org/abs/1504.08083">Fast R-CNN</a></h2>
<p>论文出处见：http://arxiv.org/abs/1504.08083 项目见：https://github.com/rbgirshick/fast-rcnn</p>
<p>R-CNN的进化版，0.3s一张图片，VOC07有70的mAP，可谓又快又强。 而且rbg的代码一般写得很好看，应该会是个很值得学习的项目。</p>
<h2 id="动机"><strong><em>动机</em></strong></h2>
<p>为何有了R-CNN和SPP-Net之后还要提出Fast RCNN（简称FRCN）？因为<strong><em>前者有三个缺点</em></strong></p>
<ul>
<li>训练的时候，<strong><em>pipeline是隔离的</em></strong>，先提proposal，然后CNN提取特征，之后用SVM分类器，最后再做bbox regression。FRCN实现了end-to-end的joint training(提proposal阶段除外)。</li>
<li><strong><em>训练时间和空间开销大</em></strong>。RCNN中ROI-centric的运算开销大，所以FRCN用了image-centric的训练方式来通过卷积的share特性来降低运算开销；RCNN提取特征给SVM训练时候需要中间要大量的磁盘空间存放特征，FRCN去掉了SVM这一步，所有的特征都暂存在显存中，就不需要额外的磁盘空间了。</li>
<li><strong><em>测试时间开销大</em></strong>。依然是因为ROI-centric的原因，这点SPP-Net已经改进，然后FRCN进一步通过single scale testing和SVD分解全连接来提速。</li>
</ul>
<h2 id="整体框架"><strong><em>整体框架</em></strong></h2>
<p>整体框架如Figure 1，如果以AlexNet（5个卷积和3个全连接）为例，大致的训练过程可以理解为：</p>
<ul>
<li>selective search在一张图片中得到约2k个object proposal(这里称为RoI)</li>
<li>缩放图片的scale得到图片金字塔，FP得到conv5的特征金字塔。</li>
<li>对于每个scale的每个ROI，求取映射关系，在conv5中crop出对应的patch。并用一个单层的SPP layer（这里称为Rol pooling layer）来统一到一样的尺度（对于AlexNet是6x6）。</li>
<li>继续经过两个全连接得到特征，这特征有分别share到两个新的全连接，连接上两个优化目标。第一个优化目标是分类，使用softmax，第二个优化目标是bbox regression，使用了一个smooth的L1-loss.</li>
</ul>
<p>除了1，上面的2-4是joint training的。 测试时候，在4之后做一个NMS即可。</p>
<p align="center">
<img src="http://hexo-pic-zhangliliang.qiniudn.com/小Q截图-20150517103126.png" width="800" >
</p>
<p>整体框架大致如上述所示了，对比回来SPP-Net，<strong><em>可以看出FRCN大致就是一个joint training版本的SPP-Net</em></strong>，改进如下：</p>
<ul>
<li>SPP-Net在实现上无法同时tuning在SPP layer两边的卷积层和全连接层。</li>
<li>SPP-Net后面的需要将第二层FC的特征放到硬盘上训练SVM，之后再额外训练bbox regressor。</li>
</ul>
<p>接下来会<strong><em>介绍FRCN里面的一些细节的motivation和效果</em></strong>。</p>
<h2 id="rol-pooling-layer"><strong><em>Rol pooling layer</em></strong></h2>
<p>Rol pooling layer的作用<strong><em>主要有两个</em></strong>，一个是<strong><em>将image中的rol定位到feature map中对应patch</em></strong>，另一个是<strong><em>用一个单层的SPP layer将这个feature map patch下采样为大小固定的feature再传入全连接层</em></strong>。</p>
<p>这里有几个细节。</p>
<ol style="list-style-type: decimal">
<li>对于<strong><em>某个rol，怎么求取对应的feature map patch？</em></strong>这个论文没有提及，笔者也还没有仔细去抠，觉得这个问题可以到代码中寻找。:)</li>
<li><strong><em>为何只是一层的SPP layer？多层的SPP layer不会更好吗？</em></strong>对于这个问题，笔者认为是因为需要读取pretrain model来finetuning的原因，比如VGG就release了一个19层的model，如果是使用多层的SPP layer就不能够直接使用这个model的parameters，而需要重新训练了。</li>
</ol>
<h2 id="multi-task-loss"><strong><em>Multi-task loss</em></strong></h2>
<p>FRCN有两个loss，以下分别介绍。</p>
<p>对于<strong><em>分类loss</em></strong>，是一个N+1路的softmax输出，其中的N是类别个数，1是背景。为何不用SVM做分类器了？在5.4作者讨论了softmax效果比SVM好，因为它引入了类间竞争。（笔者觉得这个理由略牵强，估计还是实验效果验证了softmax的performance好吧 ^_^）</p>
<p>对于<strong><em>回归loss</em></strong>，是一个4xN路输出的regressor，也就是说对于每个类别都会训练一个单独的regressor的意思，比较有意思的是，这里regressor的loss不是L2的，而是一个平滑的L1，形式如下：</p>
<p><span class="math display">\[
L_{loc}(t,t^*)=\sum_{i\in \{x,y,w,h\}}\text{smooth}_{L_1}(t_i,t_i^*) \qquad(2)
\]</span> <span class="math display">\[
\text{smooth}_{L_1}(x)=\begin{cases}0.5x^2, \text{ if } |x|&lt;1\\|x|-0.5, \text{\ otherwise }\end{cases} \qquad(1)
\]</span></p>
<p>作者这样设置的目的是<strong><em>想让loss对于离群点更加鲁棒，控制梯度的量级使得训练时不容易跑飞</em></strong>。 最后在5.1的讨论中，作者说明了Multitask loss是有助于网络的performance的。</p>
<h2 id="scale-invariance"><strong><em>Scale invariance</em></strong></h2>
<p>这里讨论object的scale问题，就是网络对于object的scale应该是要不敏感的。这里还是引用了SPP的方法，有两种。</p>
<p><strong><em>brute force （single scale）</em></strong>，也就是简单认为object不需要预先resize到类似的scale再传入网络，直接将image定死为某种scale，直接输入网络来训练就好了，然后期望网络自己能够学习到scale-invariance的表达。</p>
<p><strong><em>image pyramids （multi scale）</em></strong>，也就是要生成一个金字塔，然后对于object，在金字塔上找到一个大小比较接近227x227的投影版本，然后用这个版本去训练网络。 可以看出，2应该比1更加好，作者也在5.2讨论了，2的表现确实比1好，但是好的不算太多，大概是1个mAP左右，但是时间要慢不少，所以作者实际采用的是第一个策略，也就是single scale。</p>
<p>这里，FRCN测试之所以比SPP快，很大原因是因为这里，因为SPP用了2，而FRCN用了1。</p>
<h2 id="svd-on-fc-layers"><strong><em>SVD on fc layers</em></strong></h2>
<p>对应文中3.1，这段笔者没细看。大致意思是说全连接层耗时很多，如果能够简化全连接层的计算，那么能够提升速度。</p>
<p>具体来说，作者对全连接层的矩阵做了一个SVD分解，mAP几乎不怎么降（0.3%），但速度提速30%</p>
<h2 id="which-layers-to-finetune"><strong><em>Which layers to finetune?</em></strong></h2>
<p>对应文中4.5，作者的观察有2点</p>
<p>对于较深的网络，比如VGG，卷积层和全连接层是否一起tuning有很大的差别（66.9 vs 61.4） 有没有必要tuning所有的卷积层？答案是没有。如果留着浅层的卷积层不tuning，可以减少训练时间，而且mAP基本没有差别。</p>
<h2 id="data-augment"><strong><em>Data augment</em></strong></h2>
<p>在训练期间，作者做过的唯一一个数据增量的方式是水平翻转。 作者也试过将VOC12的数据也作为拓展数据加入到finetune的数据中，结果VOC07的mAP从66.9到了70.0，说明对于网络来说，数据越多就是越好的。</p>
<h2 id="are-more-proposals-always-better"><strong><em>Are more proposals always better？</em></strong></h2>
<p>对应文章的5.5，答案是NO。 作者将proposal的方法粗略地分成了sparse（比如selective search）和dense（sliding windows）。 如Figure 3所示，不管是哪种方法，盲目增加proposal个数反而会损害到mAP的。</p>
<p align="center">
<img src="http://hexo-pic-zhangliliang.qiniudn.com/小Q截图-20150517125419.png" width="800" >
</p>
<p>作者引用了文献11的一句话来说明：<strong><em>““[sparse proposals] may improve detection quality by reducing spurious false positives.”</em></strong></p>
</body>
</html>
