<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h2 id="identity-mappings-in-deep-residual-networks">Identity Mappings in Deep Residual Networks</h2>
<p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, ArXiv, 2016</p>
<h3 id="summary">Summary</h3>
<p>This is follow-up work to the ResNets paper. It studies the propagation formulations behind the connections of deep residual networks and performs ablation experiments. A residual block can be represented with the equations <span class="math inline">\(y_l = h(x_l) + F(x_l, W_l); x_{l+1} = f(y_l)\)</span>. <span class="math inline">\(x_l\)</span> is the input to the <span class="math inline">\(l\)</span>-th unit and <span class="math inline">\(x_{l+1}\)</span> is the output of the <span class="math inline">\(l\)</span>-th unit. In the original ResNets paper, <span class="math inline">\(h(x_l) = x_l\)</span>, <span class="math inline">\(f\)</span> is ReLu, and <span class="math inline">\(F\)</span> consists of 2-3 convolutional layers (bottleneck architecture) with BN and ReLU in between. In this paper, they propose a residual block with both <span class="math inline">\(h(x)\)</span> and <span class="math inline">\(f(x)\)</span> as identity mappings, which trains faster and performs better than their earlier baseline. Main contributions:</p>
<ul>
<li>Identity skip connections work much better than other multiplicative interactions that they experiment with:
<ul>
<li>Scaling (<span class="math inline">\(h(x) = \lambda x\)</span>): Gradients can explode or vanish depending on whether modulating scalar <span class="math inline">\(\lambda &gt; 1\)</span> or <span class="math inline">\(&lt; 1\)</span>.</li>
<li>Gating (<span class="math inline">\(1-g(x)\)</span> for skip connection and <span class="math inline">\(g(x)\)</span> for function <span class="math inline">\(F\)</span>): For gradients to propagate freely, <span class="math inline">\(g(x)\)</span> should approach 1, but F gets suppressed, hence suboptimal. This is similar to highway networks. g(x) is a 1x1 convolutional layer.</li>
<li>Gating (shortcut-only): Setting high biases pushes initial g(x) towards identity mapping, and test error is much closer to baseline.</li>
<li>1x1 convolutional shortcut: These work well for shallower networks (~34 layers), but training error becomes high for deeper networks, probably because they impede gradient propagation.</li>
</ul></li>
<li>Experiments on activations.
<ul>
<li>BN after addition messes up information flow, and performs considerably worse.</li>
<li>ReLU before addition forces the signal to be non-negative, so the signal is monotonically increasing, while ideally a residual function should be free to take values in (-inf, inf).</li>
<li>BN + ReLU pre-activation works best. This also prevents overfitting, due to BN's regularizing effect. Input signals to all weight layers are normalized.</li>
</ul></li>
</ul>
<h3 id="strengths">Strengths</h3>
<ul>
<li><p>Thorough set of experiments to show that identity shortcut connections are easiest for the network to learn. Activation of any deeper unit can be written as the sum of the activation of a shallower unit and a residual function. This also implies that gradients can be directly propagated to shallower units. This is in contrast to usual feedforward networks, where gradients are essentially a series of matrix-vector products, that may vanish, as networks grow deeper.</p></li>
<li><p>Improved accuracies than their previous ResNets paper.</p></li>
</ul>
<h3 id="weaknesses-notes">Weaknesses / Notes</h3>
<ul>
<li><p>Residual units are useful and share the same core idea that worked in LSTM units. Even though stacked non-linear layers are capable of asymptotically approximating any arbitrary function, it is clear from recent work that residual functions are much easier to approximate than the complete function. The <a href="http://arxiv.org/abs/1602.07261">latest Inception paper</a> also reports that training is accelerated and performance is improved by using identity skip connections across Inception modules.</p></li>
<li><p>It seems like the degradation problem, which serves as motivation for residual units, exists in the first place for non-idempotent activation functions such as sigmoid, hyperbolic tan. This merits further investigation, especially with recent work on function-preserving transformations such as <a href="http://arxiv.org/abs/1603.01670">Network Morphism</a>, which expands the Net2Net idea to sigmoid, tanh, by using parameterized activations, initialized to identity mappings.</p></li>
</ul>
<h2 id="identity-mappings-in-deep-residual-networks-1">Identity Mappings in Deep Residual Networks</h2>
<p><a href="https://github.com/KaimingHe/resnet-1k-layers">Code is avaiable at here</a></p>
<p>They analyze the <span style="color:red"><strong><em>propagation formulations</em></strong></span> behind the residual building blocks, which suggest that the <span style="color:red"><strong><em>forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation</em></strong></span>.</p>
<h3 id="resnets">ResNets</h3>
<p align="center">
<img src="http://img.blog.csdn.net/20151216160852064" width="300" >
</p>
<p>ResNets consist of many stacked “Residual Units”. Each unit can be expressed in a general form: <span class="math display">\[\begin{cases}
\mathbf{y}_l=h(\mathbf{x}_l)+F(\mathbf{x}_l,\mathbf{W}_l)\\
\mathbf{x}_{l+1}=f(\mathbf{y}_l)\end{cases}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{x}_l\)</span> and <span class="math inline">\(\mathbf{x}_{l+1}\)</span> are input and output of the <span class="math inline">\(l\)</span>-th unit, and <span class="math inline">\(F\)</span> is a <span style="color:red"><strong><em>residual function</em></strong></span>.</p>
<p>In &quot;<span style="color:red"><strong><em>Deep Residual Learning for Image Recognition</em></strong></span>&quot;, <span class="math inline">\(h(\mathbf{x}_l)=\mathbf{x}_l\)</span> is an identity mapping and <span class="math inline">\(f\)</span> is a ReLU function.</p>
<p align="center">
<img src="https://dl.dropboxusercontent.com/s/7z4pcawz8lpxkjl/Screenshot_2016-05-09_20-50-43.png" width="400" >
</p>
<h3 id="analysis-of-deep-residual-networks">Analysis of Deep Residual Networks</h3>
<p><span class="math display">\[\mathbf{x}_{l+1}=\mathbf{x}_l+F(\mathbf{x}_l,W_l)\Rightarrow \mathbf{x}_{l+2}=\mathbf{x}_{l+1}+F(\mathbf{x}_{l+1},W_{l+1})
= \mathbf{x}_l+F(\mathbf{x}_l+W_l)+F(\mathbf{x}_{l+1},W_{l+1})\]</span> <span class="math display">\[\mathbf{x}_L=\mathbf{x}_l+\sum_{i=l}^{L-1}F(\mathbf{x}_{i},W_{i})\]</span> for any deeper unit <span class="math inline">\(L\)</span> and any shallower unit <span class="math inline">\(l\)</span>.</p>
<p>The equation above also leads to nice backward propagation properties. Denoting the loss function as <span class="math inline">\(\varepsilon\)</span>, from the chain rule of backpropagation we have: <span class="math display">\[
\frac{\partial \varepsilon}{\partial x_l}=\frac{\partial \varepsilon}{\partial x_L}\frac{\partial x_L}{\partial x_l}=\frac{\partial \varepsilon}{\partial x_l}(1+\frac{\partial}{\partial x_l}\sum_{i=1}^{L-1}F(x_i,W_i))
\]</span></p>
<h3 id="on-the-importance-of-identity-skip-connections">On the Importance of Identity Skip Connections</h3>
<p><span class="math display">\[\mathbf{x}_{l+1}=\lambda_l \mathbf{x}_l+F(\mathbf{x}_l,W_l)\]</span> <span class="math display">\[\mathbf{x}_L=\prod_{i=1}^{L-1}\lambda_i \mathbf{x}_l+\sum_{i=l}^{L-1}\hat{F}(\mathbf{x}_{i},W_{i})\]</span> <span class="math display">\[
\frac{\partial \varepsilon}{\partial x_l}=\frac{\partial \varepsilon}{\partial x_L}\frac{\partial x_L}{\partial x_l}=\frac{\partial \varepsilon}{\partial x_l}(\prod_{i=1}^{L-1}\lambda_i+\frac{\partial}{\partial x_l}\sum_{i=1}^{L-1}\hat{F}(x_i,W_i))
\]</span></p>
|Various types of shortcut connections|case|on shortcut|on <span class="math inline">\(F\)</span>|error %|remark| |---|---|---|---|---|---| |
<p align="center">
<img src="https://dl.dropboxusercontent.com/s/oxy3p3u09zppeba/Screenshot_2016-05-09_21-07-47.png" width="200" >
</p>
|original |1|1|6.61|| |
<p align="center">
<img src="https://dl.dropboxusercontent.com/s/q9qev76sijorsvz/Screenshot_2016-05-09_21-08-41.png" width="200" >
</p>
|constant scaling|<span class="math inline">\(\begin{cases}0\\0.5\\0.5\end{cases}\)</span>|<span class="math inline">\(\begin{cases}1\\1\\0.5\end{cases}\)</span>|<span class="math inline">\(\begin{cases}fail\\fail\\12.35\end{cases}\)</span>|This is a plain net| |
<p align="center">
<img src="https://dl.dropboxusercontent.com/s/ddx5plurxcjvibq/Screenshot_2016-05-09_21-12-23.png" width="200" >
</p>
|exclusive gating|<span class="math inline">\(\begin{cases}1-g(x)\\1-g(x)\\1-g(x)\end{cases}\)</span>|<span class="math inline">\(\begin{cases}g(x)\\g(x)\\g(x)\end{cases}\)</span>|<span class="math inline">\(\begin{cases}fail\\8.70\\9.81\end{cases}\)</span>|<span class="math inline">\(\begin{cases}init \ b_g \ to \ -5\\init \ b_g =-6\\init \ b_g =-7\end{cases}\)</span>| |
<p align="center">
<img src="https://dl.dropboxusercontent.com/s/ql347063jkcww47/Screenshot_2016-05-09_21-12-34.png" width="200" >
</p>
|shortcut only gating|<span class="math inline">\(\begin{cases}1-g(x)\\1-g(x)\end{cases}\)</span>|<span class="math inline">\(\begin{cases}1\\1\end{cases}\)</span>|<span class="math inline">\(\begin{cases}12.86\\6.91\end{cases}\)</span>|<span class="math inline">\(\begin{cases}init \ b_g = 0\\init \ b_g =-6\end{cases}\)</span>| |
<p align="center">
<img src="https://dl.dropboxusercontent.com/s/g3bjkt28l7lo9vr/Screenshot_2016-05-09_21-12-44.png" width="200" >
</p>
|<span class="math inline">\(1\times 1\)</span> conv shortcut|1x1 conv|1|12.22|| |
<p align="center">
<img src="https://dl.dropboxusercontent.com/s/nt6upy5l6qqx32m/Screenshot_2016-05-09_21-12-55.png" width="200" >
</p>
<p>|dropout shortcut|dropout 0.5|1|fail||</p>
<p align="center">
<p><img src="https://dl.dropboxusercontent.com/s/9xh9enr2prgqwtx/Screenshot_2016-05-09_21-29-11.png" width="800" ></p>
<h3 id="experiments-on-activation">Experiments on Activation</h3>
<p align="center">
<p><img src="https://dl.dropboxusercontent.com/s/jrp8cvxm0dm70cd/Screenshot_2016-05-09_21-31-11.png" width="800" ></p>
<h3 id="analysis">Analysis</h3>
<p>We find that the impact of pre-activation is twofold. 1. <span style="color:red"><strong><em>the optimization is further eased</em></strong></span> (comparing with the baseline ResNet) because <span class="math inline">\(f\)</span> is an identity mapping. 2. using BN as <span style="color:red"><strong><em>pre-activation improves regularization of the models.</em></strong></span></p>
</body>
</html>
