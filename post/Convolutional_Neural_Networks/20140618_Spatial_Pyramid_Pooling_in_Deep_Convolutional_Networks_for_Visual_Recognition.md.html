<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h2 id="spatial-pyramid-pooling-in-deep-convolutional-networks-for-visual-recognition"><a href="https://arxiv.org/abs/1406.4729">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</a></h2>
<p>对应的论文是：http://arxiv.org/pdf/1406.4729v2.pdf 对应的slide：http://research.microsoft.com/en-us/um/people/kahe/eccv14sppnet/sppnet_ilsvrc2014.pdf</p>
<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/CX8CCHKlfOE" frameborder="0" allowfullscreen>
</iframe>
</center>
<h2 id="思路概述"><strong><em>思路概述</em></strong></h2>
<p>如下图所示，由于传统的CNN限制了<strong><em>输入必须固定大小（比如AlexNet是224x224）</em></strong>，所以在实际使用中往往需要对原图片进行<strong><em>crop或者warp的操作</em></strong></p>
<ul>
<li>crop：截取原图片的一个固定大小的patch</li>
<li>warp：将原图片的ROI缩放到一个固定大小的patch</li>
</ul>
<p>无论是crop还是warp，都无法保证在不失真的情况下将图片传入到CNN当中。</p>
<ul>
<li>crop：物体可能会产生<strong><em>截断</em></strong>，尤其是长宽比大的图片。</li>
<li>warp：物体被拉伸，失去“<strong><em>原形</em></strong>”，尤其是长宽比大的图片</li>
</ul>
<p align="center">
<img src="http://hexo-pic-zhangliliang.qiniudn.com/小Q截图-20140913171023.png" width="600" >
</p>
<p><strong><em>Sptial Pyramid Pooling，以下简称SPP</em></strong>，为的就是解决上述的问题，做到的效果为：<strong><em>不管输入的图片是什么尺度，都能够正确的传入网络</em></strong>。</p>
<p>思路很直观，首先发现了，<strong><em>CNN的卷积层是可以处理任意尺度的输入的</em></strong>，只是在全连接层处<strong><em>有限制尺度</em></strong>——换句话说，如果找到一个方法，在<strong><em>全连接层之前将其输入限制到等长，那么就解决了这个问题</em></strong>。</p>
<p>然后解决问题的方法就是SPP了。</p>
<h2 id="从bow到spm"><strong><em>从BoW到SPM</em></strong></h2>
<p>SPP的思想来源于SPM，然后SPM的思想来源自BoW。关于BoW和SPM，找到了两篇相关的博文，就不在这里展开了。 - <a href="http://blog.csdn.net/v_JULY_v/article/details/6555899">第九章三续：SIFT算法的应用—目标识别之Bag-of-words模型</a> - <a href="http://blog.csdn.net/jwh_bupt/article/details/9625469">Spatial Pyramid 小结</a></p>
<p>最后<strong><em>做到的效果如下图</em></strong>：</p>
<p align="center">
<img src="http://hexo-pic-zhangliliang.qiniudn.com/小Q截图-20140913192418.png" width="600" >
</p>
<p>如果原图<strong><em>输入是224x224</em></strong>，对于conv5出来后的输出，是<strong><em>13x13x256</em></strong>的，可以理解成有<strong><em>256个这样的filter</em></strong>，每个filter对应一张<strong><em>13x13的reponse map</em></strong>。</p>
<p>如果像上图那样将reponse map分成<strong><em>4x4 2x2 1x1三张子图</em></strong>，做max pooling后，出来的特征就是固定长度的<strong><em>(16+4+1)x256那么多的维度</em></strong>了。</p>
<p>如果原图的输入不是224x224，出来的特征依然是(16+4+1)x256</p>
<p><a href="">直觉地说，可以理解成将原来固定大小为(3x3)窗口的pool5改成了自适应窗口大小，窗口的大小和reponse map成比例，保证了经过pooling后出来的feature的长度是一致的</a></p>
<h2 id="如何训练网络"><strong><em>如何训练网络</em></strong></h2>
<p>理论上说，SPP-net支持直接<strong><em>以多尺度的原始图片</em></strong>作为输入后直接BP即可。实际上，caffe等实现中，为了计算的方便，输入是固定了尺度了的。</p>
<p>所以<strong><em>为了使得在固定输出尺度的情况下也能够做到SPP-net的效果</em></strong>，就需要定义一个新的SSP-layer</p>
<p>作者以输入224x224举例，这时候conv5出来的reponse map为13x13，计算出来的步长如下图所示。</p>
<p align="center">
<img src="http://hexo-pic-zhangliliang.qiniudn.com/小Q截图-20140913200352.png" width="600" >
</p>
<p>具体的计算方法，看一眼2.3的Single-size training部分就明白了。 如果输入改成180x180，这时候conv5出来的reponse map为10x10，类似的方法，能够得到新的pooling参数。</p>
<p>两种尺度下，在SSP后，输出的特征维度都是(9+4+1)x256，之后接全连接层即可。 训练的时候，224x224的图片通过随机crop得到，180x180的图片通过缩放224x224的图片得到。之后，迭代训练，即用224的图片训练一个epoch，之后180的图片训练一个epoth，交替地进行。</p>
<h2 id="如何测试网络"><strong><em>如何测试网络</em></strong></h2>
<p>作者说了一句话：Note that the above single/multi-size solutions are for training only. At the testing stage, it is straightforward to apply SPP-net on images of any sizes. 笔者觉得没有那么简单吧，毕竟caffe对于test网络也是有固定尺度的要求的。</p>
<h2 id="实验"><strong><em>实验</em></strong></h2>
<p>之后是大量的实验。</p>
<h2 id="分类实验"><strong><em>分类实验</em></strong></h2>
<p>如下图，一句话概括就是，都有提高。</p>
<p align="center">
<img src="http://hexo-pic-zhangliliang.qiniudn.com/小Q截图-20140913201353.png" width="600" >
</p>
<p>一些细节：</p>
<ol style="list-style-type: decimal">
<li>为了保证公平，test时候的做法是将图片缩放到短边为256，然后取10crop。这里的金字塔为{6x6 3x3 2x2 1x1}（笔者注意到，这里算是增加了特征，因为常规pool5后来说，只有6x6；这里另外多了9+4+1个特征）</li>
<li>作者将金字塔减少为{4x4 3x3 2x2 1x1}，这样子，每个filter的feature从原来的36减少为30，但依然有提高。（笔者认为这个还是保留意见比较好）</li>
<li>其实这部分的实验比较多，详见论文，不在这里写了。</li>
<li>在ILSVRC14上的cls track，作者是第三名</li>
</ol>
<h2 id="定位实验"><strong><em>定位实验</em></strong></h2>
<p>这里详细说说笔者较为关心的voc07上面的定位实验。用来对比的对象是RCNN。方法简述：</p>
<ol style="list-style-type: decimal">
<li>提取region proposal部分依然用的是<strong><em>selective search</em></strong></li>
<li>CNN部分，结构用的是<strong><em>ZF-5</em></strong>（单尺度训练），金字塔用了{6x6 3x3 2x2 1x1}，共50个bin</li>
<li>分类器也是用了<strong><em>SVM</em></strong>，后处理也是用了cls-specific regression</li>
</ol>
<p>所以主要差别是在<strong><em>第二步</em></strong>，做出的主要改进在于SPP-net能够一次得到整个feature map，大大减少了计算proposal的特征时候的运算开销。</p>
<p>具体做法，将图片缩放到s∈{480,576,688,864,1200}的大小，于是得到了6个feature map。尽量让region在s集合中对应的尺度接近224x224，然后选择对应的feature map进行提取。（具体如何提取？后面的附录会说）</p>
最后效果如图：
<p align="center">
<img src="http://hexo-pic-zhangliliang.qiniudn.com/小Q截图-20140913210935.png" width="500" >
</p>
<p>准确率<strong><em>从58.5提高到了59.2</em></strong>，而且速度快了<strong><em>24x</em></strong> 如果用两个模型综合，又提高了一点，到60.9</p>
<p align="center">
<img src="http://hexo-pic-zhangliliang.qiniudn.com/小Q截图-20140913211154.png" width="600" >
</p>
<h2 id="附录"><strong><em>附录</em></strong></h2>
<p><a href="">如何将图像的ROI映射到feature map？</a></p>
<p>总体的映射思路为：In our implementation, we project the corner point of a window onto a pixel in the feature maps, such that this corner point (in the image domain) is closest to the center of the receptive field of that pixel.</p>
<p>我的理解是：</p>
<ol style="list-style-type: decimal">
<li>映射的是ROI的两个角点，左上角和右下角，这两个角点就可以唯一确定ROI的位置了。</li>
<li>将feature map的pixel映射回来图片空间</li>
<li>从映射回来的pixel中选择一个距离角点最近的pixel，作为映射。</li>
</ol>
<p>如果以ZF-5为例子，具体的计算公式为：</p>
<p align="center">
<img src="http://hexo-pic-zhangliliang.qiniudn.com/小Q截图-20140914163853.png" width="400" >
</p>
<p>这里有几个变量</p>
<ul>
<li>139代表的是感受野的直径，计算这个也需要一点技巧了：如果一个filter的kernelsize=x,stride=y，而输出的reponse map的长度是n，那么其对应的感受野的长度为：n+(n-1)<em>(stride-1)+2</em>((kernelsize-1)/2)</li>
</ul>
<p><span class="math inline">\(\begin{cases} conv5(3,1) \ 1+0\times 0+2\times 1=1+2=3\\ conv4(3,1) \ 3+2\times 0+2\times 1=3+2=5\\ conv3(3,1) \ 5+4\times 0+2\times 1=5+2=7\\ pool2(3,2) \ 7+6\times 1+2\times 1=7+6+2=15\\ conv2(5,2) \ 15+14\times 1+2\times 2=15+14+4=33\\ pool1(3,2) \ 33+32\times 1+2\times 1=33+32+2=67\\ conv1(7,2) \ 67+66\times 1+2\times 3=67+66+6=139 \end{cases}\)</span></p>
<ul>
<li>16是effective stride，这里笔者理解为，将conv5的pixel映射到图片空间后，两个pixel之间的stride。（计算方法，所有stride连乘，对于ZF-5为2x2x2x2=16）</li>
<li>63和75怎么计算，还没有推出来</li>
</ul>
</body>
</html>
