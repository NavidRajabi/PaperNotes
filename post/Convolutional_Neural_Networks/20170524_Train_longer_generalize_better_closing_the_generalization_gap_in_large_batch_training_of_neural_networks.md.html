<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<h1 id="train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks"><a href="https://arxiv.org/abs/1705.08741">Train longer, generalize better: closing the generalization gap in large batch training of neural networks</a></h1>
<h2 id="pytorch-code">Pytorch <a href="https://github.com/eladhoffer/bigBatch.git">CODE</a></h2>
<p>TODO: they found that the generalization gap induced by mini-batch SGD can be completely fixed just by using more updates. This along with the &quot;<a href="https://www.reddit.com/r/MachineLearning/comments/6d0p7h/r_the_marginal_value_of_adaptive_gradient_methods/">Marginal Value of Adaptive Gradient Descent</a>&quot; paper both seem to point to the loss curves having less direct value than we might expect...perhaps there's some value in attempting to somehow keep track of local shape during training and use it to guide us out of sharp minima. (Momentum is already sort of doing this...but maybe there's value in intentionally dropping minibatch size and/or increasing learning rate after it looks like loss has plateaued...)</p>
</body>
</html>
