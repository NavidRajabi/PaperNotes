<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="pandoc.css" type="text/css" />
</head>
<body>
<h1 id="batch-normalization">Batch Normalization</h1>
<h2 id="tldr">TLDR</h2>
<p>The authors introduce Batch Normalization, a technique to normalize unit activations to zero mean and unit variance within the network. The authors show that in Feedforward and Convolutional Networks, Batch Normalization leads to faster training and better accuracies. BN also acts as a regularizer, reducing the need for Dropout, etc. Using an ensemble of batch normalized networks the authors achieve state of the art on ILSVRC.</p>
<h2 id="key-points">Key Points</h2>
<ul>
<li><p>Reduces internal covariant shift.</p></li>
<li><p>Reduces the dependence of gradients on the scale of the parameters or their initial values.</p></li>
<li><p>Regularizes the model and reduces the need for dropout, photometric distortions, local response normalization and other regularization techniques.</p></li>
<li><p>Allows use of saturating nonlinearities and higher learning rates.</p></li>
<li><p>Batch Normalization was applied to models trained for MNIST and Inception Network for ImageNet. All the above-mentioned advantages were validated in the experiments. Interestingly, Batch Normalization with sigmoid achieved an accuracy of 69.8% (overall best, using any nonlinearity, was 74.8%) while Inception model (sigmoid nonlinearity), without Batch Normalisation, worked only as good as a random guess.</p></li>
<li><p>Network training is complicated because the input distributions to higher level change as the parameter in lower layers are changing: Internal Covariate Shift. Solution: Normalize within the network.</p></li>
<li><p>BN: Normalize input to nonlinearity to have zero mean and unit variance. Then add two additional parameters (scaling and bias) per unit to preserve expressability of the network. Statistics are calculated per minibatch.</p></li>
<li><p>Network parameters increase, but not by much: 2 parameter per unit that has batch normalization applied to it.</p></li>
<li><p>Works well for fully connected and convolutional layers. Authors didn't try RNNs.</p></li>
<li><p>Change to make when adding BN: Increase learning rate, remove/decrease dropout and l2 regularization, accelerate learning rate decay, shuffle training examples more thoroughly.</p></li>
</ul>
<h2 id="future-work">Future Work</h2>
<p>While BN Transform does enhance the overall deep network training task, its precise effect on gradient propagation is still not well understood. A future extension of Batch Normalisation would be in the domain of Recurrent Neural Networks where internal covariate shift and vanishing gradients are more severe. It remains to be explored if it can also help with domain adaption by easily generalizing to new data distributions. ## Bibtex</p>
<pre><code>@inproceedings{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International conference on machine learning},
  pages={448--456},
  year={2015}
}</code></pre>
</body>
</html>
