<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>RL codes</title>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX","output/HTML-CSS"],
    extensions: [],
    TeX: {
      extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
    },
    showMathMenu: false
  });
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js">
</script>
</head>
<body>
<div class="container">
<h1>Reinforcement Learning codes</h1>
<table class="table table-striped">
<thead>
<tr><th>Name</th><th>Descriptions</th><th>Illustration</th></tr>
</thead>
<tbody>

<!------------------------------------------------------->
<tr><td><a href=""></a></td>
<td>
</td>
<td><img src="" alt="" width="200"></td></tr>
<!------------------------------------------------------->
<tr><td><a href="https://github.com/ikostrikov/pytorch-a2c-ppo-acktr">pytorch-a2c-ppo-acktr</a></td>
<td>PyTorch implementation of Advantage Actor Critic (A2C), Proximal Policy Optimization (PPO) and Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation (ACKTR).
</td>
<td><img src="https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/raw/master/imgs/beamrider.png" alt="" width="200"></td></tr>

<tr><td><a href="https://github.com/ruotianluo/irl-imitation">irl-imitation</a></td>
<td>Implementations of model-based Inverse Reinforcement Learning (IRL) algorithms in python/Tensorflow. Deep MaxEnt, MaxEnt, LPIRL
</td>
<td><img src="https://github.com/ruotianluo/irl-imitation/raw/master/imgs/rmap_maxent_10.jpg" alt="" width="200"></td></tr>


<tr><td><a href="https://github.com/Kaixhin/NoisyNet-A3C">NoisyNet-A3C</a></td>
<td>NoisyNet [1] (LSTM) asynchronous advantage actor-critic (A3C) [2] on the CartPole-v1 environment. This repo has a minimalistic design and a classic control environment to enable quick investigation of different hyperparameters.
</td>
<td><img src="https://github.com/Kaixhin/NoisyNet-A3C/raw/master/figures/good-noisynet-a3c.png" alt="" width="200"></td></tr>

<tr><td><a href="https://github.com/transedward/pytorch-dqn">dqn.Pytorch</a></td>
<td>Deep Q-Learning Network in pytorch
</td>
<td><img src="" alt="http://shws.cc.oita-u.ac.jp/shibata/KissSystem.png" width="200"></td></tr>

<tr><td><a href="https://github.com/jingweiz/pytorch-rl">rl.pytorch</a></td>
<td>Deep Reinforcement Learning with pytorch & visdom
</td>
<td><img src="" alt="" width="200"></td></tr>

<tr><td><a href="https://github.com/ikostrikov/pytorch-a3c">a3c.PyTorch</a></td>
<td>PyTorch implementation of Asynchronous Advantage Actor Critic (A3C) from "Asynchronous Methods for Deep Reinforcement Learning".
</td>
<td><img src="https://camo.githubusercontent.com/126a8ecce999b884451c9ca090ba4cfb3783195e/687474703a2f2f672e7265636f726469742e636f2f4265697143396c3730422e676966" alt="" width="200"></td></tr>

<tr><td><a href="https://github.com/JamesChuanggg/pytorch-REINFORCE">REINFORCE.pytorch</a></td>
<td>PyTorch Implementation of REINFORCE for both discrete & continuous control
</td>
<td><img src="" alt="" width="200"></td></tr>

<tr><td><a href="https://github.com/jinfagang/pytorch_chatbot">chatbot.PyTorch</a></td>
<td>A Marvelous ChatBot implement using PyTorch.
</td>
<td><img src="https://camo.githubusercontent.com/b2af91d8860ddef7321dbaebb9a7b2ca6fa2ef33/687474703a2f2f6f66777a63756e7a692e626b742e636c6f7564646e2e636f6d2f454974544977717063724173726578712e706e67" alt="" width="200"></td></tr>


<tr><td><a href="https://github.com/onlytailei/pytorch-rl">rl.Pytorch</a></td>
<td>Deep Reinforcement Learning with pytorch & visdom (the branch for A3C continuous control)
</td>
<td><img src="" alt="" width="200"></td></tr>

<tr><td><a href="https://github.com/XingxingZhang/dress">dress</a></td>
<td>Sentence Simplification with Deep Reinforcement Learning
</td>
<td><img src="" alt="" width="200"></td></tr>

<tr><td><a href="https://github.com/ghliu/pytorch-ddpg">pytorch-ddpg</a></td>
<td>Implementation of the Deep Deterministic Policy Gradient (DDPG) using PyTorch
</td>
<td><img src="https://github.com/ghliu/pytorch-ddpg/raw/master/output/Pendulum-v0-run0/validate_reward.png" alt="" width="200"></td></tr>


<tr><td><a href="https://github.com/jinfagang/rl_atari_pytorch">rl_atari_pytorch</a></td>
<td>ReinforcementLearning Learn Play Atari Using DDPG and LSTM.
</td>
<td><img src="" alt="" width="200"></td></tr>

<tr><td><a href="https://github.com/ghliu/pytorch-ddpg">DDPG.pytorch</a></td>
<td>Deep Deterministic Policy Gradient on PyTorch
</td>
<td><img src="https://github.com/ghliu/pytorch-ddpg/raw/master/output/Pendulum-v0-run0/validate_reward.png" alt="" width="200"></td></tr>



<tr><td><a href="https://github.com/Kaixhin/NoisyNet-A3C">NoisyNet-A3C.PyTorch</a></td>
<td>Noisy Networks for Exploration
</td>
<td><img src="https://github.com/Kaixhin/NoisyNet-A3C/raw/master/figures/good-noisynet-a3c.png" alt="" width="200"></td></tr>


<tr>
<td>
<a href="https://github.com/pemami4911/neural-combinatorial-rl-pytorch">neural-combinatorial-rl.pytorch</a>
</td>
<td>
PyTorch implementation of Neural Combinatorial Optimization with Reinforcement Learning.
</td>
<td>
<img src="https://camo.githubusercontent.com/5593b2c8184c4bc08372f919063e826d9bcc2c67/687474703a2f2f7333332e706f7374696d672e6f72672f79747836336b7763762f7768617469735f6167656e746e65745f706e672e706e67" alt="" width="200">
</td>
</tr>


<tr><td><a href="https://github.com/JannerM/spatial-reasoning">spatial-reasoning.pytorch</a></td>
<td>Code for the paper "Representation Learning for Grounded Spatial Reasoning" https://arxiv.org/abs/1707.03938
</td>
<td><img src="https://github.com/JannerM/spatial-reasoning/raw/master/logs/example/predictions.png" alt="" width="200"></td></tr>



<tr>
<td>
<a href="https://github.com/facebookresearch/clevr-iep">clevr-iep.PyTorch</a>
</td>
<td>
This is the code for the paper: Inferring and Executing Programs for Visual Reasoning. Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Judy Hoffman, Fei-Fei Li, Larry Zitnick, Ross Girshick
arXiv 2017
</td>
<td>
<img src="https://github.com/facebookresearch/clevr-iep/raw/master/img/system.png" alt="" width="200">
</td>
</tr>


<tr><td><a href="https://github.com/atgambardella/pytorch-es">es.PyTorch</a></td>
<td>Evolution Strategies in PyTorch
</td>
<td><img src="https://4.bp.blogspot.com/-tm1px1Jz2G4/WNQ9OlLVvUI/AAAAAAAATKE/EOo699MnBU0ThBbAaReLNMcIXoj3YkJaQCLcB/s1600/es.jpg" alt="" width="200"></td></tr>

</tbody>
</table>
</div>
</body>
</html>
