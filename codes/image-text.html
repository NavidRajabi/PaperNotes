<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Im2text codes</title>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX","output/HTML-CSS"],
    extensions: [],
    TeX: {
      extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
    },
    showMathMenu: false
  });
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js">
</script>
</head>
<body>
<div class="container">
<h1>Captioning codes</h1>
<table class="table table-striped">
<thead>
<tr><th>Name</th><th>Descriptions</th><th>Illustration</th></tr>
</thead>
<tbody>

<!------------------------------------------------------->
<tr><td><a href=""></a></td>
<td>
</td>
<td><img src="" alt="" width="200"></td></tr>
<!------------------------------------------------------->
<tr><td><a href="https://github.com/ruotianluo/neuraltalk2.pytorch">neuraltalk2.pytorch</a></td>
<td>image captioning model in pytorch(finetunable cnn in branch "with_finetune")
</td>
<td><img src="" alt="" width="200"></td></tr>


<tr><td><a href="https://github.com/markdtw/vqa-winner-cvprw-2017">vqa-winner-cvprw-2017</a></td>
<td>2017 VQA Challenge Winner (CVPR'17 Workshop).Pytorch implementation of Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge by Teney et al.
</td>
<td><img src="https://camo.githubusercontent.com/1194ae93cb823754f0f2c2194dc3f73aa60b0cf5/68747470733a2f2f692e696d6775722e636f6d2f7068424849715a2e706e67" alt="" width="200"></td></tr>

<tr><td><a href="https://github.com/Rangozhang/VideoCaption">VideoCaption</a></td>
<td>Video captioning using LSTM and CNN. This is the Visual Learning project done by Rui Zhang, Yujia Huang and Yu Zhang
</td>
<td><img src="https://github.com/Rangozhang/VideoCaption/raw/master/VideoCaption.jpg" alt="" width="200"></td></tr>

<tr>
<td>
<a href="https://github.com/nightrome/cocostuff">COCO-Stuff_10K</a></td>
<td>
COCO-Stuff augments the popular COCO dataset with pixel-level stuff annotations. These annotations can be used for scene understanding tasks like semantic segmentation, object detection and image captioning.
</td>
<td>
<img src="https://camo.githubusercontent.com/d10b897e15344334e449104a824aff6c29125dc2/687474703a2f2f63616c76696e2e696e662e65642e61632e756b2f77702d636f6e74656e742f75706c6f6164732f646174612f636f636f7374756666646174617365742f636f636f73747566662d6578616d706c65732e706e67" alt="" width="200">
</td>
</tr>

<tr><td><a href="https://github.com/ruotianluo/self-critical.pytorch">self-critical.pytorch</a></td>
<td>Unofficial pytorch implementation for Self-critical Sequence Training for Image Captioning
</td>
<td><img src="" alt="" width="200"></td></tr>

<tr><td><a href="https://github.com/erfannoury/order-embedding-disc" order-embedding-disc.theano<="" a=""></a></td>
<td>Implementation of caption-image retrieval from the paper "Order-Embeddings of Images and Language"
</td>
<td><img src="" alt="" width="200"></td></tr>

<tr><td><a href="https://github.com/nutszebra/multimodal_word2vec">multimodal_word2vec.python</a></td>
<td>implementation of Combining Language and Vision with a Multimodal Skip-gram Model
</td>
<td><img src="" alt="" width="200"></td></tr>

<tr><td><a href="https://github.com/ronghanghu/n2nmn">n2nmn</a></td>
<td>R. Hu, J. Andreas, M. Rohrbach, T. Darrell, K. Saenko, Learning to Reason: End-to-End Module Networks for Visual Question Answering. in ICCV, 2017. (PDF)
</td>
<td><img src="http://ronghanghu.com/wp-content/uploads/nmn3-e1492671105251-1024x381.jpg" alt="" width="200"></td></tr>

<tr><td><a href="https://github.com/ilija139/vqa-soft">vqa-soft</a></td>
<td>Accompanying code for "A Simple Loss Function for Improving the Convergence and Accuracy of Visual Question Answering Models" CVPR 2017 VQA workshop paper.
</td>
<td><img src="" alt="" width="200"></td></tr>


<tr><td><a href="https://github.com/LuoweiZhou/video-to-text">video-to-text</a></td>
<td>Video to text model based on NeuralTalk2
</td>
<td><img src="" alt="" width="200"></td></tr>


<tr><td><a href="https://github.com/zsdonghao/im2txt2im">im2txt2im</a></td>
<td>I2T2I: Text-to-Image Synthesis with textual data augmentation
</td>
<td><img src="https://github.com/zsdonghao/im2txt2im/raw/master/img/qualitative.jpeg" alt="" width="200"></td></tr>


<tr><td><a href="https://github.com/peteanderson80/bottom-up-attention"> bottom-up-attention</a></td>
<td>Bottom-up attention model for image captioning and VQA, based on Faster R-CNN and Visual Genome
</td>
<td><img src="https://github.com/peteanderson80/bottom-up-attention/raw/master/data/demo/rcnn_example_2.png?raw=true" alt="" width="200"></td></tr>

<tr><td><a href="https://github.com/Yugnaynehc/ssta-captioning">ssta-captioning.pytorch</a></td>
<td>Repository for paper: Saliency-Based Spatio-Temporal Attention for Video Captioning
</td>
<td><img src="https://github.com/Yugnaynehc/ssta-captioning/raw/master/diagram/framework.png" alt="" width="200"></td></tr>

<tr><td><a href="https://github.com/VisionLearningGroup/caption-guided-saliency">caption-guided-saliency.Tensorflow</a></td>
<td>Supplementary material to "Top-down Visual Saliency Guided by Captions" (CVPR 2017)
</td>
<td><img src="https://camo.githubusercontent.com/cc7a141b449b4c08a7ac4a19203e96b9ff9a3371/68747470733a2f2f7777772e64726f70626f782e636f6d2f732f337232783566776461346e6b6174752f766964656f373032332e6769663f7261773d31" alt="" width="200"></td></tr>

<tr>
<td>
<a href="https://github.com/Cadene/vqa.pytorch">vqa.pytorch</a>
</td>
<td>
Visual Question Answering in Pytorch
</td>
<td>
<img src="https://avisingh599.github.io/images/vqa/challenge.png" alt="" width="200">
</td>
</tr>

</tbody>
</table>
</div>
</body>
</html>
