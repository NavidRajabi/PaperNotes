<!DOCTYPE html>
<html>
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.4.0">
  <meta charset="utf-8">
  <meta name="viewport" content=
  "width=device-width, initial-scale=1">
  <title>Im2text codes</title>
  <link rel="stylesheet" href=
  "https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">

  <script src=
  "https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src=
  "https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX","output/HTML-CSS"],
    extensions: [],
    TeX: {
      extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
    },
    showMathMenu: false
  });
  </script>
  <script type="text/javascript" src=
  "https://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
</head>
<body>
  <div class="container">
    <h1>Captioning codes</h1>
    <table class="table table-striped">
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
          <th>Illustration</th>
        </tr>
      </thead>
      <tbody>
        <!--===================================================-->
        <tr>
          <td>
            <a href=""></a>
          </td>
          <td></td>
          <td></td>
        </tr>
        <!--===================================================-->
        <tr>
          <td>
            <a href=
            "https://github.com/ruotianluo/neuraltalk2.pytorch">neuraltalk2.pytorch</a>
          </td>
          <td>image captioning model in pytorch(finetunable cnn in
          branch "with_finetune")</td>
          <td></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/markdtw/vqa-winner-cvprw-2017">vqa-winner-cvprw-2017.pytorch</a>
          </td>
          <td>2017 VQA Challenge Winner (CVPR'17 Workshop). Pytorch
          implementation of Tips and Tricks for Visual Question
          Answering: Learnings from the 2017 Challenge by Teney et
          al.</td>
          <td><img src=
          "https://camo.githubusercontent.com/1194ae93cb823754f0f2c2194dc3f73aa60b0cf5/68747470733a2f2f692e696d6775722e636f6d2f7068424849715a2e706e67"
          alt="" width="200"></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/ruotianluo/self-critical.pytorch">self-critical.pytorch</a>
          </td>
          <td>Unofficial pytorch implementation for Self-critical
          Sequence Training for Image Captioning</td>
          <td></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/erfannoury/order-embedding-disc">order-embedding-disc.theano&lt;=""
            a=""&gt;</a>
          </td>
          <td>Implementation of caption-image retrieval from the
          paper "Order-Embeddings of Images and Language"</td>
          <td></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/ryankiros/visual-semantic-embedding">
            visual-semantic-embedding.theano&lt;="" a=""&gt;</a>
          </td>
          <td>Code for the image-sentence ranking methods from
          "Unifying Visual-Semantic Embeddings with Multimodal
          Neural Language Models" (Kiros, Salakhutdinov, Zemel.
          2014).</td>
          <td></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/nutszebra/multimodal_word2vec">multimodal_word2vec.chainer</a>
          </td>
          <td>implementation of Combining Language and Vision with
          a Multimodal Skip-gram Model</td>
          <td></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/ronghanghu/n2nmn">n2nmn.tensorflow</a>
          </td>
          <td>R. Hu, J. Andreas, M. Rohrbach, T. Darrell, K.
          Saenko, Learning to Reason: End-to-End Module Networks
          for Visual Question Answering. in ICCV, 2017. (PDF)</td>
          <td><img src=
          "http://ronghanghu.com/wp-content/uploads/nmn3-e1492671105251-1024x381.jpg"
          alt="" width="200"></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/ilija139/vqa-soft">vqa-soft.pytorch</a>
          </td>
          <td>Accompanying code for "A Simple Loss Function for
          Improving the Convergence and Accuracy of Visual Question
          Answering Models" CVPR 2017 VQA workshop paper.</td>
          <td></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/LuoweiZhou/video-to-text">video-to-text.torch</a>
          </td>
          <td>Video to text model based on NeuralTalk2</td>
          <td></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/peteanderson80/bottom-up-attention">
            bottom-up-attention.caffe</a>
          </td>
          <td>Bottom-up attention model for image captioning and
          VQA, based on Faster R-CNN and Visual Genome</td>
          <td><img src=
          "https://github.com/peteanderson80/bottom-up-attention/raw/master/data/demo/rcnn_example_2.png?raw=true"
          alt="" width="200"></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/Yugnaynehc/ssta-captioning">ssta-captioning.pytorch</a>
          </td>
          <td>Repository for paper: Saliency-Based Spatio-Temporal
          Attention for Video Captioning</td>
          <td><img src=
          "https://github.com/Yugnaynehc/ssta-captioning/raw/master/diagram/framework.png"
          alt="" width="200"></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/VisionLearningGroup/caption-guided-saliency">
            caption-guided-saliency.Tensorflow</a>
          </td>
          <td>Supplementary material to "Top-down Visual Saliency
          Guided by Captions" (CVPR 2017)</td>
          <td><img src=
          "https://camo.githubusercontent.com/cc7a141b449b4c08a7ac4a19203e96b9ff9a3371/68747470733a2f2f7777772e64726f70626f782e636f6d2f732f337232783566776461346e6b6174752f766964656f373032332e6769663f7261773d31"
          alt="" width="200"></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/Cadene/vqa.pytorch">vqa.pytorch</a>
          </td>
          <td>Visual Question Answering in Pytorch</td>
          <td><img src=
          "https://avisingh599.github.io/images/vqa/challenge.png"
          alt="" width="200"></td>
        </tr>
      </tbody>
    </table>
  </div>
</body>
</html>
