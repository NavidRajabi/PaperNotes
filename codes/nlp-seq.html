<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8" />
<title>NLP & Ses2Seq codes</title>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX","output/HTML-CSS"],
    extensions: [],
    TeX: {
      extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
    },
    showMathMenu: false
  });
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js">
</script>
<style type="text/css">
    table.primary tr:nth-child(odd) {
       background-color: rgba(0, 0, 0, 0) !important;
    }
    table.primary tr:nth-child(even) {
       background-color: rgba(0, 0, 0, 0);
    }
</style>
</head>
<body>
<h1>NLP & Seq2Seq codes</h1>
<table border="3" style="width:80%">
<!--<caption><em><center></center></em></caption>-->
<tr><th>Name</th><th>Descriptions</th><th>Illustration</th></tr>
<!------------------------------------------------------->
<tr><td><a href=""></a></td>
<td>
</td>
<td><img src="" alt="" width="200"></td></tr>
<!------------------------------------------------------->
<tr><td><a href="https://github.com/niangaotuantuan/Publications-of-Deep-Learning-in-NLP">Publications-of-Deep-Learning-in-NLP</a></td>
<td>collect the publications and related resources of Deep Learning in NLP
</td>
<td><img src="" alt="" width="200"></td></tr>


<tr><td><a href="https://github.com/facebookresearch/MUSE">MUSE</a></td>
<td>A library for Multilingual Unsupervised or Supervised word Embeddings
</td>
<td><img src="https://camo.githubusercontent.com/e8a19eb6772e722fb3fe2cd787e14ed7c4e17ddd/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6172726976616c2f6f75746c696e655f616c6c2e706e67" alt="" width="200"></td></tr>

<tr><td><a href="https://github.com/ExplorerFreda/Structured-Self-Attentive-Sentence-Embedding">Structured-Self-Attentive-Sentence-Embedding</a></td>
<td>An open-source implementation of the paper A Structured Self-Attentive Sentence Embedding published by IBM and MILA.
</td>
<td><img src="" alt="" width="200"></td></tr>

<tr><td><a href="https://github.com/lium-lst/nmtpytorch">nmtpytorch</a></td>
<td>Neural Machine Translation Framework in PyTorch
</td>
<td><img src="" alt="" width="200"></td></tr>

<tr><td><a href="https://github.com/ruotianluo/NeuralDialog-CVAE-pytorch">NeuralDialog-CVAE-pytorch</a></td>
<td>Knowledge-Guided CVAE for dialog generation
</td>
<td><img src="" alt="" width="200"></td></tr>

<tr><td><a href="">bandit-nmt</a></td>
<td>This is code repo for our EMNLP 2017 paper "Reinforcement Learning for Bandit Neural Machine Translation with Simulated Human Feedback", which implements the A2C algorithm on top of a neural encoder-decoder model and benchmarks the combination under simulated noisy rewards.
</td>
<td><img src="" alt="" width="200"></td></tr>

<tr><td><a href="https://github.com/ruotianluo/dbs">Diverse Beam Search</a></td>
<td>
This code implements Diverse Beam Search (DBS) - a replacement for beam search that generates diverse sequences from sequence models like LSTMs. This repository lets you generate diverse image-captions for models trained using the popular neuraltalk2 repository. A demo of our implementation on captioning is available at dbs.cloudcv.org
</td>
<td><img src="https://camo.githubusercontent.com/7dbd345b986b691eede2b8611285114d1fc90e32/68747470733a2f2f7332322e706f7374696d672e6f72672f686f6f7233726963782f64625f636f7665725f325f312e706e67" alt="" width="200"></td></tr>

<tr><td><a href="https://github.com/facebookresearch/SentEval">SentEval</a></td>
<td>SentEval is a library for evaluating the quality of sentence embeddings. We assess their generalization power by using them as features on a broad and diverse set of "transfer" tasks (more details here). Our goal is to ease the study and the development of general-purpose fixed-size sentence representations.
</td>
<td><img src="" alt="" width="200"></td></tr>

<tr><td><a href="https://github.com/fh295/SentenceRepresentation">SentenceRepresentation</a></td>
<td>This code acompanies the paper 'Learning Sentence Representations from Unlabelled Data' Felix Hill, KyungHyun Cho and Anna Korhonen 2016.
</td>
<td><img src="" alt="" width="200"></td></tr>

<tr><td><a href="https://github.com/spro/RARNN">RARNN</a></td>
<td>Recursive Application of Recurrent Neural Networks.
A simple model for intent parsing that supports complex nested intents.
</td>
<td><img src="https://camo.githubusercontent.com/f490fbe26a07b98dbb8aca0b96f5673acf399894/68747470733a2f2f692e696d6775722e636f6d2f44424e48726e522e706e67" alt="" width="200"></td></tr>


<tr><td><a href="https://github.com/taolei87/sru">sru</a></td>
<td>Training RNNs as Fast as CNNs (https://arxiv.org/abs/1709.02755)
</td>
<td><img src="https://github.com/taolei87/sru/raw/master/imgs/speed.png" alt="" width="200"></td></tr>

<tr><td><a href="https://github.com/LiyuanLucasLiu/LM-LSTM-CRF">LM-LSTM-CRF</a></td>
<td>This project provides high-performance character-aware sequence labeling tools and tutorials. Model details can be accessed here, and the implementation is based on the PyTorch library.
<p>LM-LSTM-CRF achieves F1 score of 91.71+/-0.10 on the CoNLL 2003 NER dataset, without using any additional corpus or resource.</p>
</td>
<td><img src="https://github.com/LiyuanLucasLiu/LM-LSTM-CRF/raw/master/docs/framework.png" alt="" width="200"></td></tr>

<tr><td><a href="https://github.com/XenderLiu/Listen-Attend-and-Spell-Pytorch">Listen-Attend-and-Spell-Pytorch</a></td>
<td>Listen Attend and Spell (LAS) implement in pytorch
</td>
<td><img src="" alt="" width="200"></td></tr>

<tr><td><a href="https://github.com/NickShahML/rnn.wgan">rnn.wgan</a></td>
<td>Code for training and evaluation of the model from "Language Generation with Recurrent Generative Adversarial Networks without Pre-training" https://arxiv.org/abs/1706.01399
</td>
<td><img src="" alt="" width="200"></td></tr>

<tr><td><a href="https://github.com/facebookresearch/InferSent">InferSent</a></td>
<td>Sentence embeddings (InferSent) and training code for NLI.
</td>
<td><img src="https://camo.githubusercontent.com/eacfae9d9987988db2774e703609473bc10ed311/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f73656e746576616c2f696e66657273656e742f76697375616c697a6174696f6e2e706e67" alt="" width="200"></td></tr>

<tr><td><a href="https://github.com/facebookresearch/fairseq">fairseq.Torch</a></td>
<td>Facebook AI Research Sequence-to-Sequence Toolkit
</td>
<td><img src="" alt="" width="200"></td></tr>

<tr><td><a href="https://github.com/iamalbert/pytorch-wordemb">wordemb.PyTorch</a></td>
<td>Load pretrained word embeddings (word2vec, glove format) into torch.FloatTensor for PyTorch
</td>
<td><img src="" alt="" width="200"></td></tr>

<tr><td><a href="">seq2seq-attn.Torch</a></td>
<td>Sequence-to-sequence model with LSTM encoder/decoders and attention
</td>
<td><img src="https://github.com/JonghwanMun/TextguidedATT" alt="" width="200"></td></tr>

<tr><td><a href="https://github.com/mingdachen/gated-attention-reader">gated-attention-reader.PyTorch</a></td>
<td>Tensorflow/Pytorch implementation of Gated Attention Reader
</td>
<td><img src="" alt="" width="200"></td></tr>

<tr><td><a href="https://github.com/maciejkula/glove-python">glove.python</a></td>
<td>Toy Python implementation of http://www-nlp.stanford.edu/projects/glove/
</td>
<td><img src="" alt="" width="200"></td></tr>


<tr><td><a href="https://github.com/cheng6076/Variational-LSTM-Autoencoder">Variational-LSTM-Autoencoder.Torch</a></td>
<td>Variational Seq2Seq model
</td>
<td><img src="http://yanran.li/images/infoflow_5.png" alt="" width="200"></td></tr>

<tr><td><a href="https://github.com/PkuRainBow/Hard-Aware-Deeply-Cascaed-Embedding">Hard-Aware-Deeply-Cascaed-Embedding.c++</a></td>
<td>source code for the paper "Hard-Aware-Deeply-Cascaed-Embedding"
</td>
<td><img src="" alt="" width="200"></td></tr>

<tr><td><a href="https://github.com/spro/pytorch-seq2seq-intent-parsing">seq2seq-intent-parsing.PyTorch</a></td>
<td>Intent parsing and slot filling in PyTorch with seq2seq + attention
</td>
<td><img src="https://camo.githubusercontent.com/4125995f183d3158103b46eeb5ffdea4eef0ef52/68747470733a2f2f692e696d6775722e636f6d2f56316c747668492e706e67" alt="" width="200"></td></tr>


<tr><td><a href="https://github.com/Cloud-CV/diverse-beam-search">diverse-beam-search.Torch</a></td>
<td>
</td>
<td><img src="" alt="" width="200"></td></tr>

<tr><td><a href="https://github.com/guillitte/pytorch-sentiment-neuron">sentiment-neuron.PyTorch</a></td>
<td>Pytorch version of generating-reviews-discovering-sentiment : https://github.com/openai/generating-reviews-discovering-sentiment
</td>
<td><img src="" alt="" width="200"></td></tr>

<tr><td><a href="https://github.com/rizar/actor-critic-public">actor-critic-public</a></td>
<td>The source code for "An Actor Critic Algorithm for Structured Prediction"
</td>
<td><img src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-11-08/0d24a0695c9fc669e643bad51d4e14f056329dec/5-Figure1-1.png" alt="" width="200"></td></tr>

<tr><td><a href="https://github.com/2014mchidamb/TorchGlove">TorchGlove.PyTorch</a></td>
<td>PyTorch implementation of Global Vectors for Word Representation. http://suriyadeepan.github.io/2016-06-28-easy-seq2seq/
</td>
<td><img src="https://github.com/2014mchidamb/TorchGlove/raw/master/glove.png" alt="" width="200"></td></tr>

<tr>
<td>
<a href="https://github.com/dallascard/TreeLSTM">TreeLSTM.PyTorch</a>
</td>
<td>
An attempt to implement the Constinuency Tree LSTM in "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks"
</td>
<td>
<img src="https://adeshpande3.github.io/assets/NLP28.png" alt="" width="200">
</td>
</tr>

<tr>
<td>
<a href="https://github.com/ttpro1995/treelstm.pytorch">treelstm.pytorch</a>
</td>
<td>
A PyTorch based implementation of Tree-LSTM from Kai Sheng Tai's paper Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks.
</td>
<td>
<img src="" alt="" width="200">
</td>
</tr>

<tr>
<td>
<a href="https://github.com/OpenNMT/OpenNMT-py">OpenNMT.PyTorch</a>
</td>
<td>
This is a Pytorch port of OpenNMT, an open-source (MIT) neural machine translation system. Full documentation is available here.
</td>
<td>
<img src="https://camo.githubusercontent.com/6340603acc1062d8ec6d274283a48fc7562bc8ba/687474703a2f2f6f70656e6e6d742e6769746875622e696f2f73696d706c652d6174746e2e706e67" alt="" width="200">
</td>
</tr>


<tr>
<td>
<a href="https://github.com/eladhoffer/seq2seq.pytorch">seq2seq.pytorch</a>
</td>
<td>
This is a complete suite for training sequence-to-sequence models in PyTorch. It consists of several models and code to both train and infer using them.
</td>
<td>
<img src="https://raw.githubusercontent.com/MaximumEntropy/Seq2Seq-PyTorch/master//images/Seq2Seq.png" alt="" width="200">
</td>
</tr>


<tr>
<td>
<a href="https://github.com/matthewfl/nlp-entity-convnet">nlp-entity-convnet.Python</a>
</td>
<td>
Convolutional network for entity linking (Naacl 2016)
</td>
<td>
<img src="" alt="" width="200">
</td>
</tr>

<tr>
<td>
<a href="">attention-is-all-you-need.pytorch</a>
</td>
<td>
This is a PyTorch implementation of the Transformer model in "Attention is All You Need" (Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin, arxiv, 2017).
</td>
<td>
<img src="https://camo.githubusercontent.com/88e8f36ce61dedfd2491885b8df2f68c4d1f92f5/687474703a2f2f696d6775722e636f6d2f316b72463252362e706e67" alt="" width="200">
</td>
</tr>

<tr>
<td>
<a href="https://github.com/ZeweiChu/nmt-seq2seq">nmt-seq2seq.PyTorch</a>
</td>
<td>
seq2seq model written in Pytorch
</td>
<td>
<img src="" alt="" width="200">
</td>
</tr>

<tr>
<td>
<a href="https://github.com/talolard/DenseContinuousSentances">DenseContinuousSentances.Tensorflow</a>
</td>
<td>
Working towards implementing Generating Sentences from a Continuous Space but with DenseNet
</td>
<td>
<img src="https://image.slidesharecdn.com/generatingsentencesfromacontinuousspace-160704233328/95/generating-sentences-from-a-continuous-space-2-638.jpg?cb=1467675299" alt="" width="200">
</td>
</tr>

<tr><td><a href="https://github.com/allenai/bi-att-flow">bi-att-flow.Tensorflow</a></td>
<td>Bidirectional Attention Flow
</td>
<td><img src="https://allenai.github.io/bi-att-flow/BiDAF.png" alt="" width="200"></td></tr>


