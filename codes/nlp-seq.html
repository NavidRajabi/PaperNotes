<!DOCTYPE html>
<html>
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.4.0">
  <meta charset="utf-8">
  <meta name="viewport" content=
  "width=device-width, initial-scale=1">
  <title>NLP & Seq2Seq codes</title>
  <link rel="stylesheet" href=
  "https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">

  <script src=
  "https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src=
  "https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX","output/HTML-CSS"],
    extensions: [],
    TeX: {
      extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
    },
    showMathMenu: false
  });
  </script>
  <script type="text/javascript" src=
  "https://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
</head>
<body>
  <div class="container">
    <h1>NLP & Seq2Seq codes</h1>
    <table class="table table-striped">
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
          <th>Illustration</th>
        </tr>
      </thead>
      <tbody>
        <!--===================================================-->
        <tr>
          <td>
            <a href=""></a>
          </td>
          <td></td>
          <td></td>
        </tr>
        <!--===================================================-->
        <tr>
          <td>
            <a href=
            "https://github.com/niangaotuantuan/Publications-of-Deep-Learning-in-NLP">
            Publications-of-Deep-Learning-in-NLP</a>
          </td>
          <td>collect the publications and related resources of
          Deep Learning in NLP</td>
          <td></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/facebookresearch/MIXER">MIXER.torch</a>
          </td>
          <td>Mixed Incremental Cross-Entropy REINFORCE ICLR 2016</td>
          <td></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/facebookresearch/MUSE">MUSE.pytorch</a>
          </td>
          <td>A library for Multilingual Unsupervised or Supervised
          word Embeddings</td>
          <td><img src=
          "https://camo.githubusercontent.com/e8a19eb6772e722fb3fe2cd787e14ed7c4e17ddd/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6172726976616c2f6f75746c696e655f616c6c2e706e67"
          alt="" width="200"></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/ExplorerFreda/Structured-Self-Attentive-Sentence-Embedding">
            Structured-Self-Attentive-Sentence-Embedding.pytorch</a>
          </td>
          <td>An open-source implementation of the paper A
          Structured Self-Attentive Sentence Embedding published by
          IBM and MILA. Requires spaCy as well.</td>
          <td></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/lium-lst/nmtpytorch">nmt.pytorch</a>
          </td>
          <td>Neural Machine Translation Framework in PyTorch</td>
          <td></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/ruotianluo/NeuralDialog-CVAE-pytorch">
            NeuralDialog-CVAE.pytorch</a>
          </td>
          <td>Knowledge-Guided CVAE for dialog generation</td>
          <td></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/khanhptnk/bandit-nmt">bandit-nmt.pytorch</a>
          </td>
          <td>This is code repo for our EMNLP 2017 paper
          "Reinforcement Learning for Bandit Neural Machine
          Translation with Simulated Human Feedback", which
          implements the A2C algorithm on top of a neural
          encoder-decoder model and benchmarks the combination
          under simulated noisy rewards.</td>
          <td></td>
        </tr>
        <tr>
          <td>
            <a href="https://github.com/ruotianluo/dbs">Diverse
            Beam Search.torch</a>
          </td>
          <td>This code implements Diverse Beam Search (DBS) - a
          replacement for beam search that generates diverse
          sequences from sequence models like LSTMs. This
          repository lets you generate diverse image-captions for
          models trained using the popular neuraltalk2 repository.
          A demo of our implementation on captioning is available
          at dbs.cloudcv.org</td>
          <td><img src=
          "https://camo.githubusercontent.com/7dbd345b986b691eede2b8611285114d1fc90e32/68747470733a2f2f7332322e706f7374696d672e6f72672f686f6f7233726963782f64625f636f7665725f325f312e706e67"
          alt="" width="200"></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/facebookresearch/SentEval">SentEval.pytorch</a>
          </td>
          <td>SentEval is a library for evaluating the quality of
          sentence embeddings. We assess their generalization power
          by using them as features on a broad and diverse set of
          "transfer" tasks (more details here). Our goal is to ease
          the study and the development of general-purpose
          fixed-size sentence representations.</td>
          <td></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/taolei87/sru">SRU.pytorch</a>
          </td>
          <td>Training RNNs as Fast as CNNs
          (https://arxiv.org/abs/1709.02755)</td>
          <td><img src=
          "https://github.com/taolei87/sru/raw/master/imgs/speed.png"
          alt="" width="200"></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/XenderLiu/Listen-Attend-and-Spell-Pytorch">
            Listen-Attend-and-Spell-Pytorch</a>
          </td>
          <td>Listen Attend and Spell (LAS) implement in
          pytorch</td>
          <td></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/NickShahML/rnn.wgan">rnn.wgan.Tensorflow</a>
          </td>
          <td>Code for training and evaluation of the model from
          "Language Generation with Recurrent Generative
          Adversarial Networks without Pre-training"
          https://arxiv.org/abs/1706.01399</td>
          <td></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/facebookresearch/InferSent">InferSent.pytorch</a>
          </td>
          <td>Sentence embeddings (InferSent) and training code for
          NLI.</td>
          <td><img src=
          "https://camo.githubusercontent.com/eacfae9d9987988db2774e703609473bc10ed311/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f73656e746576616c2f696e66657273656e742f76697375616c697a6174696f6e2e706e67"
          alt="" width="200"></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/facebookresearch/fairseq">fairseq.Torch</a>
          </td>
          <td>Facebook AI Research Sequence-to-Sequence
          Toolkit</td>
          <td></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/iamalbert/pytorch-wordemb">wordemb.PyTorch</a>
          </td>
          <td>Load pretrained word embeddings (word2vec, glove
          format) into torch.FloatTensor for PyTorch</td>
          <td></td>
        </tr>
        <tr>
          <td>
            <a href="">seq2seq-attn.Torch</a>
          </td>
          <td>Sequence-to-sequence model with LSTM encoder/decoders
          and attention</td>
          <td><img src=
          "https://github.com/JonghwanMun/TextguidedATT" alt=""
          width="200"></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/mingdachen/gated-attention-reader">gated-attention-reader.PyTorch</a>
          </td>
          <td>Tensorflow/Pytorch implementation of Gated Attention
          Reader</td>
          <td></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/maciejkula/glove-python">glove.python</a>
          </td>
          <td>Toy Python implementation of
          http://www-nlp.stanford.edu/projects/glove/</td>
          <td></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/cheng6076/Variational-LSTM-Autoencoder">
            Variational-LSTM-Autoencoder.Torch</a>
          </td>
          <td>Variational Seq2Seq model</td>
          <td><img src="http://yanran.li/images/infoflow_5.png"
          alt="" width="200"></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/PkuRainBow/Hard-Aware-Deeply-Cascaed-Embedding">
            Hard-Aware-Deeply-Cascaed-Embedding.c++</a>
          </td>
          <td>source code for the paper
          "Hard-Aware-Deeply-Cascaed-Embedding"</td>
          <td></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/spro/pytorch-seq2seq-intent-parsing">
            seq2seq-intent-parsing.PyTorch</a>
          </td>
          <td>Intent parsing and slot filling in PyTorch with
          seq2seq + attention</td>
          <td><img src=
          "https://camo.githubusercontent.com/4125995f183d3158103b46eeb5ffdea4eef0ef52/68747470733a2f2f692e696d6775722e636f6d2f56316c747668492e706e67"
          alt="" width="200"></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/Cloud-CV/diverse-beam-search">diverse-beam-search.Torch</a>
          </td>
          <td></td>
          <td></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/guillitte/pytorch-sentiment-neuron">
            sentiment-neuron.PyTorch</a>
          </td>
          <td>Pytorch version of
          generating-reviews-discovering-sentiment :
          https://github.com/openai/generating-reviews-discovering-sentiment</td>
          <td></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/rizar/actor-critic-public">actor-critic-public.theano</a>
          </td>
          <td>The source code for "An Actor Critic Algorithm for
          Structured Prediction"</td>
          <td><img src=
          "https://ai2-s2-public.s3.amazonaws.com/figures/2016-11-08/0d24a0695c9fc669e643bad51d4e14f056329dec/5-Figure1-1.png"
          alt="" width="200"></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/2014mchidamb/TorchGlove">TorchGlove.PyTorch</a>
          </td>
          <td>PyTorch implementation of Global Vectors for Word
          Representation.
          http://suriyadeepan.github.io/2016-06-28-easy-seq2seq/</td>
          <td><img src=
          "https://github.com/2014mchidamb/TorchGlove/raw/master/glove.png"
          alt="" width="200"></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/dallascard/TreeLSTM">TreeLSTM.PyTorch</a>
          </td>
          <td>An attempt to implement the Constinuency Tree LSTM in
          "Improved Semantic Representations From Tree-Structured
          Long Short-Term Memory Networks"</td>
          <td><img src=
          "https://adeshpande3.github.io/assets/NLP28.png" alt=""
          width="200"></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/ttpro1995/treelstm.pytorch">treelstm.pytorch</a>
          </td>
          <td>A PyTorch based implementation of Tree-LSTM from Kai
          Sheng Tai's paper Improved Semantic Representations From
          Tree-Structured Long Short-Term Memory Networks.</td>
          <td></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/OpenNMT/OpenNMT-py">OpenNMT.PyTorch</a>
          </td>
          <td>This is a Pytorch port of OpenNMT, an open-source
          (MIT) neural machine translation system. Full
          documentation is available here.</td>
          <td><img src=
          "https://camo.githubusercontent.com/6340603acc1062d8ec6d274283a48fc7562bc8ba/687474703a2f2f6f70656e6e6d742e6769746875622e696f2f73696d706c652d6174746e2e706e67"
          alt="" width="200"></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/eladhoffer/seq2seq.pytorch">seq2seq.pytorch</a>
          </td>
          <td>This is a complete suite for training
          sequence-to-sequence models in PyTorch. It consists of
          several models and code to both train and infer using
          them.</td>
          <td><img src=
          "https://raw.githubusercontent.com/MaximumEntropy/Seq2Seq-PyTorch/master//images/Seq2Seq.png"
          alt="" width="200"></td>
        </tr>
        <tr>
          <td>
            <a href="">attention-is-all-you-need.pytorch</a>
          </td>
          <td>This is a PyTorch implementation of the Transformer
          model in "Attention is All You Need" (Ashish Vaswani,
          Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
          Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin, arxiv,
          2017).</td>
          <td><img src=
          "https://camo.githubusercontent.com/88e8f36ce61dedfd2491885b8df2f68c4d1f92f5/687474703a2f2f696d6775722e636f6d2f316b72463252362e706e67"
          alt="" width="200"></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/ZeweiChu/nmt-seq2seq">nmt-seq2seq.PyTorch</a>
          </td>
          <td>seq2seq model written in Pytorch</td>
          <td></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/eske/seq2seq">seq2seq.Tensorflow</a>
          </td>
          <td>Attention-based sequence to sequence learning</td>
          <td></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/talolard/DenseContinuousSentances">DenseContinuousSentances.Tensorflow</a>
          </td>
          <td>Working towards implementing Generating Sentences
          from a Continuous Space but with DenseNet</td>
          <td><img src=
          "https://image.slidesharecdn.com/generatingsentencesfromacontinuousspace-160704233328/95/generating-sentences-from-a-continuous-space-2-638.jpg?cb=1467675299"
          alt="" width="200"></td>
        </tr>
        <tr>
          <td>
            <a href=
            "https://github.com/allenai/bi-att-flow">bi-att-flow.Tensorflow</a>
          </td>
          <td>Bidirectional Attention Flow</td>
          <td><img src=
          "https://allenai.github.io/bi-att-flow/BiDAF.png" alt=""
          width="200"></td>
        </tr>
      </tbody>
    </table>
  </div>
</body>
</html>
